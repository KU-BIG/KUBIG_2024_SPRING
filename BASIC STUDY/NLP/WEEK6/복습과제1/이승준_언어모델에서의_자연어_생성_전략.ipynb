{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"hSfah6SLfhVG"},"source":["# 텍스트 생성 방법 : Transformers를 이용한 언어생성에 서로 다른 디코딩 방법 사용"]},{"cell_type":"markdown","metadata":{"id":"9l31dGxwgTMZ"},"source":["#소개\n","최근 몇 년 동안, OpenAI의 유명한 모델 GPT2처럼 수백만 개의 웹 페이지에서 훈련된 Transformer 기반 대형 언어 모델의 등장으로 개방형 언어 생성에 대한 관심이 증가하고 있습니다. 개선된 Transformer 아키텍처와 대규모 unsupervised training data 외에도, 더 나은 Decoding 방법도 중요한 역할을 했습니다.\n","\n","이 실습 자료는 다양한 Decoding 전략에 대한 간략한 개요를 제공합니다.\n","\n","다음 모든 기능은 auto-regressive 언어 생성에 사용할 수 있습니다. 요약하자면, auto-regressive 언어 생성은 word sequence의 확률 분포가 다음 던어 분포에 관한 P식의 결과로 분해될 수 있고\n","\n","$$ P(w_{1:T} | W_0 ) = \\prod_{t=1}^T P(w_{t} | w_{1: t-1}, W_0) \\text{ ,with }  w_{1: 0} = \\emptyset, $$\n","\n","W0가 초기 Context word sequence의 결과로 분해될 수 있다는 가정에 기초합니다.\n","\n","Word sequence의 길이 T는 보통 즉시 결정되며, P식에서 EOS 토큰이 생성된 timestep t=T와 부합합니다.\n","\n","이번 실습에서는 가장 두드러진 decoding 방법으로 Greedy search, Beam search, Top-K sampling, Top-p sampling을 주로 둘러볼 것입니다.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"sz0J-hPCoZEl"},"source":["Transformers를 설치하고 Model을 load하겠습니다."]},{"cell_type":"code","metadata":{"id":"XbzZ_IVTtoQe","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1708501368669,"user_tz":-540,"elapsed":6152,"user":{"displayName":"이승준","userId":"02916235139337086073"}},"outputId":"4c68acd1-bbe4-4c60-e1c1-cd2fb58e5bce"},"source":["!pip install transformers"],"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.37.2)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.1)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.25.2)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.12.25)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n","Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.2)\n","Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.2)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.2)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2023.6.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.9.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.6)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.2.2)\n"]}]},{"cell_type":"markdown","metadata":{"id":"X4jS5zNKknHT"},"source":["이번 실습을 위해 SKT에서 공개한 KoGPT-2 모델을 사용해보도록 하겠습니다 :-)\n","\n","* KoGPT-2 : gpt 2 를 한국어로 fine tuning  "]},{"cell_type":"code","metadata":{"id":"b1eVsFQQgdBk","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1708501502777,"user_tz":-540,"elapsed":45715,"user":{"displayName":"이승준","userId":"02916235139337086073"}},"outputId":"fc0c80c5-ae82-4a20-fe94-1463cebf9602"},"source":["!curl -s https://packagecloud.io/install/repositories/github/git-lfs/script.deb.sh | sudo bash\n","!apt-get install git-lfs\n","!git lfs install\n","!git clone https://huggingface.co/taeminlee/kogpt2"],"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Detected operating system as Ubuntu/jammy.\n","Checking for curl...\n","Detected curl...\n","Checking for gpg...\n","Detected gpg...\n","Detected apt version as 2.4.11\n","Running apt-get update... done.\n","Installing apt-transport-https... done.\n","Installing /etc/apt/sources.list.d/github_git-lfs.list...done.\n","Importing packagecloud gpg key... Packagecloud gpg key imported to /etc/apt/keyrings/github_git-lfs-archive-keyring.gpg\n","done.\n","Running apt-get update... done.\n","\n","The repository is setup! You can now install packages.\n","Reading package lists... Done\n","Building dependency tree... Done\n","Reading state information... Done\n","The following packages will be upgraded:\n","  git-lfs\n","1 upgraded, 0 newly installed, 0 to remove and 35 not upgraded.\n","Need to get 7,421 kB of archives.\n","After this operation, 6,006 kB of additional disk space will be used.\n","Get:1 https://packagecloud.io/github/git-lfs/ubuntu jammy/main amd64 git-lfs amd64 3.4.1 [7,421 kB]\n","Fetched 7,421 kB in 1s (8,231 kB/s)\n","(Reading database ... 121753 files and directories currently installed.)\n","Preparing to unpack .../git-lfs_3.4.1_amd64.deb ...\n","Unpacking git-lfs (3.4.1) over (3.0.2-1ubuntu0.2) ...\n","Setting up git-lfs (3.4.1) ...\n","Git LFS initialized.\n","Processing triggers for man-db (2.10.2-1) ...\n","Git LFS initialized.\n","Cloning into 'kogpt2'...\n","remote: Enumerating objects: 56, done.\u001b[K\n","remote: Total 56 (delta 0), reused 0 (delta 0), pack-reused 56\u001b[K\n","Unpacking objects: 100% (56/56), 1.53 MiB | 4.87 MiB/s, done.\n","Filtering content: 100% (3/3), 1.41 GiB | 51.88 MiB/s, done.\n"]}]},{"cell_type":"code","metadata":{"id":"ckGerDmxgjOb","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1708501556599,"user_tz":-540,"elapsed":8981,"user":{"displayName":"이승준","userId":"02916235139337086073"}},"outputId":"86fde8f3-7c5e-495f-e953-38be7f1b7d49"},"source":["import torch\n","from tokenizers import SentencePieceBPETokenizer\n","from transformers import GPT2Config, GPT2LMHeadModel\n","\n","tokenizer = SentencePieceBPETokenizer(\"/content/kogpt2/vocab.json\", \"/content/kogpt2/merges.txt\")\n","\n","config = GPT2Config(vocab_size=50000)\n","config.pad_token_id = tokenizer.token_to_id('<pad>')\n","model = GPT2LMHeadModel(config)\n","\n","model_dir = '/content/kogpt2/pytorch_model.bin'\n","\n","model.load_state_dict(torch.load(model_dir, map_location='cuda'), strict=False)\n","model.to('cuda')"],"execution_count":3,"outputs":[{"output_type":"execute_result","data":{"text/plain":["GPT2LMHeadModel(\n","  (transformer): GPT2Model(\n","    (wte): Embedding(50000, 768)\n","    (wpe): Embedding(1024, 768)\n","    (drop): Dropout(p=0.1, inplace=False)\n","    (h): ModuleList(\n","      (0-11): 12 x GPT2Block(\n","        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","        (attn): GPT2Attention(\n","          (c_attn): Conv1D()\n","          (c_proj): Conv1D()\n","          (attn_dropout): Dropout(p=0.1, inplace=False)\n","          (resid_dropout): Dropout(p=0.1, inplace=False)\n","        )\n","        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","        (mlp): GPT2MLP(\n","          (c_fc): Conv1D()\n","          (c_proj): Conv1D()\n","          (act): NewGELUActivation()\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","    )\n","    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","  )\n","  (lm_head): Linear(in_features=768, out_features=50000, bias=False)\n",")"]},"metadata":{},"execution_count":3}]},{"cell_type":"markdown","metadata":{"id":"BYdGXD4opfuk"},"source":["### **Greedy Search**\n","\n","Greedy search는 단순히 가장 높은 확률을 가진 단어를 다음 단어로 선택합니다.   \n","$w_t = argmax_{w}P(w | w_{1:t-1})$ 는 각각의 timestep $t$ 입니다. 아래 그림은 greedy search을 보여줍니다.   \n","\n","![Greedy Search](https://raw.githubusercontent.com/patrickvonplaten/scientific_images/master/greedy_search.png)\n","\n","알고리즘은 단어 \"The\"에서 시작하여 다음 단어로 가장 높은 확률의 단어인 \"nice\" 등을 선택하는 탐욕법입니다. 그러므로 최종적으로 생성된 Word sequence는 \"The\", \"nice\", \"woman\"이며 전반적인 확률은 0.5x0.4 = 0.2로 계산됩니다.\n","\n","다음 문맥 (\"I\", \"enjoy\", \"walking\", \"with\", \"my\", \"cute\", \"dog\")에서 GPT2를 사용하여 Word sequence를 생성할 수 있습니다.\n","\n","Transformers에서 다음과 같은 greedy search를 사용하는 방법을 살펴보겠습니다.\n","\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"OWLd_J6lXz_t","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1708501617989,"user_tz":-540,"elapsed":3981,"user":{"displayName":"이승준","userId":"02916235139337086073"}},"outputId":"ad6d6ddb-2031-43a0-bc3b-4722a1627001"},"source":["# encode context the generation is conditioned on\n","def tokenizing(text):\n","    return torch.tensor(tokenizer.encode(text, add_special_tokens=False).ids).unsqueeze(0).to('cuda')\n","\n","\n","input_ids = tokenizing(\"이순신은 조선 중기의 무신이다.\")\n","\n","# generate text until the output length (which includes the context length) reaches 50\n","# 생성 모델은 generate 함수를 통해 다음 token을 생성해낼 수 있습니다.\n","greedy_output = model.generate(input_ids, max_length=100)\n","\n","print(\"Output:\\n\" + 100 * '-')\n","print(tokenizer.decode(greedy_output.tolist()[0], skip_special_tokens=True))\n"],"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Output:\n","----------------------------------------------------------------------------------------------------\n","이순신은 조선 중기의 무신이다.</s><s> 이 목록은 대한민국의 음악 그룹인 '빅스'의 멤버인 리키가 부른 노래들이다.</s><s> 이 목록은 대한민국의 음악 그룹인 '빅스'의 멤버 리키가 부른 노래들이다.</s><s> 이 목록은 대한민국의 음악 그룹인 '빅스'의 멤버 리키가 부른 노래들이다.</s><s> 이 목록은 대한민국의 음악 그룹인 '빅스'의 멤버 리키가 부른 노래들이다.</s><s> 이 목록은 대한민국의 음악 그룹인 '빅스'\n"]}]},{"cell_type":"markdown","metadata":{"id":"ZNIDMgkds-VO"},"source":["GPT2로 짧은 텍스트를 생성했습니다.   \n","생성된 단어 문맥은 합리적이지만 모델은 비슷한 단어를 반복하는 수준입니다.   \n","이러한 현상은 일반적인 언어생성 모델에서 나타나는 공통된 문제이며 특히 Greedy search와 Beam search에서 훨씬 더 그런 현상이 두드러져 보입니다. (Vijayakumar et al., 2016 and Shao et al., 2017에서 관련 내용을 확인 할 수 있습니다.)   \n","\n","Greedy search의 주요 단점은 그림에서 볼수 있듯이 낮은 확률 단어 이후에 나올수 있는 더 높은 확률의 단어를 놓친다는 점입니다.\n","\n","예를 들면 단어 \"has\"는 0.9의 높은 조건부 확률을 가지고 있지만, 첫 검색단어중 두번째로 높은 조건부 확률 단어인 \"dog\" 이후에  숨어있는 형태입니다. 따라서 Greedy search는 \"The\",\"dog\",\"has\"라는 Word sequence를 놓치게 됩니다.\n","\n","이러한 문제는 Beam search에서 완화할 수 있습니다."]},{"cell_type":"markdown","metadata":{"id":"Y73RcrRPn-Bn"},"source":["### **Beam search**\n","\n","Beam search는 각 Time step에서 가장 확률이 높은 Hypotheses의 num_beams를 유지하고 결국 전체 확률이 가장 높은 hypothesis를 선택하는 것으로 숨겨진 높은 확률 Word sequence를 놓칠 위험을 줄입니다.\n","\n","`num_beams =2`라고 가정하고 Toy example을 설명하겠습니다.\n","\n","![Beam search](https://raw.githubusercontent.com/patrickvonplaten/scientific_images/master/beam_search.png)\n","\n","Time step=1일때, Beam search는 가장 가능성 높은 Hypothesis \"The\",\"nice\"외에도 두번째로 가능성 높은 Hypothesis인 \"The\",\"dog\"를 추적합니다.\n","\n","Time step=2일때, Beam search는 Word sequence 확률 0.2를 가진 (\"The\",\"nice\",\"woman\") 보다 확률 0.36을 가진 (\"The\", \"dog\", \"has\")가 높다는 것을 찾습니다. 이것으로 Toy example에서 가장 가능성 높은 Word sequence를 발견 할 수 있다는 것을 보였습니다.\n","\n","Beam search는 항상 Greedy search보다 높은 확률의 결과 Sequence를 찾는 것이 가능합니다. 그러나 이것이 가장 가능성 높은 결과를 찾은 것이라고는 보장할 수 없습니다.\n","\n","`transformers`에서 Beam search를 사용하는 방법을 살펴볼 것입니다. 모든 Beam Hypotheses가 EOS토큰에 닿으면 생성이 완료되도록 `num_beams > 1` 과 `eqrly_stopping=True`로 파라미터를 설정합니다.\n"]},{"cell_type":"code","metadata":{"id":"R1R5kx30Ynej","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1708501639144,"user_tz":-540,"elapsed":1371,"user":{"displayName":"이승준","userId":"02916235139337086073"}},"outputId":"6916c9fe-02ec-45f4-9ef4-bd81d3e20568"},"source":["# activate beam search and early_stopping\n","beam_output = model.generate(\n","    input_ids,\n","    max_length=50,\n","    num_beams=5,\n","    early_stopping=True\n",")\n","\n","print(\"Output:\\n\" + 100 * '-')\n","print(tokenizer.decode(beam_output.tolist()[0], skip_special_tokens=True))\n"],"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Output:\n","----------------------------------------------------------------------------------------------------\n","이순신은 조선 중기의 무신이다.</s><s> 그 후, 그 후, 그 뒤, 그 뒤, 그 뒤, 그 뒤, 그 뒤, 그 뒤, 그 뒤, 그 뒤, 그 뒤, 그 뒤, 그 뒤, 그 뒤\n"]}]},{"cell_type":"markdown","metadata":{"id":"UCeY_atbBMr7"},"source":["결과는 틀림없이 더 유창하게 보이지만 여전히 동일한 Word sequence를 반복하는 문제를 포함합니다.\n","\n","단순한 해결법은 Paulus et al. (2017)과 Klein et al. (2017)의 논문에서 제안된 n-grams 패널티를 도입하는 것입니다. 가장 일반적인 n-grams 패널티는 이미 나타난 n-gram에 대해 다음 단어로 생성될 확률을 0으로 설정하여 두번 나타나지 않도록 하는 방법입니다.\n","\n","`no_repeat_ngram_size=2`을 설정한다면 2-gram이 두번 나타나는 것을 막을 수 있습니다."]},{"cell_type":"code","metadata":{"id":"jy3iVJgfnkMi","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1708501660683,"user_tz":-540,"elapsed":500,"user":{"displayName":"이승준","userId":"02916235139337086073"}},"outputId":"ec0d606b-4ac1-4f12-e033-b1578a7650c7"},"source":["# set no_repeat_ngram_size to 2\n","beam_output = model.generate(\n","    input_ids,\n","    max_length=50,\n","    num_beams=5,\n","    no_repeat_ngram_size=2, # 반복 막기\n","    early_stopping=True\n",")\n","\n","print(\"Output:\\n\" + 100 * '-')\n","print(tokenizer.decode(beam_output.tolist()[0], skip_special_tokens=True))\n"],"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["Output:\n","----------------------------------------------------------------------------------------------------\n","이순신은 조선 중기의 무신이다.</s><s> 이 목록은 대한민국의 보물 제327호로 지정되어 있는 보물 1호, 보물 2호로 지정된 삽화집(보물 제224호)에 대한 간략한 소개를 담고 있다.<br></s>\n"]}]},{"cell_type":"markdown","metadata":{"id":"qXTJqkIlFR4L"},"source":["더이상 반복이 나타나지 않는 다는 것을 볼 수 있습니다. 하지만 n-gram 패널티는 신중하게 사용되어야 합니다. 예를 들면 city New York에 대해 생성된 기사는 n-gram을 사용하지 않는 것이 좋습니다. 2-gram을 사용하게 될 경우 시의 이름이 전체 텍스트에서 한 번만 나타나기 때문입니다.\n","\n","Beam search의 또 다른 중요한 특징은 생성된 Top beam을 비교하여 목적에 가장 적합한 Beam을 선택할 수 있다는 것입니다.\n","\n","Transformer에서 num_return_sequences 파라미터를 return 해야 하는 최대 num_beams 보다 작거나 같도록 설정합니다. `num_return_sequences <= num_beams`로 설정된 코드를 확인할 수 있습니다."]},{"cell_type":"code","metadata":{"id":"5ClO3VphqGp6","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1708501671423,"user_tz":-540,"elapsed":825,"user":{"displayName":"이승준","userId":"02916235139337086073"}},"outputId":"132ae663-e0c6-4ad7-8cc7-0eda54da7816"},"source":["# set return_num_sequences > 1\n","beam_outputs = model.generate(\n","    input_ids,\n","    max_length=50,\n","    num_beams=5,\n","    no_repeat_ngram_size=2,\n","    num_return_sequences=5,\n","    early_stopping=True\n",")\n","\n","# now we have 3 output sequences\n","print(\"Output:\\n\" + 100 * '-')\n","for i, beam_output in enumerate(beam_outputs):\n","  print(\"{}: {}\".format(i, tokenizer.decode(beam_output.tolist(), skip_special_tokens=True)))"],"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["Output:\n","----------------------------------------------------------------------------------------------------\n","0: 이순신은 조선 중기의 무신이다.</s><s> 그 뒤, 그는 《삼국유사》를 편찬하여 《동국여지승람》을 편찬하였고, 《조선왕조실록》과 《고려사》의 편찬에도 참여하였다.《</s>\n","1: 이순신은 조선 중기의 무신이다.</s><s> 그 뒤, 그는 《삼국유사》를 편찬하여 《동국여지승람》을 편찬하였고, 《조선왕조실록》과 《고려사》의 편찬에 참여하였다.《</s>\n","2: 이순신은 조선 중기의 무신이다.</s><s> 그 뒤, 그는 《삼국유사》를 편찬하여 《동국여지승람》을 편찬하였고, 《조선왕조실록》과 《고려사》의 편찬에도 참여하였다. &lt\n","3: 이순신은 조선 중기의 무신이다.</s><s> 그 뒤, 그는 《삼국유사》를 편찬하여 《동국여지승람》을 편찬하였고, 《조선왕조실록》과 《고려사》의 편찬에 참여하였다.》</s>\n","4: 이순신은 조선 중기의 무신이다.</s><s> 그 뒤, 그는 《삼국유사》를 편찬하여 《동국여지승람》을 편찬하였고, 《조선왕조실록》과 《고려사》의 편찬에도 참여하였다. &</s>\n"]}]},{"cell_type":"markdown","metadata":{"id":"FyRYm1OUI0zN"},"source":["코드 결과를 통해 볼수 있듯이 5개의 Beam hypotheses는 서로 약간 다를 뿐이며 5개만 사용했을 경우 놀랄만한 결과는 아닙니다.\n","\n","개방형 생성에서는 Beam search가 최선의 선택사항이 아닐수 있는 몇 가지 이유가 최근에 제시되었습니다.\n","\n","- Beam search는 Machine translation 또는 Text summarization처럼 원하는 문장 생성 길이가 예측 가능한 Task에서는 잘 작동할 수 있습니다. 하지만 Dialog 또는 Story Generation Task처럼 출력길이가 크게 달라질 수 있는 개방형 생성에서는 원활하게 작동하지 않습니다. (\n","[Murray et al. (2018)](https://arxiv.org/abs/1808.10006), [Yang et al. (2018)](https://arxiv.org/abs/1808.09582))\n","\n","- Beam search은 반복 생성 문제에 취약합니다. 특히 Story Generation Task에서 n-gram또는 기타 패널티를 통해 문장을 제어하는 것이 어렵습니다. 왜냐하면 \"반복이 없는 구문\"과 \"n=gram반복 주기\" 사이에서 적당한 trade-off를 찾기 위해 많은 finetuning이 필요하기 때문입니다.\n","\n","- [Ari Holtzman et al. (2019)](https://arxiv.org/abs/1904.09751) 논문에 따르면 고품질 인간 언어는 높은 확률의 다음 단어 분포를 따르지 않는다고 주장합니다. 쉽게 말하자면 인간입장에서 우리는 지루하거나 예측 가능한 문장이 아니라 우리를 놀라게 할 수 있는 문장생성을 원한다고 합니다. 저자는 모델이 인간 텍스트 대비 beam search text를 그래프로 보여주면서 beam search text가 그다지 놀랍지 않은 문장이라는 것을 보여줬습니다.\n","\n","\n","![alt text](https://blog.fastforwardlabs.com/images/2019/05/Screen_Shot_2019_05_08_at_3_06_36_PM-1557342561886.png)"]},{"cell_type":"markdown","metadata":{"id":"hWQ2EWfYRFJZ"},"source":["### **Sampling**\n","\n","가장 기본적인 형태의 Sampling은 조건부 확률 분포에 따라 다음 단어 $w_t$를 무작위로 선택하는 것을 의미합니다.\n","\n","\n","$$w_t \\sim P(w|w_{1:t-1})$$\n","\n","위의 예를 들어, 아래 사진은 Sampling할 때 언어 생성을 시각화한 형태입니다.\n","\n","\n","![vanilla_sampling](https://raw.githubusercontent.com/patrickvonplaten/scientific_images/master/sampling_search.png)\n","\n","\n","Sampling을 이용한 언어생성은 더이상 결정론적이지 않습니다. 단어\n","$\\text{\"car\"}$ 는 조건부확률 $P(w | \\text{\"The\"})$에서 샘플링 된 후, $P(w | \\text{\"The\"}, \\text{\"car\"})$에서 $\\text{\"drives\"}$를 샘플링 합니다.\n","\n","\n","\n","`transformers`에서 `do_sample=True`를 설정하고 `top_k=0`을 통해 *Top-K* sampling을 비활성화 합니다."]},{"cell_type":"code","metadata":{"id":"aRAz4D-Ks0_4","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1708501801842,"user_tz":-540,"elapsed":841,"user":{"displayName":"이승준","userId":"02916235139337086073"}},"outputId":"dda299e4-d75c-4af9-b30e-5914c197d447"},"source":["# activate sampling and deactivate top_k by setting top_k sampling to 0\n","sample_output = model.generate(\n","    input_ids,\n","    do_sample=True, # 완전 random sampling\n","    max_length=50,\n","    top_k=0 # w/o top_k 추출\n",")\n","\n","print(\"Output:\\n\" + 100 * '-')\n","print(tokenizer.decode(sample_output.tolist()[0], skip_special_tokens=True))"],"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["Output:\n","----------------------------------------------------------------------------------------------------\n","이순신은 조선 중기의 무신이다.</s><s> 미국을 비롯한 한국전쟁의 대표적인 전쟁학자이자 여류 문호도이자 통일 이론가인 이두동, 1932.</s><s> 그는 생애부터 통일 철학에 대한 기초 지식인이며 한국의 평화와 통일을 담당할 통일\n"]}]},{"cell_type":"markdown","metadata":{"id":"WWjxm9otWZgY"},"source":["흥미롭게도 본문은 괜찮은 것 같지만 자세히 보면 매우 일관성 없는 문장입니다. *3-grams*의 *new hand sense* 와*local batte harness* 라는 문장은 이상하고 사람이 쓴것처럼 보이지 않습니다. 이것은 sampling word sequences를 할때 모델이 일관성없이 횡설수설하는 문장을 발생시키는 큰 문제입니다. ([Ari Holtzman et al. (2019)](https://arxiv.org/abs/1904.09751)).\n","\n","한가지 트릭은 [softmax](https://en.wikipedia.org/wiki/Softmax_function#Smooth_arg_max). 의 이른바 `temperature`를 낮추어 분포 $P(w|w_{1:t-1})$를 더 선명하게 만드는 것입니다. 높은 확률의 단어의 가능성은 증가시키고 낮은 확률의 단어 가능성은 감소시키는 효과가 있습니다.\n","\n","temperature를 적용한다면 다음과 같은 그림을 보일 수 있습니다.\n","\n","![top_p_sampling](https://github.com/patrickvonplaten/scientific_images/blob/master/sampling_search_with_temp.png?raw=true)\n","\n","step=1의 다음 단어 분포는 더욱 선명해졌기 때문에 단어 $\\text{\"car\"}$를 선택할 확률이 거의 없습니다.\n","\n","\n","`temperature=0.7`를 설정하여 라이브러리에서 분포를 어떻게 변화시키는지 알아보겠습니다."]},{"cell_type":"code","metadata":{"id":"WgJredc-0j0Z","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1708501870532,"user_tz":-540,"elapsed":604,"user":{"displayName":"이승준","userId":"02916235139337086073"}},"outputId":"20aa9e40-a35f-4397-9944-5fda25bdbdd5"},"source":["# use temperature to decrease the sensitivity to low probability candidates\n","sample_output = model.generate(\n","    input_ids,\n","    do_sample=True,\n","    max_length=50,\n","    top_k=0,\n","    temperature=0.7\n",")\n","\n","print(\"Output:\\n\" + 100 * '-')\n","print(tokenizer.decode(sample_output.tolist()[0], skip_special_tokens=True))"],"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["Output:\n","----------------------------------------------------------------------------------------------------\n","이순신은 조선 중기의 무신이다.</s><s> 이 날 경기에서 KIA와 두산은 윤석민과 노경은을 선발로 내세웠다.</s><s> 올 시즌 처음으로 선발 등판하는 두 번째 선발투수라는 점 때문에 많은 기대를 모으고 있다.</s><s> 하지만 윤석민은\n"]}]},{"cell_type":"code","source":["# use temperature to decrease the sensitivity to low probability candidates\n","sample_output = model.generate(\n","    input_ids,\n","    do_sample=True,\n","    max_length=50,\n","    top_k=0,\n","    temperature=0.1\n",")\n","\n","print(\"Output:\\n\" + 100 * '-')\n","print(tokenizer.decode(sample_output.tolist()[0], skip_special_tokens=True))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KojVBeFTpa5t","executionInfo":{"status":"ok","timestamp":1708501917903,"user_tz":-540,"elapsed":783,"user":{"displayName":"이승준","userId":"02916235139337086073"}},"outputId":"eefef830-b45e-4748-fee0-65cb03d09216"},"execution_count":17,"outputs":[{"output_type":"stream","name":"stdout","text":["Output:\n","----------------------------------------------------------------------------------------------------\n","이순신은 조선 중기의 무신이다.</s><s> 이 목록은 다음과 같다.</s><s> 이 목록은 다음과 같다.</s><s> 이 목록은 다음과 같다.</s><s> 이 목록은 다음과 같다.</s><s> 이 목록은 다음과 같다.</s><s> 이 목록은 다음과 같다.</s>\n"]}]},{"cell_type":"markdown","metadata":{"id":"wn_k6Bykf_vg"},"source":["이제 이상한 n-gram이 적고 출력 문장이 조금 더 일관성 있게 생성됩니다. temperature를 적용하면 분포가 덜 랜덤하지만 `temperature` $ \\to 0$,을 설정한다면 temperature가 적용된 sampling은 greedy decoding과 같아지며 이전과 동일한 문제를 겪습니다."]},{"cell_type":"markdown","metadata":{"id":"KDhbxGrehxlz"},"source":["### **Top-K Sampling**\n","\n","[Fan et. al (2018)](https://arxiv.org/pdf/1805.04833.pdf)\n","\n","***Top-K*** sampling은 간단하지만 매우 강력한 생플링 방식을 도입했습니다. . *Top-K* sampling에서 가장 가능성 높은 다음 단어는 필터링 되고 확률 질량은 K 다음 단어에만 재분배됩니다. GPT2는 Top-K Sampling방식을 채택했는데, 이것이 Story Gerneration Task에 성공한 이유중 하나입니다.\n","\n","Top-K Sampling을 더 잘 설명하기 위해 위의 예제에서 두 Sampling step에 사용되는 범위를 3단어에서 10단어로 확장합니다.\n","\n","![top_k_sampling](https://raw.githubusercontent.com/patrickvonplaten/scientific_images/master/top_k_sampling.png)\n","\n","\n","K=6을 설정하면 두 Sampling steps에서 Sampling pool을 6개의 단어로 제한합니다. $V_{\\text{top-K}}$로 정의되는 가장 높은 6개의 단어로  sampling pool을 제한합니다.\n","\n","첫 step에서 전체 확률 질량의 2/3인 0.68정도에 해당하는 단어에서 디코딩되지만, 두번째 step에서 거의 모든 확률질량인 0.99에서 디코딩합니다.\n","\n","그럼에도 불구하고 그것이 두번째 sampling step에서 $\\text{\"not\", \"the\", \"small\", \"told\"}$ 와 같은 다소 이상한 후보들을 성공적으로 제거가 가능했습니다."]},{"cell_type":"code","metadata":{"id":"HBtDOdD0wx3l","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1708501945194,"user_tz":-540,"elapsed":785,"user":{"displayName":"이승준","userId":"02916235139337086073"}},"outputId":"32473d46-76d9-4dbf-9438-00d7d97c7db3"},"source":["# set top_k to 50\n","sample_output = model.generate(\n","    input_ids,\n","    do_sample=True,\n","    max_length=50,\n","    top_k=50\n",")\n","\n","print(\"Output:\\n\" + 100 * '-')\n","print(tokenizer.decode(sample_output.tolist()[0], skip_special_tokens=True))"],"execution_count":18,"outputs":[{"output_type":"stream","name":"stdout","text":["Output:\n","----------------------------------------------------------------------------------------------------\n","이순신은 조선 중기의 무신이다.</s><s> 한편, 대한민국 제1차 세계대전 발발 이후 미국이 미국민들을 해쳐간 것을 반성하자는 '미친군'이라는 용어로는 '미국'이라는 용어를 사용할 수 없다는 지적이 나오기도 했다.</s><s>\n"]}]},{"cell_type":"markdown","metadata":{"id":"dzeNQJNhr3EH"},"source":["지금까지 기법중 가장 인간적으로 보이는 텍스트를 생성했습니다. Top-K Sampling의 한 가지 우려되는 점은 다음 단어 확률 분포 $P(w|w_{1:t-1})$에서 필터링된 단어 수를 동적으로 조정하지 않는 점입니다. 예를들면 위 그림에서 첫번째 step의 단어들은 전반적으로 평평한 분포에서 Sampling 되지만, 두번째 step의 어떤 단어들은 매우 Sharp한 분포에서 Sampling 될 수 있기 때문에 문제가 될 수 있습니다.\n","\n","\n","Step $t=1$에서 Top-K은 꽤 합리적인 후보처럼 보이는 $\\text{\"people\", \"big\", \"house\", \"cat\"}$을 샘플링하는 가능성을 배제합니다. 반면에 Step $t=2$에서 단어 Sample pool에 단어 $\\text{\"down\", \"a\"}$와 같은 부적절한 단어를 포함합니다. 그러므로 Sample pool이 고정크기 K로 제한되면 모형이 Sharp한 분포에서 횡설수설한 단어를 고를 위험이있고 평평한 분포에서는 문장의 창의성이 제한될 수 있습니다. ([Ari Holtzman et al. (2019)](https://arxiv.org/abs/1904.09751))"]},{"cell_type":"markdown","metadata":{"id":"wV6AHjet0Slr"},"source":["### **Top-p (nucleus) sampling**\n","\n","Top-p sampling은 가장 가능성 높은 단어 K 개에서만 Sample을 추출하는 방법이 아니라 누적확률이 확률 p를 초과하는 최소한의 단어 집합에서 Sample을 추출합니다.\n","\n","그 후 확률 질량이 단어 집합 사이에 재분배 됩니다. 이 방법은 다음 단어의 확률 분포에 따라 단어 집합의 크기가 동적으로 증가하거나 감소할 수 있습니다.\n","\n","![top_p_sampling](https://github.com/patrickvonplaten/scientific_images/blob/master/top_p_sampling.png?raw=true)\n","\n","\n","\n","$p=0.92$을 설정할 경우, 상위 p Sample 추출은 $V_{\\text{top-p}}$로 정의된 확률 질량의 $p=92\\%$를 초과할 최소 단어 수를 선택합니다.\n","첫번째 예에서 가장 가능성 높은 9개의 단어 (\"nice\", \"dog\", \"car\" ...  house)가 포함된 반면, 두번째 예에서는 상위 3개의 단어(\"drives\", \"is\", \"turns\")만 선택해도 92%를 초과하게 됩니다. 즉 높은 확률의 단어에만 Sampling 하고 그렇지 않은 단어는 Sampling할 확률이 매우 적습니다."]},{"cell_type":"code","metadata":{"id":"EvwIc7YAx77F","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1708502003590,"user_tz":-540,"elapsed":1019,"user":{"displayName":"이승준","userId":"02916235139337086073"}},"outputId":"cd80b366-dd7b-479a-968f-e6d2d81bb9f9"},"source":["# deactivate top_k sampling and sample only from 92% most likely words\n","sample_output = model.generate(\n","    input_ids,\n","    do_sample=True,\n","    max_length=50,\n","    top_p=0.92,\n","    top_k=0\n",")\n","\n","print(\"Output:\\n\" + 100 * '-')\n","print(tokenizer.decode(sample_output.tolist()[0], skip_special_tokens=True))"],"execution_count":19,"outputs":[{"output_type":"stream","name":"stdout","text":["Output:\n","----------------------------------------------------------------------------------------------------\n","이순신은 조선 중기의 무신이다.</s><s> 그 외에도 그리스도교에서는 사능에 의해서 하느님께서 가능하듯이 그것을 참 말로 보아야 할 것이다.</s><s> 그리고 파라나스의 전능하신 교리를 들은 무신들은 믿는 사람이 오직 하나님을\n"]}]},{"cell_type":"markdown","metadata":{"id":"894AH8E03pv7"},"source":["이론적으로는 Top-p가 Top-K보다 더 성능이 좋아 보이지만, 두 방법 모두 실제로 잘 작동합니다. Top-p는 또한 Top-K와 함께 사용될 수 있는데, 이것은 매우 낮은 순위의 단어를 피하면서도 일부 동적 선택을 허용할 수 있습니다.\n","\n","독립적으로 샘플링된 다중 출력을 얻기 위하여 파라미터를 다시 설정하도록 코드를 구성 할 수도 있습니다. `num_return_sequences > 1`:"]},{"cell_type":"code","metadata":{"id":"3kY8P9VG8Gi9","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1708502015788,"user_tz":-540,"elapsed":842,"user":{"displayName":"이승준","userId":"02916235139337086073"}},"outputId":"46a9bfbe-fdf1-40cc-9889-4370db052a40"},"source":["# set top_k = 50 and set top_p = 0.95 and num_return_sequences = 3\n","sample_outputs = model.generate(\n","    input_ids,\n","    do_sample=True,\n","    max_length=50,\n","    top_k=20,\n","    top_p=0.90,\n","    num_return_sequences=3\n",")\n","\n","print(\"Output:\\n\" + 100 * '-')\n","for i, sample_output in enumerate(sample_outputs):\n","  print(\"{}: {}\".format(i, tokenizer.decode(sample_output.tolist(), skip_special_tokens=True)))"],"execution_count":20,"outputs":[{"output_type":"stream","name":"stdout","text":["Output:\n","----------------------------------------------------------------------------------------------------\n","0: 이순신은 조선 중기의 무신이다.</s><s> 이 때, 그는 자신이 세운 성(城)과 그 주위에 있던 건물들을 전부 파괴하고, 성당의 이름을 ‘성(城)’으로 바꾸고, 성을 성(城)이라고 명명하여 성을 세웠다\n","1: 이순신은 조선 중기의 무신이다.</s><s> 또한, 조선 숙종 때 성리학자 이덕형이 사림파의 학설을 지지하여, 송시열의 《송씨조씨》(송씨조씨의 일화)라는 책을 편찬하였으며, 이 책은 사림\n","2: 이순신은 조선 중기의 무신이다.</s><s> 이에 따라 이 전 대통령은 이 전 대통령의 부인 김윤옥 여사에 대한 조사 방침은 일단 보류한 채 향후 수사 진행 상황을 지켜보겠다는 뜻을 전한 것으로 알려졌다.</s><s> 검찰 관계자는 \"이 전 대통령의 조사\n"]}]},{"cell_type":"markdown","metadata":{"id":"etg-3zdz4jQK"},"source":["### **Conclusion**\n","\n","- ad-hoc decoding방법에 따르면 Top-p와 Top-k sampling은 기존 Greedy-, Beam search 보다 개방형 언어생성에서 더 유창한 문장을 생성하는 것으로 보입니다.\n","\n","- 그러나 Top-p와 Top-k sampling 또한 Greedy-, Beam search 처럼 반복적 Word sequencd에 대한 문제가 발생합니다. ([Welleck et al. (2020)](https://arxiv.org/abs/2002.02492))\n","\n","- ([Welleck et al. (2019)](https://arxiv.org/pdf/1908.04319.pdf))에 따르면 Beam search가 모델 트레이닝 목적함수를 잘 조정한다면 Top-p보다 더 유찬한 텍스트를 생선한다는 연구도 있습니다.\n","\n","- 개방형 언어생성은 빠르게 발전하는 분야이며, 무엇이 적합하다고 단정할 수 없으므로 특정 사용 사례에서 가장 잘 작동하는 방법이 무엇인지 고려해야 합니다.\n","\n","- 언어별, 모델별로 최적의 생성 방식을 찾아내야 합니다.\n"]},{"cell_type":"code","source":[],"metadata":{"id":"iq7bpv2mrQSk"},"execution_count":null,"outputs":[]}]}