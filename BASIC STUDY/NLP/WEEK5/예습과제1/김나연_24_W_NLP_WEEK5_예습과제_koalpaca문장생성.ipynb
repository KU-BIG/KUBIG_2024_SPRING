{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "fsnGgar7gx83"
      },
      "source": [
        "5주차 예습과제에서는 6주차 내용인 GPT의 생성 능력을 맛보고자 합니다. 사용할 모델은 koalpaca라는 모델인데, 한국어 버전의 alpaca 모델입니다.\n",
        "\n",
        "alpaca는 llama라는 모델을 instruction fine-tuning 해서 사람의 지시를 잘 따르는 모델입니다.\n",
        "\n",
        "llama라는 모델은 LLaMA-13B의 경우 GPT-3보다 10배이상 작음에도 불구하고 대부분의 평가서 GPT-3보다 우수한 성능을 보이며, 더 나아가 LLaMA-65B의 경우 대부분의 벤치마크에서 Chinchilla, Gopher, GPT-3, PaLM와 유사하거나 더 뛰어난 결과를 보이는 훌륭한 모델입니다.\n",
        "\n",
        "그러면 지금부터 koalpaca로 생성 모델을 한번 경험해보도록 하겠습니다!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "7crJC7X_eb1v"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Wed Feb 14 16:28:25 2024       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.154.05             Driver Version: 535.154.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  NVIDIA GeForce RTX 3080 Ti     Off | 00000000:01:00.0  On |                  N/A |\n",
            "|  0%   58C    P5              33W / 400W |  11197MiB / 12288MiB |     46%      Default |\n",
            "|                                         |                      |                  N/A |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "|    0   N/A  N/A      1660      G   /usr/lib/xorg/Xorg                          334MiB |\n",
            "|    0   N/A  N/A      1855      G   /usr/bin/gnome-shell                         80MiB |\n",
            "|    0   N/A  N/A      3220      G   ...sion,SpareRendererForSitePerProcess      178MiB |\n",
            "|    0   N/A  N/A      4786      G   ...17577218,4752912574222447472,262144      184MiB |\n",
            "|    0   N/A  N/A    103724      C   /bin/python3                                826MiB |\n",
            "|    0   N/A  N/A    104118      C   /bin/python3                               9570MiB |\n",
            "+---------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "# GPU 켜져 있는지 확인\n",
        "\n",
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "o186Sm7Uef5V"
      },
      "outputs": [],
      "source": [
        "!pip install -q -U bitsandbytes\n",
        "!pip install -q -U git+https://github.com/huggingface/transformers.git\n",
        "!pip install -q -U git+https://github.com/huggingface/peft.git\n",
        "!pip install -q -U git+https://github.com/huggingface/accelerate.git\n",
        "!pip install -q datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "VgCKqAXMekYz"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/jj/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "syg88baNin2b"
      },
      "source": [
        "peft는 parameter efficient fine-tuning의 약자로, 큰 모델을 코랩에서 돌릴 수 있도록 쪼개서 만든 것입니다.\n",
        "\n",
        "오래 걸립니다!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "GlZv-twPerjy"
      },
      "outputs": [],
      "source": [
        "from peft import PeftModel, PeftConfig"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "gYf0UQ3fetzP"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "adapter_config.json: 100%|██████████| 428/428 [00:00<00:00, 1.10MB/s]\n",
            "config.json: 100%|██████████| 686/686 [00:00<00:00, 1.16MB/s]\n",
            "model.safetensors.index.json: 100%|██████████| 52.5k/52.5k [00:00<00:00, 45.0MB/s]\n",
            "model-00001-of-00028.safetensors: 100%|██████████| 946M/946M [01:24<00:00, 11.2MB/s]\n",
            "model-00002-of-00028.safetensors: 100%|██████████| 843M/843M [01:15<00:00, 11.2MB/s]\n",
            "model-00003-of-00028.safetensors: 100%|██████████| 843M/843M [01:15<00:00, 11.2MB/s]\n",
            "model-00004-of-00028.safetensors: 100%|██████████| 1.00G/1.00G [01:30<00:00, 11.2MB/s]\n",
            "model-00005-of-00028.safetensors: 100%|██████████| 896M/896M [01:19<00:00, 11.2MB/s]\n",
            "model-00006-of-00028.safetensors: 100%|██████████| 1.00G/1.00G [01:31<00:00, 11.0MB/s]\n",
            "model-00007-of-00028.safetensors: 100%|██████████| 896M/896M [01:20<00:00, 11.1MB/s]\n",
            "model-00008-of-00028.safetensors: 100%|██████████| 1.00G/1.00G [01:33<00:00, 10.8MB/s]\n",
            "model-00009-of-00028.safetensors: 100%|██████████| 896M/896M [01:21<00:00, 11.0MB/s]\n",
            "model-00010-of-00028.safetensors: 100%|██████████| 1.00G/1.00G [01:35<00:00, 10.5MB/s]\n",
            "model-00011-of-00028.safetensors: 100%|██████████| 896M/896M [01:19<00:00, 11.2MB/s]\n",
            "model-00012-of-00028.safetensors: 100%|██████████| 1.00G/1.00G [01:30<00:00, 11.1MB/s]\n",
            "model-00013-of-00028.safetensors: 100%|██████████| 896M/896M [01:20<00:00, 11.1MB/s]\n",
            "model-00014-of-00028.safetensors: 100%|██████████| 1.00G/1.00G [01:35<00:00, 10.5MB/s]\n",
            "model-00015-of-00028.safetensors: 100%|██████████| 896M/896M [01:21<00:00, 11.0MB/s]\n",
            "model-00016-of-00028.safetensors: 100%|██████████| 1.00G/1.00G [01:29<00:00, 11.2MB/s]\n",
            "model-00017-of-00028.safetensors: 100%|██████████| 896M/896M [01:26<00:00, 10.4MB/s]\n",
            "model-00018-of-00028.safetensors: 100%|██████████| 1.00G/1.00G [01:40<00:00, 10.0MB/s]\n",
            "model-00019-of-00028.safetensors: 100%|██████████| 896M/896M [01:45<00:00, 8.49MB/s]\n",
            "model-00020-of-00028.safetensors: 100%|██████████| 1.00G/1.00G [01:54<00:00, 8.81MB/s]\n",
            "model-00021-of-00028.safetensors: 100%|██████████| 896M/896M [01:44<00:00, 8.55MB/s]\n",
            "model-00022-of-00028.safetensors: 100%|██████████| 1.00G/1.00G [02:01<00:00, 8.25MB/s]\n",
            "model-00023-of-00028.safetensors: 100%|██████████| 896M/896M [01:46<00:00, 8.39MB/s]\n",
            "model-00024-of-00028.safetensors: 100%|██████████| 1.00G/1.00G [02:05<00:00, 8.02MB/s]\n",
            "model-00025-of-00028.safetensors: 100%|██████████| 896M/896M [01:49<00:00, 8.20MB/s]\n",
            "model-00026-of-00028.safetensors: 100%|██████████| 1.00G/1.00G [02:05<00:00, 7.99MB/s]\n",
            "model-00027-of-00028.safetensors: 100%|██████████| 896M/896M [01:47<00:00, 8.32MB/s]\n",
            "model-00028-of-00028.safetensors: 100%|██████████| 518M/518M [01:00<00:00, 8.54MB/s]\n",
            "Downloading shards: 100%|██████████| 28/28 [43:59<00:00, 94.27s/it] \n",
            "Loading checkpoint shards: 100%|██████████| 28/28 [00:53<00:00,  1.93s/it]\n",
            "generation_config.json: 100%|██████████| 100/100 [00:00<00:00, 204kB/s]\n",
            "adapter_model.bin: 100%|██████████| 26.2M/26.2M [00:02<00:00, 11.4MB/s]\n",
            "tokenizer_config.json: 100%|██████████| 210/210 [00:00<00:00, 435kB/s]\n",
            "tokenizer.json: 100%|██████████| 1.65M/1.65M [00:00<00:00, 1.69MB/s]\n",
            "special_tokens_map.json: 100%|██████████| 185/185 [00:00<00:00, 391kB/s]\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "PeftModelForCausalLM(\n",
              "  (base_model): LoraModel(\n",
              "    (model): GPTNeoXForCausalLM(\n",
              "      (gpt_neox): GPTNeoXModel(\n",
              "        (embed_in): Embedding(30080, 5120)\n",
              "        (emb_dropout): Dropout(p=0.0, inplace=False)\n",
              "        (layers): ModuleList(\n",
              "          (0-39): 40 x GPTNeoXLayer(\n",
              "            (input_layernorm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
              "            (post_attention_layernorm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
              "            (post_attention_dropout): Dropout(p=0.0, inplace=False)\n",
              "            (post_mlp_dropout): Dropout(p=0.0, inplace=False)\n",
              "            (attention): GPTNeoXAttention(\n",
              "              (rotary_emb): GPTNeoXRotaryEmbedding()\n",
              "              (query_key_value): lora.Linear4bit(\n",
              "                (base_layer): Linear4bit(in_features=5120, out_features=15360, bias=True)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Dropout(p=0.05, inplace=False)\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=5120, out_features=8, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=8, out_features=15360, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "              )\n",
              "              (dense): Linear4bit(in_features=5120, out_features=5120, bias=True)\n",
              "              (attention_dropout): Dropout(p=0.0, inplace=False)\n",
              "            )\n",
              "            (mlp): GPTNeoXMLP(\n",
              "              (dense_h_to_4h): Linear4bit(in_features=5120, out_features=20480, bias=True)\n",
              "              (dense_4h_to_h): Linear4bit(in_features=20480, out_features=5120, bias=True)\n",
              "              (act): GELUActivation()\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "        (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
              "      )\n",
              "      (embed_out): Linear(in_features=5120, out_features=30080, bias=False)\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "peft_model_id = \"beomi/qlora-koalpaca-polyglot-12.8b-50step\"\n",
        "config = PeftConfig.from_pretrained(peft_model_id)\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16\n",
        ")\n",
        "model = AutoModelForCausalLM.from_pretrained(config.base_model_name_or_path, quantization_config=bnb_config, device_map={\"\":0})\n",
        "model = PeftModel.from_pretrained(model, peft_model_id)\n",
        "tokenizer = AutoTokenizer.from_pretrained(config.base_model_name_or_path)\n",
        "\n",
        "model.eval()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "ViGa1Tvyi8fI"
      },
      "source": [
        "next token generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "sHacSUa3fgz0"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "인간처럼 생각하고, 행동하는 '지능'을 통해 인류가 이제까지 풀지 못했던 수수께끼에 도전한다. 그는 인공지능 분야의 최고 권위자로, MIT 인공지능연구소 소장을 역임했다. IBM 연구원으로 인공 지능 및 시스템 신경망 분야 연구를 선도했으며, 이후 하버드대학교\n"
          ]
        }
      ],
      "source": [
        "prompt = '인간처럼 생각하고, 행동하는 \\'지능\\'을 통해 인류가 이제까지 풀지 못했던'\n",
        "with torch.no_grad():\n",
        "  tokens = tokenizer.encode(prompt, return_tensors='pt').to(device='cuda', non_blocking=True)\n",
        "  gen_tokens = model.generate(tokens, do_sample=True, temperature=0.8, max_length=64)\n",
        "  generated = tokenizer.batch_decode(gen_tokens)[0]\n",
        "\n",
        "print(generated)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "zO4qeMpZi54_"
      },
      "source": [
        " QA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "0JBJKYJcfLuX"
      },
      "outputs": [],
      "source": [
        "def gen(x):\n",
        "    q = f\"### 질문: {x}\\n\\n### 답변:\"\n",
        "    # print(q)\n",
        "    gened = model.generate(\n",
        "        **tokenizer(\n",
        "            q,\n",
        "            return_tensors='pt',\n",
        "            return_token_type_ids=False\n",
        "        ).to('cuda'),\n",
        "        max_new_tokens=50,\n",
        "        early_stopping=True,\n",
        "        do_sample=True,\n",
        "        eos_token_id=2,\n",
        "    )\n",
        "    print(tokenizer.decode(gened[0]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "55GdkP9afi6U"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/jj/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:447: UserWarning: `num_beams` is set to 1. However, `early_stopping` is set to `True` -- this flag is only used in beam-based generation modes. You should set `num_beams>1` or unset `early_stopping`.\n",
            "  warnings.warn(\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "### 질문: 건강하게 살기 위한 세 가지 방법은?\n",
            "\n",
            "### 답변: ① 마음을 편안하게 다스려야 건강해진다. 우리의 인체는 자율신경에 의해 조절되는 ‘기(氣)’로 구성되어 있어서, 사람이 분노를 하면 그 기운이 머리 쪽으로 올라가는데 이것\n"
          ]
        }
      ],
      "source": [
        "gen('건강하게 살기 위한 세 가지 방법은?')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "VjacO_6njJWR"
      },
      "source": [
        "top_k와 같은 parameter를 조절하면서 실험해보세요!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "8JaD2vpPfkuU"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "인간처럼 생각하고, 행동하는 '지능'을 통해 인류가 이제까지 풀지 못했던 문제를 해결하는 것이 가능해진다면, 우리는 그것을 인류의 진화라고 부를 수 있고, 또 그렇게 불러야 마땅하다. 인공지능 연구의 역사 인공지능의 역사를 돌이켜보면\n"
          ]
        }
      ],
      "source": [
        "prompt = '인간처럼 생각하고, 행동하는 \\'지능\\'을 통해 인류가 이제까지 풀지 못했던'\n",
        "with torch.no_grad():\n",
        "  tokens = tokenizer.encode(prompt, return_tensors='pt').to(device='cuda', non_blocking=True)\n",
        "  gen_tokens = model.generate(tokens, do_sample=True, temperature=0.8, max_length=64, top_k=50)\n",
        "  generated = tokenizer.batch_decode(gen_tokens)[0]\n",
        "\n",
        "print(generated)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
