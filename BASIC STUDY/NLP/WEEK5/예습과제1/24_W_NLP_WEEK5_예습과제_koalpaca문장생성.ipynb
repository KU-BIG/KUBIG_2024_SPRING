{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "5주차 예습과제에서는 6주차 내용인 GPT의 생성 능력을 맛보고자 합니다. 사용할 모델은 koalpaca라는 모델인데, 한국어 버전의 alpaca 모델입니다.\n",
        "\n",
        "alpaca는 llama라는 모델을 instruction fine-tuning 해서 사람의 지시를 잘 따르는 모델입니다.\n",
        "\n",
        "llama라는 모델은 LLaMA-13B의 경우 GPT-3보다 10배이상 작음에도 불구하고 대부분의 평가서 GPT-3보다 우수한 성능을 보이며, 더 나아가 LLaMA-65B의 경우 대부분의 벤치마크에서 Chinchilla, Gopher, GPT-3, PaLM와 유사하거나 더 뛰어난 결과를 보이는 훌륭한 모델입니다.\n",
        "\n",
        "그러면 지금부터 koalpaca로 생성 모델을 한번 경험해보도록 하겠습니다!"
      ],
      "metadata": {
        "id": "fsnGgar7gx83"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7crJC7X_eb1v"
      },
      "outputs": [],
      "source": [
        "# GPU 켜져 있는지 확인\n",
        "\n",
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q -U bitsandbytes\n",
        "!pip install -q -U git+https://github.com/huggingface/transformers.git\n",
        "!pip install -q -U git+https://github.com/huggingface/peft.git\n",
        "!pip install -q -U git+https://github.com/huggingface/accelerate.git\n",
        "!pip install -q datasets"
      ],
      "metadata": {
        "id": "o186Sm7Uef5V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig"
      ],
      "metadata": {
        "id": "VgCKqAXMekYz"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "peft는 parameter efficient fine-tuning의 약자로, 큰 모델을 코랩에서 돌릴 수 있도록 쪼개서 만든 것입니다.\n",
        "\n",
        "오래 걸립니다!"
      ],
      "metadata": {
        "id": "syg88baNin2b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import PeftModel, PeftConfig"
      ],
      "metadata": {
        "id": "GlZv-twPerjy"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "peft_model_id = \"beomi/qlora-koalpaca-polyglot-12.8b-50step\"\n",
        "config = PeftConfig.from_pretrained(peft_model_id)\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16\n",
        ")\n",
        "model = AutoModelForCausalLM.from_pretrained(config.base_model_name_or_path, quantization_config=bnb_config, device_map={\"\":0})\n",
        "model = PeftModel.from_pretrained(model, peft_model_id)\n",
        "tokenizer = AutoTokenizer.from_pretrained(config.base_model_name_or_path)\n",
        "\n",
        "model.eval()"
      ],
      "metadata": {
        "id": "gYf0UQ3fetzP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "next token generation"
      ],
      "metadata": {
        "id": "ViGa1Tvyi8fI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = '인간처럼 생각하고, 행동하는 \\'지능\\'을 통해 인류가 이제까지 풀지 못했던'\n",
        "with torch.no_grad():\n",
        "  tokens = tokenizer.encode(prompt, return_tensors='pt').to(device='cuda', non_blocking=True)\n",
        "  gen_tokens = model.generate(tokens, do_sample=True, temperature=0.8, max_length=64)\n",
        "  generated = tokenizer.batch_decode(gen_tokens)[0]\n",
        "\n",
        "print(generated)"
      ],
      "metadata": {
        "id": "sHacSUa3fgz0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " QA"
      ],
      "metadata": {
        "id": "zO4qeMpZi54_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def gen(x):\n",
        "    q = f\"### 질문: {x}\\n\\n### 답변:\"\n",
        "    # print(q)\n",
        "    gened = model.generate(\n",
        "        **tokenizer(\n",
        "            q,\n",
        "            return_tensors='pt',\n",
        "            return_token_type_ids=False\n",
        "        ).to('cuda'),\n",
        "        max_new_tokens=50,\n",
        "        early_stopping=True,\n",
        "        do_sample=True,\n",
        "        eos_token_id=2,\n",
        "    )\n",
        "    print(tokenizer.decode(gened[0]))"
      ],
      "metadata": {
        "id": "0JBJKYJcfLuX"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gen('건강하게 살기 위한 세 가지 방법은?')"
      ],
      "metadata": {
        "id": "55GdkP9afi6U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "top_k와 같은 parameter를 조절하면서 실험해보세요!"
      ],
      "metadata": {
        "id": "VjacO_6njJWR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = '인간처럼 생각하고, 행동하는 \\'지능\\'을 통해 인류가 이제까지 풀지 못했던'\n",
        "with torch.no_grad():\n",
        "  tokens = tokenizer.encode(prompt, return_tensors='pt').to(device='cuda', non_blocking=True)\n",
        "  gen_tokens = model.generate(tokens, do_sample=True, temperature=0.8, max_length=64, top_k=50)\n",
        "  generated = tokenizer.batch_decode(gen_tokens)[0]\n",
        "\n",
        "print(generated)"
      ],
      "metadata": {
        "id": "8JaD2vpPfkuU"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}