{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[],"authorship_tag":"ABX9TyPmRp689CknMq3p3GrNe0mR"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"l9umFW4qIalb"},"outputs":[],"source":["import numpy as np"]},{"cell_type":"markdown","source":["## CBOW model 구현\n","\n","simpleCBOW 클래스 만들어 준다. 그전 activation function, loss function, optimizer를 정의"],"metadata":{"id":"6sHXcx_pIf8g"}},{"cell_type":"code","source":["# coding: utf-8\n","def softmax(x):\n","    if x.ndim == 2:\n","        x = x - x.max(axis=1, keepdims=True)\n","        x = np.exp(x)\n","        x /= x.sum(axis=1, keepdims=True)\n","    elif x.ndim == 1:\n","        x = x - np.max(x)\n","        x = np.exp(x) / np.sum(np.exp(x))\n","\n","    return x\n","\n","\n","def cross_entropy_error(y, t):\n","    if y.ndim == 1:\n","        t = t.reshape(1, t.size)\n","        y = y.reshape(1, y.size)\n","\n","    # 정답 데이터가 원핫 벡터일 경우 정답 레이블 인덱스로 변환\n","    if t.size == y.size:\n","        t = t.argmax(axis=1)\n","\n","    batch_size = y.shape[0]\n","\n","    return -np.sum(np.log(y[np.arange(batch_size), t] + 1e-7)) / batch_size\n"],"metadata":{"id":"OVudqrS_JHaP"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["activation function, loss function 정의"],"metadata":{"id":"weMqQfyMJcF9"}},{"cell_type":"code","source":["# coding: utf-8\n","\n","class MatMul:\n","    def __init__(self, W):\n","        self.params = [W]\n","        self.grads = [np.zeros_like(W)]\n","        self.x = None\n","\n","    def forward(self, x):\n","        W, = self.params\n","        out = np.dot(x, W)\n","        self.x = x\n","        return out\n","\n","    def backward(self, dout):\n","        W, = self.params\n","        dx = np.dot(dout, W.T)\n","        dW = np.dot(self.x.T, dout)\n","        self.grads[0][...] = dW\n","        return dx\n","\n","\n","class Affine:\n","    def __init__(self, W, b):\n","        self.params = [W, b]\n","        self.grads = [np.zeros_like(W), np.zeros_like(b)]\n","        self.x = None\n","\n","    def forward(self, x):\n","        W, b = self.params\n","        out = np.dot(x, W) + b\n","        self.x = x\n","        return out\n","\n","    def backward(self, dout):\n","        W, b = self.params\n","        dx = np.dot(dout, W.T)\n","        dW = np.dot(self.x.T, dout)\n","        db = np.sum(dout, axis=0)\n","\n","        self.grads[0][...] = dW\n","        self.grads[1][...] = db\n","        return dx\n","\n","\n","class Softmax:\n","    def __init__(self):\n","        self.params, self.grads = [], []\n","        self.out = None\n","\n","    def forward(self, x):\n","        self.out = softmax(x)\n","        return self.out\n","\n","    def backward(self, dout):\n","        dx = self.out * dout\n","        sumdx = np.sum(dx, axis=1, keepdims=True)\n","        dx -= self.out * sumdx\n","        return dx\n","\n","\n","class SoftmaxWithLoss:\n","    def __init__(self):\n","        self.params, self.grads = [], []\n","        self.y = None  # softmax의 출력\n","        self.t = None  # 정답 레이블\n","\n","    def forward(self, x, t):\n","        self.t = t\n","        self.y = softmax(x)\n","\n","        # 정답 레이블이 원핫 벡터일 경우 정답의 인덱스로 변환\n","        if self.t.size == self.y.size:\n","            self.t = self.t.argmax(axis=1)\n","\n","        loss = cross_entropy_error(self.y, self.t)\n","        return loss\n","\n","    def backward(self, dout=1):\n","        batch_size = self.t.shape[0]\n","\n","        dx = self.y.copy()\n","        dx[np.arange(batch_size), self.t] -= 1\n","        dx *= dout\n","        dx = dx / batch_size\n","\n","        return dx\n","\n","class SigmoidWithLoss:\n","    def __init__(self):\n","        self.params, self.grads = [], []\n","        self.loss = None\n","        self.y = None  # sigmoid의 출력\n","        self.t = None  # 정답 데이터\n","\n","    def forward(self, x, t):\n","        self.t = t\n","        self.y = 1 / (1 + np.exp(-x))\n","\n","        self.loss = cross_entropy_error(np.c_[1 - self.y, self.y], self.t)\n","\n","        return self.loss\n","\n","    def backward(self, dout=1):\n","        batch_size = self.t.shape[0]\n","\n","        dx = (self.y - self.t) * dout / batch_size\n","        return dx"],"metadata":{"id":"PIu-naM2Jg69"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["그외 학습에 필요한 class 정의"],"metadata":{"id":"eF1lAF8eJfKt"}},{"cell_type":"code","source":["# coding: utf-8\n","import numpy\n","import time\n","import matplotlib.pyplot as plt\n","\n","\n","class Trainer:\n","    def __init__(self, model, optimizer):\n","        self.model = model\n","        self.optimizer = optimizer\n","        self.loss_list = []\n","        self.eval_interval = None\n","        self.current_epoch = 0\n","\n","    def fit(self, x, t, max_epoch=10, batch_size=32, max_grad=None, eval_interval=20):\n","        data_size = len(x)\n","        max_iters = data_size // batch_size\n","        self.eval_interval = eval_interval\n","        model, optimizer = self.model, self.optimizer\n","        total_loss = 0\n","        loss_count = 0\n","\n","        start_time = time.time()\n","        for epoch in range(max_epoch):\n","            # 뒤섞기\n","            idx = numpy.random.permutation(numpy.arange(data_size))\n","            x = x[idx]\n","            t = t[idx]\n","\n","            for iters in range(max_iters):\n","                batch_x = x[iters*batch_size:(iters+1)*batch_size]\n","                batch_t = t[iters*batch_size:(iters+1)*batch_size]\n","\n","                # 기울기 구해 매개변수 갱신\n","                loss = model.forward(batch_x, batch_t)\n","                model.backward()\n","                params, grads = remove_duplicate(model.params, model.grads)  # 공유된 가중치를 하나로 모음\n","                if max_grad is not None:\n","                    clip_grads(grads, max_grad)\n","                optimizer.update(params, grads)\n","                total_loss += loss\n","                loss_count += 1\n","\n","                # 평가\n","                if (eval_interval is not None) and (iters % eval_interval) == 0:\n","                    avg_loss = total_loss / loss_count\n","                    elapsed_time = time.time() - start_time\n","                    print('| 에폭 %d |  반복 %d / %d | 시간 %d[s] | 손실 %.2f'\n","                          % (self.current_epoch + 1, iters + 1, max_iters, elapsed_time, avg_loss))\n","                    self.loss_list.append(float(avg_loss))\n","                    total_loss, loss_count = 0, 0\n","\n","            self.current_epoch += 1\n","\n","    def plot(self, ylim=None):\n","        x = numpy.arange(len(self.loss_list))\n","        if ylim is not None:\n","            plt.ylim(*ylim)\n","        plt.plot(x, self.loss_list, label='train')\n","        plt.xlabel('반복 (x' + str(self.eval_interval) + ')')\n","        plt.ylabel('손실')\n","        plt.show()"],"metadata":{"id":"UONj6-s1Kdag"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# coding: utf-8\n","class Adam:\n","    '''\n","    Adam (http://arxiv.org/abs/1412.6980v8)\n","    '''\n","    def __init__(self, lr=0.001, beta1=0.9, beta2=0.999):\n","        self.lr = lr\n","        self.beta1 = beta1\n","        self.beta2 = beta2\n","        self.iter = 0\n","        self.m = None\n","        self.v = None\n","\n","    def update(self, params, grads):\n","        if self.m is None:\n","            self.m, self.v = [], []\n","            for param in params:\n","                self.m.append(np.zeros_like(param))\n","                self.v.append(np.zeros_like(param))\n","\n","        self.iter += 1\n","        lr_t = self.lr * np.sqrt(1.0 - self.beta2**self.iter) / (1.0 - self.beta1**self.iter)\n","\n","        for i in range(len(params)):\n","            self.m[i] += (1 - self.beta1) * (grads[i] - self.m[i])\n","            self.v[i] += (1 - self.beta2) * (grads[i]**2 - self.v[i])\n","\n","            params[i] -= lr_t * self.m[i] / (np.sqrt(self.v[i]) + 1e-7)\n"],"metadata":{"id":"PunPSDiaKevs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def preprocess(text):\n","    text = text.lower()\n","    text = text.replace('.', ' .')\n","    words = text.split(' ')\n","\n","    word_to_id = {}\n","    id_to_word = {}\n","    for word in words:\n","        if word not in word_to_id:\n","            new_id = len(word_to_id)\n","            word_to_id[word] = new_id\n","            id_to_word[new_id] = word\n","\n","    corpus = np.array([word_to_id[w] for w in words])\n","\n","    return corpus, word_to_id, id_to_word\n","\n","def convert_one_hot(corpus, vocab_size):\n","    '''원핫 표현으로 변환\n","\n","    :param corpus: 단어 ID 목록(1차원 또는 2차원 넘파이 배열)\n","    :param vocab_size: 어휘 수\n","    :return: 원핫 표현(2차원 또는 3차원 넘파이 배열)\n","    '''\n","    N = corpus.shape[0]\n","\n","    if corpus.ndim == 1:\n","        one_hot = np.zeros((N, vocab_size), dtype=np.int32)\n","        for idx, word_id in enumerate(corpus):\n","            one_hot[idx, word_id] = 1\n","\n","    elif corpus.ndim == 2:\n","        C = corpus.shape[1]\n","        one_hot = np.zeros((N, C, vocab_size), dtype=np.int32)\n","        for idx_0, word_ids in enumerate(corpus):\n","            for idx_1, word_id in enumerate(word_ids):\n","                one_hot[idx_0, idx_1, word_id] = 1\n","\n","    return one_hot\n","\n","def create_contexts_target(corpus, window_size=1):\n","    '''맥락과 타깃 생성\n","\n","    :param corpus: 말뭉치(단어 ID 목록)\n","    :param window_size: 윈도우 크기(윈도우 크기가 1이면 타깃 단어 좌우 한 단어씩이 맥락에 포함)\n","    :return:\n","    '''\n","    target = corpus[window_size:-window_size]\n","    contexts = []\n","\n","    for idx in range(window_size, len(corpus)-window_size):\n","        cs = []\n","        for t in range(-window_size, window_size + 1):\n","            if t == 0:\n","                continue\n","            cs.append(corpus[idx + t])\n","        contexts.append(cs)\n","\n","    return np.array(contexts), np.array(target)\n","\n","def remove_duplicate(params, grads):\n","    '''\n","    매개변수 배열 중 중복되는 가중치를 하나로 모아\n","    그 가중치에 대응하는 기울기를 더한다.\n","    '''\n","    params, grads = params[:], grads[:]  # copy list\n","\n","    while True:\n","        find_flg = False\n","        L = len(params)\n","\n","        for i in range(0, L - 1):\n","            for j in range(i + 1, L):\n","                # 가중치 공유 시\n","                if params[i] is params[j]:\n","                    grads[i] += grads[j]  # 경사를 더함\n","                    find_flg = True\n","                    params.pop(j)\n","                    grads.pop(j)\n","                # 가중치를 전치행렬로 공유하는 경우(weight tying)\n","                elif params[i].ndim == 2 and params[j].ndim == 2 and \\\n","                     params[i].T.shape == params[j].shape and np.all(params[i].T == params[j]):\n","                    grads[i] += grads[j].T\n","                    find_flg = True\n","                    params.pop(j)\n","                    grads.pop(j)\n","\n","                if find_flg: break\n","            if find_flg: break\n","\n","        if not find_flg: break\n","\n","    return params, grads"],"metadata":{"id":"xYG8UfQBKhL5"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## SimpleCBOW\n","\n","직접 레이어를 쌓아 구현\n","\n","window size = 1"],"metadata":{"id":"3JqDsFuZKm-z"}},{"cell_type":"code","source":["class SimpleCBOW:\n","    def __init__(self, vocab_size, hidden_size):\n","        V, H = vocab_size, hidden_size\n","\n","        # 가중치 초기화\n","        W_in = 0.01 * np.random.randn(V, H).astype('f')\n","        W_out = 0.01 * np.random.randn(H, V).astype('f')\n","\n","        # 계층 생성\n","        self.in_layer0 = MatMul(W_in)\n","        self.in_layer1 = MatMul(W_in)\n","        self.out_layer = MatMul(W_out)\n","\n","        # lost function 정의\n","        self.loss_layer = SoftmaxWithLoss()\n","\n","        # 모든 가중치와 기울기를 리스트에 모으기\n","        layers = [self.in_layer0, self.in_layer1, self.out_layer]\n","        self.params, self.grads = [], []\n","        for layer in layers:\n","            self.params += layer.params\n","            self.grads += layer.grads\n","\n","        # 인스턴스 변수에 단어의 분산 표현을 저장\n","        self.word_vecs = W_in\n","\n","    def forward(self, contexts, target):\n","        h0 = self.in_layer0.forward(contexts[:, 0])\n","        h1 = self.in_layer1.forward(contexts[:, 1])\n","        h = (h0 + h1) * 0.5\n","\n","        score = self.out_layer.forward(h)\n","        loss = self.loss_layer.forward(score, target)\n","        return loss\n","\n","    def backward(self, dout=1):\n","        ds = self.loss_layer.backward(dout)\n","        da = self.out_layer.backward(ds)\n","        da *= 0.5\n","        self.in_layer1.backward(da)\n","        self.in_layer0.backward(da)\n","        return None"],"metadata":{"id":"s65hjy3ANVwx"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Train SimpleCBOW"],"metadata":{"id":"ALNuMHYiOm8-"}},{"cell_type":"code","source":["# coding: utf-8\n","\n","window_size = 1\n","hidden_size = 5\n","batch_size = 3\n","max_epoch = 1000\n","\n","text = 'Yesterday all my troubles seemed so far away'\n","corpus, word_to_id, id_to_word = preprocess(text)\n","\n","vocab_size = len(word_to_id)\n","contexts, target = create_contexts_target(corpus, window_size)\n","target = convert_one_hot(target, vocab_size)\n","contexts = convert_one_hot(contexts, vocab_size)\n","\n","model = SimpleCBOW(vocab_size, hidden_size)\n","optimizer = Adam()\n","trainer = Trainer(model, optimizer)\n","\n","trainer.fit(contexts, target, max_epoch, batch_size)\n","trainer.plot()\n","\n","word_vecs = model.word_vecs\n","for word_id, word in id_to_word.items():\n","    print(word, word_vecs[word_id])"],"metadata":{"id":"SSTwMcHBOsEz"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["학습 후 모델에 저장된 `가중치 행렬의 행`들을 보면, 이것이 각 단어의 `임베딩 벡터 `"],"metadata":{"id":"3B_NlowaO--X"}},{"cell_type":"code","source":["word_vecs = model.word_vecs\n","for word_id, word in id_to_word.items():\n","  print(word, word_vecs[word_id])"],"metadata":{"id":"mO_Fdql5PM_a"},"execution_count":null,"outputs":[]}]}