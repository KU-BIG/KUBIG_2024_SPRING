{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[],"authorship_tag":"ABX9TyMOFscEbHRG7S4Z8YAv1fqJ"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["## 1. IMDB\n","\n","영화 정보 제공 웹사이트 -> 리뷰들을 감정분석용 데이터 셋으로 많이 씀\n","kaggle 의 감성분석 튜토리얼 진행\n","\n","*  영어 데이터 사용\n","*  train set & test set 모두 25,000 개의 긍정 부정 리뷰로 구성 (긍정-label 1 , 부정 - label 0)\n"],"metadata":{"id":"Cq17JekGm7hP"}},{"cell_type":"markdown","source":["## 2. Load Files"],"metadata":{"id":"6PHUF7-uqk2Y"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"7lCb6zyxsiu0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import pandas as pd\n","from bs4 import BeautifulSoup\n","from nltk.stem import WordNetLemmatizer\n","import re\n","import nltk\n","from nltk.corpus import stopwords\n","nltk.download('stopwords')\n","from nltk.stem import PorterStemmer\n","nltk.download('wordnet')\n","nltk.download('omw-1.4')\n","from nltk.stem import WordNetLemmatizer\n","import matplotlib as plt"],"metadata":{"id":"2ahgQ61Msk1R"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# tsv 파일은 탭으로 구분된 파일\n","# header=0 은 첫 행이 열 이름\n","# delimiter=\"\\t\" 파일이 탭으로 구분됨\n","# quoting = 3, 따옴표 인식 x, 일반 문자 처리\n","\n","train = pd.read_csv('/content/drive/MyDrive/24-winter KUBIG NLP/WEEK 2/WEEK 2 복습과제 IMDB 텍스트 감성분석/labeledTrainData.tsv', header=0, delimiter=\"\\t\", quoting=3)\n","test = pd.read_csv('/content/drive/MyDrive/24-winter KUBIG NLP/WEEK 2/WEEK 2 복습과제 IMDB 텍스트 감성분석/testData.tsv', header=0, delimiter='\\t', quoting=3)\n","submit = pd.read_csv('/content/drive/MyDrive/24-winter KUBIG NLP/WEEK 2/WEEK 2 복습과제 IMDB 텍스트 감성분석/sampleSubmission.csv')"],"metadata":{"id":"u_mjHxN9s2Px"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 3. 데이터 탐색\n"],"metadata":{"id":"AJSBFi5MwPoz"}},{"cell_type":"code","source":["train.info()"],"metadata":{"id":"deStPSn0wSy1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train.head()"],"metadata":{"id":"kAEFYn5swT_Z"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["test.info()"],"metadata":{"id":"o-y_Qm4dwVDb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["test.head()"],"metadata":{"id":"3YtzALc4wXdo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train['sentiment'].value_counts()"],"metadata":{"id":"J0iWB3gZwXl8"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 4. Text Preprocessing"],"metadata":{"id":"lAJIPca20c7H"}},{"cell_type":"markdown","source":["### 4-1. html 태그 제거\n","\n","리뷰에서는 `<br>` 이라는 html 줄바꿈 태그가 보임!\n","\n","`BeautifulSoup` 를 이용해 태그 지우자"],"metadata":{"id":"-Bd4PBiR0gxM"}},{"cell_type":"code","source":["train[\"review\"][0]"],"metadata":{"id":"ajf_9tRD0lc_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from bs4 import BeautifulSoup"],"metadata":{"id":"Qds2RLHq5V04"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["example1 = BeautifulSoup(train[\"review\"][0])\n","example1 = example1.get_text()\n","print(\"before deleting:\" , train[\"review\"][0])\n","print(\"after deleting:\" , example1)"],"metadata":{"id":"HS5027jz5YF2"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 4-2. 정규표현식(re)로 알파벳만 남기기\n","\n","html 태그는 제거했으나, 각종 특수문자가 여전히 남아있음\n","re를 통해 알파벳이 아닌것을 공백으로 대체"],"metadata":{"id":"q_GvcOHc5jnS"}},{"cell_type":"code","source":["import re\n","letters_only = re.sub(\"[^a-zA-Z]\", \" \", example1)"],"metadata":{"id":"ZJ57gDKEkkOR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train[\"review\"][0]"],"metadata":{"id":"tSbhg6N4kNP3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["letters_only"],"metadata":{"id":"ktqRlABfkr0x"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 4-3. 토큰화 (Tokenizing)\n","\n","*토큰은 의미를 갖는 최소분석단위 (한국어는 형태소 , 영어는 띄어쓰기 단위)\n","*토큰화는 corpus 덩어리를 작은 토큰 단위로 쪼개주는 자겅ㅂ\n","\n","split() 함수를 써주면, 띄어쓰기로 토큰화\n","lower_case 함수로 소문자 변환하는 이유는 대소문자가 다른 단어로 구분되기 때문에 복잡성 저하"],"metadata":{"id":"uuSxhUJUksI8"}},{"cell_type":"code","source":["lower_case = letters_only.lower()\n","token_words = lower_case.split()\n","print(\"토큰화 이후 생성된 토큰(단어) 개수\", len(token_words))"],"metadata":{"id":"t1JkbWrTuHpj"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["아래는 nltk 와 keras 에서 토큰화를 수행해주는 도구들"],"metadata":{"id":"bmTb5S2Q2FEC"}},{"cell_type":"code","source":["# from nltk.tokenize import word_tokenize\n","# word_tokenize(lower_case)\n","# from nltk.tokenize import WordPunctTokenizer\n","# WordPunctTokenizer().tokenize(lower_case)\n","# from tensorflow.keras.preprocessing.text import text_to_word_sequence\n","# text_to_word_sequence(lower_case)"],"metadata":{"id":"tVwvNyoA2CmM"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 4-4.불용어 제거\n","\n","i we our 같이 크게 중요하지 않은 단어들 제거\n","\n","nltk 내장 불용어는 179개"],"metadata":{"id":"WGH6CZKC2Mad"}},{"cell_type":"code","source":["import nltk\n","nltk.download('stopwords')"],"metadata":{"id":"tlantcnm2Y5d"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from nltk.corpus import stopwords\n","stopwords_list = stopwords.words('english')\n","print(\"nltk에 내장된 불용어 개수: \", len(stopwords_list))\n","print(\"불용어 예시: \", stopwords_list[:10])\n","non_stopwords = [w for w in token_words if not w in stopwords_list]\n","print(\"예시 review에서 불용어 제거하고 남은 토큰 개수: \",len(non_stopwords))"],"metadata":{"id":"Yh9-_zxG2dZh"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 4-5. 어간 추출\n","\n","nltk 에선 어간을 추출해주는 도구를 제공합니다.\n","100% 정확하지는 않음\n","\n","* ALIZE → AL\n","* ANCE → 제거\n","* ICAL → IC\n","\n","* formalize → formal\n","* allowance → allow\n","* electricical → electric"],"metadata":{"id":"7iCpN9-T3jf9"}},{"cell_type":"code","source":["from nltk.stem import PorterStemmer\n","porter_stemmer = PorterStemmer()\n","stemmed_words = [porter_stemmer.stem(w) for w in non_stopwords]"],"metadata":{"id":"sJ5C4x6T4YTm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["stemmed_words[1:10]"],"metadata":{"id":"fLwJ7-5F5QMs"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 4-6. 표제어 추출\n","\n","표제어 추출은 어간 추출과 다르게 어떤 품사로 쓰였는지를 고려함\n","\n","stemming(어간 추출)을 하는 것보다 lemmatization(표제어 추출)을 하는 것이 더 효과적"],"metadata":{"id":"h23d4bng5UBD"}},{"cell_type":"code","source":["nltk.download('wordnet')\n","nltk.download('omw-1.4')\n","from nltk.stem import WordNetLemmatizer\n","wordnet_lemmatizer = WordNetLemmatizer()"],"metadata":{"id":"-7WbtFKK50Ot"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["lemmatized_words = [wordnet_lemmatizer.lemmatize(w) for w in non_stopwords]\n","lemmatized_words = [wordnet_lemmatizer.lemmatize(w,\"v\") for w in lemmatized_words]"],"metadata":{"id":"Gkk20DQx59vI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(non_stopwords[21:40]) # none\n","print(stemmed_words[21:40]) # stemming\n","print(lemmatized_words[21:40]) # lemmatization"],"metadata":{"id":"fiCOCOVB6CjI"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 하나의 함수로 표현\n","\n","앞선 6가지 전처리 과정을 하나의 함수로 통합"],"metadata":{"id":"QfiDk3SA6IrZ"}},{"cell_type":"code","source":["stopwords_list= set(stopwords.words('english'))\n","wordnet_lemmatizer = WordNetLemmatizer()\n","\n","def review_to_words(raw_review):\n","    except_tag = BeautifulSoup(raw_review).get_text() # html 태그 제거\n","    letters_only = re.sub(\"[^a-zA-Z]\", \" \", except_tag) # 정규표현식으로 알파벳 남기기\n","    token_words = letters_only.lower().split() # 소문자로 통합 후 토큰화\n","    non_stopwords = [w for w in token_words if not w in stopwords_list] # 불용어 제거\n","    lemmatized_words = [wordnet_lemmatizer.lemmatize(w) for w in non_stopwords] # 표제어 추출\n","    lemmatized_words = [wordnet_lemmatizer.lemmatize(w,\"v\") for w in lemmatized_words]\n","    words = \" \".join(lemmatized_words)\n","    return words"],"metadata":{"id":"Lo_q46Xq9B8X"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 5. BoW (Bag of Words) 형태로 변환\n","\n","단어의 출현 빈도에 집중해 텍스트를 수치화하는 표현 방식\n","\n","예를 들어 아래처럼 두개의 노래구절이 있는 경우"],"metadata":{"id":"rsRmrvCh-XPC"}},{"cell_type":"code","source":["lyric1 = \"but I don't want to stay in the middle\" # ditto\n","lyric2 = \"like you a little don't want no riddle\""],"metadata":{"id":"ibxyfO0p_5nY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["vocab = [\"but\", \"I\", \"don't\", \"want\", \"to\", \"stay\", \"in\", \"the\", \"middle\", \"like\", \"you\", \"a\", \"little\", \"no\", \"riddle\"]"],"metadata":{"id":"YTFQeCU5AHMc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 각 단어에 인덱스 부여\n","# 인덱스 위치에 단어의 빈도 표현\n","\n","lyric1 = [1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0]\n","lyric2 = [0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1]"],"metadata":{"id":"qpUKdEN7AJmT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["num_reviews = train['review'].size\n","\n","clean_train_reviews = []\n","for i in range(0, num_reviews):\n","     if (i + 1) % 5000 == 0 :  #실행이 잘되는지 확인하기 위해 5000개 실행될때마다 확인문구\n","         print('Review {} of {}'.format(i+1, num_reviews))\n","     clean_train_reviews.append(review_to_words(train['review'][i]))"],"metadata":{"id":"GHwtaex7BEVr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.feature_extraction.text import CountVectorizer\n","\n","vectorizer = CountVectorizer(analyzer = 'word', # 학습 단위\n","                             tokenizer = None,\n","                             preprocessor = None,\n","                             stop_words = None,\n","                             min_df = 2, # 토큰이 나타날 최소 문서 개수\n","                             ngram_range=(1, 2), # 단어의 묶음 개수\n","                             max_features = 4000) # 토큰의 최대 개수, 즉 컬럼의 최대 개수"],"metadata":{"id":"rhVdJ5EZBF5O"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 참고 TF-IDF\n","\n","단어 빈도, 문서빈도 역수 사용\n","\n","* 단어빈도 : 특정 단어가 한 문서 내에서 출현한 빈도\n","* 문서빈도 : 특정 단어가 출현한 전체 문서 개수"],"metadata":{"id":"kPV0CmscF6Td"}},{"cell_type":"code","source":["from sklearn.feature_extraction.text import TfidfVectorizer\n","\n","vectorizer = TfidfVectorizer(min_df = 4, # 토큰이 나타날 최소 문서 개수\n","                             analyzer = 'word', # 학습 단위\n","                             ngram_range = (1, 2), # 단어의 묶음 개수\n","                             max_features = 1000) # 토큰의 최대 개수, 즉 컬럼의 최대 개수"],"metadata":{"id":"a82JSxFDFwO0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_data_features = vectorizer.fit_transform(clean_train_reviews)\n","train_data_features.shape"],"metadata":{"id":"W0e84J8uFyjj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["vocab = vectorizer.get_feature_names()\n","vocab[:10]"],"metadata":{"id":"ltXr2iXLFz7e"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 7. Modeling\n","\n","baseline 코드로 제시된 트리 계열 알고리즘으로 학습 시키기"],"metadata":{"id":"El1vIunzF1DF"}},{"cell_type":"code","source":["from sklearn.ensemble import RandomForestClassifier\n","from lightgbm import LGBMClassifier\n","from xgboost import XGBClassifier\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import roc_auc_score\n","from sklearn.ensemble import StackingClassifier\n","from sklearn.linear_model import LogisticRegression\n","\n","x = train_data_features\n","y = train['sentiment']\n","\n","x_train, x_val, y_train, y_val = train_test_split(x, y, test_size=0.3, random_state=42)\n","\n","# 트리 알고리즘 3개를 사용\n","rf = RandomForestClassifier(n_estimators = 200,\n","                            n_jobs = -1,\n","                            random_state=42,\n","                            max_depth=20)\n","\n","xgb = XGBClassifier(n_estimators=200,\n","                    max_depth=10,\n","                    learning_rate=0.05,\n","                    objective='binary:logistic')\n","\n","lgbm = LGBMClassifier(n_estimators=200,\n","                    max_depth=10,\n","                    metric='binary_logloss')"],"metadata":{"id":"8qddf5EwHOo3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["xgb.fit(x_train, y_train)\n","y_pred_xgb = xgb.predict_proba(x_val)"],"metadata":{"id":"cxqZqUtrHSTo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["lgbm.fit(x_train, y_train)\n","y_pred_lgbm = lgbm.predict_proba(x_val)"],"metadata":{"id":"YbeyYBG9HQUA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["rf.fit(x_train, y_train)\n","y_pred_rf = rf.predict_proba(x_val)"],"metadata":{"id":"T6cCEbzAHV1i"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["학습된 모델들의 roc_auc_score 구하기"],"metadata":{"id":"ARyNW3L1HX5M"}},{"cell_type":"code","source":["print('Random Forest AUC Score :', roc_auc_score(y_val, y_pred_rf[:,1]))\n","print('XGBoost AUC Score :', roc_auc_score(y_val, y_pred_xgb[:,1]))\n","print('LGBM AUC Score :', roc_auc_score(y_val, y_pred_lgbm[:,1]))"],"metadata":{"id":"ddsROWzLHW1G"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 8. test set 추론  "],"metadata":{"id":"hRmh_Z1DHcOK"}},{"cell_type":"code","source":["num_reviews = test['review'].size\n","\n","clean_test_reviews = []\n","for i in range(0, num_reviews):\n","     if (i + 1) % 5000 == 0 :  #실행이 잘 되는지 확인하기 위해 5000개 실행될때마다 확인문구\n","         print('Review {} of {}'.format(i+1, num_reviews))\n","     clean_test_reviews.append(review_to_words(test['review'][i]))"],"metadata":{"id":"WsGzLpSBHfbu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["test_data_features = vectorizer.fit_transform(clean_test_reviews)\n","test_data_features = test_data_features.toarray()"],"metadata":{"id":"WM1HwFKjHgc4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 3개 알고리즘 중 원하는 것으로 predict\n","result = lgbm.predict(test_data_features)\n","submit['sentiment'] = result"],"metadata":{"id":"S8STbBaFHhQq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["submit.head(10)"],"metadata":{"id":"819E5YUNHkgM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"PkCkPSGFHktu"},"execution_count":null,"outputs":[]}]}