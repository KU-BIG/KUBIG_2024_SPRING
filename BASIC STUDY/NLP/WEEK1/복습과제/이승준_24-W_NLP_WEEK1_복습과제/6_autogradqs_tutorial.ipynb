{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[],"authorship_tag":"ABX9TyMQJ6IOJ9GFM/GpIr0lzkQM"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"ZUM5Rw0RDoQa"},"outputs":[],"source":["%matplotlib inline"]},{"cell_type":"markdown","source":["## torch.autograd 를 사용한 자동미분\n","\n","신경망 학습시에는 **역전파** 알고리즘 주로 사용\n","-> 매개변수는 손실함수의 변화도 (gradient)에 따라 조정됨\n","-> 변화도 계산을 위해 pytorch 에서는 torch.autograd라는 자동 미분 엔진 사용\n","-> 모든 계산 그래프에 대해 변화도 자동계산 지원"],"metadata":{"id":"v49kHieMDums"}},{"cell_type":"code","source":["import torch\n","\n","x = torch.ones(5) # input tensor\n","y = torch.zeros(3) # expected output\n","w = torch.randn(5, 3, requires_grad=True) # 기울기\n","b = torch.randn(3, requires_grad=True) # 상수\n","z = torch.matmul(x, w)+b # 연산\n","loss = torch.nn.functional.binary_cross_entropy_with_logits(z, y) # 손실"],"metadata":{"id":"h9mO0fRpRvAe"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Tensor, Function 과 연산 그래프\n","\n","해당 코드는 ** 연산 그래프 ** 정의\n","\n","w, b 는 최적화 해야하는 매개변수, 손실함수 변화도 계산 할 수 있어야 함. 해당 텐서에 requires_grad 속성 설정\n","\n"],"metadata":{"id":"-y49c9PLSUvw"}},{"cell_type":"code","source":["print (\"Gradient function for z = \", z.grad_fn)\n","print (\"Gradient function for loss = \", loss.grad_fn)"],"metadata":{"id":"8WXbOPzCTaVB"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 변화도 계산하기\n","\n","매개변수 내에서 가중치 최적화 - > 매개변수 손실함수의 도함수 계산\n","loss.backward() 호출 후 값 가져오기"],"metadata":{"id":"9TfRNOqnTeGo"}},{"cell_type":"code","source":["loss.backward() # 호출 한번씩 밖에 안됨.\n","print(w.grad)\n","print(b.grad)"],"metadata":{"id":"pfTBUK30UQ8n"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 변화도 추적 멈추기\n","\n","순전파 연산만 필요한 경우 -> torch.no_grad 연산으로 둘러싸 연산 추적 멈추기"],"metadata":{"id":"FDTYtRMzUmka"}},{"cell_type":"code","source":["z = torch .matmul(x, w)+b\n","print(z.requires_grad)\n","\n","with torch.no_grad():\n","    z = torch.matmul(x, w)+b\n","print(z.requires_grad)"],"metadata":{"id":"q7bmqh3mUUmx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# detach 메소드 적용해 같은 결과 얻기\n","\n","z = torch.matmul(x, w)+b\n","z_det = z.detach()\n","print(z_det.requires_grad)"],"metadata":{"id":"h62JlCMXVMra"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 연산 그래프에 대한 추가 정보\n","\n","DAG = directed acyclic graph\n","\n","autograd는 데이터의 실행된 모든 연산들의 기록을 function,\n","\n","순전파단계에서 autograd는 두가지 작업 수행\n","\n","*   요청된 연산 수행해 결과 텐서 내기\n","*   DAG에 연산의 변화도 기능 유지\n","\n","역전파 단계는 DAG의 root 에서 .backward() 호출될떄 시작.  \n","역전파 단계에서 autograd는\n","\n","*   각 grad_fn 에서 변화도 계산\n","*   각 텐서의  .grad 속성에 계산 결과를 쌓고\n","*   연쇄법칙으로 모든 텐서에 propagate\n","\n"],"metadata":{"id":"xHTNjd-OVdrw"}},{"cell_type":"markdown","source":["## Optional Reading  텐서 변화도, 야코비안 곱\n","\n","출력함수가 임의의 텐서라면 pytorch는 실제 변화도 아닌 야코비안 곱을 계산!\n"],"metadata":{"id":"XtW97mCNbTCS"}},{"cell_type":"code","source":["inp = torch.eye(4, 5, requires_grad=True)\n","out = (inp+1).pow(2)\n","out.backward(torch.ones_like(inp), retain_graph=True)\n","print(\"First call\\n\", inp.grad)\n","out.backward(torch.ones_like(inp), retain_graph=True)\n","print(\"\\nSecond call\\n\", inp.grad)\n","inp.grad.zero_()\n","out.backward(torch.ones_like(inp), retain_graph=True)\n","print(\"\\nCall after zeroing gradients\\n\", inp.grad)"],"metadata":{"id":"-2V0h26yb4jf"},"execution_count":null,"outputs":[]}]}