{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[],"authorship_tag":"ABX9TyOBjskMRPaR49udR85+rjnH"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"r3HWUhbrcLBo"},"outputs":[],"source":["%matplot inline"]},{"cell_type":"markdown","source":["## 모델 매개변수 최적화\n","\n","데이터에 매개변수를 최적화하여 모델을 학습하고, 테스트 하는 과정\n","\n","모델 학습은 반복적인 과정을 거침\n","* 각 반복 단계에서 모델은 출력을 추측\n","* 추측과 정답사이의 loss를 계산\n","* 매개변수에 대한 오류의 도함수를 수집\n","* 경사하강법으로 파라미터를 **최적화**"],"metadata":{"id":"WvKwkdlfdLCe"}},{"cell_type":"markdown","source":["## 기본 코드"],"metadata":{"id":"Y6nfa3-Bd9Iy"}},{"cell_type":"code","source":["import torch\n","from torch import nn\n","from torch.utils.data import DataLoader\n","from torchvision import datasets\n","from torchvision.transforms import ToTensor\n","\n","training_data = datasets.FashionMNIST(\n","    root='data',\n","    train=True,\n","    download=True,\n","    transform=ToTensor()\n",")\n","\n","test_data = datasets.FashionMNIST(\n","    root='data',\n","    train=False,\n","    download=True,\n","    transform=ToTensor()\n",")\n","\n","train_dataloader = DataLoader(training_data, batch_size=64)\n","test_dataloader = DataLoader(test_data, batch_size=64)\n","\n","class NN(nn.Module):\n","    def __init__(self):\n","        super(NN, self).__init__()\n","        self.flatten = nn.Flatten()\n","        self.linear_relu_stack = nn.Sequential(\n","            nn.Linear(28*28, 512),\n","            nn.ReLU(),\n","            nn.Linear(512, 512),\n","            nn.ReLU(),\n","            nn.Linear(512, 10),\n","        )\n","\n","    def forward(self, x):\n","        x = self.flatten(x)\n","        logits = self.linear_relu_stack(x)\n","        return logits\n","\n","model = NN()\n"],"metadata":{"id":"R6tdM1EAd_UB"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 하이퍼파라미터\n","\n","모델 최적화 과정을 제어할 수 있는 조절 가능한 매개변수\n","서로 다른 하이퍼파라미터 값은 모델학습과 수렴율에 영향을 미칠 수 있다.\n","\n","학습시 다음의 하이퍼 파라미터 정의\n","* 에폭 수 (epoch) - 데이터셋을 반복하는 횟수\n","* 배치 크기 (batch size) - 매개변수가 갱신되기 전 신경망을 통해 전파된 데이터 샘플 수\n","* 학습률 (learning rate) - 각 배치/에폭에서 모델의 매개변수를 조절하는 비율. 값이 작을수록 학습속도가 느려지고, 값이 클 수록 학습 중 에측할 수 없는 동작이 발생 가능함"],"metadata":{"id":"DnDkweUVgKMK"}},{"cell_type":"code","source":["learinig_rate = 1e-3\n","batch_size = 64\n","epochs = 5"],"metadata":{"id":"ubwbxY_8iPxF"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 최적화 단계\n","\n","하이퍼 파라미터 설정 이후 최적화  단계를 통해 모델을 학습 후 최적화\n","\n","각 반복단계 = 에폭\n","\n","하나의 에폭은 두 부분으로 구성된다\n","\n","* 학습단계 -> 학습용 데이터셋을 반복하고 최적의 매개변수로 수렴\n","* 검증/테스트 단계 -> 모델 성능이 개선되고 있는지 보기 위해 데이터셋을 반복\n","\n","아래 설명하는 개념들은 학습단계에 포함"],"metadata":{"id":"D8xiMdx_iTDB"}},{"cell_type":"markdown","source":["## 손실함수\n","\n","일반적인 손실함수는 MSE, nll, Crossentropy loss 등이 있음"],"metadata":{"id":"k8XpyM0-sTgn"}},{"cell_type":"code","source":["# 손실함수 초기화\n","loss_fn = nn.CrossEntropyLoss()"],"metadata":{"id":"Qj7tTpb2sq7_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 옵티마이저\n","\n","최적화는 각 학습 단계에서 오류 줄이기 위해 파라미터 조절하는 과정.\n","\n","최적화 알고리즘을 (여기서는 확률적 경사하강범 SGD) 정의함\n","모든 최적화 절차는 optimizer 객체에 캡슐화(encapsulate 됨)\n","\n"],"metadata":{"id":"hSC6xMYiswta"}},{"cell_type":"code","source":["optimizer = torch.optim.SGD(model.parameters(), lr=learinig_rate)"],"metadata":{"id":"6wlbzm_xvkjv"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["학습 단계에서 최적화는 세 단계\n","* optimizer.zero.grad 를 호출해 매개변수 변화도 재설정, 기본적으로 변화도는 더해지기 떄문에 중복계산을 막기 위해 0으로 설정\n","* loss.backward() 호출해 예측 손실을 역전파 함. 각 매개변ㅅ수에 대한 손실의 변화도를 저장\n","* 변화도 계산 이후에 optimizer.step 호출해 역전파 단계에서 수집된 변화도로 매개변수 조정"],"metadata":{"id":"b7WIKJ1AwcLZ"}},{"cell_type":"markdown","source":["## 전체 구현\n","\n","최적화 코드 반복해 수행하는 train loop, 데스트 데이터로 모델의 성능 측정하는 test loop 정의"],"metadata":{"id":"T-awko1SxCi4"}},{"cell_type":"code","source":["def train_loop(dataloader, model, loss_fn, optimizer):\n","    size = len(dataloader.dataset)\n","    for batch, (X, y) in enumerate(dataloader):\n","        # 예측과 손실 계산\n","        pred = model(X)\n","        loss = loss_fn(pred, y)\n","\n","        # 역전파\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","\n","        if batch % 100 == 0:\n","            loss, current = loss.item(), batch * len(X)\n","            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n","\n","def test_loop(dataloader, model, loss_fn):\n","    size = len(dataloader.dataset)\n","    num_batches = len(dataloader)\n","    test_loss, correct = 0, 0\n","\n","    with torch.no_grad():\n","        for X, y in dataloader:\n","            pred = model(X)\n","            test_loss += loss_fn(pred, y).item() # 손실 값 누적\n","            corect += (pred.argmax(1) == y).type(torch.float).sum().item() # 정확히 예측된 값 누적\n","\n","    test_loss /= num_batches # 테스트 손실값 평균 계산\n","    correct /= size # 정확도 평균계산\n","    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"],"metadata":{"id":"UBhYF_F1xEjO"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["손실함수, 옵티마이저 초기화 이후 train loop, test loop 에 전달함. 모델의 성능 향상을 알아보기 위해 자유롭게 에폭 수를 증가시킬 수 있음"],"metadata":{"id":"c9AoCjHB0QkS"}},{"cell_type":"code","source":["loss_fn = nn.CrossEntropyLoss()\n","optimizer = torch.optim.SGD(model.parameters(), lr=learinig_rate)\n","\n","epochs = 10\n","for t in range(epochs):\n","    print(f\"Epoch {t+1}\\n-------------------------------\")\n","    train_loop(train_dataloader, model, loss_fn, optimizer)\n","print(\"done!!\")"],"metadata":{"id":"tSiLsTPS6HAJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"udIMN8M46MRu"},"execution_count":null,"outputs":[]}]}