{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "참고 (딥러닝을 이용한 자연어 처리)\n",
        ": https://wikidocs.net/94600\n",
        "\n",
        "위 링크에는 keras framework로 신경망을 구현한 반면, 이번 과제에서는 **pytorch** framework로 구현해보도록 합니다.\n",
        "\n",
        "참고 : https://www.deeplearningwizard.com/deep_learning/practical_pytorch/pytorch_recurrent_neuralnetwork/\n",
        "https://wonhwa.tistory.com/35"
      ],
      "metadata": {
        "id": "i1sviZfjra6c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pJdUylezMQ19",
        "outputId": "cc9afade-673a-4ea6-b8bc-96a6918d7493"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j3Piwi4SkyZs",
        "outputId": "ce688bea-7e79-4122-b209-8ecc6792ed70"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda is available.\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from datetime import datetime\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"{device}\" \" is available.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 데이터 불러오기 (네이버 쇼핑 리뷰 감성 분석 데이터)"
      ],
      "metadata": {
        "id": "7VWSo5CAjxu8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install konlpy\n",
        "!pip install mecab-python\n",
        "!bash <(curl -s https://raw.githubusercontent.com/konlpy/konlpy/master/scripts/mecab.sh)"
      ],
      "metadata": {
        "id": "1IXiir7_oFO6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import urllib.request\n",
        "from collections import Counter\n",
        "from konlpy.tag import Mecab\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences"
      ],
      "metadata": {
        "id": "312yGF-nn-7Z"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 데이터 로드하기\n",
        "urllib.request.urlretrieve(\"https://raw.githubusercontent.com/bab2min/corpus/master/sentiment/naver_shopping.txt\", filename=\"ratings_total.txt\")\n",
        "total_data = pd.read_table('ratings_total.txt', names = ['ratings', 'reviews'])\n",
        "\n",
        "# 데이터 개수 확인\n",
        "print('리뷰 개수 : ', len(total_data))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O1XyR9D-oSsJ",
        "outputId": "2012d01f-ac18-4485-b4d8-386b7dfe1956"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "리뷰 개수 :  200000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "total_data[:5] # 상위 5개 데이터만 샘플로 확인"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "w_uJez_mol6E",
        "outputId": "853ab44a-a38d-4900-955f-ec9ad4fe37b8"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   ratings                                            reviews\n",
              "0        5                                            배공빠르고 굿\n",
              "1        2                      택배가 엉망이네용 저희집 밑에층에 말도없이 놔두고가고\n",
              "2        5  아주좋아요 바지 정말 좋아서2개 더 구매했어요 이가격에 대박입니다. 바느질이 조금 ...\n",
              "3        2  선물용으로 빨리 받아서 전달했어야 하는 상품이었는데 머그컵만 와서 당황했습니다. 전...\n",
              "4        5                  민트색상 예뻐요. 옆 손잡이는 거는 용도로도 사용되네요 ㅎㅎ"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-13595c5d-a3ff-4651-9abb-8e2e717b462f\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ratings</th>\n",
              "      <th>reviews</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>5</td>\n",
              "      <td>배공빠르고 굿</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>택배가 엉망이네용 저희집 밑에층에 말도없이 놔두고가고</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>5</td>\n",
              "      <td>아주좋아요 바지 정말 좋아서2개 더 구매했어요 이가격에 대박입니다. 바느질이 조금 ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2</td>\n",
              "      <td>선물용으로 빨리 받아서 전달했어야 하는 상품이었는데 머그컵만 와서 당황했습니다. 전...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>민트색상 예뻐요. 옆 손잡이는 거는 용도로도 사용되네요 ㅎㅎ</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-13595c5d-a3ff-4651-9abb-8e2e717b462f')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-13595c5d-a3ff-4651-9abb-8e2e717b462f button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-13595c5d-a3ff-4651-9abb-8e2e717b462f');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-ad05b41c-d17b-4bc5-ad13-631b751c14dd\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-ad05b41c-d17b-4bc5-ad13-631b751c14dd')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-ad05b41c-d17b-4bc5-ad13-631b751c14dd button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "total_data['ratings'].value_counts()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k3dAgmVr-yXd",
        "outputId": "31c6e3f3-56db-4202-de91-20aabec76b79"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5    81170\n",
              "2    63948\n",
              "1    36007\n",
              "4    18783\n",
              "Name: ratings, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 감성 분석을 위한 라벨링\n",
        "total_data['label'] = np.select([total_data.ratings > 3], [1], default=0) # 4,5점은 긍정 1 / 1,2 점은 부정 0 으로 라벨링\n",
        "total_data.drop_duplicates(subset=['reviews'], inplace=True) # 중복 제거\n",
        "\n",
        "# 훈련 데이터와 테스트 데이터 split\n",
        "train_data, test_data = train_test_split(total_data, test_size = 0.2, random_state = 42)"
      ],
      "metadata": {
        "id": "1R4QJEaxorat"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data['label'].value_counts()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6lI6XJRUqTIp",
        "outputId": "b0e38c9b-d2b4-439e-f957-c720b7b541c0"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1    80003\n",
              "0    79923\n",
              "Name: label, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 데이터 정제 및 전처리"
      ],
      "metadata": {
        "id": "vMftA9UKqiBl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 한글과 공백을 제외하고 모두 제거 (train)\n",
        "# [^ㄱ-ㅎㅏ-ㅣ가-힣 ]: 정규 표현식으로, 한글(모음과 자음)과 띄어쓰기를 제외한 문자들을 매칭\n",
        "train_data['reviews'] = train_data['reviews'].str.replace(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\",\"\")\n",
        "train_data['reviews'].replace('', np.nan, inplace=True) # 공백은 Null 값으로 변경\n",
        "\n",
        "#왜 test에서 제거하는 코드는 있는데 train에서는 null값 제거하지 않는가??\n",
        "#---> 공백도 일종의 표현이라 생각한건지..?\n",
        "#---> 아니면 확인해봤더니 isna().sum() 값이 0이라 굳이 하지 않은 건지?\n",
        "\n",
        "# test data에도 동일하게 적용\n",
        "test_data.drop_duplicates(subset = ['reviews'], inplace=True) # 중복 제거\n",
        "test_data['reviews'] = test_data['reviews'].str.replace(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\",\"\")\n",
        "test_data['reviews'].replace('', np.nan, inplace=True) # 공백은 Null 값으로 변경\n",
        "test_data = test_data.dropna(how='any') # Null 값 제거"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i5JDUsBpqj45",
        "outputId": "910eb6a2-2a1b-44ee-de39-46df18779d8e"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-12-9b4311d83b86>:3: FutureWarning: The default value of regex will change from True to False in a future version.\n",
            "  train_data['reviews'] = train_data['reviews'].str.replace(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\",\"\")\n",
            "<ipython-input-12-9b4311d83b86>:8: FutureWarning: The default value of regex will change from True to False in a future version.\n",
            "  test_data['reviews'] = test_data['reviews'].str.replace(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\",\"\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_data['reviews'].isna().sum() #train_data에 공백이었던 것 없음"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fducGuW8_end",
        "outputId": "eb86dd2e-163a-42a4-da1c-0d5b79f3121f"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 토큰화"
      ],
      "metadata": {
        "id": "cn6IM4UQq8ad"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Mecab 모델로 형태소 분석 및 토큰화\n",
        "mecab = Mecab()\n",
        "\n",
        "# 불용어 설정\n",
        "# stopword.txt 파일이 저장된 경로를 정확히 입력해주세요\n",
        "with open('/content/drive/MyDrive/KUBIG 24-w/week1/stopword.txt') as f:\n",
        "    list_file = f.readlines()\n",
        "\n",
        "stopwords_list = []\n",
        "for stopword in list_file:\n",
        "  stopwords = re.sub('[\\n]', '', stopword)\n",
        "  stopwords_list.append(stopwords)\n",
        "\n",
        "# train data 토큰화\n",
        "train_data['tokenized'] = train_data['reviews'].apply(mecab.morphs)\n",
        "train_data['tokenized'] = train_data['tokenized'].apply(lambda x: [item for item in x if item not in stopwords_list])\n",
        "\n",
        "# test data 토큰화\n",
        "test_data['tokenized'] = test_data['reviews'].apply(mecab.morphs)\n",
        "test_data['tokenized'] = test_data['tokenized'].apply(lambda x: [item for item in x if item not in stopwords_list])"
      ],
      "metadata": {
        "id": "4u0NrFSaq9vd"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 정수 인코딩"
      ],
      "metadata": {
        "id": "_CD_HiUHroCe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# train과 test를 위한 X,Y data 분류\n",
        "\n",
        "X_train = train_data['tokenized'].values\n",
        "y_train = train_data['label'].values\n",
        "X_test= test_data['tokenized'].values\n",
        "y_test = test_data['label'].values"
      ],
      "metadata": {
        "id": "4H9Bu8yxr03N"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 단어 집합 생성 및 정수 인코딩\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(X_train)"
      ],
      "metadata": {
        "id": "aiQQp7KCrpR6"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.word_index"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WPQpegBGBYKp",
        "outputId": "6d83bd98-66ad-47f7-be20-ba005b343f59"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'고': 1,\n",
              " '네요': 2,\n",
              " '도': 3,\n",
              " '좋': 4,\n",
              " '는': 5,\n",
              " '어요': 6,\n",
              " '은': 7,\n",
              " '는데': 8,\n",
              " '아요': 9,\n",
              " '잘': 10,\n",
              " '있': 11,\n",
              " '구매': 12,\n",
              " '안': 13,\n",
              " '게': 14,\n",
              " '배송': 15,\n",
              " '했': 16,\n",
              " '너무': 17,\n",
              " '한': 18,\n",
              " '같': 19,\n",
              " '지': 20,\n",
              " '거': 21,\n",
              " '먹': 22,\n",
              " '합니다': 23,\n",
              " '다': 24,\n",
              " '되': 25,\n",
              " '재': 26,\n",
              " '요': 27,\n",
              " '기': 28,\n",
              " '쓰': 29,\n",
              " '않': 30,\n",
              " '없': 31,\n",
              " '해서': 32,\n",
              " '사용': 33,\n",
              " '았': 34,\n",
              " '만': 35,\n",
              " '보다': 36,\n",
              " '었': 37,\n",
              " '겠': 38,\n",
              " '주': 39,\n",
              " '아서': 40,\n",
              " '그냥': 41,\n",
              " '해': 42,\n",
              " '보': 43,\n",
              " '제품': 44,\n",
              " '서': 45,\n",
              " '면': 46,\n",
              " '어서': 47,\n",
              " '가격': 48,\n",
              " '만족': 49,\n",
              " '생각': 50,\n",
              " '주문': 51,\n",
              " '입니다': 52,\n",
              " '라': 53,\n",
              " '더': 54,\n",
              " '받': 55,\n",
              " '니': 56,\n",
              " '맛': 57,\n",
              " '할': 58,\n",
              " '많이': 59,\n",
              " '개': 60,\n",
              " '시': 61,\n",
              " '사이즈': 62,\n",
              " '듯': 63,\n",
              " '빠르': 64,\n",
              " '샀': 65,\n",
              " '왔': 66,\n",
              " '번': 67,\n",
              " '작': 68,\n",
              " '음': 69,\n",
              " '맛있': 70,\n",
              " 'ㅠㅠ': 71,\n",
              " '포장': 72,\n",
              " '수': 73,\n",
              " '별로': 74,\n",
              " '두': 75,\n",
              " '입': 76,\n",
              " '맞': 77,\n",
              " '던': 78,\n",
              " '저렴': 79,\n",
              " '괜찮': 80,\n",
              " '세요': 81,\n",
              " '넘': 82,\n",
              " '못': 83,\n",
              " '상품': 84,\n",
              " '정말': 85,\n",
              " '데': 86,\n",
              " '에요': 87,\n",
              " 'ㅠ': 88,\n",
              " '냄새': 89,\n",
              " '부분': 90,\n",
              " '감사': 91,\n",
              " '아주': 92,\n",
              " '용': 93,\n",
              " '인데': 94,\n",
              " 'ㅎㅎ': 95,\n",
              " '으면': 96,\n",
              " '진짜': 97,\n",
              " '긴': 98,\n",
              " '보내': 99,\n",
              " '알': 100,\n",
              " '많': 101,\n",
              " '적': 102,\n",
              " '인': 103,\n",
              " '구입': 104,\n",
              " '구요': 105,\n",
              " '정도': 106,\n",
              " '반품': 107,\n",
              " '크': 108,\n",
              " '느낌': 109,\n",
              " '맘': 110,\n",
              " '아직': 111,\n",
              " '중': 112,\n",
              " '줄': 113,\n",
              " '써': 114,\n",
              " '다고': 115,\n",
              " '불편': 116,\n",
              " '깔끔': 117,\n",
              " '봤': 118,\n",
              " '모르': 119,\n",
              " '싶': 120,\n",
              " '완전': 121,\n",
              " '전': 122,\n",
              " '다시': 123,\n",
              " 'ㅜㅜ': 124,\n",
              " '달': 125,\n",
              " '건': 126,\n",
              " '편하': 127,\n",
              " '은데': 128,\n",
              " '쓸': 129,\n",
              " '엄청': 130,\n",
              " '넣': 131,\n",
              " '다가': 132,\n",
              " '려고': 133,\n",
              " '처음': 134,\n",
              " '사진': 135,\n",
              " 'ㅎ': 136,\n",
              " '디자인': 137,\n",
              " '비': 138,\n",
              " '효과': 139,\n",
              " '튼튼': 140,\n",
              " '물': 141,\n",
              " '빠른': 142,\n",
              " '랑': 143,\n",
              " '상태': 144,\n",
              " '걸': 145,\n",
              " '분': 146,\n",
              " '선물': 147,\n",
              " '대비': 148,\n",
              " '추천': 149,\n",
              " '굿': 150,\n",
              " '함': 151,\n",
              " '색상': 152,\n",
              " '살': 153,\n",
              " '예요': 154,\n",
              " '라고': 155,\n",
              " '길': 156,\n",
              " '얇': 157,\n",
              " '원': 158,\n",
              " '싸': 159,\n",
              " '색': 160,\n",
              " '신': 161,\n",
              " '엔': 162,\n",
              " '싼': 163,\n",
              " '집': 164,\n",
              " '기대': 165,\n",
              " '귀찮': 166,\n",
              " 'ㅋㅋ': 167,\n",
              " '실망': 168,\n",
              " '더니': 169,\n",
              " '째': 170,\n",
              " '산': 171,\n",
              " '교환': 172,\n",
              " '계속': 173,\n",
              " '감': 174,\n",
              " '향': 175,\n",
              " '없이': 176,\n",
              " '마음': 177,\n",
              " '셨': 178,\n",
              " '진': 179,\n",
              " '건지': 180,\n",
              " '이쁘': 181,\n",
              " '물건': 182,\n",
              " '돈': 183,\n",
              " '설치': 184,\n",
              " '아쉽': 185,\n",
              " '말': 186,\n",
              " '오래': 187,\n",
              " '해야': 188,\n",
              " '박스': 189,\n",
              " '항상': 190,\n",
              " '된': 191,\n",
              " '이랑': 192,\n",
              " '후': 193,\n",
              " '파': 194,\n",
              " '크기': 195,\n",
              " '는지': 196,\n",
              " '큰': 197,\n",
              " '아기': 198,\n",
              " '였': 199,\n",
              " '별': 200,\n",
              " '놓': 201,\n",
              " '택배': 202,\n",
              " '늦': 203,\n",
              " '이거': 204,\n",
              " '내': 205,\n",
              " '불량': 206,\n",
              " 'ㅡㅡ': 207,\n",
              " '죠': 208,\n",
              " '와서': 209,\n",
              " '처럼': 210,\n",
              " '곳': 211,\n",
              " '보이': 212,\n",
              " '믿': 213,\n",
              " '조립': 214,\n",
              " '잘못': 215,\n",
              " '뭐': 216,\n",
              " '예쁘': 217,\n",
              " '질': 218,\n",
              " 'ㅜ': 219,\n",
              " '이나': 220,\n",
              " '남': 221,\n",
              " '스럽': 222,\n",
              " '근데': 223,\n",
              " '양': 224,\n",
              " '붙': 225,\n",
              " '잇': 226,\n",
              " '면서': 227,\n",
              " '마감': 228,\n",
              " '시켰': 229,\n",
              " '재질': 230,\n",
              " '꺼': 231,\n",
              " 'ㄷ': 232,\n",
              " '버렸': 233,\n",
              " '건데': 234,\n",
              " '날': 235,\n",
              " '확인': 236,\n",
              " 'ㅋ': 237,\n",
              " '역시': 238,\n",
              " '님': 239,\n",
              " '셔서': 240,\n",
              " '필요': 241,\n",
              " '최고': 242,\n",
              " '옷': 243,\n",
              " '손': 244,\n",
              " '세': 245,\n",
              " '드': 246,\n",
              " '시원': 247,\n",
              " '기분': 248,\n",
              " '더라구요': 249,\n",
              " '매우': 250,\n",
              " '인지': 251,\n",
              " '후기': 252,\n",
              " '이뻐요': 253,\n",
              " '힘들': 254,\n",
              " '만큼': 255,\n",
              " '때문': 256,\n",
              " '대': 257,\n",
              " '해도': 258,\n",
              " '비싸': 259,\n",
              " '고정': 260,\n",
              " '빨리': 261,\n",
              " '예뻐요': 262,\n",
              " '늘': 263,\n",
              " '씩': 264,\n",
              " '나요': 265,\n",
              " '니까': 266,\n",
              " '편': 267,\n",
              " '나오': 268,\n",
              " '품질': 269,\n",
              " '들어가': 270,\n",
              " '햇': 271,\n",
              " '씁니다': 272,\n",
              " '라서': 273,\n",
              " '이렇게': 274,\n",
              " '이건': 275,\n",
              " 'ㄴ': 276,\n",
              " '만들': 277,\n",
              " 'ㄱ': 278,\n",
              " '빼': 279,\n",
              " '새': 280,\n",
              " '될': 281,\n",
              " '적당': 282,\n",
              " '쉽': 283,\n",
              " '성비': 284,\n",
              " '임': 285,\n",
              " '앞': 286,\n",
              " '짧': 287,\n",
              " '으니': 288,\n",
              " '하루': 289,\n",
              " '소리': 290,\n",
              " '떨어지': 291,\n",
              " '조아': 292,\n",
              " '가지': 293,\n",
              " '다는': 294,\n",
              " '여름': 295,\n",
              " '네여': 296,\n",
              " '가볍': 297,\n",
              " '점': 298,\n",
              " '신경': 299,\n",
              " '전화': 300,\n",
              " '사람': 301,\n",
              " '살짝': 302,\n",
              " '뜯': 303,\n",
              " '문제': 304,\n",
              " '원래': 305,\n",
              " '그런': 306,\n",
              " '팩': 307,\n",
              " '그렇': 308,\n",
              " '도착': 309,\n",
              " '샀어요': 310,\n",
              " '피부': 311,\n",
              " 'ㅈ': 312,\n",
              " 'ㅅ': 313,\n",
              " '전혀': 314,\n",
              " '여러': 315,\n",
              " '리뷰': 316,\n",
              " '부드럽': 317,\n",
              " '녹': 318,\n",
              " '비추': 319,\n",
              " '성': 320,\n",
              " '최악': 321,\n",
              " '제대로': 322,\n",
              " '됩니다': 323,\n",
              " '시켜': 324,\n",
              " '머리': 325,\n",
              " '다르': 326,\n",
              " '처리': 327,\n",
              " '마다': 328,\n",
              " '차': 329,\n",
              " '잡': 330,\n",
              " '뚜껑': 331,\n",
              " '추가': 332,\n",
              " '별루': 333,\n",
              " '그래요': 334,\n",
              " '그런지': 335,\n",
              " '아닌': 336,\n",
              " '자꾸': 337,\n",
              " '걱정': 338,\n",
              " '어야': 339,\n",
              " '쪽': 340,\n",
              " '자주': 341,\n",
              " 'ㅋㅋㅋ': 342,\n",
              " '편해요': 343,\n",
              " '봐요': 344,\n",
              " '애': 345,\n",
              " '버리': 346,\n",
              " '한테': 347,\n",
              " '연락': 348,\n",
              " '먼지': 349,\n",
              " '빨': 350,\n",
              " '어도': 351,\n",
              " '티': 352,\n",
              " '천': 353,\n",
              " '금방': 354,\n",
              " '색깔': 355,\n",
              " '친절': 356,\n",
              " '듭니다': 357,\n",
              " '려구요': 358,\n",
              " '케이스': 359,\n",
              " '주일': 360,\n",
              " '못하': 361,\n",
              " '심하': 362,\n",
              " '불': 363,\n",
              " '드렸': 364,\n",
              " '배': 365,\n",
              " '며': 366,\n",
              " '찾': 367,\n",
              " '꼼꼼': 368,\n",
              " '됐': 369,\n",
              " '충전': 370,\n",
              " '덜': 371,\n",
              " '묻': 372,\n",
              " '세탁': 373,\n",
              " '썼': 374,\n",
              " '게요': 375,\n",
              " '가성': 376,\n",
              " '담': 377,\n",
              " '온': 378,\n",
              " '연결': 379,\n",
              " '커서': 380,\n",
              " '붙이': 381,\n",
              " '열': 382,\n",
              " '지금': 383,\n",
              " '타': 384,\n",
              " '실': 385,\n",
              " '길이': 386,\n",
              " '상': 387,\n",
              " '친구': 388,\n",
              " '빠지': 389,\n",
              " '이용': 390,\n",
              " '다니': 391,\n",
              " '마세요': 392,\n",
              " '닦': 393,\n",
              " '줬': 394,\n",
              " '짜증': 395,\n",
              " '유통': 396,\n",
              " '차이': 397,\n",
              " '구멍': 398,\n",
              " '다리': 399,\n",
              " '장': 400,\n",
              " '절대': 401,\n",
              " '촉촉': 402,\n",
              " '힘': 403,\n",
              " '제일': 404,\n",
              " '속': 405,\n",
              " '화면': 406,\n",
              " '아용': 407,\n",
              " '짱': 408,\n",
              " '끝': 409,\n",
              " '커버': 410,\n",
              " '환불': 411,\n",
              " '눈': 412,\n",
              " '착용': 413,\n",
              " '고급': 414,\n",
              " '막': 415,\n",
              " '엄마': 416,\n",
              " '신선': 417,\n",
              " '고민': 418,\n",
              " '넉넉': 419,\n",
              " '끼': 420,\n",
              " 'ㅎㅎㅎ': 421,\n",
              " '고요': 422,\n",
              " '참고': 423,\n",
              " '고기': 424,\n",
              " '비해': 425,\n",
              " '그대로': 426,\n",
              " 'ㅂ': 427,\n",
              " '빨랐': 428,\n",
              " '위': 429,\n",
              " '으나': 430,\n",
              " '치': 431,\n",
              " '할게요': 432,\n",
              " '의사': 433,\n",
              " '걸로': 434,\n",
              " '기한': 435,\n",
              " '정리': 436,\n",
              " '가능': 437,\n",
              " '겟': 438,\n",
              " '끈': 439,\n",
              " '짜': 440,\n",
              " '반': 441,\n",
              " '따뜻': 442,\n",
              " '너무너무': 443,\n",
              " '러': 444,\n",
              " '걸렸': 445,\n",
              " '문': 446,\n",
              " '이게': 447,\n",
              " '바닥': 448,\n",
              " '구성': 449,\n",
              " '바르': 450,\n",
              " '통': 451,\n",
              " '어용': 452,\n",
              " '모양': 453,\n",
              " '따로': 454,\n",
              " '발': 455,\n",
              " '운동': 456,\n",
              " '판매': 457,\n",
              " '자체': 458,\n",
              " '키': 459,\n",
              " '됨': 460,\n",
              " '삿': 461,\n",
              " '간편': 462,\n",
              " '비닐': 463,\n",
              " '깨끗': 464,\n",
              " '걍': 465,\n",
              " '퀄리티': 466,\n",
              " '첨': 467,\n",
              " '의자': 468,\n",
              " '일반': 469,\n",
              " '바람': 470,\n",
              " '편리': 471,\n",
              " '본': 472,\n",
              " '느리': 473,\n",
              " '정품': 474,\n",
              " '꼭': 475,\n",
              " '오늘': 476,\n",
              " '써요': 477,\n",
              " '기사': 478,\n",
              " '져서': 479,\n",
              " '소재': 480,\n",
              " '스': 481,\n",
              " '똑같': 482,\n",
              " '볼': 483,\n",
              " '얼굴': 484,\n",
              " '기존': 485,\n",
              " '꾸준히': 486,\n",
              " '가방': 487,\n",
              " '뒤': 488,\n",
              " '빨라서': 489,\n",
              " '그랬': 490,\n",
              " '후회': 491,\n",
              " '할께요': 492,\n",
              " '기스': 493,\n",
              " '시키': 494,\n",
              " '핏': 495,\n",
              " '나사': 496,\n",
              " '애기': 497,\n",
              " '그러': 498,\n",
              " '무게': 499,\n",
              " '블랙': 500,\n",
              " '스러워': 501,\n",
              " '납니다': 502,\n",
              " '비싼': 503,\n",
              " '맛나': 504,\n",
              " '사은품': 505,\n",
              " '원단': 506,\n",
              " '엉망': 507,\n",
              " '서비스': 508,\n",
              " '빨라요': 509,\n",
              " '손잡이': 510,\n",
              " '굳': 511,\n",
              " '건조': 512,\n",
              " '딱딱': 513,\n",
              " '약하': 514,\n",
              " '강추': 515,\n",
              " '바지': 516,\n",
              " '플라스틱': 517,\n",
              " '두께': 518,\n",
              " '이제': 519,\n",
              " '번창': 520,\n",
              " '무': 521,\n",
              " '갈': 522,\n",
              " '라는': 523,\n",
              " '대로': 524,\n",
              " '확실히': 525,\n",
              " '는데요': 526,\n",
              " '부족': 527,\n",
              " '져': 528,\n",
              " '지나': 529,\n",
              " '선택': 530,\n",
              " '보고': 531,\n",
              " '기능': 532,\n",
              " '걸리': 533,\n",
              " '아무리': 534,\n",
              " '판매자': 535,\n",
              " '짜리': 536,\n",
              " 'ㄹ': 537,\n",
              " '보관': 538,\n",
              " '났': 539,\n",
              " '아도': 540,\n",
              " '설명': 541,\n",
              " '봐야': 542,\n",
              " '마시': 543,\n",
              " '높': 544,\n",
              " '맛없': 545,\n",
              " '나쁘': 546,\n",
              " '젤': 547,\n",
              " '평': 548,\n",
              " '으려고': 549,\n",
              " '보여요': 550,\n",
              " '밑': 551,\n",
              " '고무': 552,\n",
              " '지퍼': 553,\n",
              " '깨져서': 554,\n",
              " '목': 555,\n",
              " '어렵': 556,\n",
              " '중간': 557,\n",
              " '마리': 558,\n",
              " '싱싱': 559,\n",
              " '뭔가': 560,\n",
              " '김치': 561,\n",
              " '조절': 562,\n",
              " '길래': 563,\n",
              " '벌써': 564,\n",
              " '사세요': 565,\n",
              " '쿠션': 566,\n",
              " '조': 567,\n",
              " '올': 568,\n",
              " '져요': 569,\n",
              " '써야': 570,\n",
              " '라도': 571,\n",
              " '허접': 572,\n",
              " '검수': 573,\n",
              " '나무': 574,\n",
              " '핑크': 575,\n",
              " '보단': 576,\n",
              " '보통': 577,\n",
              " '한번': 578,\n",
              " '전체': 579,\n",
              " '단': 580,\n",
              " '거나': 581,\n",
              " '직접': 582,\n",
              " '아깝': 583,\n",
              " '형': 584,\n",
              " '무난': 585,\n",
              " '놀': 586,\n",
              " '접착력': 587,\n",
              " '옆': 588,\n",
              " '커요': 589,\n",
              " '귀엽': 590,\n",
              " '으': 591,\n",
              " '요즘': 592,\n",
              " '신발': 593,\n",
              " '부모': 594,\n",
              " '예정': 595,\n",
              " '자국': 596,\n",
              " '씻': 597,\n",
              " '께': 598,\n",
              " '미리': 599,\n",
              " '왓': 600,\n",
              " '쓰레기': 601,\n",
              " '가루': 602,\n",
              " '스티커': 603,\n",
              " '아들': 604,\n",
              " '이유': 605,\n",
              " '여행': 606,\n",
              " '어여': 607,\n",
              " '깔': 608,\n",
              " '싫': 609,\n",
              " '마': 610,\n",
              " '허리': 611,\n",
              " '아쉬워': 612,\n",
              " '광고': 613,\n",
              " '엇': 614,\n",
              " '마트': 615,\n",
              " '짐': 616,\n",
              " '남편': 617,\n",
              " '건가요': 618,\n",
              " '찍': 619,\n",
              " '밖에': 620,\n",
              " '비슷': 621,\n",
              " '제거': 622,\n",
              " '거품': 623,\n",
              " '넓': 624,\n",
              " '여서': 625,\n",
              " '마스크': 626,\n",
              " '겨울': 627,\n",
              " '중국': 628,\n",
              " '한쪽': 629,\n",
              " '셔야': 630,\n",
              " '작동': 631,\n",
              " '패드': 632,\n",
              " '착용감': 633,\n",
              " '짱짱': 634,\n",
              " 'ㅡ': 635,\n",
              " '죽': 636,\n",
              " '합': 637,\n",
              " '나가': 638,\n",
              " '이불': 639,\n",
              " 'ㅁ': 640,\n",
              " '첫': 641,\n",
              " '아프': 642,\n",
              " '무겁': 643,\n",
              " '그닥': 644,\n",
              " 'ㅠㅠㅠ': 645,\n",
              " '쓸께요': 646,\n",
              " '설명서': 647,\n",
              " '봅니다': 648,\n",
              " '예전': 649,\n",
              " '낮': 650,\n",
              " '터져서': 651,\n",
              " '썩': 652,\n",
              " '용량': 653,\n",
              " '흰색': 654,\n",
              " '밤': 655,\n",
              " '던데': 656,\n",
              " '파손': 657,\n",
              " '털': 658,\n",
              " '약해요': 659,\n",
              " '화': 660,\n",
              " '느': 661,\n",
              " '컬러': 662,\n",
              " '기름': 663,\n",
              " '드립니다': 664,\n",
              " '습니당': 665,\n",
              " '신랑': 666,\n",
              " '높이': 667,\n",
              " '내요': 668,\n",
              " '개월': 669,\n",
              " '테이프': 670,\n",
              " '문의': 671,\n",
              " '건강': 672,\n",
              " '저번': 673,\n",
              " '글': 674,\n",
              " '어쩔': 675,\n",
              " '밝': 676,\n",
              " '두껍': 677,\n",
              " '색감': 678,\n",
              " '바': 679,\n",
              " '당': 680,\n",
              " '대박': 681,\n",
              " '지워': 682,\n",
              " '아침': 683,\n",
              " '몸': 684,\n",
              " '요청': 685,\n",
              " '방': 686,\n",
              " '나왔': 687,\n",
              " '인터넷': 688,\n",
              " '여자': 689,\n",
              " '값': 690,\n",
              " '솔직히': 691,\n",
              " '먹이': 692,\n",
              " '비지떡': 693,\n",
              " '떼': 694,\n",
              " '기본': 695,\n",
              " '견고': 696,\n",
              " '업체': 697,\n",
              " '은지': 698,\n",
              " '유리': 699,\n",
              " '부탁': 700,\n",
              " '대충': 701,\n",
              " '트': 702,\n",
              " '확정': 703,\n",
              " '옴': 704,\n",
              " '교체': 705,\n",
              " '늘어나': 706,\n",
              " '양념': 707,\n",
              " '성능': 708,\n",
              " '세트': 709,\n",
              " '솜': 710,\n",
              " '귀여워': 711,\n",
              " '려': 712,\n",
              " '아예': 713,\n",
              " '매트': 714,\n",
              " '비교': 715,\n",
              " '공간': 716,\n",
              " '보냈': 717,\n",
              " '회사': 718,\n",
              " '앉': 719,\n",
              " '필름': 720,\n",
              " '상자': 721,\n",
              " '괜히': 722,\n",
              " '뜨': 723,\n",
              " '고구마': 724,\n",
              " '화이트': 725,\n",
              " '아이스': 726,\n",
              " '굉장히': 727,\n",
              " '냉동': 728,\n",
              " '선': 729,\n",
              " '텐데': 730,\n",
              " '고장': 731,\n",
              " '울': 732,\n",
              " '달라요': 733,\n",
              " '활용': 734,\n",
              " '검정': 735,\n",
              " '좁': 736,\n",
              " '컵': 737,\n",
              " '갔': 738,\n",
              " '떨어져요': 739,\n",
              " '남자': 740,\n",
              " '백': 741,\n",
              " '모서리': 742,\n",
              " '개인': 743,\n",
              " '그렇게': 744,\n",
              " '안쪽': 745,\n",
              " '정': 746,\n",
              " '커': 747,\n",
              " '생기': 748,\n",
              " '뻔': 749,\n",
              " '밥': 750,\n",
              " '유용': 751,\n",
              " '가족': 752,\n",
              " '까': 753,\n",
              " '끼우': 754,\n",
              " '자극': 755,\n",
              " '간': 756,\n",
              " '난': 757,\n",
              " '답변': 758,\n",
              " '소음': 759,\n",
              " '떨어져서': 760,\n",
              " '기간': 761,\n",
              " '꽉': 762,\n",
              " '봉': 763,\n",
              " '배달': 764,\n",
              " '빠릅니다': 765,\n",
              " '다신': 766,\n",
              " '팬티': 767,\n",
              " '취소': 768,\n",
              " '당황': 769,\n",
              " '가장': 770,\n",
              " '느려요': 771,\n",
              " '신청': 772,\n",
              " '부실': 773,\n",
              " '이틀': 774,\n",
              " '침대': 775,\n",
              " '거리': 776,\n",
              " '평소': 777,\n",
              " '검': 778,\n",
              " '누가': 779,\n",
              " '급하': 780,\n",
              " '한지': 781,\n",
              " '수납': 782,\n",
              " '강아지': 783,\n",
              " '졌어요': 784,\n",
              " '급': 785,\n",
              " '익': 786,\n",
              " '트러블': 787,\n",
              " '즈': 788,\n",
              " '발송': 789,\n",
              " '못했': 790,\n",
              " '시킬': 791,\n",
              " '약해서': 792,\n",
              " '인가': 793,\n",
              " '그럭저럭': 794,\n",
              " '놀랐': 795,\n",
              " '하고': 796,\n",
              " '고객': 797,\n",
              " '아닌데': 798,\n",
              " '미': 799,\n",
              " '써서': 800,\n",
              " '아님': 801,\n",
              " '풀': 802,\n",
              " '나와서': 803,\n",
              " '뻑뻑': 804,\n",
              " '양말': 805,\n",
              " '돼서': 806,\n",
              " '불안': 807,\n",
              " '내용물': 808,\n",
              " '바꾸': 809,\n",
              " '아쉬웠': 810,\n",
              " '푹신': 811,\n",
              " '지인': 812,\n",
              " '돼요': 813,\n",
              " '이미': 814,\n",
              " '뿐': 815,\n",
              " '간식': 816,\n",
              " '입맛': 817,\n",
              " '밖': 818,\n",
              " '쉬': 819,\n",
              " '문자': 820,\n",
              " '베': 821,\n",
              " '을까': 822,\n",
              " '부위': 823,\n",
              " '붙여': 824,\n",
              " '크림': 825,\n",
              " '더라고요': 826,\n",
              " '빠져서': 827,\n",
              " '이걸': 828,\n",
              " '덕분': 829,\n",
              " '딸': 830,\n",
              " '느라': 831,\n",
              " '고소': 832,\n",
              " '정확': 833,\n",
              " '귀': 834,\n",
              " '품평': 835,\n",
              " '부착': 836,\n",
              " '더군요': 837,\n",
              " '왔어요': 838,\n",
              " '낫': 839,\n",
              " '으시': 840,\n",
              " '찝찝': 841,\n",
              " '얼룩': 842,\n",
              " '다양': 843,\n",
              " '고리': 844,\n",
              " '올리': 845,\n",
              " '드리': 846,\n",
              " '했었': 847,\n",
              " '버림': 848,\n",
              " '실패': 849,\n",
              " '봐서': 850,\n",
              " '특히': 851,\n",
              " '성분': 852,\n",
              " '코': 853,\n",
              " '채': 854,\n",
              " '시작': 855,\n",
              " '겉': 856,\n",
              " '아가': 857,\n",
              " '단단': 858,\n",
              " '껄': 859,\n",
              " '머': 860,\n",
              " '뿌리': 861,\n",
              " '실리콘': 862,\n",
              " '느렸': 863,\n",
              " '모기': 864,\n",
              " '졌': 865,\n",
              " 'ㅣ': 866,\n",
              " '간단': 867,\n",
              " '나중': 868,\n",
              " '피': 869,\n",
              " '흠': 870,\n",
              " '새로': 871,\n",
              " '을게요': 872,\n",
              " '아쉬운': 873,\n",
              " '식': 874,\n",
              " '사이': 875,\n",
              " '그것': 876,\n",
              " '가죽': 877,\n",
              " '생': 878,\n",
              " '나와요': 879,\n",
              " '가요': 880,\n",
              " '편안': 881,\n",
              " '품절': 882,\n",
              " '답답': 883,\n",
              " '봉지': 884,\n",
              " '촉감': 885,\n",
              " '심해요': 886,\n",
              " '싸구려': 887,\n",
              " '종류': 888,\n",
              " '분리': 889,\n",
              " '실밥': 890,\n",
              " '모자': 891,\n",
              " '아파요': 892,\n",
              " '종이': 893,\n",
              " '아무': 894,\n",
              " '관리': 895,\n",
              " '작업': 896,\n",
              " '서랍': 897,\n",
              " '청소': 898,\n",
              " '나름': 899,\n",
              " '고생': 900,\n",
              " '쫌': 901,\n",
              " '니다': 902,\n",
              " '커피': 903,\n",
              " '방지': 904,\n",
              " '이해': 905,\n",
              " '부담': 906,\n",
              " '닿': 907,\n",
              " '줘야': 908,\n",
              " '곤': 909,\n",
              " '라면': 910,\n",
              " '꼼꼼히': 911,\n",
              " '챙겨': 912,\n",
              " '올려': 913,\n",
              " '인가요': 914,\n",
              " '금액': 915,\n",
              " '박': 916,\n",
              " '다면': 917,\n",
              " '땀': 918,\n",
              " '느끼': 919,\n",
              " '샴푸': 920,\n",
              " '상판': 921,\n",
              " '겁나': 922,\n",
              " '바랍니다': 923,\n",
              " '보여서': 924,\n",
              " '방법': 925,\n",
              " '그런가': 926,\n",
              " '몰랐': 927,\n",
              " '열심히': 928,\n",
              " '적극': 929,\n",
              " '용도': 930,\n",
              " '니당': 931,\n",
              " '력': 932,\n",
              " '답': 933,\n",
              " '갖': 934,\n",
              " '아래': 935,\n",
              " '칼': 936,\n",
              " '흡수': 937,\n",
              " '경우': 938,\n",
              " '쓴': 939,\n",
              " '기다리': 940,\n",
              " '접': 941,\n",
              " '밴드': 942,\n",
              " '엉덩이': 943,\n",
              " '가슴': 944,\n",
              " '보풀': 945,\n",
              " '부': 946,\n",
              " '워': 947,\n",
              " '편합니다': 948,\n",
              " '적당히': 949,\n",
              " '바느질': 950,\n",
              " '마니': 951,\n",
              " '책상': 952,\n",
              " '김': 953,\n",
              " '시킨': 954,\n",
              " '전하': 955,\n",
              " '빠름': 956,\n",
              " '프로': 957,\n",
              " '꽤': 958,\n",
              " '품': 959,\n",
              " '여기저기': 960,\n",
              " '준': 961,\n",
              " '신축성': 962,\n",
              " '장착': 963,\n",
              " '꽃': 964,\n",
              " '심플': 965,\n",
              " '붙였': 966,\n",
              " '줘서': 967,\n",
              " '실물': 968,\n",
              " '아닙니다': 969,\n",
              " '며칠': 970,\n",
              " '우유': 971,\n",
              " '느림': 972,\n",
              " '니깐': 973,\n",
              " '나와': 974,\n",
              " '편한': 975,\n",
              " '나머지': 976,\n",
              " '지저분': 977,\n",
              " '다행': 978,\n",
              " '빠져요': 979,\n",
              " '힘드': 980,\n",
              " '향도': 981,\n",
              " '도움': 982,\n",
              " '깨': 983,\n",
              " '닫': 984,\n",
              " '끊': 985,\n",
              " '홈': 986,\n",
              " '돼': 987,\n",
              " '터지': 988,\n",
              " '라인': 989,\n",
              " '아닌가요': 990,\n",
              " '사료': 991,\n",
              " '집니다': 992,\n",
              " '허술': 993,\n",
              " '빠져': 994,\n",
              " '바퀴': 995,\n",
              " '휘': 996,\n",
              " '음식': 997,\n",
              " '빠짐': 998,\n",
              " '틀': 999,\n",
              " '심해서': 1000,\n",
              " ...}"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**fit_on_texts** : Tokenizer 객체에 텍스트 데이터를 입력하여 단어 집합(vocabulary)을 생성하고, 각 단어에 고유한 정수를 할당하는 메서드"
      ],
      "metadata": {
        "id": "ct6boB5EA_m-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# vocab_size 설\n",
        "vocab_size = len(tokenizer.word_index)\n",
        "\n",
        "# 텍스트 시퀀스 -> 정수 시퀀스\n",
        "tokenizer = Tokenizer(vocab_size, oov_token = 'OOV')\n",
        "tokenizer.fit_on_texts(X_train)\n",
        "X_train = tokenizer.texts_to_sequences(X_train)\n",
        "X_test = tokenizer.texts_to_sequences(X_test)"
      ],
      "metadata": {
        "id": "nUz8VtqgsFjB"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "vocab_size를 토크나이저의 인자로 설정 -> 이보다 큰 숫자가 부여된 단어들은 OOV로 변환\n"
      ],
      "metadata": {
        "id": "1ZT2mMKvBvQ2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(X_train[:3])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8dZ8BWYSB5r9",
        "outputId": "d3c567d8-25af-44b8-f3a3-f8f0d058695c"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1856, 22270, 2811, 134, 115, 44, 134, 66, 9, 10513, 59, 74, 227, 6, 142, 14533, 26, 47, 5337, 30, 266], [2186, 458, 6, 127, 281, 9133, 327, 15, 9743, 6134, 12746, 192, 15, 54, 406, 139, 213, 2, 9743, 630, 3659, 12, 48, 1097, 514, 24, 214, 2, 23, 352, 26, 1338, 247, 3], [118, 57, 182, 3, 5]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(X_test[:3])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Bnrp6joB68C",
        "outputId": "939a14c2-8a53-47ca-fb28-7974b80a3c92"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[18, 651, 2, 711, 4, 103, 2, 169, 223], [312, 3800, 60, 3753, 1576], [16, 4, 65, 3, 49, 149, 5, 8, 20, 10, 468, 6, 255, 530, 57, 11, 3]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 패딩\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "OYZGDvYhsbF2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print('리뷰의 최대 길이 :',max(len(review) for review in X_train))\n",
        "print('리뷰의 평균 길이 :',sum(map(len, X_train))/len(X_train))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WULaQy5-scVq",
        "outputId": "aad78272-2853-46ee-e0f3-9c1ae2fafc3d"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "리뷰의 최대 길이 : 86\n",
            "리뷰의 평균 길이 : 15.648862598952014\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "num_tokens = [len(review) for review in X_train]\n",
        "\n",
        "plt.title('all text length')\n",
        "plt.hist(num_tokens, bins=30) #데이터 길이 분포\n",
        "plt.xlabel('length of samples')\n",
        "plt.ylabel('number of samples')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "fmlo_6gVO3az",
        "outputId": "0a577503-2a4e-4340-fd0f-6fdb80408480"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "select_length = 40\n",
        "\n",
        "def below_threshold_len(max_len, nested_list):\n",
        "    cnt = 0\n",
        "    for s in nested_list:\n",
        "        if(len(s) <= max_len):\n",
        "            cnt = cnt + 1\n",
        "\n",
        "    print('전체 샘플 중 길이가 %s 이하인 샘플의 비율: %s'%(max_len, (cnt / len(nested_list))))\n",
        "\n",
        "below_threshold_len(select_length, X_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "44WPTfENOWAa",
        "outputId": "0a4852d4-65e2-4ff7-bb86-b18465aa46ff"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "전체 샘플 중 길이가 40 이하인 샘플의 비율: 0.9564986306166602\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 최대 길이를 40으로 잡고 패딩\n",
        "max_len = 40\n",
        "X_train = pad_sequences(X_train, maxlen=max_len)\n",
        "X_test = pad_sequences(X_test, maxlen=max_len)"
      ],
      "metadata": {
        "id": "LiyYv--EsjAO"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model 변경 함수\n",
        "def get_model(model, model_params):\n",
        "    models = {\n",
        "        \"rnn\": RNNModel,\n",
        "        \"lstm\": LSTMModel,\n",
        "        \"gru\": GRUModel,\n",
        "    }\n",
        "    return models.get(model.lower())(**model_params) #모델 인스턴스 생성하여 반환"
      ],
      "metadata": {
        "id": "NZPf38qXttio"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(X_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i3IeLhWGC42I",
        "outputId": "b2e18bbf-2c11-4afa-806e-6906a68d253d"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "159926"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(X_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LOD1wP6vC6vE",
        "outputId": "75f856da-3935-4cb1-d502-dc06337089ed"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "39982"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### DataLoader 생성"
      ],
      "metadata": {
        "id": "JXExaNzZLUG1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class LabeledNumpyArrayDataset(Dataset):\n",
        "    def __init__(self, numpy_data, numpy_labels, transform=None):\n",
        "        self.data = numpy_data\n",
        "        self.labels = numpy_labels\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        sample = self.data[index]\n",
        "        label = self.labels[index]\n",
        "\n",
        "        if self.transform:\n",
        "            sample = self.transform(sample)\n",
        "\n",
        "        return sample, label"
      ],
      "metadata": {
        "id": "IyVTAop2L9Kv"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 64\n",
        "\n",
        "# 레이블링된 데이터셋 객체 생성\n",
        "train_dataset = LabeledNumpyArrayDataset(X_train, y_train)\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "test_dataset = LabeledNumpyArrayDataset(X_test, y_test)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)"
      ],
      "metadata": {
        "id": "ROX3dxWBLWA2"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x, y = next(iter(train_dataloader))\n",
        "print(x.shape)\n",
        "print(y.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l7raW0ZCF7lG",
        "outputId": "019c4c1a-f072-427b-dec1-a469c739df57"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([64, 40])\n",
            "torch.Size([64])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z0ixF4JpHiI4",
        "outputId": "9e63ac1b-91b2-4f77-c911-e2e2024c07c5"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[   0,    0,    0,  ...,  545,    8, 3098],\n",
              "        [   0,    0,    0,  ...,  702, 1602,  273],\n",
              "        [   0,    0,    0,  ..., 7938, 4401,  168],\n",
              "        ...,\n",
              "        [   0,    0,    0,  ...,    9,  123,    5],\n",
              "        [   0,    0,    0,  ...,   65,  111,  358],\n",
              "        [   0,    0,    0,  ...,    2,    5,    3]], dtype=torch.int32)"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "haA1MtU1HjVB",
        "outputId": "6e5f1244-3bf9-4595-dfa9-b6967d92408b"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1,\n",
              "        0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0,\n",
              "        0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1])"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 모델 학습 (Vanilla RNN)"
      ],
      "metadata": {
        "id": "2TMAHztyso_m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class RNNModel(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, layer_dim, output_dim, dropout_prob, device):\n",
        "        super(RNNModel, self).__init__()\n",
        "        self.device = device\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.layer_dim = layer_dim\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim) #단어 집합의 크기가 vocab_size이고 각 단어를 embedding_dim 차원의 임베딩 벡터로 매핑\n",
        "\n",
        "        self.rnn = nn.RNN(\n",
        "            embedding_dim, hidden_dim, layer_dim, batch_first=True, dropout=dropout_prob\n",
        "        )\n",
        "        # Fully connected layer\n",
        "        self.fc = nn.Linear(embedding_dim, output_dim)\n",
        "\n",
        "    def forward(self, text):\n",
        "        embedded = self.embedding(text)\n",
        "\n",
        "        h0 = torch.zeros(self.layer_dim, text.size(0), self.hidden_dim).requires_grad_()\n",
        "        #.requires_grad_() : 이 텐서를 기울기를 계산해야 하는 변수로 지정\n",
        "        h0 = h0.to(self.device)\n",
        "        out, h0 = self.rnn(embedded, h0.detach())\n",
        "        #어차피 h0.detach() 설정하여 그래디언트 계산 중지한다면..\n",
        "        #처음에 왜 requires_grad_() 한건지??\n",
        "\n",
        "        # 현재 out의 차원은 (batch_size, seq_length, hidden_size)입니다.\n",
        "        # 이를 fully connected layer에 fit하게 차원을 변경(batch_size, hidden_size)해주어야 합니다.\n",
        "        # 가장 마지막 output 선택\n",
        "        out = out[:, -1, :] # 64 x 64 size\n",
        "\n",
        "        \"\"\"\n",
        "        문제1: 이제 out을 우리가 원하는 ouput_dim 차원으로 변환해주어야 합니다.\n",
        "        빈칸에 들어갈 인스턴스 변수를 채워넣어주세요.\n",
        "        \"\"\"\n",
        "        out = self.fc(out)\n",
        "        return out"
      ],
      "metadata": {
        "id": "PaX9qNpYljQi"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**nn.embedding**\n",
        "![image.png](data:image/png;base64,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)"
      ],
      "metadata": {
        "id": "g9YguFoPDzoK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_epoch = 15\n",
        "batch_size = 64\n",
        "embedding_dim = 64\n",
        "hidden_dim = 64\n",
        "layer_dim = 1\n",
        "output_dim = 4\n",
        "\n",
        "model = RNNModel(vocab_size, embedding_dim, hidden_dim, layer_dim, output_dim, 0.5, device)\n",
        "model = model.to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr= 0.005)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()"
      ],
      "metadata": {
        "id": "yLCP04LFvZlD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9175e80f-61e1-43b2-bee4-ba50b51bd645"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
            "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "iter = 0\n",
        "for epoch in range(num_epoch):\n",
        "    for i, (text, labels) in enumerate(train_dataloader):\n",
        "        model.train()\n",
        "\n",
        "        text = text.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        logits = model(text).to(device)\n",
        "\n",
        "\n",
        "\n",
        "        \"\"\"\n",
        "        문제2: loss를 구하기 위해서 위에서 정의한 변수 중 무엇을 사용하면 될까요? 빈칸을 채워넣어주세요.\n",
        "        \"\"\"\n",
        "        loss = criterion(logits, labels)\n",
        "\n",
        "        loss.backward()\n",
        "\n",
        "        \"\"\"\n",
        "        문제3: 역전파를 거친 후 매개변수(가중치)를 업데이트하기 위해서 필요한 메서드는 무엇이었나요?\n",
        "        빈칸을 채워넣어주세요.\n",
        "        \"\"\"\n",
        "        optimizer.step()\n",
        "\n",
        "        iter += 1\n",
        "\n",
        "        if iter % 500 == 0:\n",
        "            model.eval()\n",
        "            # Calculate Accuracy\n",
        "            correct = 0\n",
        "            total = 0\n",
        "            # Iterate through test dataset\n",
        "            for test_text, test_labels in test_dataloader:\n",
        "                test_text = test_text.to(device)\n",
        "                test_labels = test_labels.to(device)\n",
        "                # Forward pass only to get logits/output\n",
        "                outputs = model(test_text)\n",
        "\n",
        "                # Get predictions from the maximum value\n",
        "                _, predicted = torch.max(outputs.data, 1)\n",
        "\n",
        "                # Total number of labels\n",
        "                total += test_labels.size(0)\n",
        "\n",
        "                # Total correct predictions\n",
        "                correct += (predicted == test_labels).sum()\n",
        "\n",
        "            accuracy = 100 * correct / total\n",
        "\n",
        "            # Print Loss\n",
        "            print('Iteration: {}. Loss: {}. Accuracy: {}'.format(iter, loss.item(), accuracy))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xFQCXBDf17hW",
        "outputId": "a8079331-ed68-4625-c20a-a6665481746e"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration: 500. Loss: 0.20847074687480927. Accuracy: 88.169677734375\n",
            "Iteration: 1000. Loss: 0.20165172219276428. Accuracy: 87.65694427490234\n",
            "Iteration: 1500. Loss: 0.2251133769750595. Accuracy: 88.14216613769531\n",
            "Iteration: 2000. Loss: 0.2152765393257141. Accuracy: 88.22720336914062\n",
            "Iteration: 2500. Loss: 0.2377128154039383. Accuracy: 88.46231079101562\n",
            "Iteration: 3000. Loss: 0.26740187406539917. Accuracy: 88.31724548339844\n",
            "Iteration: 3500. Loss: 0.20263995230197906. Accuracy: 87.94707489013672\n",
            "Iteration: 4000. Loss: 0.37734198570251465. Accuracy: 87.96708679199219\n",
            "Iteration: 4500. Loss: 0.2162439227104187. Accuracy: 87.9795913696289\n",
            "Iteration: 5000. Loss: 0.2566618323326111. Accuracy: 88.18468475341797\n",
            "Iteration: 5500. Loss: 0.26671701669692993. Accuracy: 88.36226654052734\n",
            "Iteration: 6000. Loss: 0.23056435585021973. Accuracy: 88.42729187011719\n",
            "Iteration: 6500. Loss: 0.2864078879356384. Accuracy: 87.78950500488281\n",
            "Iteration: 7000. Loss: 0.25202497839927673. Accuracy: 87.8995590209961\n",
            "Iteration: 7500. Loss: 0.3547215759754181. Accuracy: 87.52688598632812\n",
            "Iteration: 8000. Loss: 0.19541233777999878. Accuracy: 87.21174621582031\n",
            "Iteration: 8500. Loss: 0.2774474620819092. Accuracy: 87.53189086914062\n",
            "Iteration: 9000. Loss: 0.4049246609210968. Accuracy: 85.65604400634766\n",
            "Iteration: 9500. Loss: 0.2463236153125763. Accuracy: 86.61397552490234\n",
            "Iteration: 10000. Loss: 0.2555873692035675. Accuracy: 86.308837890625\n",
            "Iteration: 10500. Loss: 0.20056092739105225. Accuracy: 85.76609802246094\n",
            "Iteration: 11000. Loss: 0.23989765346050262. Accuracy: 87.45435333251953\n",
            "Iteration: 11500. Loss: 0.2402416318655014. Accuracy: 87.79701232910156\n",
            "Iteration: 12000. Loss: 0.16702887415885925. Accuracy: 87.84703063964844\n",
            "Iteration: 12500. Loss: 0.4183937907218933. Accuracy: 87.83452606201172\n",
            "Iteration: 13000. Loss: 0.2800566256046295. Accuracy: 88.21219635009766\n",
            "Iteration: 13500. Loss: 0.26240479946136475. Accuracy: 87.98709869384766\n",
            "Iteration: 14000. Loss: 0.3922768533229828. Accuracy: 87.76449584960938\n",
            "Iteration: 14500. Loss: 0.3134988248348236. Accuracy: 87.84703063964844\n",
            "Iteration: 15000. Loss: 0.21784056723117828. Accuracy: 88.1296615600586\n",
            "Iteration: 15500. Loss: 0.27590394020080566. Accuracy: 87.39933013916016\n",
            "Iteration: 16000. Loss: 0.3380581736564636. Accuracy: 86.56395721435547\n",
            "Iteration: 16500. Loss: 0.13529641926288605. Accuracy: 87.45435333251953\n",
            "Iteration: 17000. Loss: 0.321268767118454. Accuracy: 87.41934204101562\n",
            "Iteration: 17500. Loss: 0.21053297817707062. Accuracy: 88.05963134765625\n",
            "Iteration: 18000. Loss: 0.32518085837364197. Accuracy: 88.08213806152344\n",
            "Iteration: 18500. Loss: 0.3163357675075531. Accuracy: 87.50688171386719\n",
            "Iteration: 19000. Loss: 0.16554400324821472. Accuracy: 87.72947692871094\n",
            "Iteration: 19500. Loss: 0.15017817914485931. Accuracy: 87.72947692871094\n",
            "Iteration: 20000. Loss: 0.23143218457698822. Accuracy: 88.09464263916016\n",
            "Iteration: 20500. Loss: 0.12608163058757782. Accuracy: 87.95207977294922\n",
            "Iteration: 21000. Loss: 0.30927562713623047. Accuracy: 87.94957733154297\n",
            "Iteration: 21500. Loss: 0.4278658926486969. Accuracy: 87.6644515991211\n",
            "Iteration: 22000. Loss: 0.41868534684181213. Accuracy: 87.9395751953125\n",
            "Iteration: 22500. Loss: 0.19171302020549774. Accuracy: 87.71196746826172\n",
            "Iteration: 23000. Loss: 0.1690261960029602. Accuracy: 88.0121078491211\n",
            "Iteration: 23500. Loss: 0.2427709698677063. Accuracy: 88.3597640991211\n",
            "Iteration: 24000. Loss: 0.2915826737880707. Accuracy: 88.14216613769531\n",
            "Iteration: 24500. Loss: 0.25393739342689514. Accuracy: 87.86954498291016\n",
            "Iteration: 25000. Loss: 0.07935281097888947. Accuracy: 87.9295654296875\n",
            "Iteration: 25500. Loss: 0.2963983714580536. Accuracy: 87.55439758300781\n",
            "Iteration: 26000. Loss: 0.07994909584522247. Accuracy: 87.81451416015625\n",
            "Iteration: 26500. Loss: 0.38253554701805115. Accuracy: 87.2342529296875\n",
            "Iteration: 27000. Loss: 0.34900519251823425. Accuracy: 86.60647583007812\n",
            "Iteration: 27500. Loss: 0.3333488702774048. Accuracy: 87.6269302368164\n",
            "Iteration: 28000. Loss: 0.20291219651699066. Accuracy: 87.68445587158203\n",
            "Iteration: 28500. Loss: 0.2604807913303375. Accuracy: 86.86659240722656\n",
            "Iteration: 29000. Loss: 0.22466643154621124. Accuracy: 87.0091552734375\n",
            "Iteration: 29500. Loss: 0.31779658794403076. Accuracy: 87.72947692871094\n",
            "Iteration: 30000. Loss: 0.24312342703342438. Accuracy: 87.55439758300781\n",
            "Iteration: 30500. Loss: 0.21734045445919037. Accuracy: 87.84203338623047\n",
            "Iteration: 31000. Loss: 0.23463264107704163. Accuracy: 87.73197937011719\n",
            "Iteration: 31500. Loss: 0.26913294196128845. Accuracy: 87.77449798583984\n",
            "Iteration: 32000. Loss: 0.28503233194351196. Accuracy: 87.72197723388672\n",
            "Iteration: 32500. Loss: 0.27902358770370483. Accuracy: 87.62443542480469\n",
            "Iteration: 33000. Loss: 0.4234417974948883. Accuracy: 87.79701232910156\n",
            "Iteration: 33500. Loss: 0.17848366498947144. Accuracy: 87.20924377441406\n",
            "Iteration: 34000. Loss: 0.5051177144050598. Accuracy: 87.03416442871094\n",
            "Iteration: 34500. Loss: 0.1780252456665039. Accuracy: 86.73152923583984\n",
            "Iteration: 35000. Loss: 0.3660930395126343. Accuracy: 86.6614990234375\n",
            "Iteration: 35500. Loss: 0.3555006980895996. Accuracy: 86.67900848388672\n",
            "Iteration: 36000. Loss: 0.19932417571544647. Accuracy: 86.00370025634766\n",
            "Iteration: 36500. Loss: 0.299123078584671. Accuracy: 86.3813705444336\n",
            "Iteration: 37000. Loss: 0.2737323045730591. Accuracy: 86.34635925292969\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 모델학습 (LSTM, Long short term memory)"
      ],
      "metadata": {
        "id": "7C3Q3Cd3lkYw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LSTMModel(nn.Module):\n",
        "\n",
        "    def __init__(self,vocab_size, embedding_dim, hidden_dim, layer_dim, output_dim, dropout_prob, device):\n",
        "        super(LSTMModel, self).__init__()\n",
        "\n",
        "        self.device = device\n",
        "\n",
        "        # Defining the number of layers and the nodes in each layer\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.layer_dim = layer_dim\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "\n",
        "        # LSTM layers\n",
        "        self.lstm = nn.LSTM(\n",
        "            embedding_dim, hidden_dim, layer_dim, batch_first=True, dropout=dropout_prob\n",
        "        )\n",
        "\n",
        "        # Fully connected layer\n",
        "        self.fc = nn.Linear(embedding_dim, output_dim)\n",
        "\n",
        "    def forward(self, text):\n",
        "        embedded = self.embedding(text)\n",
        "\n",
        "        h0 = torch.zeros(self.layer_dim, text.size(0), self.hidden_dim).requires_grad_()\n",
        "        h0 = h0.to(self.device)\n",
        "\n",
        "        # 초기에 cell state를 영행렬로 초기화\n",
        "        c0 = torch.zeros(self.layer_dim, text.size(0), self.hidden_dim).requires_grad_()\n",
        "        c0 = c0.to(self.device)\n",
        "\n",
        "        out, (hn, cn) = self.lstm(embedded, (h0.detach(), c0.detach()))\n",
        "\n",
        "        # 현재 out의 차원은 (batch_size, seq_length, hidden_size)입니다.\n",
        "        # 이를 fully connected layer에 fit하게 차원을 변경(batch_size, hidden_size)해주어야 합니다.\n",
        "        out = out[:, -1, :]\n",
        "\n",
        "        # fc layer를 통해 (batch_size, output_dim)로 차원을 변경해줍니다.\n",
        "        out = self.fc(out)\n",
        "\n",
        "        return out"
      ],
      "metadata": {
        "id": "5j23sTsP2Sv_"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_epoch = 15\n",
        "batch_size = 64\n",
        "embedding_dim = 64\n",
        "hidden_dim = 64\n",
        "layer_dim = 1\n",
        "output_dim = 4 #2?\n",
        "\n",
        "model = LSTMModel(vocab_size, embedding_dim, hidden_dim, layer_dim, output_dim, 0.5, device)\n",
        "model = model.to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr= 0.005)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()"
      ],
      "metadata": {
        "id": "Q-PZquMSZ6Fp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9199af4e-fd43-4a4e-ad3b-47f0c0c5ad6a"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
            "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "iter = 0\n",
        "for epoch in range(num_epoch):\n",
        "    for i, (text, labels) in enumerate(train_dataloader):\n",
        "        model.train()\n",
        "\n",
        "        text = text.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        logits = model(text).to(device)\n",
        "\n",
        "        loss = criterion(logits, labels)\n",
        "        loss.backward()\n",
        "\n",
        "        # Updating parameters\n",
        "        optimizer.step()\n",
        "\n",
        "        iter += 1\n",
        "\n",
        "        if iter % 1000 == 0:\n",
        "            model.eval()\n",
        "            # Calculate Accuracy\n",
        "            correct = 0\n",
        "            total = 0\n",
        "            # Iterate through test dataset\n",
        "            for test_text, test_labels in test_dataloader:\n",
        "                test_text = test_text.to(device)\n",
        "                test_labels = test_labels.to(device)\n",
        "                # Forward pass only to get logits/output\n",
        "                outputs = model(test_text)\n",
        "\n",
        "                # Get predictions from the maximum value\n",
        "                _, predicted = torch.max(outputs.data, 1)\n",
        "\n",
        "                # Total number of labels\n",
        "                total += test_labels.size(0)\n",
        "\n",
        "                # Total correct predictions\n",
        "                correct += (predicted == test_labels).sum()\n",
        "\n",
        "            accuracy = 100 * correct / total\n",
        "\n",
        "            # Print Loss\n",
        "            print('Iteration: {}. Loss: {}. Accuracy: {}'.format(iter, loss.item(), accuracy))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dvcL3dQVYDx5",
        "outputId": "d0fa4e5e-8e3d-4ba9-b424-aeaae80d9f2a"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration: 1000. Loss: 0.15838727355003357. Accuracy: 90.31564331054688\n",
            "Iteration: 2000. Loss: 0.23593564331531525. Accuracy: 91.22354888916016\n",
            "Iteration: 3000. Loss: 0.21899352967739105. Accuracy: 91.39112854003906\n",
            "Iteration: 4000. Loss: 0.18348872661590576. Accuracy: 91.53118896484375\n",
            "Iteration: 5000. Loss: 0.10562261939048767. Accuracy: 91.54119110107422\n",
            "Iteration: 6000. Loss: 0.11862608790397644. Accuracy: 91.49617767333984\n",
            "Iteration: 7000. Loss: 0.0966348648071289. Accuracy: 91.43114471435547\n",
            "Iteration: 8000. Loss: 0.1370687633752823. Accuracy: 91.27857971191406\n",
            "Iteration: 9000. Loss: 0.1325879991054535. Accuracy: 91.26107025146484\n",
            "Iteration: 10000. Loss: 0.17780989408493042. Accuracy: 91.08599090576172\n",
            "Iteration: 11000. Loss: 0.21121563017368317. Accuracy: 90.96593475341797\n",
            "Iteration: 12000. Loss: 0.1261020451784134. Accuracy: 90.98094177246094\n",
            "Iteration: 13000. Loss: 0.056481584906578064. Accuracy: 90.93341827392578\n",
            "Iteration: 14000. Loss: 0.1327892392873764. Accuracy: 90.99344635009766\n",
            "Iteration: 15000. Loss: 0.049667615443468094. Accuracy: 91.08098602294922\n",
            "Iteration: 16000. Loss: 0.300117164850235. Accuracy: 90.58076477050781\n",
            "Iteration: 17000. Loss: 0.09006408601999283. Accuracy: 90.65079498291016\n",
            "Iteration: 18000. Loss: 0.11319158226251602. Accuracy: 90.23810577392578\n",
            "Iteration: 19000. Loss: 0.164947971701622. Accuracy: 90.63078308105469\n",
            "Iteration: 20000. Loss: 0.08354493230581284. Accuracy: 90.5982666015625\n",
            "Iteration: 21000. Loss: 0.19698503613471985. Accuracy: 90.43819427490234\n",
            "Iteration: 22000. Loss: 0.05777369439601898. Accuracy: 90.54324340820312\n",
            "Iteration: 23000. Loss: 0.010647863149642944. Accuracy: 90.40818786621094\n",
            "Iteration: 24000. Loss: 0.08525288105010986. Accuracy: 90.39318084716797\n",
            "Iteration: 25000. Loss: 0.034709107130765915. Accuracy: 90.24060821533203\n",
            "Iteration: 26000. Loss: 0.07917476445436478. Accuracy: 90.13056182861328\n",
            "Iteration: 27000. Loss: 0.03294777125120163. Accuracy: 90.29312896728516\n",
            "Iteration: 28000. Loss: 0.08566650748252869. Accuracy: 90.27062225341797\n",
            "Iteration: 29000. Loss: 0.03915569931268692. Accuracy: 90.29813385009766\n",
            "Iteration: 30000. Loss: 0.06424678862094879. Accuracy: 90.43819427490234\n",
            "Iteration: 31000. Loss: 0.12035337835550308. Accuracy: 90.15056610107422\n",
            "Iteration: 32000. Loss: 0.11065202206373215. Accuracy: 90.28312683105469\n",
            "Iteration: 33000. Loss: 0.08216328173875809. Accuracy: 89.97048950195312\n",
            "Iteration: 34000. Loss: 0.08041148632764816. Accuracy: 90.33565521240234\n",
            "Iteration: 35000. Loss: 0.02846168726682663. Accuracy: 90.31314086914062\n",
            "Iteration: 36000. Loss: 0.019110057502985. Accuracy: 90.18558502197266\n",
            "Iteration: 37000. Loss: 0.030444368720054626. Accuracy: 90.13306427001953\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "label이 0, 1뿐이면 **output_dim**을 **2**로 설정하는게 맞지 않나..?   \n",
        "or **output_dim = 1** & **nn.BCEWithLogitsLoss()**"
      ],
      "metadata": {
        "id": "v64Qwp3OMJI5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_epoch = 15\n",
        "batch_size = 64\n",
        "embedding_dim = 64\n",
        "hidden_dim = 64\n",
        "layer_dim = 1\n",
        "output_dim = 2\n",
        "\n",
        "model = LSTMModel(vocab_size, embedding_dim, hidden_dim, layer_dim, output_dim, 0.5, device)\n",
        "model = model.to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr= 0.005)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()"
      ],
      "metadata": {
        "id": "oYwS41MGL2ND"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "iter = 0\n",
        "for epoch in range(num_epoch):\n",
        "    for i, (text, labels) in enumerate(train_dataloader):\n",
        "        model.train()\n",
        "\n",
        "        text = text.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        logits = model(text).to(device)\n",
        "\n",
        "        loss = criterion(logits, labels)\n",
        "        loss.backward()\n",
        "\n",
        "        # Updating parameters\n",
        "        optimizer.step()\n",
        "\n",
        "        iter += 1\n",
        "\n",
        "        if iter % 1000 == 0:\n",
        "            model.eval()\n",
        "            # Calculate Accuracy\n",
        "            correct = 0\n",
        "            total = 0\n",
        "            # Iterate through test dataset\n",
        "            for test_text, test_labels in test_dataloader:\n",
        "                test_text = test_text.to(device)\n",
        "                test_labels = test_labels.to(device)\n",
        "                # Forward pass only to get logits/output\n",
        "                outputs = model(test_text)\n",
        "\n",
        "                # Get predictions from the maximum value\n",
        "                _, predicted = torch.max(outputs.data, 1)\n",
        "\n",
        "                # Total number of labels\n",
        "                total += test_labels.size(0)\n",
        "\n",
        "                # Total correct predictions\n",
        "                correct += (predicted == test_labels).sum()\n",
        "\n",
        "            accuracy = 100 * correct / total\n",
        "\n",
        "            # Print Loss\n",
        "            print('Iteration: {}. Loss: {}. Accuracy: {}'.format(iter, loss.item(), accuracy))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uarRx57KL3UP",
        "outputId": "c0ebde92-f492-414a-934a-19da53efef64"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration: 1000. Loss: 0.1879459023475647. Accuracy: 90.13056182861328\n",
            "Iteration: 2000. Loss: 0.1423189491033554. Accuracy: 91.19603729248047\n",
            "Iteration: 3000. Loss: 0.3947441279888153. Accuracy: 91.23855590820312\n",
            "Iteration: 4000. Loss: 0.16750533878803253. Accuracy: 91.50117492675781\n",
            "Iteration: 5000. Loss: 0.1264519989490509. Accuracy: 91.47116088867188\n",
            "Iteration: 6000. Loss: 0.2065981924533844. Accuracy: 91.42613983154297\n",
            "Iteration: 7000. Loss: 0.2191588431596756. Accuracy: 91.4111328125\n",
            "Iteration: 8000. Loss: 0.248113751411438. Accuracy: 91.20854187011719\n",
            "Iteration: 9000. Loss: 0.26823821663856506. Accuracy: 91.04096984863281\n",
            "Iteration: 10000. Loss: 0.09668954461812973. Accuracy: 91.26856994628906\n",
            "Iteration: 11000. Loss: 0.04609474539756775. Accuracy: 91.09849548339844\n",
            "Iteration: 12000. Loss: 0.16489103436470032. Accuracy: 90.92341613769531\n",
            "Iteration: 13000. Loss: 0.0997127965092659. Accuracy: 91.01596069335938\n",
            "Iteration: 14000. Loss: 0.03291650861501694. Accuracy: 90.7158203125\n",
            "Iteration: 15000. Loss: 0.07050096243619919. Accuracy: 90.8333740234375\n",
            "Iteration: 16000. Loss: 0.054048504680395126. Accuracy: 90.85839080810547\n",
            "Iteration: 17000. Loss: 0.379537969827652. Accuracy: 90.8333740234375\n",
            "Iteration: 18000. Loss: 0.04787250608205795. Accuracy: 90.5357437133789\n",
            "Iteration: 19000. Loss: 0.09927181154489517. Accuracy: 90.4131851196289\n",
            "Iteration: 20000. Loss: 0.1305140107870102. Accuracy: 90.61577606201172\n",
            "Iteration: 21000. Loss: 0.1120411828160286. Accuracy: 90.38817596435547\n",
            "Iteration: 22000. Loss: 0.072560615837574. Accuracy: 90.46070861816406\n",
            "Iteration: 23000. Loss: 0.06977660208940506. Accuracy: 90.56075286865234\n",
            "Iteration: 24000. Loss: 0.053877074271440506. Accuracy: 90.63328552246094\n",
            "Iteration: 25000. Loss: 0.09601585566997528. Accuracy: 90.65829467773438\n",
            "Iteration: 26000. Loss: 0.0763326957821846. Accuracy: 90.54324340820312\n",
            "Iteration: 27000. Loss: 0.07672203332185745. Accuracy: 90.47321319580078\n",
            "Iteration: 28000. Loss: 0.06819310039281845. Accuracy: 90.6758041381836\n",
            "Iteration: 29000. Loss: 0.1874975860118866. Accuracy: 90.43569946289062\n",
            "Iteration: 30000. Loss: 0.03325051814317703. Accuracy: 90.4957275390625\n",
            "Iteration: 31000. Loss: 0.12601006031036377. Accuracy: 90.20309448242188\n",
            "Iteration: 32000. Loss: 0.10719487816095352. Accuracy: 90.27812957763672\n",
            "Iteration: 33000. Loss: 0.036805953830480576. Accuracy: 90.22560119628906\n",
            "Iteration: 34000. Loss: 0.04314480721950531. Accuracy: 90.24060821533203\n",
            "Iteration: 35000. Loss: 0.03152500092983246. Accuracy: 90.24311065673828\n",
            "Iteration: 36000. Loss: 0.06439772993326187. Accuracy: 90.21309661865234\n",
            "Iteration: 37000. Loss: 0.08180133253335953. Accuracy: 90.2506103515625\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 모델 학습 (Gated Recurrent Unit (GRU))"
      ],
      "metadata": {
        "id": "CEmWGtFslpiz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GRUModel(nn.Module):\n",
        "\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, layer_dim, output_dim, dropout_prob, device):\n",
        "        super(GRUModel, self).__init__()\n",
        "\n",
        "        # Defining the number of layers and the nodes in each layer\n",
        "        self.device = device\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.layer_dim = layer_dim\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "\n",
        "        # GRU layers\n",
        "        self.gru = nn.GRU(\n",
        "            embedding_dim, hidden_dim, layer_dim, batch_first=True, dropout=dropout_prob\n",
        "        )\n",
        "\n",
        "        # Fully connected layer\n",
        "        self.fc = nn.Linear(embedding_dim, output_dim)\n",
        "\n",
        "    def forward(self, text):\n",
        "\n",
        "        embedded = self.embedding(text)\n",
        "\n",
        "        # Initializing hidden state for first input with zeros\n",
        "        h0 = torch.zeros(self.layer_dim, text.size(0), self.hidden_dim).requires_grad_()\n",
        "        h0 = h0.to(self.device)\n",
        "\n",
        "        # Forward propagation by passing in the input and hidden state into the model\n",
        "        out, _ = self.gru(embedded, h0.detach())\n",
        "\n",
        "        # Reshaping the outputs in the shape of (batch_size, seq_length, hidden_size)\n",
        "        # so that it can fit into the fully connected layer\n",
        "        out = out[:, -1, :]\n",
        "\n",
        "        # Convert the final state to our desired output shape (batch_size, output_dim)\n",
        "        out = self.fc(out)\n",
        "\n",
        "        return out\n"
      ],
      "metadata": {
        "id": "zpaGyyY2ltWF"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_epoch = 15\n",
        "batch_size = 64\n",
        "embedding_dim = 64\n",
        "hidden_dim = 64\n",
        "layer_dim = 1\n",
        "output_dim = 4\n",
        "\n",
        "model = GRUModel(vocab_size, embedding_dim, hidden_dim, layer_dim, output_dim, 0.5, device)\n",
        "model = model.to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr= 0.005)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()"
      ],
      "metadata": {
        "id": "q59GPyZSaZjX"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "iter = 0\n",
        "for epoch in range(num_epoch):\n",
        "    for i, (text, labels) in enumerate(train_dataloader):\n",
        "        model.train()\n",
        "\n",
        "        text = text.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        logits = model(text).to(device)\n",
        "        # Calculate Loss: softmax --> cross entropy loss\n",
        "        loss = criterion(logits, labels)\n",
        "        loss.backward()\n",
        "\n",
        "        # Updating parameters\n",
        "        optimizer.step()\n",
        "\n",
        "        iter += 1\n",
        "\n",
        "        if iter % 1000 == 0:\n",
        "            model.eval()\n",
        "            # Calculate Accuracy\n",
        "            correct = 0\n",
        "            total = 0\n",
        "            # Iterate through test dataset\n",
        "            for test_text, test_labels in test_dataloader:\n",
        "                test_text = test_text.to(device)\n",
        "                test_labels = test_labels.to(device)\n",
        "                # Forward pass only to get logits/output\n",
        "                outputs = model(test_text)\n",
        "\n",
        "                # Get predictions from the maximum value\n",
        "                _, predicted = torch.max(outputs.data, 1)\n",
        "\n",
        "                # Total number of labels\n",
        "                total += test_labels.size(0)\n",
        "\n",
        "                # Total correct predictions\n",
        "                correct += (predicted == test_labels).sum()\n",
        "\n",
        "            accuracy = 100 * correct / total\n",
        "\n",
        "            # Print Loss\n",
        "            print('Iteration: {}. Loss: {}. Accuracy: {}'.format(iter, loss.item(), accuracy))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gLd0utoBacFW",
        "outputId": "a718674c-9711-4e87-ec28-b699186abd22"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration: 1000. Loss: 0.29596734046936035. Accuracy: 90.49322509765625\n",
            "Iteration: 2000. Loss: 0.2221606969833374. Accuracy: 91.08849334716797\n",
            "Iteration: 3000. Loss: 0.1845347285270691. Accuracy: 91.3711166381836\n",
            "Iteration: 4000. Loss: 0.11902143806219101. Accuracy: 91.02596282958984\n",
            "Iteration: 5000. Loss: 0.23776155710220337. Accuracy: 91.28107452392578\n",
            "Iteration: 6000. Loss: 0.03446044772863388. Accuracy: 91.36361694335938\n",
            "Iteration: 7000. Loss: 0.19462139904499054. Accuracy: 91.10599517822266\n",
            "Iteration: 8000. Loss: 0.14771480858325958. Accuracy: 90.85338592529297\n",
            "Iteration: 9000. Loss: 0.182330921292305. Accuracy: 91.10099792480469\n",
            "Iteration: 10000. Loss: 0.14506076276302338. Accuracy: 90.88089752197266\n",
            "Iteration: 11000. Loss: 0.0778849869966507. Accuracy: 90.54574584960938\n",
            "Iteration: 12000. Loss: 0.17788353562355042. Accuracy: 90.9109115600586\n",
            "Iteration: 13000. Loss: 0.24977344274520874. Accuracy: 90.78585815429688\n",
            "Iteration: 14000. Loss: 0.1578587293624878. Accuracy: 90.6482925415039\n",
            "Iteration: 15000. Loss: 0.19678229093551636. Accuracy: 90.54324340820312\n",
            "Iteration: 16000. Loss: 0.17515602707862854. Accuracy: 89.82792663574219\n",
            "Iteration: 17000. Loss: 0.058135196566581726. Accuracy: 90.26811981201172\n",
            "Iteration: 18000. Loss: 0.07116945087909698. Accuracy: 90.42819213867188\n",
            "Iteration: 19000. Loss: 0.12110001593828201. Accuracy: 90.44820404052734\n",
            "Iteration: 20000. Loss: 0.11536829173564911. Accuracy: 90.3681640625\n",
            "Iteration: 21000. Loss: 0.08115585148334503. Accuracy: 90.2606201171875\n",
            "Iteration: 22000. Loss: 0.21564380824565887. Accuracy: 90.40818786621094\n",
            "Iteration: 23000. Loss: 0.07807998359203339. Accuracy: 90.32064819335938\n",
            "Iteration: 24000. Loss: 0.12702400982379913. Accuracy: 90.47571563720703\n",
            "Iteration: 25000. Loss: 0.16012203693389893. Accuracy: 90.15056610107422\n",
            "Iteration: 26000. Loss: 0.10780581831932068. Accuracy: 90.29063415527344\n",
            "Iteration: 27000. Loss: 0.163776233792305. Accuracy: 89.98799896240234\n",
            "Iteration: 28000. Loss: 0.1524306684732437. Accuracy: 89.91796112060547\n",
            "Iteration: 29000. Loss: 0.13328228890895844. Accuracy: 90.05052185058594\n",
            "Iteration: 30000. Loss: 0.09876896440982819. Accuracy: 89.88294982910156\n",
            "Iteration: 31000. Loss: 0.18309913575649261. Accuracy: 90.01300811767578\n",
            "Iteration: 32000. Loss: 0.12714733183383942. Accuracy: 89.9529800415039\n",
            "Iteration: 33000. Loss: 0.05543147400021553. Accuracy: 89.54029083251953\n",
            "Iteration: 34000. Loss: 0.05618460103869438. Accuracy: 90.0255126953125\n",
            "Iteration: 35000. Loss: 0.07903633266687393. Accuracy: 89.76539611816406\n",
            "Iteration: 36000. Loss: 0.14140404760837555. Accuracy: 89.76289367675781\n",
            "Iteration: 37000. Loss: 0.1812649965286255. Accuracy: 90.04801940917969\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 모델 성능 비교"
      ],
      "metadata": {
        "id": "mjuXGLI7eA2X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "옵티마이저, 파라미터 등을 바꿔가며 모델의 성능을 향상시켜보세요.\n",
        "\n",
        "이후 세 가지 모델의 성능 차이를 비교하고, 자유롭게 해석해보세요!"
      ],
      "metadata": {
        "id": "cH2yfHfheC-l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "test accuracy 기준으로 비교했을 때, RNN의 성능이 가장 낮고 LSTM과 GRU의 성능은 비슷함. LSTM, GRU는 게이트 메커니즘을 사용하여 장기 의존성을 더 잘 학습할 수 있기 때문에 더 나은 성능을 보였을 것. (데이터가 장기 의존성이 중요한 경우)"
      ],
      "metadata": {
        "id": "QAGY94vyNDg2"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nPXaC8hyODno"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}