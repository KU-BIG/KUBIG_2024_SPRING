{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# < 3주차 과제 1 : MLP 마음대로 다뤄보기>\n","- dataset을 임의로 선정해서 직접 분석 해보기(제공한 코드를 활용해서 해보기)\n","- activation functions 중 relu사용시 함수 직접 정의\n","- lr, optimizer 등 바꿔보기\n","- hidden layer/neuron 수를 바꾸기\n","- 전처리도 추가\n","- 모든 시도를 올려주세요!\n","- 제일 높은 acc를 보인 시도를 명시해주세요!\n"],"metadata":{"id":"sgAYo4nrw2F4"}},{"cell_type":"code","execution_count":1,"metadata":{"id":"fX437IL6qbI-","executionInfo":{"status":"ok","timestamp":1706621108718,"user_tz":-540,"elapsed":1856,"user":{"displayName":"Insu Shin","userId":"15408006383984625639"}}},"outputs":[],"source":["import torch\n","from torch import nn, optim\n","from sklearn.datasets import load_breast_cancer\n","from torch.utils.data import  TensorDataset, DataLoader\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score\n","import numpy as np\n","from sklearn.preprocessing import StandardScaler\n","import matplotlib.pyplot as plt"]},{"cell_type":"markdown","source":["## 아래 데이터셋 중 원하는 데이터셋 하나를 선택하여, 코드 과제 진행하기!\n","- 1) load_digits() <br>\n","- 2) load_wine()"],"metadata":{"id":"oxkFzBDNmWNk"}},{"cell_type":"markdown","source":["\"인수\" 라고 써있는 코드만 돌릴 것!"],"metadata":{"id":"2c8spvxFOzts"}},{"cell_type":"code","source":["# 데이터셋 종류 :\n","data = load_breast_cancer()"],"metadata":{"id":"FywYbfsKtjcR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#  인수\n","from sklearn.datasets import load_wine, load_digits\n","data2 = load_wine()"],"metadata":{"id":"kFoarsJOETOb","executionInfo":{"status":"ok","timestamp":1706621113006,"user_tz":-540,"elapsed":568,"user":{"displayName":"Insu Shin","userId":"15408006383984625639"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["input = data.data\n","output = data.target"],"metadata":{"id":"C2P0hqZ9yBGm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 인수\n","input = data2.data\n","output = data2.target"],"metadata":{"id":"Pf5dyCCMG7kM","executionInfo":{"status":"ok","timestamp":1706621114851,"user_tz":-540,"elapsed":7,"user":{"displayName":"Insu Shin","userId":"15408006383984625639"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["# 인수\n","len(input)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MZHd-G4mFCOT","executionInfo":{"status":"ok","timestamp":1706621117702,"user_tz":-540,"elapsed":630,"user":{"displayName":"Insu Shin","userId":"15408006383984625639"}},"outputId":"e7a2424f-dbe8-4469-8c52-52b74242c4bb"},"execution_count":4,"outputs":[{"output_type":"execute_result","data":{"text/plain":["178"]},"metadata":{},"execution_count":4}]},{"cell_type":"code","source":["# 인수\n","input"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ouOCQPRxJOXr","executionInfo":{"status":"ok","timestamp":1706621118677,"user_tz":-540,"elapsed":13,"user":{"displayName":"Insu Shin","userId":"15408006383984625639"}},"outputId":"d9ddf8b6-f1ff-4a80-c220-fe27578c6903"},"execution_count":5,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[1.423e+01, 1.710e+00, 2.430e+00, ..., 1.040e+00, 3.920e+00,\n","        1.065e+03],\n","       [1.320e+01, 1.780e+00, 2.140e+00, ..., 1.050e+00, 3.400e+00,\n","        1.050e+03],\n","       [1.316e+01, 2.360e+00, 2.670e+00, ..., 1.030e+00, 3.170e+00,\n","        1.185e+03],\n","       ...,\n","       [1.327e+01, 4.280e+00, 2.260e+00, ..., 5.900e-01, 1.560e+00,\n","        8.350e+02],\n","       [1.317e+01, 2.590e+00, 2.370e+00, ..., 6.000e-01, 1.620e+00,\n","        8.400e+02],\n","       [1.413e+01, 4.100e+00, 2.740e+00, ..., 6.100e-01, 1.600e+00,\n","        5.600e+02]])"]},"metadata":{},"execution_count":5}]},{"cell_type":"code","source":["# 인수\n","len(input[0])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"d_OWBkLbJpGh","executionInfo":{"status":"ok","timestamp":1706621121398,"user_tz":-540,"elapsed":9,"user":{"displayName":"Insu Shin","userId":"15408006383984625639"}},"outputId":"4a1fd194-cb62-4925-d0d5-46da0fb4411e"},"execution_count":6,"outputs":[{"output_type":"execute_result","data":{"text/plain":["13"]},"metadata":{},"execution_count":6}]},{"cell_type":"markdown","source":["변수 종류는 13개\n","- Alcohol\n","- Malic acid\n","- Ash\n","- Alcalinity of ash\n","- Magnesium\n","- Total phenols\n","- Flavanoids\n","- Nonflavanoid phenols\n","- Proanthocyanins\n","- Color intensity\n","- Hue\n","- OD280/OD315 of diluted wines\n","- Proline"],"metadata":{"id":"HOI4_rCkJvOV"}},{"cell_type":"code","source":["# 인수\n","len(output)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"38HM3ifgFEzM","executionInfo":{"status":"ok","timestamp":1706621124698,"user_tz":-540,"elapsed":808,"user":{"displayName":"Insu Shin","userId":"15408006383984625639"}},"outputId":"f069a5ae-f603-40d3-e87c-fbe5371e9502"},"execution_count":7,"outputs":[{"output_type":"execute_result","data":{"text/plain":["178"]},"metadata":{},"execution_count":7}]},{"cell_type":"code","source":["# 인수\n","# 3 classes\n","output"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VT_pf-sRJTiT","executionInfo":{"status":"ok","timestamp":1706621126473,"user_tz":-540,"elapsed":12,"user":{"displayName":"Insu Shin","userId":"15408006383984625639"}},"outputId":"82296c3d-7dff-40ae-81a2-ac7047d77dfa"},"execution_count":8,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1,\n","       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2,\n","       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n","       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n","       2, 2])"]},"metadata":{},"execution_count":8}]},{"cell_type":"code","source":["# 인수\n","# GPU에 쓸 것인가 CPU에 쓸 것인가\n","device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","\n","torch.manual_seed(777)\n","if device == \"cuda\":\n","  torch.cuda.manual_seed_all(777)"],"metadata":{"id":"SggpQfSPt85C","executionInfo":{"status":"ok","timestamp":1706621128901,"user_tz":-540,"elapsed":592,"user":{"displayName":"Insu Shin","userId":"15408006383984625639"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["# skip 해라\n","x_train, x_test, y_train, y_test = train_test_split(input, output, test_size = 0.3, random_state = 42, stratify= data.target, shuffle = True)\n","\n","x_train = torch.FloatTensor(x_train).to(device)\n","y_train = torch.LongTensor(y_train).to(device)\n","x_test = torch.FloatTensor(x_test)\n","y_test = torch.LongTensor(y_test)\n","\n","# 데이터를 tensor로 바꿔주고 gpu 연산이 가능해지도록 gpu에 옮김\n","# label 값을 왜 long 에 옮겨놓는가? loss function이 다르기 때문"],"metadata":{"id":"bLMzf-2ntYeX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 인수\n","x_train, x_test, y_train, y_test = train_test_split(input, output, test_size = 0.3, random_state = 42, stratify= data2.target, shuffle = True)\n","\n","x_train = torch.FloatTensor(x_train).to(device)\n","y_train = torch.LongTensor(y_train).to(device)\n","x_test = torch.FloatTensor(x_test)\n","y_test = torch.LongTensor(y_test)"],"metadata":{"id":"GUEQ1MEHNiiW","executionInfo":{"status":"ok","timestamp":1706621131814,"user_tz":-540,"elapsed":439,"user":{"displayName":"Insu Shin","userId":"15408006383984625639"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["print(x_train[0])\n","print(y_train[0])\n","\n","#input 30개 (속성이 30개)\n","#y의 class는 2개 (양성과 음성)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"umEdiTZkrVqS","outputId":"36b23f6d-93ed-4480-8218-72d21bb309c7","executionInfo":{"status":"ok","timestamp":1706621135102,"user_tz":-540,"elapsed":563,"user":{"displayName":"Insu Shin","userId":"15408006383984625639"}}},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([1.3750e+01, 1.7300e+00, 2.4100e+00, 1.6000e+01, 8.9000e+01, 2.6000e+00,\n","        2.7600e+00, 2.9000e-01, 1.8100e+00, 5.6000e+00, 1.1500e+00, 2.9000e+00,\n","        1.3200e+03], device='cuda:0')\n","tensor(0, device='cuda:0')\n"]}]},{"cell_type":"code","source":["# 인수\n","# array object를 줬는데, 실제로 뭐가 들어있는가\n","print(x_train[0])\n","print(y_train[0])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UW8FcSDOPcOJ","executionInfo":{"status":"ok","timestamp":1706621137046,"user_tz":-540,"elapsed":9,"user":{"displayName":"Insu Shin","userId":"15408006383984625639"}},"outputId":"04cf1f02-7422-4b32-80a6-6e1ca0e81e3e"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([1.3750e+01, 1.7300e+00, 2.4100e+00, 1.6000e+01, 8.9000e+01, 2.6000e+00,\n","        2.7600e+00, 2.9000e-01, 1.8100e+00, 5.6000e+00, 1.1500e+00, 2.9000e+00,\n","        1.3200e+03], device='cuda:0')\n","tensor(0, device='cuda:0')\n"]}]},{"cell_type":"markdown","source":["파이토치에서는 데이터셋을 좀 더 쉽게 다룰 수 있도록 유용한 도구로서 torch.utils.data.Dataset과 torch.utils.data.DataLoader를 제공합니다. 이를 사용하면 미니 배치 학습, 데이터 셔플(shuffle), 병렬 처리까지 간단히 수행할 수 있습니다. 기본적인 사용 방법은 Dataset을 정의하고, 이를 DataLoader에 전달하는 것\n","- init : class 에서 객체가 생성되면 바로 실행되는 함수\n","- len : observation 수를 정의하는 함수\n","- getitem : iteration 마다 해당하는 데이터를 돌려주는 함수"],"metadata":{"id":"combmxzmYFyn"}},{"cell_type":"code","source":["# 인수\n","# pytorch dataset 모듈을 이용해서 돌리는 방법. 알아둘 것\n","from torch.utils.data import Dataset\n","class CustomDataset(Dataset):\n","  def __init__(self):\n","    self.x_data = x_train\n","    self.y_data = [[y] for y in y_train]\n","#  데이터셋의 전처리를 해주는 부분\n","\n","  def __len__(self):\n","    return len(self.x_data)\n","#  데이터셋의 길이. 즉, 총 샘플의 수를 적어주는 부분\n","\n","  def __getitem__(self, idx):\n","    x = torch.FloatTensor(self.x_data[idx]).to(device)\n","    y = torch.LongTensor(self.y_data[idx]).to(device)\n","#  데이터셋에서 특정 1개의 샘플을 가져오는 함수\n","\n","    return x,y"],"metadata":{"id":"y38TlgXoqV5Z","executionInfo":{"status":"ok","timestamp":1706621142162,"user_tz":-540,"elapsed":416,"user":{"displayName":"Insu Shin","userId":"15408006383984625639"}}},"execution_count":13,"outputs":[]},{"cell_type":"code","source":["# 인수\n","# dataloader를 이용해서 batch 별로 불러와서 학습시킴\n","\n","batch_size = 8\n","\n","dataset = CustomDataset()\n","dataloader = DataLoader(dataset, batch_size=batch_size)"],"metadata":{"id":"x8VHwnuFqino","executionInfo":{"status":"ok","timestamp":1706621145695,"user_tz":-540,"elapsed":419,"user":{"displayName":"Insu Shin","userId":"15408006383984625639"}}},"execution_count":14,"outputs":[]},{"cell_type":"code","source":["# skip 해라\n","# node 수가 breast_cancer data 기준으로 짜여져 있다. 이 코드는 돌리지 말 것\n","\n","# input_layer와 hidden_layer의 노드 개수를 어떻게 바꿔줘야할까?\n","# hidden layer/neuron 수를 바꾸기\n","\n","model = nn.Sequential(\n","          nn.Linear(30,398, bias=True),\n","          nn.Sigmoid(),\n","          nn.Linear(398,15, bias=True),\n","          nn.Sigmoid(),\n","          nn.Linear(15,5, bias=True),\n","          nn.Softmax()\n","          ).to(device)"],"metadata":{"id":"C6V7a4tyq6Jc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 인수\n","# input이 변수 13개, output이 category 3개\n","# GPT의 도움을 받음\n","\n","# 마지막에 softmax 함수를 이용해야 확률이 함수값으로 바뀜.\n","\n","# Define the number of input features and output classes\n","input_size = 13  # 13 features in the Wine dataset\n","output_size = 3  # 3 classes in the Wine dataset\n","\n","# Modify the number of hidden layers and nodes as needed\n","hidden_size1 = 500\n","hidden_size2 = 27\n","\n","# Define the model\n","model = nn.Sequential(\n","    nn.Linear(input_size, hidden_size1, bias=True),\n","    nn.Sigmoid(),  # You can use different activation functions, e.g., nn.ReLU()\n","    nn.Linear(hidden_size1, hidden_size2, bias=True),\n","    nn.Sigmoid(),\n","    nn.Linear(hidden_size2, output_size, bias=True),\n","    nn.Softmax()  # Softmax along dimension 1 (assuming a classification task)\n",").to(device)"],"metadata":{"id":"Tf3KbkvjKufR","executionInfo":{"status":"ok","timestamp":1706621156956,"user_tz":-540,"elapsed":649,"user":{"displayName":"Insu Shin","userId":"15408006383984625639"}}},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":["Note:\n","classification 문제의 경우 주로 마지막 노드는 softmax activation function으로 마무리하게 된다.\n","\n"," https://developers.google.com/machine-learning/crash-course/multi-class-neural-networks/softmax?hl=ko\n","\n"," 무려 구글이 그렇게 한다고 한단다. 그 이유는 nn.Linear은 어쨋든 선형회귀 방정식으로 설계가 되는데, 이것을 확률로 바꾸기 위해서는 Softmax가 필요하게 된다."],"metadata":{"id":"LjwoZRTsRRbu"}},{"cell_type":"markdown","source":["class로 구현 가능\n","- init : 초기 생성 함수\n","- foward : 순전파(입력값 => 예측값 의 과정)"],"metadata":{"id":"07uV8RY7Yr_5"}},{"cell_type":"code","source":["class Model(torch.nn.Module):\n","\n","  def __init__(self):\n","    super(Model, self).__init__()\n","    self.layer1 = nn.Sequential(\n","          nn.Linear(13,64, bias=True), # input_layer = 30, hidden_layer1 = 398 # 인수 input_layer = 13, hidden layer 64\n","          nn.Sigmoid(),\n","          nn.BatchNorm1d(64) # 인수: hidden layer과 동일한 값\n","    )\n","  # activation function 이용\n","  #   nn.ReLU()\n","  #   nn.tanH()\n","  #   https://pytorch.org/docs/stable/nn.html 그 외에도 여기서 확인 가능함\n","  #   파라미터가 필요하지 않다는 것이 특징\n","\n","  # batch normazliation 1d, 파라미터 값으로 vector의 길이를 전해줌\n","  # 추후에 이미지를 다루게 된다면, 그때는 batch normalization 2d를 이용하게 됨\n","  # 그때는 파라미터 값으로 채널, 가로, 세로 길이를 전달해주게 됨\n","\n","    self.layer2 = nn.Sequential(\n","        nn.Linear(64,32, bias=True), # hidden_layer1 = 398, hidden_layer2 = 15 # 인수 hidden layer 1 : 64, hidden layer 2 : 32\n","        nn.Sigmoid()\n","    )\n","    self.layer3 = nn.Sequential(\n","          nn.Linear(32,6, bias=True), # hidden_layer1 = 15, hidden_layer2 = 10 # hidden layer 2; 32, hidden layer 3: 6\n","        nn.Sigmoid()\n","    )\n","    self.layer4 = nn.Sequential(\n","        nn.Linear(6, 3, bias=True), # hidden_layer3 = 10, output_layer = 5\n","        nn.Softmax()\n","    )\n","\n","  def forward(self,x):\n","    output = self.layer1(x)\n","    output = self.layer2(output)\n","    output = self.layer3(output)\n","    output = self.layer4(output)\n","    return output"],"metadata":{"id":"a0zLstbMqxEZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def init_weights(layer):\n","    if isinstance(layer, nn.Linear):\n","        torch.nn.init.xavier_uniform(layer.weight)\n","        layer.bias.data.fill_(0.01)\n","\n","        #xavier사용\n","        # Layer의 weight를 어떤 분포를 가지도록 초기화시켜줌+global minimum찾기 위해서"],"metadata":{"id":"kqcqqkECrSGK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 인수\n","# 클래스로 직접 짠 모형\n","model2 = Model().to(device)\n","model2.apply(init_weights)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oMDUBFg6rUpw","outputId":"77760278-62b5-4dc9-e940-c722c41df8d6","executionInfo":{"status":"ok","timestamp":1706618087324,"user_tz":-540,"elapsed":5,"user":{"displayName":"Insu Shin","userId":"15408006383984625639"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-17-6196a47a4462>:3: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n","  torch.nn.init.xavier_uniform(layer.weight)\n"]},{"output_type":"execute_result","data":{"text/plain":["Model(\n","  (layer1): Sequential(\n","    (0): Linear(in_features=13, out_features=64, bias=True)\n","    (1): Sigmoid()\n","    (2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","  )\n","  (layer2): Sequential(\n","    (0): Linear(in_features=64, out_features=32, bias=True)\n","    (1): Sigmoid()\n","  )\n","  (layer3): Sequential(\n","    (0): Linear(in_features=32, out_features=6, bias=True)\n","    (1): Sigmoid()\n","  )\n","  (layer4): Sequential(\n","    (0): Linear(in_features=6, out_features=3, bias=True)\n","    (1): Softmax(dim=None)\n","  )\n",")"]},"metadata":{},"execution_count":18}]},{"cell_type":"code","source":["# 인수\n","# dataloader를 이용해서 만든 모형\n","print(model)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pNflwwSTWX1b","executionInfo":{"status":"ok","timestamp":1706621170449,"user_tz":-540,"elapsed":717,"user":{"displayName":"Insu Shin","userId":"15408006383984625639"}},"outputId":"644819fd-8b73-4e95-d4e3-069ef34adfdb"},"execution_count":16,"outputs":[{"output_type":"stream","name":"stdout","text":["Sequential(\n","  (0): Linear(in_features=13, out_features=500, bias=True)\n","  (1): Sigmoid()\n","  (2): Linear(in_features=500, out_features=27, bias=True)\n","  (3): Sigmoid()\n","  (4): Linear(in_features=27, out_features=3, bias=True)\n","  (5): Softmax(dim=None)\n",")\n"]}]},{"cell_type":"code","source":["# 인수\n","print(model2)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OwZt5CetrYFb","outputId":"279bbe17-3dfb-47ce-ebb4-b78166fe20e2","executionInfo":{"status":"ok","timestamp":1706618121270,"user_tz":-540,"elapsed":436,"user":{"displayName":"Insu Shin","userId":"15408006383984625639"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Model(\n","  (layer1): Sequential(\n","    (0): Linear(in_features=13, out_features=64, bias=True)\n","    (1): Sigmoid()\n","    (2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","  )\n","  (layer2): Sequential(\n","    (0): Linear(in_features=64, out_features=32, bias=True)\n","    (1): Sigmoid()\n","  )\n","  (layer3): Sequential(\n","    (0): Linear(in_features=32, out_features=6, bias=True)\n","    (1): Sigmoid()\n","  )\n","  (layer4): Sequential(\n","    (0): Linear(in_features=6, out_features=3, bias=True)\n","    (1): Softmax(dim=None)\n","  )\n",")\n"]}]},{"cell_type":"code","source":["# 인수\n","model.parameters()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jDDUJrQ0MIhP","executionInfo":{"status":"ok","timestamp":1706621174600,"user_tz":-540,"elapsed":434,"user":{"displayName":"Insu Shin","userId":"15408006383984625639"}},"outputId":"74692dc7-3af8-4ecd-94f4-4856ea0eed5f"},"execution_count":17,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<generator object Module.parameters at 0x788c4b89bd80>"]},"metadata":{},"execution_count":17}]},{"cell_type":"code","source":["# 인수\n","model2.parameters()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"A4xICx-0WcbK","executionInfo":{"status":"ok","timestamp":1706618254714,"user_tz":-540,"elapsed":3,"user":{"displayName":"Insu Shin","userId":"15408006383984625639"}},"outputId":"b70ce7e2-e16c-4691-fe81-e8672a584c99"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<generator object Module.parameters at 0x7ab6e0d22490>"]},"metadata":{},"execution_count":30}]},{"cell_type":"code","source":["loss_fn  = nn.CrossEntropyLoss().to(device)\n","\n","# 여러가지 optimizer 시도해보기\n","# lr 바꿔보기\n","\n","\n","optimizer = optim.Adam(model.parameters(), lr= 0.01)\n","\n","# 인수\n","# optimizer = optim.SGD(model.parameters(), lr = 0.5, momentum = 0.1)\n","\n","# https://pytorch.org/docs/stable/optim.html#module-torch.optim 페이지 참조\n","# optimizer = optim.SGD(model.parameters(), lr=0.1, momentum=0.9)\n","# sgd 등등"],"metadata":{"id":"AYFp-eTErh7b","executionInfo":{"status":"ok","timestamp":1706621178206,"user_tz":-540,"elapsed":1929,"user":{"displayName":"Insu Shin","userId":"15408006383984625639"}}},"execution_count":18,"outputs":[]},{"cell_type":"markdown","source":["SGD 보다는 Adam이 더 잘 나오는 것 같다"],"metadata":{"id":"cT-eMSNTSx_b"}},{"cell_type":"code","source":["# 인수\n","# model\n","losses = []\n","for epoch in range(1000): # 인수: change 100 to 1000\n","\n","  optimizer.zero_grad()\n","  hypothesis = model(x_train)\n","\n","  # 비용 함수\n","  cost = loss_fn(hypothesis, y_train)\n","  cost.backward()\n","  optimizer.step()\n","  losses.append(cost.item())\n","\n","  if epoch % 10 == 0:\n","    print(epoch, cost.item())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"90QxHvlIrjS7","outputId":"6f2ad40f-69a9-4b3e-9912-2064c1346fef","executionInfo":{"status":"ok","timestamp":1706621197519,"user_tz":-540,"elapsed":2015,"user":{"displayName":"Insu Shin","userId":"15408006383984625639"}}},"execution_count":20,"outputs":[{"output_type":"stream","name":"stdout","text":["0 0.6654204726219177\n","10 0.6335322856903076\n","20 0.6647904515266418\n","30 0.640836238861084\n","40 0.6146565675735474\n","50 0.6096700429916382\n","60 0.6066498756408691\n","70 0.5987682938575745\n","80 0.5928642153739929\n","90 0.5874531269073486\n","100 0.5876943469047546\n","110 0.5844358205795288\n","120 0.5897173881530762\n","130 0.5843123197555542\n","140 0.5833465456962585\n","150 0.5858109593391418\n","160 0.6323438882827759\n","170 0.6675360202789307\n","180 0.6050803661346436\n","190 0.6008906960487366\n","200 0.5988869071006775\n","210 0.5852044820785522\n","220 0.5898149609565735\n","230 0.5887541770935059\n","240 0.6047810316085815\n","250 0.8482786417007446\n","260 0.706588089466095\n","270 0.6245086193084717\n","280 0.6172299385070801\n","290 0.6003454327583313\n","300 0.5926333069801331\n","310 0.5978312492370605\n","320 0.5871542096138\n","330 0.5861281156539917\n","340 0.587567925453186\n","350 0.5841338634490967\n","360 0.5831354260444641\n","370 0.582676351070404\n","380 0.5810275077819824\n","390 0.5789951682090759\n","400 0.5883436799049377\n","410 0.5758383274078369\n","420 0.5998111963272095\n","430 0.5898849964141846\n","440 0.6103910803794861\n","450 0.6341637372970581\n","460 0.5971906185150146\n","470 0.5909880995750427\n","480 0.5827502608299255\n","490 0.5769133567810059\n","500 0.5743898153305054\n","510 0.5712958574295044\n","520 0.5708067417144775\n","530 0.570435106754303\n","540 0.5701601505279541\n","550 0.5699921250343323\n","560 0.5698491930961609\n","570 0.5697273015975952\n","580 0.5696086883544922\n","590 0.5694571733474731\n","600 0.5691962838172913\n","610 0.5688012838363647\n","620 0.5686105489730835\n","630 0.5685447454452515\n","640 0.5685118436813354\n","650 0.5684816837310791\n","660 0.568455159664154\n","670 0.5684310793876648\n","680 0.5684090256690979\n","690 0.5683883428573608\n","700 0.5683689117431641\n","710 0.5683503150939941\n","720 0.5683325529098511\n","730 0.5683155059814453\n","740 0.5682989358901978\n","750 0.5682827830314636\n","760 0.5682669281959534\n","770 0.5682505369186401\n","780 0.5682329535484314\n","790 0.5682123899459839\n","800 0.5681864619255066\n","810 0.5681551098823547\n","820 0.5681292414665222\n","830 0.5681164264678955\n","840 0.5681054592132568\n","850 0.5680949687957764\n","860 0.5680854320526123\n","870 0.5680760145187378\n","880 0.5680670738220215\n","890 0.568058431148529\n","900 0.5680500864982605\n","910 0.5680420398712158\n","920 0.5680341124534607\n","930 0.5680264830589294\n","940 0.5680191516876221\n","950 0.5680120587348938\n","960 0.5680050253868103\n","970 0.5679982304573059\n","980 0.5679914951324463\n","990 0.5679851770401001\n"]}]},{"cell_type":"code","source":["plt.plot(losses)\n","plt.show()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":430},"id":"81ASYrW7roFM","outputId":"51b359fe-3e52-4e61-faa4-ce2b55b2ae51","executionInfo":{"status":"ok","timestamp":1706621203335,"user_tz":-540,"elapsed":792,"user":{"displayName":"Insu Shin","userId":"15408006383984625639"}}},"execution_count":21,"outputs":[{"output_type":"display_data","data":{"text/plain":["<Figure size 640x480 with 1 Axes>"],"image/png":"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\n"},"metadata":{}}]},{"cell_type":"code","source":["with torch.no_grad():\n","  model = model.to('cpu')\n","  y_pred = model(x_test)\n","  y_pred = y_pred.detach().numpy()\n","  predicted = np.argmax(y_pred, axis =1)\n","  accuracy = (accuracy_score(predicted, y_test))"],"metadata":{"id":"h4kJzpLErqhZ","executionInfo":{"status":"ok","timestamp":1706621216559,"user_tz":-540,"elapsed":6,"user":{"displayName":"Insu Shin","userId":"15408006383984625639"}}},"execution_count":22,"outputs":[]},{"cell_type":"code","source":["print(f'model의 output은 :  {y_pred[0]}')\n","print(f'argmax를 한 후의 output은 {predicted[0]}')\n","print(f'accuracy는 {accuracy}')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vyIKhs3Nr6Ay","outputId":"55fdf4c3-19f3-47de-8a56-b97266d9b819","executionInfo":{"status":"ok","timestamp":1706621219844,"user_tz":-540,"elapsed":743,"user":{"displayName":"Insu Shin","userId":"15408006383984625639"}}},"execution_count":23,"outputs":[{"output_type":"stream","name":"stdout","text":["model의 output은 :  [9.9987328e-01 1.3960319e-05 1.1275225e-04]\n","argmax를 한 후의 output은 0\n","accuracy는 0.9814814814814815\n"]}]},{"cell_type":"markdown","source":["| Hidden layer1 | Hidden layer 2 | Accuracy |\n","|----------|----------|----------|\n","| 64 | 32 | 0.388 |\n","| 128 | 9 | 0.333 |\n","| 500 | 27 | 0.981 |"],"metadata":{"id":"jo_8uMUESIaz"}},{"cell_type":"markdown","source":["생각보다 정확도가 좋지 않다. 뭐가 문제일까.."],"metadata":{"id":"uz_6pJmDUpjV"}},{"cell_type":"markdown","source":["# < 3주차 과제 2 : CNN 맛보기>"],"metadata":{"id":"3RzRM7xThZV_"}},{"cell_type":"code","source":["from __future__ import print_function\n","import argparse\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","from torchvision import datasets, transforms\n","from torch.autograd import Variable"],"metadata":{"id":"56xqgtLxhZw6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Training settings\n","\n","batch_size = 64\n","\n","# MNIST Dataset\n","train_dataset = datasets.MNIST(root='./data/',\n","                               train=True,\n","                               transform=transforms.ToTensor(),\n","                               download=True)\n","\n","test_dataset = datasets.MNIST(root='./data/',\n","                              train=False,\n","                              transform=transforms.ToTensor())\n","\n","# Data Loader (Input Pipeline)\n","train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n","                                           batch_size=batch_size,\n","                                           shuffle=True)\n","\n","test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n","                                          batch_size=batch_size,\n","                                          shuffle=False)"],"metadata":{"id":"TzkF2bFNhcQ2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"OzYXuveFYYO0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class Net(nn.Module):\n","  def __init__(self):\n","    super(Net, self).__init__()\n","    self.conv1 = nn.Conv2d(1, 10, kernel_size = 5)\n","    self.conv2 = nn.Conv2d(10, 20, kernel_size = 5)\n","    self.mp = nn.MaxPool2d(2)\n","    self.fc = nn.Linear(320, 10) ### : 알맞는 input은? 인수: 320?\n","\n","  def forward(self, x):\n","    in_size = x.size(0)\n","    x = F.relu(self.mp(self.conv1(x)))\n","    x = F.relu(self.mp(self.conv2(x)))\n","    x = x.view(in_size, -1)\n","    x = self.fc(x)\n","    return F.log_softmax(x)"],"metadata":{"id":"tLCSvgganBrH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model = Net()\n","optimizer = optim.SGD(model.parameters(), lr = 0.01, momentum = 0.5)"],"metadata":{"id":"lkYZ4pUdnUHc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def train(epoch):\n","  model.train()\n","  for batch_idx, (data, target) in enumerate(train_loader):\n","    data, target = Variable(data), Variable(target)\n","    optimizer.zero_grad()\n","    output = model(data)\n","    loss = F.nll_loss(output, target)\n","    loss.backward()\n","    optimizer.step()\n","    if batch_idx % 10 == 0:\n","      print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n","          epoch, batch_idx * len(data), len(train_loader.dataset),\n","          100. * batch_idx / len(train_loader), loss.item()))"],"metadata":{"id":"IzUrEM3EnXJb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def test():\n","    model.eval() #model.eval() 의 기능은?\n","    test_loss = 0\n","    correct = 0\n","    for data, target in test_loader:\n","        data, target = Variable(data, volatile=True), Variable(target)\n","        output = model(data)\n","        test_loss += F.nll_loss(output, target, size_average=False).data # nll_loss?? / cross entropy loss와의 관계 확인!\n","        pred = output.data.max(1, keepdim=True)[1]\n","        correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n","\n","    test_loss /= len(test_loader.dataset)\n","    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n","        test_loss, correct, len(test_loader.dataset),\n","        100. * correct / len(test_loader.dataset)))"],"metadata":{"id":"EFi0gYJGn2aa"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for epoch in range(1, 10):\n","    train(epoch)\n","    test()"],"metadata":{"id":"zSvSZb_Bn4Nx","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1706618993230,"user_tz":-540,"elapsed":113279,"user":{"displayName":"Insu Shin","userId":"15408006383984625639"}},"outputId":"b287043e-17f3-419c-9a2b-4beee1b3f491"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-36-7bda15c12f57>:15: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n","  return F.log_softmax(x)\n"]},{"output_type":"stream","name":"stdout","text":["Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.309633\n","Train Epoch: 1 [640/60000 (1%)]\tLoss: 2.251448\n","Train Epoch: 1 [1280/60000 (2%)]\tLoss: 2.242977\n","Train Epoch: 1 [1920/60000 (3%)]\tLoss: 2.207593\n","Train Epoch: 1 [2560/60000 (4%)]\tLoss: 2.162951\n","Train Epoch: 1 [3200/60000 (5%)]\tLoss: 2.102299\n","Train Epoch: 1 [3840/60000 (6%)]\tLoss: 2.019460\n","Train Epoch: 1 [4480/60000 (7%)]\tLoss: 1.811223\n","Train Epoch: 1 [5120/60000 (9%)]\tLoss: 1.668923\n","Train Epoch: 1 [5760/60000 (10%)]\tLoss: 1.465732\n","Train Epoch: 1 [6400/60000 (11%)]\tLoss: 1.086230\n","Train Epoch: 1 [7040/60000 (12%)]\tLoss: 0.819841\n","Train Epoch: 1 [7680/60000 (13%)]\tLoss: 0.987055\n","Train Epoch: 1 [8320/60000 (14%)]\tLoss: 0.616366\n","Train Epoch: 1 [8960/60000 (15%)]\tLoss: 0.798320\n","Train Epoch: 1 [9600/60000 (16%)]\tLoss: 0.818280\n","Train Epoch: 1 [10240/60000 (17%)]\tLoss: 0.646864\n","Train Epoch: 1 [10880/60000 (18%)]\tLoss: 0.558093\n","Train Epoch: 1 [11520/60000 (19%)]\tLoss: 0.718366\n","Train Epoch: 1 [12160/60000 (20%)]\tLoss: 0.549295\n","Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.644585\n","Train Epoch: 1 [13440/60000 (22%)]\tLoss: 0.463055\n","Train Epoch: 1 [14080/60000 (23%)]\tLoss: 0.504924\n","Train Epoch: 1 [14720/60000 (25%)]\tLoss: 0.586940\n","Train Epoch: 1 [15360/60000 (26%)]\tLoss: 0.293004\n","Train Epoch: 1 [16000/60000 (27%)]\tLoss: 0.206684\n","Train Epoch: 1 [16640/60000 (28%)]\tLoss: 0.507724\n","Train Epoch: 1 [17280/60000 (29%)]\tLoss: 0.348500\n","Train Epoch: 1 [17920/60000 (30%)]\tLoss: 0.385024\n","Train Epoch: 1 [18560/60000 (31%)]\tLoss: 0.319393\n","Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.365475\n","Train Epoch: 1 [19840/60000 (33%)]\tLoss: 0.625806\n","Train Epoch: 1 [20480/60000 (34%)]\tLoss: 0.208679\n","Train Epoch: 1 [21120/60000 (35%)]\tLoss: 0.404146\n","Train Epoch: 1 [21760/60000 (36%)]\tLoss: 0.278933\n","Train Epoch: 1 [22400/60000 (37%)]\tLoss: 0.162366\n","Train Epoch: 1 [23040/60000 (38%)]\tLoss: 0.325599\n","Train Epoch: 1 [23680/60000 (39%)]\tLoss: 0.300749\n","Train Epoch: 1 [24320/60000 (41%)]\tLoss: 0.289710\n","Train Epoch: 1 [24960/60000 (42%)]\tLoss: 0.277447\n","Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.411843\n","Train Epoch: 1 [26240/60000 (44%)]\tLoss: 0.606290\n","Train Epoch: 1 [26880/60000 (45%)]\tLoss: 0.266027\n","Train Epoch: 1 [27520/60000 (46%)]\tLoss: 0.445331\n","Train Epoch: 1 [28160/60000 (47%)]\tLoss: 0.269846\n","Train Epoch: 1 [28800/60000 (48%)]\tLoss: 0.126355\n","Train Epoch: 1 [29440/60000 (49%)]\tLoss: 0.382862\n","Train Epoch: 1 [30080/60000 (50%)]\tLoss: 0.299263\n","Train Epoch: 1 [30720/60000 (51%)]\tLoss: 0.369120\n","Train Epoch: 1 [31360/60000 (52%)]\tLoss: 0.294950\n","Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.422124\n","Train Epoch: 1 [32640/60000 (54%)]\tLoss: 0.329154\n","Train Epoch: 1 [33280/60000 (55%)]\tLoss: 0.191021\n","Train Epoch: 1 [33920/60000 (57%)]\tLoss: 0.274862\n","Train Epoch: 1 [34560/60000 (58%)]\tLoss: 0.248242\n","Train Epoch: 1 [35200/60000 (59%)]\tLoss: 0.239656\n","Train Epoch: 1 [35840/60000 (60%)]\tLoss: 0.146279\n","Train Epoch: 1 [36480/60000 (61%)]\tLoss: 0.382051\n","Train Epoch: 1 [37120/60000 (62%)]\tLoss: 0.429804\n","Train Epoch: 1 [37760/60000 (63%)]\tLoss: 0.275295\n","Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.349125\n","Train Epoch: 1 [39040/60000 (65%)]\tLoss: 0.170281\n","Train Epoch: 1 [39680/60000 (66%)]\tLoss: 0.141919\n","Train Epoch: 1 [40320/60000 (67%)]\tLoss: 0.237413\n","Train Epoch: 1 [40960/60000 (68%)]\tLoss: 0.320655\n","Train Epoch: 1 [41600/60000 (69%)]\tLoss: 0.268855\n","Train Epoch: 1 [42240/60000 (70%)]\tLoss: 0.275706\n","Train Epoch: 1 [42880/60000 (71%)]\tLoss: 0.352089\n","Train Epoch: 1 [43520/60000 (72%)]\tLoss: 0.237686\n","Train Epoch: 1 [44160/60000 (74%)]\tLoss: 0.361200\n","Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.250664\n","Train Epoch: 1 [45440/60000 (76%)]\tLoss: 0.225168\n","Train Epoch: 1 [46080/60000 (77%)]\tLoss: 0.288510\n","Train Epoch: 1 [46720/60000 (78%)]\tLoss: 0.239452\n","Train Epoch: 1 [47360/60000 (79%)]\tLoss: 0.191501\n","Train Epoch: 1 [48000/60000 (80%)]\tLoss: 0.234011\n","Train Epoch: 1 [48640/60000 (81%)]\tLoss: 0.141333\n","Train Epoch: 1 [49280/60000 (82%)]\tLoss: 0.217695\n","Train Epoch: 1 [49920/60000 (83%)]\tLoss: 0.294738\n","Train Epoch: 1 [50560/60000 (84%)]\tLoss: 0.134396\n","Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.191942\n","Train Epoch: 1 [51840/60000 (86%)]\tLoss: 0.407460\n","Train Epoch: 1 [52480/60000 (87%)]\tLoss: 0.177647\n","Train Epoch: 1 [53120/60000 (88%)]\tLoss: 0.121798\n","Train Epoch: 1 [53760/60000 (90%)]\tLoss: 0.221779\n","Train Epoch: 1 [54400/60000 (91%)]\tLoss: 0.106711\n","Train Epoch: 1 [55040/60000 (92%)]\tLoss: 0.203716\n","Train Epoch: 1 [55680/60000 (93%)]\tLoss: 0.127381\n","Train Epoch: 1 [56320/60000 (94%)]\tLoss: 0.118135\n","Train Epoch: 1 [56960/60000 (95%)]\tLoss: 0.421941\n","Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.145222\n","Train Epoch: 1 [58240/60000 (97%)]\tLoss: 0.269845\n","Train Epoch: 1 [58880/60000 (98%)]\tLoss: 0.178886\n","Train Epoch: 1 [59520/60000 (99%)]\tLoss: 0.266154\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-39-f52337105c2a>:6: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n","  data, target = Variable(data, volatile=True), Variable(target)\n","/usr/local/lib/python3.10/dist-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n","  warnings.warn(warning.format(ret))\n"]},{"output_type":"stream","name":"stdout","text":["\n","Test set: Average loss: 0.1722, Accuracy: 9486/10000 (95%)\n","\n","Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.160414\n","Train Epoch: 2 [640/60000 (1%)]\tLoss: 0.313018\n","Train Epoch: 2 [1280/60000 (2%)]\tLoss: 0.114778\n","Train Epoch: 2 [1920/60000 (3%)]\tLoss: 0.074981\n","Train Epoch: 2 [2560/60000 (4%)]\tLoss: 0.280264\n","Train Epoch: 2 [3200/60000 (5%)]\tLoss: 0.162230\n","Train Epoch: 2 [3840/60000 (6%)]\tLoss: 0.341868\n","Train Epoch: 2 [4480/60000 (7%)]\tLoss: 0.171879\n","Train Epoch: 2 [5120/60000 (9%)]\tLoss: 0.099763\n","Train Epoch: 2 [5760/60000 (10%)]\tLoss: 0.195877\n","Train Epoch: 2 [6400/60000 (11%)]\tLoss: 0.112799\n","Train Epoch: 2 [7040/60000 (12%)]\tLoss: 0.070197\n","Train Epoch: 2 [7680/60000 (13%)]\tLoss: 0.107087\n","Train Epoch: 2 [8320/60000 (14%)]\tLoss: 0.230121\n","Train Epoch: 2 [8960/60000 (15%)]\tLoss: 0.097375\n","Train Epoch: 2 [9600/60000 (16%)]\tLoss: 0.196230\n","Train Epoch: 2 [10240/60000 (17%)]\tLoss: 0.075814\n","Train Epoch: 2 [10880/60000 (18%)]\tLoss: 0.211486\n","Train Epoch: 2 [11520/60000 (19%)]\tLoss: 0.149819\n","Train Epoch: 2 [12160/60000 (20%)]\tLoss: 0.086941\n","Train Epoch: 2 [12800/60000 (21%)]\tLoss: 0.299311\n","Train Epoch: 2 [13440/60000 (22%)]\tLoss: 0.201057\n","Train Epoch: 2 [14080/60000 (23%)]\tLoss: 0.132593\n","Train Epoch: 2 [14720/60000 (25%)]\tLoss: 0.100121\n","Train Epoch: 2 [15360/60000 (26%)]\tLoss: 0.112631\n","Train Epoch: 2 [16000/60000 (27%)]\tLoss: 0.085376\n","Train Epoch: 2 [16640/60000 (28%)]\tLoss: 0.311335\n","Train Epoch: 2 [17280/60000 (29%)]\tLoss: 0.123313\n","Train Epoch: 2 [17920/60000 (30%)]\tLoss: 0.172020\n","Train Epoch: 2 [18560/60000 (31%)]\tLoss: 0.174551\n","Train Epoch: 2 [19200/60000 (32%)]\tLoss: 0.057423\n","Train Epoch: 2 [19840/60000 (33%)]\tLoss: 0.140511\n","Train Epoch: 2 [20480/60000 (34%)]\tLoss: 0.075602\n","Train Epoch: 2 [21120/60000 (35%)]\tLoss: 0.174976\n","Train Epoch: 2 [21760/60000 (36%)]\tLoss: 0.074019\n","Train Epoch: 2 [22400/60000 (37%)]\tLoss: 0.101726\n","Train Epoch: 2 [23040/60000 (38%)]\tLoss: 0.203391\n","Train Epoch: 2 [23680/60000 (39%)]\tLoss: 0.117750\n","Train Epoch: 2 [24320/60000 (41%)]\tLoss: 0.115031\n","Train Epoch: 2 [24960/60000 (42%)]\tLoss: 0.185174\n","Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.033425\n","Train Epoch: 2 [26240/60000 (44%)]\tLoss: 0.052613\n","Train Epoch: 2 [26880/60000 (45%)]\tLoss: 0.122936\n","Train Epoch: 2 [27520/60000 (46%)]\tLoss: 0.114398\n","Train Epoch: 2 [28160/60000 (47%)]\tLoss: 0.295643\n","Train Epoch: 2 [28800/60000 (48%)]\tLoss: 0.214601\n","Train Epoch: 2 [29440/60000 (49%)]\tLoss: 0.250364\n","Train Epoch: 2 [30080/60000 (50%)]\tLoss: 0.110774\n","Train Epoch: 2 [30720/60000 (51%)]\tLoss: 0.271776\n","Train Epoch: 2 [31360/60000 (52%)]\tLoss: 0.096556\n","Train Epoch: 2 [32000/60000 (53%)]\tLoss: 0.152815\n","Train Epoch: 2 [32640/60000 (54%)]\tLoss: 0.071152\n","Train Epoch: 2 [33280/60000 (55%)]\tLoss: 0.153322\n","Train Epoch: 2 [33920/60000 (57%)]\tLoss: 0.296068\n","Train Epoch: 2 [34560/60000 (58%)]\tLoss: 0.128068\n","Train Epoch: 2 [35200/60000 (59%)]\tLoss: 0.282928\n","Train Epoch: 2 [35840/60000 (60%)]\tLoss: 0.249421\n","Train Epoch: 2 [36480/60000 (61%)]\tLoss: 0.352617\n","Train Epoch: 2 [37120/60000 (62%)]\tLoss: 0.160996\n","Train Epoch: 2 [37760/60000 (63%)]\tLoss: 0.267905\n","Train Epoch: 2 [38400/60000 (64%)]\tLoss: 0.149729\n","Train Epoch: 2 [39040/60000 (65%)]\tLoss: 0.085017\n","Train Epoch: 2 [39680/60000 (66%)]\tLoss: 0.452558\n","Train Epoch: 2 [40320/60000 (67%)]\tLoss: 0.206487\n","Train Epoch: 2 [40960/60000 (68%)]\tLoss: 0.109880\n","Train Epoch: 2 [41600/60000 (69%)]\tLoss: 0.063028\n","Train Epoch: 2 [42240/60000 (70%)]\tLoss: 0.121328\n","Train Epoch: 2 [42880/60000 (71%)]\tLoss: 0.101740\n","Train Epoch: 2 [43520/60000 (72%)]\tLoss: 0.152889\n","Train Epoch: 2 [44160/60000 (74%)]\tLoss: 0.208792\n","Train Epoch: 2 [44800/60000 (75%)]\tLoss: 0.117522\n","Train Epoch: 2 [45440/60000 (76%)]\tLoss: 0.090516\n","Train Epoch: 2 [46080/60000 (77%)]\tLoss: 0.104907\n","Train Epoch: 2 [46720/60000 (78%)]\tLoss: 0.088470\n","Train Epoch: 2 [47360/60000 (79%)]\tLoss: 0.403458\n","Train Epoch: 2 [48000/60000 (80%)]\tLoss: 0.083995\n","Train Epoch: 2 [48640/60000 (81%)]\tLoss: 0.064095\n","Train Epoch: 2 [49280/60000 (82%)]\tLoss: 0.055912\n","Train Epoch: 2 [49920/60000 (83%)]\tLoss: 0.140893\n","Train Epoch: 2 [50560/60000 (84%)]\tLoss: 0.206993\n","Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.124441\n","Train Epoch: 2 [51840/60000 (86%)]\tLoss: 0.270725\n","Train Epoch: 2 [52480/60000 (87%)]\tLoss: 0.121962\n","Train Epoch: 2 [53120/60000 (88%)]\tLoss: 0.152613\n","Train Epoch: 2 [53760/60000 (90%)]\tLoss: 0.117974\n","Train Epoch: 2 [54400/60000 (91%)]\tLoss: 0.119646\n","Train Epoch: 2 [55040/60000 (92%)]\tLoss: 0.024869\n","Train Epoch: 2 [55680/60000 (93%)]\tLoss: 0.054166\n","Train Epoch: 2 [56320/60000 (94%)]\tLoss: 0.148906\n","Train Epoch: 2 [56960/60000 (95%)]\tLoss: 0.139945\n","Train Epoch: 2 [57600/60000 (96%)]\tLoss: 0.075693\n","Train Epoch: 2 [58240/60000 (97%)]\tLoss: 0.118897\n","Train Epoch: 2 [58880/60000 (98%)]\tLoss: 0.171259\n","Train Epoch: 2 [59520/60000 (99%)]\tLoss: 0.223335\n","\n","Test set: Average loss: 0.1112, Accuracy: 9672/10000 (97%)\n","\n","Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.259733\n","Train Epoch: 3 [640/60000 (1%)]\tLoss: 0.138725\n","Train Epoch: 3 [1280/60000 (2%)]\tLoss: 0.314031\n","Train Epoch: 3 [1920/60000 (3%)]\tLoss: 0.088097\n","Train Epoch: 3 [2560/60000 (4%)]\tLoss: 0.199743\n","Train Epoch: 3 [3200/60000 (5%)]\tLoss: 0.057501\n","Train Epoch: 3 [3840/60000 (6%)]\tLoss: 0.128415\n","Train Epoch: 3 [4480/60000 (7%)]\tLoss: 0.137249\n","Train Epoch: 3 [5120/60000 (9%)]\tLoss: 0.194930\n","Train Epoch: 3 [5760/60000 (10%)]\tLoss: 0.030426\n","Train Epoch: 3 [6400/60000 (11%)]\tLoss: 0.233018\n","Train Epoch: 3 [7040/60000 (12%)]\tLoss: 0.049451\n","Train Epoch: 3 [7680/60000 (13%)]\tLoss: 0.305247\n","Train Epoch: 3 [8320/60000 (14%)]\tLoss: 0.085198\n","Train Epoch: 3 [8960/60000 (15%)]\tLoss: 0.086414\n","Train Epoch: 3 [9600/60000 (16%)]\tLoss: 0.116424\n","Train Epoch: 3 [10240/60000 (17%)]\tLoss: 0.051035\n","Train Epoch: 3 [10880/60000 (18%)]\tLoss: 0.154185\n","Train Epoch: 3 [11520/60000 (19%)]\tLoss: 0.280032\n","Train Epoch: 3 [12160/60000 (20%)]\tLoss: 0.127764\n","Train Epoch: 3 [12800/60000 (21%)]\tLoss: 0.091668\n","Train Epoch: 3 [13440/60000 (22%)]\tLoss: 0.212568\n","Train Epoch: 3 [14080/60000 (23%)]\tLoss: 0.207022\n","Train Epoch: 3 [14720/60000 (25%)]\tLoss: 0.135266\n","Train Epoch: 3 [15360/60000 (26%)]\tLoss: 0.057554\n","Train Epoch: 3 [16000/60000 (27%)]\tLoss: 0.151755\n","Train Epoch: 3 [16640/60000 (28%)]\tLoss: 0.147676\n","Train Epoch: 3 [17280/60000 (29%)]\tLoss: 0.083559\n","Train Epoch: 3 [17920/60000 (30%)]\tLoss: 0.195585\n","Train Epoch: 3 [18560/60000 (31%)]\tLoss: 0.061535\n","Train Epoch: 3 [19200/60000 (32%)]\tLoss: 0.090660\n","Train Epoch: 3 [19840/60000 (33%)]\tLoss: 0.209088\n","Train Epoch: 3 [20480/60000 (34%)]\tLoss: 0.105487\n","Train Epoch: 3 [21120/60000 (35%)]\tLoss: 0.156866\n","Train Epoch: 3 [21760/60000 (36%)]\tLoss: 0.272120\n","Train Epoch: 3 [22400/60000 (37%)]\tLoss: 0.046503\n","Train Epoch: 3 [23040/60000 (38%)]\tLoss: 0.230924\n","Train Epoch: 3 [23680/60000 (39%)]\tLoss: 0.060447\n","Train Epoch: 3 [24320/60000 (41%)]\tLoss: 0.187151\n","Train Epoch: 3 [24960/60000 (42%)]\tLoss: 0.030509\n","Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.041263\n","Train Epoch: 3 [26240/60000 (44%)]\tLoss: 0.205151\n","Train Epoch: 3 [26880/60000 (45%)]\tLoss: 0.248390\n","Train Epoch: 3 [27520/60000 (46%)]\tLoss: 0.100957\n","Train Epoch: 3 [28160/60000 (47%)]\tLoss: 0.113774\n","Train Epoch: 3 [28800/60000 (48%)]\tLoss: 0.132772\n","Train Epoch: 3 [29440/60000 (49%)]\tLoss: 0.066369\n","Train Epoch: 3 [30080/60000 (50%)]\tLoss: 0.044904\n","Train Epoch: 3 [30720/60000 (51%)]\tLoss: 0.069885\n","Train Epoch: 3 [31360/60000 (52%)]\tLoss: 0.041729\n","Train Epoch: 3 [32000/60000 (53%)]\tLoss: 0.222889\n","Train Epoch: 3 [32640/60000 (54%)]\tLoss: 0.021841\n","Train Epoch: 3 [33280/60000 (55%)]\tLoss: 0.066151\n","Train Epoch: 3 [33920/60000 (57%)]\tLoss: 0.097020\n","Train Epoch: 3 [34560/60000 (58%)]\tLoss: 0.117834\n","Train Epoch: 3 [35200/60000 (59%)]\tLoss: 0.092840\n","Train Epoch: 3 [35840/60000 (60%)]\tLoss: 0.060964\n","Train Epoch: 3 [36480/60000 (61%)]\tLoss: 0.195259\n","Train Epoch: 3 [37120/60000 (62%)]\tLoss: 0.048494\n","Train Epoch: 3 [37760/60000 (63%)]\tLoss: 0.159264\n","Train Epoch: 3 [38400/60000 (64%)]\tLoss: 0.034436\n","Train Epoch: 3 [39040/60000 (65%)]\tLoss: 0.055174\n","Train Epoch: 3 [39680/60000 (66%)]\tLoss: 0.060100\n","Train Epoch: 3 [40320/60000 (67%)]\tLoss: 0.064223\n","Train Epoch: 3 [40960/60000 (68%)]\tLoss: 0.125197\n","Train Epoch: 3 [41600/60000 (69%)]\tLoss: 0.043484\n","Train Epoch: 3 [42240/60000 (70%)]\tLoss: 0.169967\n","Train Epoch: 3 [42880/60000 (71%)]\tLoss: 0.048328\n","Train Epoch: 3 [43520/60000 (72%)]\tLoss: 0.248317\n","Train Epoch: 3 [44160/60000 (74%)]\tLoss: 0.083438\n","Train Epoch: 3 [44800/60000 (75%)]\tLoss: 0.071074\n","Train Epoch: 3 [45440/60000 (76%)]\tLoss: 0.099067\n","Train Epoch: 3 [46080/60000 (77%)]\tLoss: 0.105345\n","Train Epoch: 3 [46720/60000 (78%)]\tLoss: 0.093624\n","Train Epoch: 3 [47360/60000 (79%)]\tLoss: 0.057672\n","Train Epoch: 3 [48000/60000 (80%)]\tLoss: 0.015353\n","Train Epoch: 3 [48640/60000 (81%)]\tLoss: 0.156286\n","Train Epoch: 3 [49280/60000 (82%)]\tLoss: 0.071359\n","Train Epoch: 3 [49920/60000 (83%)]\tLoss: 0.086876\n","Train Epoch: 3 [50560/60000 (84%)]\tLoss: 0.044036\n","Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.107272\n","Train Epoch: 3 [51840/60000 (86%)]\tLoss: 0.333092\n","Train Epoch: 3 [52480/60000 (87%)]\tLoss: 0.032568\n","Train Epoch: 3 [53120/60000 (88%)]\tLoss: 0.179120\n","Train Epoch: 3 [53760/60000 (90%)]\tLoss: 0.071302\n","Train Epoch: 3 [54400/60000 (91%)]\tLoss: 0.102396\n","Train Epoch: 3 [55040/60000 (92%)]\tLoss: 0.122882\n","Train Epoch: 3 [55680/60000 (93%)]\tLoss: 0.120871\n","Train Epoch: 3 [56320/60000 (94%)]\tLoss: 0.027123\n","Train Epoch: 3 [56960/60000 (95%)]\tLoss: 0.022163\n","Train Epoch: 3 [57600/60000 (96%)]\tLoss: 0.052763\n","Train Epoch: 3 [58240/60000 (97%)]\tLoss: 0.120077\n","Train Epoch: 3 [58880/60000 (98%)]\tLoss: 0.018214\n","Train Epoch: 3 [59520/60000 (99%)]\tLoss: 0.129030\n","\n","Test set: Average loss: 0.0885, Accuracy: 9717/10000 (97%)\n","\n","Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.042900\n","Train Epoch: 4 [640/60000 (1%)]\tLoss: 0.161525\n","Train Epoch: 4 [1280/60000 (2%)]\tLoss: 0.065925\n","Train Epoch: 4 [1920/60000 (3%)]\tLoss: 0.157119\n","Train Epoch: 4 [2560/60000 (4%)]\tLoss: 0.018980\n","Train Epoch: 4 [3200/60000 (5%)]\tLoss: 0.106596\n","Train Epoch: 4 [3840/60000 (6%)]\tLoss: 0.049775\n","Train Epoch: 4 [4480/60000 (7%)]\tLoss: 0.110319\n","Train Epoch: 4 [5120/60000 (9%)]\tLoss: 0.099618\n","Train Epoch: 4 [5760/60000 (10%)]\tLoss: 0.060650\n","Train Epoch: 4 [6400/60000 (11%)]\tLoss: 0.037921\n","Train Epoch: 4 [7040/60000 (12%)]\tLoss: 0.011533\n","Train Epoch: 4 [7680/60000 (13%)]\tLoss: 0.047650\n","Train Epoch: 4 [8320/60000 (14%)]\tLoss: 0.066252\n","Train Epoch: 4 [8960/60000 (15%)]\tLoss: 0.021294\n","Train Epoch: 4 [9600/60000 (16%)]\tLoss: 0.080208\n","Train Epoch: 4 [10240/60000 (17%)]\tLoss: 0.064234\n","Train Epoch: 4 [10880/60000 (18%)]\tLoss: 0.184630\n","Train Epoch: 4 [11520/60000 (19%)]\tLoss: 0.103977\n","Train Epoch: 4 [12160/60000 (20%)]\tLoss: 0.033483\n","Train Epoch: 4 [12800/60000 (21%)]\tLoss: 0.113623\n","Train Epoch: 4 [13440/60000 (22%)]\tLoss: 0.020408\n","Train Epoch: 4 [14080/60000 (23%)]\tLoss: 0.036384\n","Train Epoch: 4 [14720/60000 (25%)]\tLoss: 0.081159\n","Train Epoch: 4 [15360/60000 (26%)]\tLoss: 0.056681\n","Train Epoch: 4 [16000/60000 (27%)]\tLoss: 0.220005\n","Train Epoch: 4 [16640/60000 (28%)]\tLoss: 0.114284\n","Train Epoch: 4 [17280/60000 (29%)]\tLoss: 0.087933\n","Train Epoch: 4 [17920/60000 (30%)]\tLoss: 0.227111\n","Train Epoch: 4 [18560/60000 (31%)]\tLoss: 0.063068\n","Train Epoch: 4 [19200/60000 (32%)]\tLoss: 0.048700\n","Train Epoch: 4 [19840/60000 (33%)]\tLoss: 0.026701\n","Train Epoch: 4 [20480/60000 (34%)]\tLoss: 0.123786\n","Train Epoch: 4 [21120/60000 (35%)]\tLoss: 0.051396\n","Train Epoch: 4 [21760/60000 (36%)]\tLoss: 0.050602\n","Train Epoch: 4 [22400/60000 (37%)]\tLoss: 0.099775\n","Train Epoch: 4 [23040/60000 (38%)]\tLoss: 0.093515\n","Train Epoch: 4 [23680/60000 (39%)]\tLoss: 0.137040\n","Train Epoch: 4 [24320/60000 (41%)]\tLoss: 0.121456\n","Train Epoch: 4 [24960/60000 (42%)]\tLoss: 0.122448\n","Train Epoch: 4 [25600/60000 (43%)]\tLoss: 0.017190\n","Train Epoch: 4 [26240/60000 (44%)]\tLoss: 0.032027\n","Train Epoch: 4 [26880/60000 (45%)]\tLoss: 0.083392\n","Train Epoch: 4 [27520/60000 (46%)]\tLoss: 0.042817\n","Train Epoch: 4 [28160/60000 (47%)]\tLoss: 0.185760\n","Train Epoch: 4 [28800/60000 (48%)]\tLoss: 0.212059\n","Train Epoch: 4 [29440/60000 (49%)]\tLoss: 0.051130\n","Train Epoch: 4 [30080/60000 (50%)]\tLoss: 0.110562\n","Train Epoch: 4 [30720/60000 (51%)]\tLoss: 0.090404\n","Train Epoch: 4 [31360/60000 (52%)]\tLoss: 0.054479\n","Train Epoch: 4 [32000/60000 (53%)]\tLoss: 0.165719\n","Train Epoch: 4 [32640/60000 (54%)]\tLoss: 0.055725\n","Train Epoch: 4 [33280/60000 (55%)]\tLoss: 0.097882\n","Train Epoch: 4 [33920/60000 (57%)]\tLoss: 0.063254\n","Train Epoch: 4 [34560/60000 (58%)]\tLoss: 0.080294\n","Train Epoch: 4 [35200/60000 (59%)]\tLoss: 0.158291\n","Train Epoch: 4 [35840/60000 (60%)]\tLoss: 0.106695\n","Train Epoch: 4 [36480/60000 (61%)]\tLoss: 0.116787\n","Train Epoch: 4 [37120/60000 (62%)]\tLoss: 0.059966\n","Train Epoch: 4 [37760/60000 (63%)]\tLoss: 0.007752\n","Train Epoch: 4 [38400/60000 (64%)]\tLoss: 0.019020\n","Train Epoch: 4 [39040/60000 (65%)]\tLoss: 0.051752\n","Train Epoch: 4 [39680/60000 (66%)]\tLoss: 0.100678\n","Train Epoch: 4 [40320/60000 (67%)]\tLoss: 0.106870\n","Train Epoch: 4 [40960/60000 (68%)]\tLoss: 0.041115\n","Train Epoch: 4 [41600/60000 (69%)]\tLoss: 0.046754\n","Train Epoch: 4 [42240/60000 (70%)]\tLoss: 0.058999\n","Train Epoch: 4 [42880/60000 (71%)]\tLoss: 0.025396\n","Train Epoch: 4 [43520/60000 (72%)]\tLoss: 0.034259\n","Train Epoch: 4 [44160/60000 (74%)]\tLoss: 0.033770\n","Train Epoch: 4 [44800/60000 (75%)]\tLoss: 0.060509\n","Train Epoch: 4 [45440/60000 (76%)]\tLoss: 0.068687\n","Train Epoch: 4 [46080/60000 (77%)]\tLoss: 0.042399\n","Train Epoch: 4 [46720/60000 (78%)]\tLoss: 0.027335\n","Train Epoch: 4 [47360/60000 (79%)]\tLoss: 0.047452\n","Train Epoch: 4 [48000/60000 (80%)]\tLoss: 0.092345\n","Train Epoch: 4 [48640/60000 (81%)]\tLoss: 0.166517\n","Train Epoch: 4 [49280/60000 (82%)]\tLoss: 0.055705\n","Train Epoch: 4 [49920/60000 (83%)]\tLoss: 0.032769\n","Train Epoch: 4 [50560/60000 (84%)]\tLoss: 0.022205\n","Train Epoch: 4 [51200/60000 (85%)]\tLoss: 0.086190\n","Train Epoch: 4 [51840/60000 (86%)]\tLoss: 0.023821\n","Train Epoch: 4 [52480/60000 (87%)]\tLoss: 0.097318\n","Train Epoch: 4 [53120/60000 (88%)]\tLoss: 0.032320\n","Train Epoch: 4 [53760/60000 (90%)]\tLoss: 0.131862\n","Train Epoch: 4 [54400/60000 (91%)]\tLoss: 0.124924\n","Train Epoch: 4 [55040/60000 (92%)]\tLoss: 0.101044\n","Train Epoch: 4 [55680/60000 (93%)]\tLoss: 0.175597\n","Train Epoch: 4 [56320/60000 (94%)]\tLoss: 0.012357\n","Train Epoch: 4 [56960/60000 (95%)]\tLoss: 0.080045\n","Train Epoch: 4 [57600/60000 (96%)]\tLoss: 0.054062\n","Train Epoch: 4 [58240/60000 (97%)]\tLoss: 0.166785\n","Train Epoch: 4 [58880/60000 (98%)]\tLoss: 0.070275\n","Train Epoch: 4 [59520/60000 (99%)]\tLoss: 0.019651\n","\n","Test set: Average loss: 0.0666, Accuracy: 9793/10000 (98%)\n","\n","Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.216293\n","Train Epoch: 5 [640/60000 (1%)]\tLoss: 0.074645\n","Train Epoch: 5 [1280/60000 (2%)]\tLoss: 0.090887\n","Train Epoch: 5 [1920/60000 (3%)]\tLoss: 0.018719\n","Train Epoch: 5 [2560/60000 (4%)]\tLoss: 0.089234\n","Train Epoch: 5 [3200/60000 (5%)]\tLoss: 0.180558\n","Train Epoch: 5 [3840/60000 (6%)]\tLoss: 0.127641\n","Train Epoch: 5 [4480/60000 (7%)]\tLoss: 0.103751\n","Train Epoch: 5 [5120/60000 (9%)]\tLoss: 0.017818\n","Train Epoch: 5 [5760/60000 (10%)]\tLoss: 0.069838\n","Train Epoch: 5 [6400/60000 (11%)]\tLoss: 0.050752\n","Train Epoch: 5 [7040/60000 (12%)]\tLoss: 0.287585\n","Train Epoch: 5 [7680/60000 (13%)]\tLoss: 0.054189\n","Train Epoch: 5 [8320/60000 (14%)]\tLoss: 0.139623\n","Train Epoch: 5 [8960/60000 (15%)]\tLoss: 0.086544\n","Train Epoch: 5 [9600/60000 (16%)]\tLoss: 0.037892\n","Train Epoch: 5 [10240/60000 (17%)]\tLoss: 0.054586\n","Train Epoch: 5 [10880/60000 (18%)]\tLoss: 0.064657\n","Train Epoch: 5 [11520/60000 (19%)]\tLoss: 0.042565\n","Train Epoch: 5 [12160/60000 (20%)]\tLoss: 0.161749\n","Train Epoch: 5 [12800/60000 (21%)]\tLoss: 0.132360\n","Train Epoch: 5 [13440/60000 (22%)]\tLoss: 0.130425\n","Train Epoch: 5 [14080/60000 (23%)]\tLoss: 0.198837\n","Train Epoch: 5 [14720/60000 (25%)]\tLoss: 0.029219\n","Train Epoch: 5 [15360/60000 (26%)]\tLoss: 0.067778\n","Train Epoch: 5 [16000/60000 (27%)]\tLoss: 0.122136\n","Train Epoch: 5 [16640/60000 (28%)]\tLoss: 0.131654\n","Train Epoch: 5 [17280/60000 (29%)]\tLoss: 0.010668\n","Train Epoch: 5 [17920/60000 (30%)]\tLoss: 0.042831\n","Train Epoch: 5 [18560/60000 (31%)]\tLoss: 0.035808\n","Train Epoch: 5 [19200/60000 (32%)]\tLoss: 0.152061\n","Train Epoch: 5 [19840/60000 (33%)]\tLoss: 0.217574\n","Train Epoch: 5 [20480/60000 (34%)]\tLoss: 0.041462\n","Train Epoch: 5 [21120/60000 (35%)]\tLoss: 0.110913\n","Train Epoch: 5 [21760/60000 (36%)]\tLoss: 0.014248\n","Train Epoch: 5 [22400/60000 (37%)]\tLoss: 0.132014\n","Train Epoch: 5 [23040/60000 (38%)]\tLoss: 0.088821\n","Train Epoch: 5 [23680/60000 (39%)]\tLoss: 0.104512\n","Train Epoch: 5 [24320/60000 (41%)]\tLoss: 0.061829\n","Train Epoch: 5 [24960/60000 (42%)]\tLoss: 0.160481\n","Train Epoch: 5 [25600/60000 (43%)]\tLoss: 0.086141\n","Train Epoch: 5 [26240/60000 (44%)]\tLoss: 0.086308\n","Train Epoch: 5 [26880/60000 (45%)]\tLoss: 0.076307\n","Train Epoch: 5 [27520/60000 (46%)]\tLoss: 0.126208\n","Train Epoch: 5 [28160/60000 (47%)]\tLoss: 0.071509\n","Train Epoch: 5 [28800/60000 (48%)]\tLoss: 0.057431\n","Train Epoch: 5 [29440/60000 (49%)]\tLoss: 0.045078\n","Train Epoch: 5 [30080/60000 (50%)]\tLoss: 0.071100\n","Train Epoch: 5 [30720/60000 (51%)]\tLoss: 0.126917\n","Train Epoch: 5 [31360/60000 (52%)]\tLoss: 0.027316\n","Train Epoch: 5 [32000/60000 (53%)]\tLoss: 0.041735\n","Train Epoch: 5 [32640/60000 (54%)]\tLoss: 0.040330\n","Train Epoch: 5 [33280/60000 (55%)]\tLoss: 0.124825\n","Train Epoch: 5 [33920/60000 (57%)]\tLoss: 0.103231\n","Train Epoch: 5 [34560/60000 (58%)]\tLoss: 0.036605\n","Train Epoch: 5 [35200/60000 (59%)]\tLoss: 0.068263\n","Train Epoch: 5 [35840/60000 (60%)]\tLoss: 0.104018\n","Train Epoch: 5 [36480/60000 (61%)]\tLoss: 0.103048\n","Train Epoch: 5 [37120/60000 (62%)]\tLoss: 0.114790\n","Train Epoch: 5 [37760/60000 (63%)]\tLoss: 0.034688\n","Train Epoch: 5 [38400/60000 (64%)]\tLoss: 0.026414\n","Train Epoch: 5 [39040/60000 (65%)]\tLoss: 0.146123\n","Train Epoch: 5 [39680/60000 (66%)]\tLoss: 0.086216\n","Train Epoch: 5 [40320/60000 (67%)]\tLoss: 0.104416\n","Train Epoch: 5 [40960/60000 (68%)]\tLoss: 0.135986\n","Train Epoch: 5 [41600/60000 (69%)]\tLoss: 0.055109\n","Train Epoch: 5 [42240/60000 (70%)]\tLoss: 0.077237\n","Train Epoch: 5 [42880/60000 (71%)]\tLoss: 0.015899\n","Train Epoch: 5 [43520/60000 (72%)]\tLoss: 0.172966\n","Train Epoch: 5 [44160/60000 (74%)]\tLoss: 0.077963\n","Train Epoch: 5 [44800/60000 (75%)]\tLoss: 0.073558\n","Train Epoch: 5 [45440/60000 (76%)]\tLoss: 0.134844\n","Train Epoch: 5 [46080/60000 (77%)]\tLoss: 0.062678\n","Train Epoch: 5 [46720/60000 (78%)]\tLoss: 0.059055\n","Train Epoch: 5 [47360/60000 (79%)]\tLoss: 0.248556\n","Train Epoch: 5 [48000/60000 (80%)]\tLoss: 0.046455\n","Train Epoch: 5 [48640/60000 (81%)]\tLoss: 0.066255\n","Train Epoch: 5 [49280/60000 (82%)]\tLoss: 0.020657\n","Train Epoch: 5 [49920/60000 (83%)]\tLoss: 0.013519\n","Train Epoch: 5 [50560/60000 (84%)]\tLoss: 0.023660\n","Train Epoch: 5 [51200/60000 (85%)]\tLoss: 0.176480\n","Train Epoch: 5 [51840/60000 (86%)]\tLoss: 0.061956\n","Train Epoch: 5 [52480/60000 (87%)]\tLoss: 0.102104\n","Train Epoch: 5 [53120/60000 (88%)]\tLoss: 0.141003\n","Train Epoch: 5 [53760/60000 (90%)]\tLoss: 0.218735\n","Train Epoch: 5 [54400/60000 (91%)]\tLoss: 0.024392\n","Train Epoch: 5 [55040/60000 (92%)]\tLoss: 0.075089\n","Train Epoch: 5 [55680/60000 (93%)]\tLoss: 0.174473\n","Train Epoch: 5 [56320/60000 (94%)]\tLoss: 0.148532\n","Train Epoch: 5 [56960/60000 (95%)]\tLoss: 0.051049\n","Train Epoch: 5 [57600/60000 (96%)]\tLoss: 0.057519\n","Train Epoch: 5 [58240/60000 (97%)]\tLoss: 0.189939\n","Train Epoch: 5 [58880/60000 (98%)]\tLoss: 0.022270\n","Train Epoch: 5 [59520/60000 (99%)]\tLoss: 0.043147\n","\n","Test set: Average loss: 0.0620, Accuracy: 9806/10000 (98%)\n","\n","Train Epoch: 6 [0/60000 (0%)]\tLoss: 0.055923\n","Train Epoch: 6 [640/60000 (1%)]\tLoss: 0.089513\n","Train Epoch: 6 [1280/60000 (2%)]\tLoss: 0.013162\n","Train Epoch: 6 [1920/60000 (3%)]\tLoss: 0.096207\n","Train Epoch: 6 [2560/60000 (4%)]\tLoss: 0.037817\n","Train Epoch: 6 [3200/60000 (5%)]\tLoss: 0.032953\n","Train Epoch: 6 [3840/60000 (6%)]\tLoss: 0.139073\n","Train Epoch: 6 [4480/60000 (7%)]\tLoss: 0.188416\n","Train Epoch: 6 [5120/60000 (9%)]\tLoss: 0.038930\n","Train Epoch: 6 [5760/60000 (10%)]\tLoss: 0.088922\n","Train Epoch: 6 [6400/60000 (11%)]\tLoss: 0.018570\n","Train Epoch: 6 [7040/60000 (12%)]\tLoss: 0.036651\n","Train Epoch: 6 [7680/60000 (13%)]\tLoss: 0.048082\n","Train Epoch: 6 [8320/60000 (14%)]\tLoss: 0.061712\n","Train Epoch: 6 [8960/60000 (15%)]\tLoss: 0.150598\n","Train Epoch: 6 [9600/60000 (16%)]\tLoss: 0.026301\n","Train Epoch: 6 [10240/60000 (17%)]\tLoss: 0.048869\n","Train Epoch: 6 [10880/60000 (18%)]\tLoss: 0.080495\n","Train Epoch: 6 [11520/60000 (19%)]\tLoss: 0.093205\n","Train Epoch: 6 [12160/60000 (20%)]\tLoss: 0.017565\n","Train Epoch: 6 [12800/60000 (21%)]\tLoss: 0.038860\n","Train Epoch: 6 [13440/60000 (22%)]\tLoss: 0.073100\n","Train Epoch: 6 [14080/60000 (23%)]\tLoss: 0.069344\n","Train Epoch: 6 [14720/60000 (25%)]\tLoss: 0.066342\n","Train Epoch: 6 [15360/60000 (26%)]\tLoss: 0.153229\n","Train Epoch: 6 [16000/60000 (27%)]\tLoss: 0.022620\n","Train Epoch: 6 [16640/60000 (28%)]\tLoss: 0.305579\n","Train Epoch: 6 [17280/60000 (29%)]\tLoss: 0.005360\n","Train Epoch: 6 [17920/60000 (30%)]\tLoss: 0.054375\n","Train Epoch: 6 [18560/60000 (31%)]\tLoss: 0.035867\n","Train Epoch: 6 [19200/60000 (32%)]\tLoss: 0.010860\n","Train Epoch: 6 [19840/60000 (33%)]\tLoss: 0.161756\n","Train Epoch: 6 [20480/60000 (34%)]\tLoss: 0.136884\n","Train Epoch: 6 [21120/60000 (35%)]\tLoss: 0.146972\n","Train Epoch: 6 [21760/60000 (36%)]\tLoss: 0.026284\n","Train Epoch: 6 [22400/60000 (37%)]\tLoss: 0.091959\n","Train Epoch: 6 [23040/60000 (38%)]\tLoss: 0.140949\n","Train Epoch: 6 [23680/60000 (39%)]\tLoss: 0.144790\n","Train Epoch: 6 [24320/60000 (41%)]\tLoss: 0.083413\n","Train Epoch: 6 [24960/60000 (42%)]\tLoss: 0.313580\n","Train Epoch: 6 [25600/60000 (43%)]\tLoss: 0.083314\n","Train Epoch: 6 [26240/60000 (44%)]\tLoss: 0.205856\n","Train Epoch: 6 [26880/60000 (45%)]\tLoss: 0.043995\n","Train Epoch: 6 [27520/60000 (46%)]\tLoss: 0.047280\n","Train Epoch: 6 [28160/60000 (47%)]\tLoss: 0.078213\n","Train Epoch: 6 [28800/60000 (48%)]\tLoss: 0.078097\n","Train Epoch: 6 [29440/60000 (49%)]\tLoss: 0.128358\n","Train Epoch: 6 [30080/60000 (50%)]\tLoss: 0.069876\n","Train Epoch: 6 [30720/60000 (51%)]\tLoss: 0.050988\n","Train Epoch: 6 [31360/60000 (52%)]\tLoss: 0.008448\n","Train Epoch: 6 [32000/60000 (53%)]\tLoss: 0.119987\n","Train Epoch: 6 [32640/60000 (54%)]\tLoss: 0.059463\n","Train Epoch: 6 [33280/60000 (55%)]\tLoss: 0.034140\n","Train Epoch: 6 [33920/60000 (57%)]\tLoss: 0.107322\n","Train Epoch: 6 [34560/60000 (58%)]\tLoss: 0.039880\n","Train Epoch: 6 [35200/60000 (59%)]\tLoss: 0.024856\n","Train Epoch: 6 [35840/60000 (60%)]\tLoss: 0.041192\n","Train Epoch: 6 [36480/60000 (61%)]\tLoss: 0.008286\n","Train Epoch: 6 [37120/60000 (62%)]\tLoss: 0.020014\n","Train Epoch: 6 [37760/60000 (63%)]\tLoss: 0.109036\n","Train Epoch: 6 [38400/60000 (64%)]\tLoss: 0.020348\n","Train Epoch: 6 [39040/60000 (65%)]\tLoss: 0.083901\n","Train Epoch: 6 [39680/60000 (66%)]\tLoss: 0.034993\n","Train Epoch: 6 [40320/60000 (67%)]\tLoss: 0.030488\n","Train Epoch: 6 [40960/60000 (68%)]\tLoss: 0.006982\n","Train Epoch: 6 [41600/60000 (69%)]\tLoss: 0.076036\n","Train Epoch: 6 [42240/60000 (70%)]\tLoss: 0.020263\n","Train Epoch: 6 [42880/60000 (71%)]\tLoss: 0.018643\n","Train Epoch: 6 [43520/60000 (72%)]\tLoss: 0.034303\n","Train Epoch: 6 [44160/60000 (74%)]\tLoss: 0.077171\n","Train Epoch: 6 [44800/60000 (75%)]\tLoss: 0.054413\n","Train Epoch: 6 [45440/60000 (76%)]\tLoss: 0.071359\n","Train Epoch: 6 [46080/60000 (77%)]\tLoss: 0.085607\n","Train Epoch: 6 [46720/60000 (78%)]\tLoss: 0.019536\n","Train Epoch: 6 [47360/60000 (79%)]\tLoss: 0.014665\n","Train Epoch: 6 [48000/60000 (80%)]\tLoss: 0.020101\n","Train Epoch: 6 [48640/60000 (81%)]\tLoss: 0.108873\n","Train Epoch: 6 [49280/60000 (82%)]\tLoss: 0.056202\n","Train Epoch: 6 [49920/60000 (83%)]\tLoss: 0.068794\n","Train Epoch: 6 [50560/60000 (84%)]\tLoss: 0.040660\n","Train Epoch: 6 [51200/60000 (85%)]\tLoss: 0.137414\n","Train Epoch: 6 [51840/60000 (86%)]\tLoss: 0.041217\n","Train Epoch: 6 [52480/60000 (87%)]\tLoss: 0.103086\n","Train Epoch: 6 [53120/60000 (88%)]\tLoss: 0.088803\n","Train Epoch: 6 [53760/60000 (90%)]\tLoss: 0.111982\n","Train Epoch: 6 [54400/60000 (91%)]\tLoss: 0.021785\n","Train Epoch: 6 [55040/60000 (92%)]\tLoss: 0.025898\n","Train Epoch: 6 [55680/60000 (93%)]\tLoss: 0.130746\n","Train Epoch: 6 [56320/60000 (94%)]\tLoss: 0.200020\n","Train Epoch: 6 [56960/60000 (95%)]\tLoss: 0.024601\n","Train Epoch: 6 [57600/60000 (96%)]\tLoss: 0.075626\n","Train Epoch: 6 [58240/60000 (97%)]\tLoss: 0.042478\n","Train Epoch: 6 [58880/60000 (98%)]\tLoss: 0.036355\n","Train Epoch: 6 [59520/60000 (99%)]\tLoss: 0.128789\n","\n","Test set: Average loss: 0.0590, Accuracy: 9811/10000 (98%)\n","\n","Train Epoch: 7 [0/60000 (0%)]\tLoss: 0.027446\n","Train Epoch: 7 [640/60000 (1%)]\tLoss: 0.057849\n","Train Epoch: 7 [1280/60000 (2%)]\tLoss: 0.018565\n","Train Epoch: 7 [1920/60000 (3%)]\tLoss: 0.073099\n","Train Epoch: 7 [2560/60000 (4%)]\tLoss: 0.113838\n","Train Epoch: 7 [3200/60000 (5%)]\tLoss: 0.107023\n","Train Epoch: 7 [3840/60000 (6%)]\tLoss: 0.095157\n","Train Epoch: 7 [4480/60000 (7%)]\tLoss: 0.088077\n","Train Epoch: 7 [5120/60000 (9%)]\tLoss: 0.030950\n","Train Epoch: 7 [5760/60000 (10%)]\tLoss: 0.017883\n","Train Epoch: 7 [6400/60000 (11%)]\tLoss: 0.070430\n","Train Epoch: 7 [7040/60000 (12%)]\tLoss: 0.023073\n","Train Epoch: 7 [7680/60000 (13%)]\tLoss: 0.069428\n","Train Epoch: 7 [8320/60000 (14%)]\tLoss: 0.070496\n","Train Epoch: 7 [8960/60000 (15%)]\tLoss: 0.026751\n","Train Epoch: 7 [9600/60000 (16%)]\tLoss: 0.096535\n","Train Epoch: 7 [10240/60000 (17%)]\tLoss: 0.049664\n","Train Epoch: 7 [10880/60000 (18%)]\tLoss: 0.019569\n","Train Epoch: 7 [11520/60000 (19%)]\tLoss: 0.067366\n","Train Epoch: 7 [12160/60000 (20%)]\tLoss: 0.075393\n","Train Epoch: 7 [12800/60000 (21%)]\tLoss: 0.185842\n","Train Epoch: 7 [13440/60000 (22%)]\tLoss: 0.078642\n","Train Epoch: 7 [14080/60000 (23%)]\tLoss: 0.044130\n","Train Epoch: 7 [14720/60000 (25%)]\tLoss: 0.004308\n","Train Epoch: 7 [15360/60000 (26%)]\tLoss: 0.012831\n","Train Epoch: 7 [16000/60000 (27%)]\tLoss: 0.018223\n","Train Epoch: 7 [16640/60000 (28%)]\tLoss: 0.074166\n","Train Epoch: 7 [17280/60000 (29%)]\tLoss: 0.195055\n","Train Epoch: 7 [17920/60000 (30%)]\tLoss: 0.062608\n","Train Epoch: 7 [18560/60000 (31%)]\tLoss: 0.097297\n","Train Epoch: 7 [19200/60000 (32%)]\tLoss: 0.104665\n","Train Epoch: 7 [19840/60000 (33%)]\tLoss: 0.033049\n","Train Epoch: 7 [20480/60000 (34%)]\tLoss: 0.095092\n","Train Epoch: 7 [21120/60000 (35%)]\tLoss: 0.085473\n","Train Epoch: 7 [21760/60000 (36%)]\tLoss: 0.009824\n","Train Epoch: 7 [22400/60000 (37%)]\tLoss: 0.042958\n","Train Epoch: 7 [23040/60000 (38%)]\tLoss: 0.040430\n","Train Epoch: 7 [23680/60000 (39%)]\tLoss: 0.065965\n","Train Epoch: 7 [24320/60000 (41%)]\tLoss: 0.078175\n","Train Epoch: 7 [24960/60000 (42%)]\tLoss: 0.160488\n","Train Epoch: 7 [25600/60000 (43%)]\tLoss: 0.026237\n","Train Epoch: 7 [26240/60000 (44%)]\tLoss: 0.032473\n","Train Epoch: 7 [26880/60000 (45%)]\tLoss: 0.055328\n","Train Epoch: 7 [27520/60000 (46%)]\tLoss: 0.204015\n","Train Epoch: 7 [28160/60000 (47%)]\tLoss: 0.030061\n","Train Epoch: 7 [28800/60000 (48%)]\tLoss: 0.091662\n","Train Epoch: 7 [29440/60000 (49%)]\tLoss: 0.118311\n","Train Epoch: 7 [30080/60000 (50%)]\tLoss: 0.091772\n","Train Epoch: 7 [30720/60000 (51%)]\tLoss: 0.011336\n","Train Epoch: 7 [31360/60000 (52%)]\tLoss: 0.054833\n","Train Epoch: 7 [32000/60000 (53%)]\tLoss: 0.015297\n","Train Epoch: 7 [32640/60000 (54%)]\tLoss: 0.056661\n","Train Epoch: 7 [33280/60000 (55%)]\tLoss: 0.039641\n","Train Epoch: 7 [33920/60000 (57%)]\tLoss: 0.035275\n","Train Epoch: 7 [34560/60000 (58%)]\tLoss: 0.167531\n","Train Epoch: 7 [35200/60000 (59%)]\tLoss: 0.162255\n","Train Epoch: 7 [35840/60000 (60%)]\tLoss: 0.035104\n","Train Epoch: 7 [36480/60000 (61%)]\tLoss: 0.098472\n","Train Epoch: 7 [37120/60000 (62%)]\tLoss: 0.141814\n","Train Epoch: 7 [37760/60000 (63%)]\tLoss: 0.043672\n","Train Epoch: 7 [38400/60000 (64%)]\tLoss: 0.021736\n","Train Epoch: 7 [39040/60000 (65%)]\tLoss: 0.046346\n","Train Epoch: 7 [39680/60000 (66%)]\tLoss: 0.046020\n","Train Epoch: 7 [40320/60000 (67%)]\tLoss: 0.155541\n","Train Epoch: 7 [40960/60000 (68%)]\tLoss: 0.064748\n","Train Epoch: 7 [41600/60000 (69%)]\tLoss: 0.130234\n","Train Epoch: 7 [42240/60000 (70%)]\tLoss: 0.033083\n","Train Epoch: 7 [42880/60000 (71%)]\tLoss: 0.111044\n","Train Epoch: 7 [43520/60000 (72%)]\tLoss: 0.029032\n","Train Epoch: 7 [44160/60000 (74%)]\tLoss: 0.056263\n","Train Epoch: 7 [44800/60000 (75%)]\tLoss: 0.182619\n","Train Epoch: 7 [45440/60000 (76%)]\tLoss: 0.062299\n","Train Epoch: 7 [46080/60000 (77%)]\tLoss: 0.065266\n","Train Epoch: 7 [46720/60000 (78%)]\tLoss: 0.026260\n","Train Epoch: 7 [47360/60000 (79%)]\tLoss: 0.034884\n","Train Epoch: 7 [48000/60000 (80%)]\tLoss: 0.051989\n","Train Epoch: 7 [48640/60000 (81%)]\tLoss: 0.183837\n","Train Epoch: 7 [49280/60000 (82%)]\tLoss: 0.023652\n","Train Epoch: 7 [49920/60000 (83%)]\tLoss: 0.187438\n","Train Epoch: 7 [50560/60000 (84%)]\tLoss: 0.070486\n","Train Epoch: 7 [51200/60000 (85%)]\tLoss: 0.110575\n","Train Epoch: 7 [51840/60000 (86%)]\tLoss: 0.090918\n","Train Epoch: 7 [52480/60000 (87%)]\tLoss: 0.048859\n","Train Epoch: 7 [53120/60000 (88%)]\tLoss: 0.108675\n","Train Epoch: 7 [53760/60000 (90%)]\tLoss: 0.013450\n","Train Epoch: 7 [54400/60000 (91%)]\tLoss: 0.036736\n","Train Epoch: 7 [55040/60000 (92%)]\tLoss: 0.010199\n","Train Epoch: 7 [55680/60000 (93%)]\tLoss: 0.012617\n","Train Epoch: 7 [56320/60000 (94%)]\tLoss: 0.018639\n","Train Epoch: 7 [56960/60000 (95%)]\tLoss: 0.129703\n","Train Epoch: 7 [57600/60000 (96%)]\tLoss: 0.064940\n","Train Epoch: 7 [58240/60000 (97%)]\tLoss: 0.034275\n","Train Epoch: 7 [58880/60000 (98%)]\tLoss: 0.046366\n","Train Epoch: 7 [59520/60000 (99%)]\tLoss: 0.138037\n","\n","Test set: Average loss: 0.0568, Accuracy: 9818/10000 (98%)\n","\n","Train Epoch: 8 [0/60000 (0%)]\tLoss: 0.030902\n","Train Epoch: 8 [640/60000 (1%)]\tLoss: 0.069248\n","Train Epoch: 8 [1280/60000 (2%)]\tLoss: 0.052577\n","Train Epoch: 8 [1920/60000 (3%)]\tLoss: 0.010542\n","Train Epoch: 8 [2560/60000 (4%)]\tLoss: 0.055952\n","Train Epoch: 8 [3200/60000 (5%)]\tLoss: 0.146861\n","Train Epoch: 8 [3840/60000 (6%)]\tLoss: 0.106005\n","Train Epoch: 8 [4480/60000 (7%)]\tLoss: 0.028623\n","Train Epoch: 8 [5120/60000 (9%)]\tLoss: 0.226924\n","Train Epoch: 8 [5760/60000 (10%)]\tLoss: 0.097328\n","Train Epoch: 8 [6400/60000 (11%)]\tLoss: 0.109660\n","Train Epoch: 8 [7040/60000 (12%)]\tLoss: 0.015541\n","Train Epoch: 8 [7680/60000 (13%)]\tLoss: 0.091478\n","Train Epoch: 8 [8320/60000 (14%)]\tLoss: 0.085349\n","Train Epoch: 8 [8960/60000 (15%)]\tLoss: 0.066903\n","Train Epoch: 8 [9600/60000 (16%)]\tLoss: 0.057481\n","Train Epoch: 8 [10240/60000 (17%)]\tLoss: 0.149614\n","Train Epoch: 8 [10880/60000 (18%)]\tLoss: 0.136077\n","Train Epoch: 8 [11520/60000 (19%)]\tLoss: 0.016766\n","Train Epoch: 8 [12160/60000 (20%)]\tLoss: 0.069750\n","Train Epoch: 8 [12800/60000 (21%)]\tLoss: 0.182109\n","Train Epoch: 8 [13440/60000 (22%)]\tLoss: 0.083194\n","Train Epoch: 8 [14080/60000 (23%)]\tLoss: 0.071351\n","Train Epoch: 8 [14720/60000 (25%)]\tLoss: 0.008377\n","Train Epoch: 8 [15360/60000 (26%)]\tLoss: 0.064033\n","Train Epoch: 8 [16000/60000 (27%)]\tLoss: 0.064232\n","Train Epoch: 8 [16640/60000 (28%)]\tLoss: 0.026477\n","Train Epoch: 8 [17280/60000 (29%)]\tLoss: 0.027296\n","Train Epoch: 8 [17920/60000 (30%)]\tLoss: 0.039661\n","Train Epoch: 8 [18560/60000 (31%)]\tLoss: 0.082156\n","Train Epoch: 8 [19200/60000 (32%)]\tLoss: 0.102997\n","Train Epoch: 8 [19840/60000 (33%)]\tLoss: 0.066092\n","Train Epoch: 8 [20480/60000 (34%)]\tLoss: 0.045206\n","Train Epoch: 8 [21120/60000 (35%)]\tLoss: 0.008797\n","Train Epoch: 8 [21760/60000 (36%)]\tLoss: 0.023354\n","Train Epoch: 8 [22400/60000 (37%)]\tLoss: 0.031647\n","Train Epoch: 8 [23040/60000 (38%)]\tLoss: 0.067573\n","Train Epoch: 8 [23680/60000 (39%)]\tLoss: 0.047655\n","Train Epoch: 8 [24320/60000 (41%)]\tLoss: 0.048774\n","Train Epoch: 8 [24960/60000 (42%)]\tLoss: 0.060055\n","Train Epoch: 8 [25600/60000 (43%)]\tLoss: 0.039187\n","Train Epoch: 8 [26240/60000 (44%)]\tLoss: 0.023926\n","Train Epoch: 8 [26880/60000 (45%)]\tLoss: 0.134538\n","Train Epoch: 8 [27520/60000 (46%)]\tLoss: 0.013602\n","Train Epoch: 8 [28160/60000 (47%)]\tLoss: 0.009521\n","Train Epoch: 8 [28800/60000 (48%)]\tLoss: 0.035961\n","Train Epoch: 8 [29440/60000 (49%)]\tLoss: 0.041723\n","Train Epoch: 8 [30080/60000 (50%)]\tLoss: 0.066596\n","Train Epoch: 8 [30720/60000 (51%)]\tLoss: 0.011923\n","Train Epoch: 8 [31360/60000 (52%)]\tLoss: 0.071704\n","Train Epoch: 8 [32000/60000 (53%)]\tLoss: 0.066810\n","Train Epoch: 8 [32640/60000 (54%)]\tLoss: 0.008158\n","Train Epoch: 8 [33280/60000 (55%)]\tLoss: 0.098795\n","Train Epoch: 8 [33920/60000 (57%)]\tLoss: 0.111603\n","Train Epoch: 8 [34560/60000 (58%)]\tLoss: 0.057113\n","Train Epoch: 8 [35200/60000 (59%)]\tLoss: 0.095835\n","Train Epoch: 8 [35840/60000 (60%)]\tLoss: 0.026821\n","Train Epoch: 8 [36480/60000 (61%)]\tLoss: 0.055622\n","Train Epoch: 8 [37120/60000 (62%)]\tLoss: 0.044212\n","Train Epoch: 8 [37760/60000 (63%)]\tLoss: 0.142767\n","Train Epoch: 8 [38400/60000 (64%)]\tLoss: 0.112390\n","Train Epoch: 8 [39040/60000 (65%)]\tLoss: 0.044607\n","Train Epoch: 8 [39680/60000 (66%)]\tLoss: 0.077154\n","Train Epoch: 8 [40320/60000 (67%)]\tLoss: 0.042538\n","Train Epoch: 8 [40960/60000 (68%)]\tLoss: 0.107770\n","Train Epoch: 8 [41600/60000 (69%)]\tLoss: 0.010396\n","Train Epoch: 8 [42240/60000 (70%)]\tLoss: 0.018529\n","Train Epoch: 8 [42880/60000 (71%)]\tLoss: 0.056732\n","Train Epoch: 8 [43520/60000 (72%)]\tLoss: 0.021533\n","Train Epoch: 8 [44160/60000 (74%)]\tLoss: 0.030349\n","Train Epoch: 8 [44800/60000 (75%)]\tLoss: 0.035354\n","Train Epoch: 8 [45440/60000 (76%)]\tLoss: 0.011673\n","Train Epoch: 8 [46080/60000 (77%)]\tLoss: 0.122716\n","Train Epoch: 8 [46720/60000 (78%)]\tLoss: 0.050304\n","Train Epoch: 8 [47360/60000 (79%)]\tLoss: 0.034485\n","Train Epoch: 8 [48000/60000 (80%)]\tLoss: 0.040055\n","Train Epoch: 8 [48640/60000 (81%)]\tLoss: 0.011194\n","Train Epoch: 8 [49280/60000 (82%)]\tLoss: 0.043793\n","Train Epoch: 8 [49920/60000 (83%)]\tLoss: 0.096494\n","Train Epoch: 8 [50560/60000 (84%)]\tLoss: 0.032253\n","Train Epoch: 8 [51200/60000 (85%)]\tLoss: 0.008902\n","Train Epoch: 8 [51840/60000 (86%)]\tLoss: 0.120314\n","Train Epoch: 8 [52480/60000 (87%)]\tLoss: 0.009312\n","Train Epoch: 8 [53120/60000 (88%)]\tLoss: 0.018848\n","Train Epoch: 8 [53760/60000 (90%)]\tLoss: 0.055998\n","Train Epoch: 8 [54400/60000 (91%)]\tLoss: 0.068294\n","Train Epoch: 8 [55040/60000 (92%)]\tLoss: 0.050377\n","Train Epoch: 8 [55680/60000 (93%)]\tLoss: 0.079662\n","Train Epoch: 8 [56320/60000 (94%)]\tLoss: 0.012034\n","Train Epoch: 8 [56960/60000 (95%)]\tLoss: 0.050661\n","Train Epoch: 8 [57600/60000 (96%)]\tLoss: 0.064483\n","Train Epoch: 8 [58240/60000 (97%)]\tLoss: 0.084225\n","Train Epoch: 8 [58880/60000 (98%)]\tLoss: 0.093971\n","Train Epoch: 8 [59520/60000 (99%)]\tLoss: 0.039791\n","\n","Test set: Average loss: 0.0569, Accuracy: 9826/10000 (98%)\n","\n","Train Epoch: 9 [0/60000 (0%)]\tLoss: 0.044151\n","Train Epoch: 9 [640/60000 (1%)]\tLoss: 0.045283\n","Train Epoch: 9 [1280/60000 (2%)]\tLoss: 0.053649\n","Train Epoch: 9 [1920/60000 (3%)]\tLoss: 0.168428\n","Train Epoch: 9 [2560/60000 (4%)]\tLoss: 0.157215\n","Train Epoch: 9 [3200/60000 (5%)]\tLoss: 0.027589\n","Train Epoch: 9 [3840/60000 (6%)]\tLoss: 0.036009\n","Train Epoch: 9 [4480/60000 (7%)]\tLoss: 0.026344\n","Train Epoch: 9 [5120/60000 (9%)]\tLoss: 0.012583\n","Train Epoch: 9 [5760/60000 (10%)]\tLoss: 0.016198\n","Train Epoch: 9 [6400/60000 (11%)]\tLoss: 0.143745\n","Train Epoch: 9 [7040/60000 (12%)]\tLoss: 0.037995\n","Train Epoch: 9 [7680/60000 (13%)]\tLoss: 0.024398\n","Train Epoch: 9 [8320/60000 (14%)]\tLoss: 0.089963\n","Train Epoch: 9 [8960/60000 (15%)]\tLoss: 0.064179\n","Train Epoch: 9 [9600/60000 (16%)]\tLoss: 0.084939\n","Train Epoch: 9 [10240/60000 (17%)]\tLoss: 0.082824\n","Train Epoch: 9 [10880/60000 (18%)]\tLoss: 0.047288\n","Train Epoch: 9 [11520/60000 (19%)]\tLoss: 0.105988\n","Train Epoch: 9 [12160/60000 (20%)]\tLoss: 0.027107\n","Train Epoch: 9 [12800/60000 (21%)]\tLoss: 0.137777\n","Train Epoch: 9 [13440/60000 (22%)]\tLoss: 0.017977\n","Train Epoch: 9 [14080/60000 (23%)]\tLoss: 0.051539\n","Train Epoch: 9 [14720/60000 (25%)]\tLoss: 0.010590\n","Train Epoch: 9 [15360/60000 (26%)]\tLoss: 0.016972\n","Train Epoch: 9 [16000/60000 (27%)]\tLoss: 0.008145\n","Train Epoch: 9 [16640/60000 (28%)]\tLoss: 0.116364\n","Train Epoch: 9 [17280/60000 (29%)]\tLoss: 0.022904\n","Train Epoch: 9 [17920/60000 (30%)]\tLoss: 0.048692\n","Train Epoch: 9 [18560/60000 (31%)]\tLoss: 0.077425\n","Train Epoch: 9 [19200/60000 (32%)]\tLoss: 0.033022\n","Train Epoch: 9 [19840/60000 (33%)]\tLoss: 0.039316\n","Train Epoch: 9 [20480/60000 (34%)]\tLoss: 0.022077\n","Train Epoch: 9 [21120/60000 (35%)]\tLoss: 0.078608\n","Train Epoch: 9 [21760/60000 (36%)]\tLoss: 0.192310\n","Train Epoch: 9 [22400/60000 (37%)]\tLoss: 0.083671\n","Train Epoch: 9 [23040/60000 (38%)]\tLoss: 0.030281\n","Train Epoch: 9 [23680/60000 (39%)]\tLoss: 0.013401\n","Train Epoch: 9 [24320/60000 (41%)]\tLoss: 0.028309\n","Train Epoch: 9 [24960/60000 (42%)]\tLoss: 0.019858\n","Train Epoch: 9 [25600/60000 (43%)]\tLoss: 0.011371\n","Train Epoch: 9 [26240/60000 (44%)]\tLoss: 0.031877\n","Train Epoch: 9 [26880/60000 (45%)]\tLoss: 0.008506\n","Train Epoch: 9 [27520/60000 (46%)]\tLoss: 0.043244\n","Train Epoch: 9 [28160/60000 (47%)]\tLoss: 0.012996\n","Train Epoch: 9 [28800/60000 (48%)]\tLoss: 0.065282\n","Train Epoch: 9 [29440/60000 (49%)]\tLoss: 0.018451\n","Train Epoch: 9 [30080/60000 (50%)]\tLoss: 0.142351\n","Train Epoch: 9 [30720/60000 (51%)]\tLoss: 0.120714\n","Train Epoch: 9 [31360/60000 (52%)]\tLoss: 0.242446\n","Train Epoch: 9 [32000/60000 (53%)]\tLoss: 0.063895\n","Train Epoch: 9 [32640/60000 (54%)]\tLoss: 0.153084\n","Train Epoch: 9 [33280/60000 (55%)]\tLoss: 0.062672\n","Train Epoch: 9 [33920/60000 (57%)]\tLoss: 0.027290\n","Train Epoch: 9 [34560/60000 (58%)]\tLoss: 0.043349\n","Train Epoch: 9 [35200/60000 (59%)]\tLoss: 0.114298\n","Train Epoch: 9 [35840/60000 (60%)]\tLoss: 0.055250\n","Train Epoch: 9 [36480/60000 (61%)]\tLoss: 0.024633\n","Train Epoch: 9 [37120/60000 (62%)]\tLoss: 0.018897\n","Train Epoch: 9 [37760/60000 (63%)]\tLoss: 0.027992\n","Train Epoch: 9 [38400/60000 (64%)]\tLoss: 0.051963\n","Train Epoch: 9 [39040/60000 (65%)]\tLoss: 0.053873\n","Train Epoch: 9 [39680/60000 (66%)]\tLoss: 0.045370\n","Train Epoch: 9 [40320/60000 (67%)]\tLoss: 0.051287\n","Train Epoch: 9 [40960/60000 (68%)]\tLoss: 0.078770\n","Train Epoch: 9 [41600/60000 (69%)]\tLoss: 0.020329\n","Train Epoch: 9 [42240/60000 (70%)]\tLoss: 0.031823\n","Train Epoch: 9 [42880/60000 (71%)]\tLoss: 0.108140\n","Train Epoch: 9 [43520/60000 (72%)]\tLoss: 0.042436\n","Train Epoch: 9 [44160/60000 (74%)]\tLoss: 0.160215\n","Train Epoch: 9 [44800/60000 (75%)]\tLoss: 0.178903\n","Train Epoch: 9 [45440/60000 (76%)]\tLoss: 0.057151\n","Train Epoch: 9 [46080/60000 (77%)]\tLoss: 0.120435\n","Train Epoch: 9 [46720/60000 (78%)]\tLoss: 0.068818\n","Train Epoch: 9 [47360/60000 (79%)]\tLoss: 0.019996\n","Train Epoch: 9 [48000/60000 (80%)]\tLoss: 0.034163\n","Train Epoch: 9 [48640/60000 (81%)]\tLoss: 0.185080\n","Train Epoch: 9 [49280/60000 (82%)]\tLoss: 0.032653\n","Train Epoch: 9 [49920/60000 (83%)]\tLoss: 0.061518\n","Train Epoch: 9 [50560/60000 (84%)]\tLoss: 0.043168\n","Train Epoch: 9 [51200/60000 (85%)]\tLoss: 0.011923\n","Train Epoch: 9 [51840/60000 (86%)]\tLoss: 0.031496\n","Train Epoch: 9 [52480/60000 (87%)]\tLoss: 0.025777\n","Train Epoch: 9 [53120/60000 (88%)]\tLoss: 0.241448\n","Train Epoch: 9 [53760/60000 (90%)]\tLoss: 0.012139\n","Train Epoch: 9 [54400/60000 (91%)]\tLoss: 0.085234\n","Train Epoch: 9 [55040/60000 (92%)]\tLoss: 0.044432\n","Train Epoch: 9 [55680/60000 (93%)]\tLoss: 0.109829\n","Train Epoch: 9 [56320/60000 (94%)]\tLoss: 0.081245\n","Train Epoch: 9 [56960/60000 (95%)]\tLoss: 0.012760\n","Train Epoch: 9 [57600/60000 (96%)]\tLoss: 0.079020\n","Train Epoch: 9 [58240/60000 (97%)]\tLoss: 0.021044\n","Train Epoch: 9 [58880/60000 (98%)]\tLoss: 0.014029\n","Train Epoch: 9 [59520/60000 (99%)]\tLoss: 0.036613\n","\n","Test set: Average loss: 0.0560, Accuracy: 9820/10000 (98%)\n","\n"]}]}]}