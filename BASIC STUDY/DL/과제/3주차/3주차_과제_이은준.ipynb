{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# < 3주차 과제 1 : MLP 마음대로 다뤄보기>\n","- dataset을 임의로 선정해서 직접 분석 해보기(제공한 코드를 활용해서 해보기)\n","- activation functions 중 relu사용시 함수 직접 정의\n","- lr, optimizer 등 바꿔보기\n","- hidden layer/neuron 수를 바꾸기\n","- 전처리도 추가\n","- 모든 시도를 올려주세요!\n","- 제일 높은 acc(Accuracy)를 보인 시도를 명시해주세요!\n"],"metadata":{"id":"sgAYo4nrw2F4"}},{"cell_type":"code","execution_count":1,"metadata":{"id":"fX437IL6qbI-","executionInfo":{"status":"ok","timestamp":1706695593692,"user_tz":-540,"elapsed":5936,"user":{"displayName":"EunJun Lee","userId":"12137931568709711475"}}},"outputs":[],"source":["import torch\n","from torch import nn, optim\n","from sklearn.datasets import load_breast_cancer\n","from torch.utils.data import  TensorDataset, DataLoader\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score\n","import numpy as np\n","from sklearn.preprocessing import StandardScaler\n","import matplotlib.pyplot as plt"]},{"cell_type":"markdown","source":["## 아래 데이터셋 중 원하는 데이터셋 하나를 선택하여, 코드 과제 진행하기!\n","- 1) load_digits() <br>\n","- 2) load_wine()"],"metadata":{"id":"oxkFzBDNmWNk"}},{"cell_type":"code","source":["from sklearn.datasets import load_wine\n","# 데이터셋 종류 :\n","data = load_wine()"],"metadata":{"id":"FywYbfsKtjcR","executionInfo":{"status":"ok","timestamp":1706695593692,"user_tz":-540,"elapsed":9,"user":{"displayName":"EunJun Lee","userId":"12137931568709711475"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["input = data.data\n","output = data.target"],"metadata":{"id":"C2P0hqZ9yBGm","executionInfo":{"status":"ok","timestamp":1706695593692,"user_tz":-540,"elapsed":7,"user":{"displayName":"EunJun Lee","userId":"12137931568709711475"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","\n","torch.manual_seed(777)\n","if device == \"cuda\":\n","  torch.cuda.manual_seed_all(777)"],"metadata":{"id":"SggpQfSPt85C","executionInfo":{"status":"ok","timestamp":1706695593693,"user_tz":-540,"elapsed":8,"user":{"displayName":"EunJun Lee","userId":"12137931568709711475"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["x_train, x_test, y_train, y_test = train_test_split(input, output, test_size = 0.3, random_state = 42, stratify= data.target, shuffle = True) #stratify는 클래스의 분포를 유지, 모델이 특정 클래스에 편향되지 않도록 도움\n","\n","x_train = torch.FloatTensor(x_train).to(device)\n","y_train = torch.LongTensor(y_train).to(device)\n","x_test = torch.FloatTensor(x_test)\n","y_test = torch.LongTensor(y_test)\n","\n","# 데이터를 tensor로 바꿔주고 gpu 연산이 가능해지도록 gpu에 옮김\n","# label 값을 왜 long(정수형 레이블) 에 옮겨놓는가? -> loss function이 다르기 때문 (Neural network의 loss function은 주로 정수형 레이블을 입력으로 받아 계산)"],"metadata":{"id":"bLMzf-2ntYeX","executionInfo":{"status":"ok","timestamp":1706695594249,"user_tz":-540,"elapsed":563,"user":{"displayName":"EunJun Lee","userId":"12137931568709711475"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["print(x_train[0])\n","print(y_train)\n","\n","# input 13개 (속성이 13개)\n","# ylabel은 3개"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"umEdiTZkrVqS","outputId":"a8207c6e-c9e3-417a-b7f6-9a70ffb4cafd","executionInfo":{"status":"ok","timestamp":1706696832542,"user_tz":-540,"elapsed":8,"user":{"displayName":"EunJun Lee","userId":"12137931568709711475"}}},"execution_count":32,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([1.3750e+01, 1.7300e+00, 2.4100e+00, 1.6000e+01, 8.9000e+01, 2.6000e+00,\n","        2.7600e+00, 2.9000e-01, 1.8100e+00, 5.6000e+00, 1.1500e+00, 2.9000e+00,\n","        1.3200e+03], device='cuda:0')\n","tensor([0, 0, 0, 0, 1, 0, 2, 1, 2, 0, 1, 1, 2, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1,\n","        1, 2, 1, 0, 2, 1, 2, 2, 0, 2, 1, 1, 1, 1, 2, 0, 0, 0, 0, 0, 0, 1, 1, 2,\n","        0, 2, 0, 0, 0, 2, 0, 2, 1, 1, 1, 0, 1, 0, 1, 1, 0, 2, 1, 0, 0, 1, 1, 2,\n","        0, 0, 2, 1, 1, 1, 2, 2, 2, 2, 1, 1, 0, 0, 1, 1, 1, 2, 2, 1, 1, 2, 0, 0,\n","        0, 2, 1, 2, 1, 1, 2, 0, 1, 1, 0, 1, 2, 1, 2, 0, 2, 0, 0, 1, 1, 2, 0, 2,\n","        1, 2, 2, 1], device='cuda:0')\n"]}]},{"cell_type":"markdown","source":["파이토치에서는 데이터셋을 좀 더 쉽게 다룰 수 있도록 유용한 도구로서 torch.utils.data.Dataset과 torch.utils.data.DataLoader를 제공합니다. 이를 사용하면 미니 배치 학습, 데이터 셔플(shuffle), 병렬 처리까지 간단히 수행할 수 있습니다. 기본적인 사용 방법은 Dataset을 정의하고, 이를 DataLoader에 전달하는 것\n","- init : class 에서 객체가 생성되면 바로 실행되는 함수\n","- len : observation 수를 정의하는 함수\n","- getitem : iteration 마다 해당하는 데이터를 돌려주는 함수"],"metadata":{"id":"combmxzmYFyn"}},{"cell_type":"code","source":["from torch.utils.data import Dataset\n","class CustomDataset(Dataset):\n","  def __init__(self, x_train, y_train, device): # device는 cpu 차원을 cuda 차원으로 넘겨주는 역할을 함\n","    self.x_data = x_train\n","    self.y_data = [[y] for y in y_train]\n","    self.device = device\n","    # 데이터셋의 전처리를 해주는 부분\n","    # 여기에서는 간단한 스케일링을 예시로 추가\n","    self.x_data = self.preprocessing(self.x_data)\n","\n","  def preprocessing(self, data):\n","        # 간단한 Min-Max 스케일링을 예시로 추가\n","        min_val = min(map(min, data))\n","        max_val = max(map(max, data))\n","        scaled_data = [[(val - min_val) / (max_val - min_val) for val in row] for row in data]\n","        return scaled_data\n","\n","  def __len__(self):\n","    return len(self.x_data)\n","#  데이터셋의 길이. 즉, 총 샘플의 수를 적어주는 부분\n","\n","  def __getitem__(self, idx): # idx(인덱스)에 해당하는 샘플 반환\n","    x = torch.FloatTensor(self.x_data[idx]).to(self.device) # Tensor를 cuda차원으로 보내줌\n","    y = torch.LongTensor(self.y_data[idx]).to(self.device)\n","#  데이터셋에서 특정 1개의 샘플을 가져오는 함수\n","\n","    return x,y"],"metadata":{"id":"y38TlgXoqV5Z","executionInfo":{"status":"ok","timestamp":1706695595107,"user_tz":-540,"elapsed":5,"user":{"displayName":"EunJun Lee","userId":"12137931568709711475"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["# 해당 부분 아래서 사용 X 왜 쓴 건지 잘 모르겠음.\n","\n","batch_size = 8\n","\n","dataset = CustomDataset(x_train, y_train, device)\n","dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)"],"metadata":{"id":"x8VHwnuFqino","executionInfo":{"status":"ok","timestamp":1706695595459,"user_tz":-540,"elapsed":356,"user":{"displayName":"EunJun Lee","userId":"12137931568709711475"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["# input_layer와 hidden_layer의 노드 개수를 어떻게 바꿔줘야할까?\n","# hidden layer/neuron 수를 바꾸기\n","\n","\n","model = nn.Sequential(\n","          nn.Linear(13,398, bias=True), #첫번재 Hidden layer의 노드수는 398개\n","          nn.ReLU(),\n","          nn.Linear(398,15, bias=True), #두번재 Hidden layer의 노드수는 15개\n","          nn.ReLU(),\n","          nn.Linear(15,3, bias=True), #세번재 Hidden layer의 노드수는 3개 -> y의 Label 3개이기 때문\n","          nn.Softmax(dim=1) # 두개의 node 중 더 확률이 높은 값을 사용하려고 Softmax 사용\n","          ).to(device)"],"metadata":{"id":"C6V7a4tyq6Jc","executionInfo":{"status":"ok","timestamp":1706695595459,"user_tz":-540,"elapsed":11,"user":{"displayName":"EunJun Lee","userId":"12137931568709711475"}}},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":["class로 구현 가능\n","- init : 초기 생성 함수\n","- foward : 순전파(입력값 => 예측값 의 과정)"],"metadata":{"id":"07uV8RY7Yr_5"}},{"cell_type":"code","source":["class Model(torch.nn.Module):\n","\n","  def __init__(self, input_size):\n","    super(Model, self).__init__()\n","    self.layer1 = nn.Sequential(\n","          nn.Linear(input_size,398, bias=True), # input_layer = 13, hidden_layer1 = 398\n","          nn.ReLU(), # sigmoid() 대신 ReLU() 사용 -> sigmoid()의 기울기에 대한 단점 보완, 계산 복잡도 낮아 효율성 증가\n","        nn.BatchNorm1d(398)\n","    )\n","  # activation function 이용\n","  #   nn.ReLU()\n","  #   nn.tanH()\n","  #   https://pytorch.org/docs/stable/nn.html 그 외에도 여기서 확인 가능함\n","  #   파라미터가 필요하지 않다는 것이 특징\n","\n","  # batch normazliation 1d, 파라미터 값으로 vector의 길이를 전해줌\n","  # 추후에 이미지를 다루게 된다면, 그때는 batch normalization 2d를 이용하게 됨\n","  # 그때는 파라미터 값으로 채널, 가로, 세로 길이를 전달해주게 됨\n","\n","    self.layer2 = nn.Sequential(\n","          nn.Linear(398,15, bias=True), # hidden_layer1 = 398, hidden_layer2 = 15\n","        nn.ReLU()\n","    )\n","    self.layer3 = nn.Sequential(\n","          nn.Linear(15,10, bias=True), # hidden_layer1 = 15, hidden_layer2 = 10\n","        nn.ReLU()\n","    )\n","    self.layer4 = nn.Sequential(\n","        nn.Linear(10, 3, bias=True), # hidden_layer3 = 10, output_layer = 2\n","        nn.Softmax()\n","    )\n","\n","  def forward(self,x): # 순전파 정의 함수\n","    output = self.layer1(x)\n","    output = self.layer2(output)\n","    output = self.layer3(output)\n","    output = self.layer4(output)\n","    return output"],"metadata":{"id":"a0zLstbMqxEZ","executionInfo":{"status":"ok","timestamp":1706695595460,"user_tz":-540,"elapsed":11,"user":{"displayName":"EunJun Lee","userId":"12137931568709711475"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["def init_weights(layer): # nerual network의 weight 초기화\n","    if isinstance(layer, nn.Linear): # 주어진 layer가 선형인지 확인\n","        torch.nn.init.xavier_uniform(layer.weight) # Xavier 초기화를 적용하여 가중치를 균등하게 초기화 -> 안정성 향상\n","        layer.bias.data.fill_(0.01) #bias를 0.01로 초기화\n","\n","        #xavier사용 -> 가중치를 특정한 분포(균등 분포 또는 정규 분포)에서 뽑아낸 값으로 초기화\n","        # Layer의 weight를 어떤 분포를 가지도록 초기화시켜줌+global minimum찾기 위해서"],"metadata":{"id":"kqcqqkECrSGK","executionInfo":{"status":"ok","timestamp":1706695595460,"user_tz":-540,"elapsed":11,"user":{"displayName":"EunJun Lee","userId":"12137931568709711475"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","source":["input_size = len(x_train[0])  # x_train의 첫 번째 샘플의 특성 수\n","model = Model(input_size).to(device)\n","model.apply(init_weights)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oMDUBFg6rUpw","outputId":"fdfc5441-eb3c-43cf-9bcf-b6b1ee1cb729","executionInfo":{"status":"ok","timestamp":1706695595460,"user_tz":-540,"elapsed":11,"user":{"displayName":"EunJun Lee","userId":"12137931568709711475"}}},"execution_count":12,"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-11-1376bb522da5>:3: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n","  torch.nn.init.xavier_uniform(layer.weight) # Xavier 초기화를 적용하여 가중치를 균등하게 초기화 -> 안정성 향상\n"]},{"output_type":"execute_result","data":{"text/plain":["Model(\n","  (layer1): Sequential(\n","    (0): Linear(in_features=13, out_features=398, bias=True)\n","    (1): ReLU()\n","    (2): BatchNorm1d(398, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","  )\n","  (layer2): Sequential(\n","    (0): Linear(in_features=398, out_features=15, bias=True)\n","    (1): ReLU()\n","  )\n","  (layer3): Sequential(\n","    (0): Linear(in_features=15, out_features=10, bias=True)\n","    (1): ReLU()\n","  )\n","  (layer4): Sequential(\n","    (0): Linear(in_features=10, out_features=3, bias=True)\n","    (1): Softmax(dim=None)\n","  )\n",")"]},"metadata":{},"execution_count":12}]},{"cell_type":"code","source":["print(model)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OwZt5CetrYFb","outputId":"0b395749-7323-4f69-f079-6a9e80147e73","executionInfo":{"status":"ok","timestamp":1706695595460,"user_tz":-540,"elapsed":9,"user":{"displayName":"EunJun Lee","userId":"12137931568709711475"}}},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["Model(\n","  (layer1): Sequential(\n","    (0): Linear(in_features=13, out_features=398, bias=True)\n","    (1): ReLU()\n","    (2): BatchNorm1d(398, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","  )\n","  (layer2): Sequential(\n","    (0): Linear(in_features=398, out_features=15, bias=True)\n","    (1): ReLU()\n","  )\n","  (layer3): Sequential(\n","    (0): Linear(in_features=15, out_features=10, bias=True)\n","    (1): ReLU()\n","  )\n","  (layer4): Sequential(\n","    (0): Linear(in_features=10, out_features=3, bias=True)\n","    (1): Softmax(dim=None)\n","  )\n",")\n"]}]},{"cell_type":"code","source":["loss_fn  = nn.CrossEntropyLoss().to(device) # 2개 이상의 label에 대해 classification의 측면에 사용되기 때문에 CrossEntropyLoss() 사용하는 것이 유리함\n","# 만약 1개 label에 대한 경우는 MSE, MAE 등 사용\n","\n","# 여러가지 optimizer 시도해보기\n","# lr 바꿔보기 -> 모든 optimizer에 대해 learning rate 계속 변경 결과 0.01이 가장 좋은 결과를 보임\n","\n","\n","optimizer = optim.Adagrad(model.parameters(), lr= 0.01) # 가장 좋은 결과를 보인 optimizer -> 이유는? 아마도 batch size를 적용하지 않았기 때문으로 보임.\n","\n","# https://pytorch.org/docs/stable/optim.html#module-torch.optim 페이지 참조\n","#optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n","#optimizer = optim.Adamax(model.parameters(), lr= 0.01)\n","#optimizer = optim.Adam(model.parameters(), lr= 0.01)\n","#optimizer = optim.AdamW(model.parameters(), lr= 0.01)\n","#optimizer = optim.NAdam(model.parameters(), lr= 0.01)\n","\n","# sgd 등등"],"metadata":{"id":"AYFp-eTErh7b","executionInfo":{"status":"ok","timestamp":1706695599882,"user_tz":-540,"elapsed":4429,"user":{"displayName":"EunJun Lee","userId":"12137931568709711475"}}},"execution_count":14,"outputs":[]},{"cell_type":"code","source":["losses = []\n","for epoch in range(100):\n","\n","  optimizer.zero_grad()\n","  hypothesis = model.forward(x_train) # 반복 실행 시 CUDA 관련 문제 발생 -> 아래쪽 참고\n","\n","  # 비용 함수\n","  cost = loss_fn(hypothesis, y_train)\n","  cost.backward()\n","  optimizer.step()\n","  losses.append(cost.item()) # 그래프 그리기 위해 Loss값을 losses라는 리스트에 추가, item()은 텐서에서 loss 결과값의 scaler만 추출하는 함수\n","\n","  if epoch % 10 == 0:\n","    print(epoch, cost.item())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"90QxHvlIrjS7","outputId":"2443b99f-b040-43e6-9a54-027cdef04d42","executionInfo":{"status":"ok","timestamp":1706695601371,"user_tz":-540,"elapsed":1493,"user":{"displayName":"EunJun Lee","userId":"12137931568709711475"}}},"execution_count":15,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1518: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  return self._call_impl(*args, **kwargs)\n"]},{"output_type":"stream","name":"stdout","text":["0 1.0395019054412842\n","10 0.7279837727546692\n","20 0.6512361764907837\n","30 0.6090585589408875\n","40 0.5944061875343323\n","50 0.5824767351150513\n","60 0.5831121206283569\n","70 0.5705000758171082\n","80 0.5632441639900208\n","90 0.5571518540382385\n"]}]},{"cell_type":"code","source":["plt.plot(losses)\n","plt.show()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":430},"id":"81ASYrW7roFM","outputId":"73d17f41-0bea-43cc-edc7-9ad4d0ff761e","executionInfo":{"status":"ok","timestamp":1706695601977,"user_tz":-540,"elapsed":611,"user":{"displayName":"EunJun Lee","userId":"12137931568709711475"}}},"execution_count":16,"outputs":[{"output_type":"display_data","data":{"text/plain":["<Figure size 640x480 with 1 Axes>"],"image/png":"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\n"},"metadata":{}}]},{"cell_type":"code","source":["with torch.no_grad():\n","  model = model.to(device) # 기존의 코드는 model이 numpy 이용을 위해 cpu로 전환되기 때문에 다른 optimizer 사용을 위한 코드 재시작 시 오류가 있었음.\n","  y_pred = model(x_test.to(device)) # x_test도 cuda에 위치하도록 변경\n","  y_pred = y_pred.detach().cpu().numpy()  # GPU 사용을 위해 cpu().numpy()로 변경, detach는 gradient 빼서 메모리를 아끼는 역할을 함.\n","  predicted = np.argmax(y_pred, axis =1) # 가장 높은 확률의 y_pred index를 predicted에 저장\n","  accuracy = (accuracy_score(predicted, y_test))"],"metadata":{"id":"h4kJzpLErqhZ","executionInfo":{"status":"ok","timestamp":1706695601977,"user_tz":-540,"elapsed":8,"user":{"displayName":"EunJun Lee","userId":"12137931568709711475"}}},"execution_count":17,"outputs":[]},{"cell_type":"code","source":["print(f'model의 output은 :  {y_pred[0]}')\n","print(f'argmax를 한 후의 output은 {predicted[0]}')\n","print(f'accuracy는 {accuracy}')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vyIKhs3Nr6Ay","outputId":"c0dd6bf2-93ad-48d8-913b-27ad5b5214e2","executionInfo":{"status":"ok","timestamp":1706695601978,"user_tz":-540,"elapsed":9,"user":{"displayName":"EunJun Lee","userId":"12137931568709711475"}}},"execution_count":18,"outputs":[{"output_type":"stream","name":"stdout","text":["model의 output은 :  [9.9987888e-01 9.7835175e-05 2.3262150e-05]\n","argmax를 한 후의 output은 0\n","accuracy는 0.9629629629629629\n"]}]},{"cell_type":"markdown","source":["# < 3주차 과제 2 : CNN 맛보기>\n","- FNN은 데이터가 많이 필요하다는 단점과 패턴 인식이 어렵다는 단점들이 존재\n","- But, CNN은 FNN에 비해 더 적은 parameter를 가짐 -> 더 많은 dataset이 강요되지 않음\n","- kernel을 사용하기에 기존 패턴 인식 단점 극복"],"metadata":{"id":"3RzRM7xThZV_"}},{"cell_type":"code","source":["from __future__ import print_function\n","import argparse\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","from torchvision import datasets, transforms\n","from torch.autograd import Variable"],"metadata":{"id":"56xqgtLxhZw6","executionInfo":{"status":"ok","timestamp":1706695602354,"user_tz":-540,"elapsed":383,"user":{"displayName":"EunJun Lee","userId":"12137931568709711475"}}},"execution_count":19,"outputs":[]},{"cell_type":"code","source":["# Training settings\n","\n","batch_size = 64\n","\n","# MNIST Dataset\n","train_dataset = datasets.MNIST(root='./data/',\n","                               train=True,\n","                               transform=transforms.ToTensor(),\n","                               download=True)\n","\n","test_dataset = datasets.MNIST(root='./data/',\n","                              train=False,\n","                              transform=transforms.ToTensor())\n","\n","# Data Loader (Input Pipeline)\n","train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n","                                           batch_size=batch_size,\n","                                           shuffle=True)\n","\n","test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n","                                          batch_size=batch_size,\n","                                          shuffle=False)"],"metadata":{"id":"TzkF2bFNhcQ2","executionInfo":{"status":"ok","timestamp":1706695604057,"user_tz":-540,"elapsed":1710,"user":{"displayName":"EunJun Lee","userId":"12137931568709711475"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"6d7332e8-f33d-44a7-8bab-a422f272ce11"},"execution_count":20,"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n","Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 9912422/9912422 [00:00<00:00, 74574062.92it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n","\n","Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n","Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 28881/28881 [00:00<00:00, 108156869.49it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n","\n","Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n","Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 1648877/1648877 [00:00<00:00, 50512667.78it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n","\n","Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n","Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 4542/4542 [00:00<00:00, 1541055.55it/s]"]},{"output_type":"stream","name":"stdout","text":["Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n","\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}]},{"cell_type":"markdown","source":["# CNN 공부 시 Convolution(패턴 인식) -> Pooling(사이즈 줄임)-> Activation(비선형성 부여) -> Batch_normalization(분포 정렬) 순서로 주로 사용."],"metadata":{"id":"UZpnyoB3b3KF"}},{"cell_type":"code","source":["class Net(nn.Module):\n","  def __init__(self):\n","    super(Net, self).__init__()\n","    self.conv1 = nn.Conv2d(1, 10, kernel_size = 5) # image 파일이기 때문에 convolution을 2차원으로 설정, 흑백 이미지이기에 input_size = 1, output_size가 커질수록 parameter가 많이 필요하나 패턴을 더 다양하게 인식 가능\n","    self.conv2 = nn.Conv2d(10, 20, kernel_size = 5) # Padding 값이 0임 -> convolution 계산 시 kernel이 input을 벗어나면 안됨.\n","    self.mp = nn.MaxPool2d(2) # output 에서 가장 큰 값 하나를 return하며 size를 줄임\n","    self.fc = nn.Linear(320, 10) ### : 알맞는 input은 -> conv2를 거친 후의 feature map 크기여야함. 따라서 20*4*4 (아래 feature map 크기 구하는 방법 참조)\n","\n","  def forward(self, x):\n","    in_size = x.size(0) # 28*28\n","    x = F.relu(self.mp(self.conv1(x))) # -> feature map size가 kernel_size가 5이기에 24*24로 변환, mp로 인해 12*12로 반환\n","    x = F.relu(self.mp(self.conv2(x))) # -> feature map size가 kernel_size가 5이기에 8*8로 변환, mp로 인해 4*4로 반환 즉, [64, 20, 4, 4] 형태의 그림으로 존재함.\n","    x = x.view(in_size, -1) # -> batch를 제외한 남은 차원을 하나로 합침. 즉, [64, 320] 형태로 변환\n","    x = self.fc(x)\n","    return F.log_softmax(x) # -> 10개의 output을 softmax 처리해서 log로 반환"],"metadata":{"id":"tLCSvgganBrH","executionInfo":{"status":"ok","timestamp":1706695604057,"user_tz":-540,"elapsed":5,"user":{"displayName":"EunJun Lee","userId":"12137931568709711475"}}},"execution_count":21,"outputs":[]},{"cell_type":"code","source":["model = Net()\n","optimizer = optim.SGD(model.parameters(), lr = 0.01, momentum = 0.5)"],"metadata":{"id":"lkYZ4pUdnUHc","executionInfo":{"status":"ok","timestamp":1706695604058,"user_tz":-540,"elapsed":5,"user":{"displayName":"EunJun Lee","userId":"12137931568709711475"}}},"execution_count":22,"outputs":[]},{"cell_type":"code","source":["def train(epoch):\n","  model.train()\n","  for batch_idx, (data, target) in enumerate(train_loader):\n","    data, target = Variable(data), Variable(target) # tensor에서 variable로 변환\n","    optimizer.zero_grad()\n","    output = model(data)\n","    loss = F.nll_loss(output, target) # 모델의 출력에 소프트 맥스를 적용해 로그로 변환(forward에서 return형태와 동일) 이후 CrossEntropyLoss 적용 -> 다중 클래스 분류 문제에 사용되는 손실 함수들 중 하나\n","    loss.backward()\n","    optimizer.step()\n","    if batch_idx % 10 == 0:\n","      print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n","          epoch, batch_idx * len(data), len(train_loader.dataset),\n","          100. * batch_idx / len(train_loader), loss.item()))"],"metadata":{"id":"IzUrEM3EnXJb","executionInfo":{"status":"ok","timestamp":1706695604058,"user_tz":-540,"elapsed":5,"user":{"displayName":"EunJun Lee","userId":"12137931568709711475"}}},"execution_count":23,"outputs":[]},{"cell_type":"code","source":["def test():\n","    model.eval() #model.eval() 의 기능은?\n","    test_loss = 0\n","    correct = 0\n","    for data, target in test_loader:\n","        data, target = Variable(data, volatile=True), Variable(target)\n","        output = model(data)\n","        test_loss += F.nll_loss(output, target, size_average=False).data # nll_loss?? / cross entropy loss와의 관계 확인!\n","        pred = output.data.max(1, keepdim=True)[1]\n","        correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n","\n","    test_loss /= len(test_loader.dataset)\n","    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n","        test_loss, correct, len(test_loader.dataset),\n","        100. * correct / len(test_loader.dataset)))"],"metadata":{"id":"EFi0gYJGn2aa","executionInfo":{"status":"ok","timestamp":1706695604058,"user_tz":-540,"elapsed":4,"user":{"displayName":"EunJun Lee","userId":"12137931568709711475"}}},"execution_count":24,"outputs":[]},{"cell_type":"code","source":["for epoch in range(1, 10):\n","    train(epoch)\n","    test()"],"metadata":{"id":"zSvSZb_Bn4Nx","executionInfo":{"status":"ok","timestamp":1706695829007,"user_tz":-540,"elapsed":224953,"user":{"displayName":"EunJun Lee","userId":"12137931568709711475"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"4d3de674-2c09-4496-eded-50f499735191"},"execution_count":25,"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-21-4820c0182e17>:15: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n","  return F.log_softmax(x) # -> 10개의 output을 softmax 처리해서 log로 반환\n"]},{"output_type":"stream","name":"stdout","text":["Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.333787\n","Train Epoch: 1 [640/60000 (1%)]\tLoss: 2.310540\n","Train Epoch: 1 [1280/60000 (2%)]\tLoss: 2.303401\n","Train Epoch: 1 [1920/60000 (3%)]\tLoss: 2.287821\n","Train Epoch: 1 [2560/60000 (4%)]\tLoss: 2.272456\n","Train Epoch: 1 [3200/60000 (5%)]\tLoss: 2.252066\n","Train Epoch: 1 [3840/60000 (6%)]\tLoss: 2.247658\n","Train Epoch: 1 [4480/60000 (7%)]\tLoss: 2.187569\n","Train Epoch: 1 [5120/60000 (9%)]\tLoss: 2.163913\n","Train Epoch: 1 [5760/60000 (10%)]\tLoss: 2.149293\n","Train Epoch: 1 [6400/60000 (11%)]\tLoss: 2.066093\n","Train Epoch: 1 [7040/60000 (12%)]\tLoss: 1.952025\n","Train Epoch: 1 [7680/60000 (13%)]\tLoss: 1.835536\n","Train Epoch: 1 [8320/60000 (14%)]\tLoss: 1.660598\n","Train Epoch: 1 [8960/60000 (15%)]\tLoss: 1.417570\n","Train Epoch: 1 [9600/60000 (16%)]\tLoss: 1.141111\n","Train Epoch: 1 [10240/60000 (17%)]\tLoss: 1.011117\n","Train Epoch: 1 [10880/60000 (18%)]\tLoss: 0.854689\n","Train Epoch: 1 [11520/60000 (19%)]\tLoss: 0.832860\n","Train Epoch: 1 [12160/60000 (20%)]\tLoss: 0.572486\n","Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.703015\n","Train Epoch: 1 [13440/60000 (22%)]\tLoss: 0.749190\n","Train Epoch: 1 [14080/60000 (23%)]\tLoss: 0.558810\n","Train Epoch: 1 [14720/60000 (25%)]\tLoss: 0.598403\n","Train Epoch: 1 [15360/60000 (26%)]\tLoss: 0.609348\n","Train Epoch: 1 [16000/60000 (27%)]\tLoss: 0.559624\n","Train Epoch: 1 [16640/60000 (28%)]\tLoss: 0.439670\n","Train Epoch: 1 [17280/60000 (29%)]\tLoss: 0.371133\n","Train Epoch: 1 [17920/60000 (30%)]\tLoss: 0.570943\n","Train Epoch: 1 [18560/60000 (31%)]\tLoss: 0.348253\n","Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.535237\n","Train Epoch: 1 [19840/60000 (33%)]\tLoss: 0.459077\n","Train Epoch: 1 [20480/60000 (34%)]\tLoss: 0.542556\n","Train Epoch: 1 [21120/60000 (35%)]\tLoss: 0.367493\n","Train Epoch: 1 [21760/60000 (36%)]\tLoss: 0.542089\n","Train Epoch: 1 [22400/60000 (37%)]\tLoss: 0.341557\n","Train Epoch: 1 [23040/60000 (38%)]\tLoss: 0.417260\n","Train Epoch: 1 [23680/60000 (39%)]\tLoss: 0.518869\n","Train Epoch: 1 [24320/60000 (41%)]\tLoss: 0.449530\n","Train Epoch: 1 [24960/60000 (42%)]\tLoss: 0.609798\n","Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.413774\n","Train Epoch: 1 [26240/60000 (44%)]\tLoss: 0.298516\n","Train Epoch: 1 [26880/60000 (45%)]\tLoss: 0.300948\n","Train Epoch: 1 [27520/60000 (46%)]\tLoss: 0.374837\n","Train Epoch: 1 [28160/60000 (47%)]\tLoss: 0.414436\n","Train Epoch: 1 [28800/60000 (48%)]\tLoss: 0.297800\n","Train Epoch: 1 [29440/60000 (49%)]\tLoss: 0.232377\n","Train Epoch: 1 [30080/60000 (50%)]\tLoss: 0.442840\n","Train Epoch: 1 [30720/60000 (51%)]\tLoss: 0.445673\n","Train Epoch: 1 [31360/60000 (52%)]\tLoss: 0.304161\n","Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.332907\n","Train Epoch: 1 [32640/60000 (54%)]\tLoss: 0.241728\n","Train Epoch: 1 [33280/60000 (55%)]\tLoss: 0.265309\n","Train Epoch: 1 [33920/60000 (57%)]\tLoss: 0.213752\n","Train Epoch: 1 [34560/60000 (58%)]\tLoss: 0.241322\n","Train Epoch: 1 [35200/60000 (59%)]\tLoss: 0.289027\n","Train Epoch: 1 [35840/60000 (60%)]\tLoss: 0.389594\n","Train Epoch: 1 [36480/60000 (61%)]\tLoss: 0.238783\n","Train Epoch: 1 [37120/60000 (62%)]\tLoss: 0.153158\n","Train Epoch: 1 [37760/60000 (63%)]\tLoss: 0.488259\n","Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.219549\n","Train Epoch: 1 [39040/60000 (65%)]\tLoss: 0.320606\n","Train Epoch: 1 [39680/60000 (66%)]\tLoss: 0.162327\n","Train Epoch: 1 [40320/60000 (67%)]\tLoss: 0.181148\n","Train Epoch: 1 [40960/60000 (68%)]\tLoss: 0.381228\n","Train Epoch: 1 [41600/60000 (69%)]\tLoss: 0.157185\n","Train Epoch: 1 [42240/60000 (70%)]\tLoss: 0.219173\n","Train Epoch: 1 [42880/60000 (71%)]\tLoss: 0.242442\n","Train Epoch: 1 [43520/60000 (72%)]\tLoss: 0.204465\n","Train Epoch: 1 [44160/60000 (74%)]\tLoss: 0.385940\n","Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.295545\n","Train Epoch: 1 [45440/60000 (76%)]\tLoss: 0.238064\n","Train Epoch: 1 [46080/60000 (77%)]\tLoss: 0.290930\n","Train Epoch: 1 [46720/60000 (78%)]\tLoss: 0.265414\n","Train Epoch: 1 [47360/60000 (79%)]\tLoss: 0.315341\n","Train Epoch: 1 [48000/60000 (80%)]\tLoss: 0.460405\n","Train Epoch: 1 [48640/60000 (81%)]\tLoss: 0.278927\n","Train Epoch: 1 [49280/60000 (82%)]\tLoss: 0.210826\n","Train Epoch: 1 [49920/60000 (83%)]\tLoss: 0.181036\n","Train Epoch: 1 [50560/60000 (84%)]\tLoss: 0.326154\n","Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.191835\n","Train Epoch: 1 [51840/60000 (86%)]\tLoss: 0.240852\n","Train Epoch: 1 [52480/60000 (87%)]\tLoss: 0.205442\n","Train Epoch: 1 [53120/60000 (88%)]\tLoss: 0.181959\n","Train Epoch: 1 [53760/60000 (90%)]\tLoss: 0.247258\n","Train Epoch: 1 [54400/60000 (91%)]\tLoss: 0.181522\n","Train Epoch: 1 [55040/60000 (92%)]\tLoss: 0.226252\n","Train Epoch: 1 [55680/60000 (93%)]\tLoss: 0.165659\n","Train Epoch: 1 [56320/60000 (94%)]\tLoss: 0.274683\n","Train Epoch: 1 [56960/60000 (95%)]\tLoss: 0.122391\n","Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.142147\n","Train Epoch: 1 [58240/60000 (97%)]\tLoss: 0.178061\n","Train Epoch: 1 [58880/60000 (98%)]\tLoss: 0.313970\n","Train Epoch: 1 [59520/60000 (99%)]\tLoss: 0.192378\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-24-f52337105c2a>:6: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n","  data, target = Variable(data, volatile=True), Variable(target)\n","/usr/local/lib/python3.10/dist-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n","  warnings.warn(warning.format(ret))\n"]},{"output_type":"stream","name":"stdout","text":["\n","Test set: Average loss: 0.1904, Accuracy: 9445/10000 (94%)\n","\n","Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.144356\n","Train Epoch: 2 [640/60000 (1%)]\tLoss: 0.311371\n","Train Epoch: 2 [1280/60000 (2%)]\tLoss: 0.255028\n","Train Epoch: 2 [1920/60000 (3%)]\tLoss: 0.413055\n","Train Epoch: 2 [2560/60000 (4%)]\tLoss: 0.196605\n","Train Epoch: 2 [3200/60000 (5%)]\tLoss: 0.076946\n","Train Epoch: 2 [3840/60000 (6%)]\tLoss: 0.178278\n","Train Epoch: 2 [4480/60000 (7%)]\tLoss: 0.220730\n","Train Epoch: 2 [5120/60000 (9%)]\tLoss: 0.303001\n","Train Epoch: 2 [5760/60000 (10%)]\tLoss: 0.197868\n","Train Epoch: 2 [6400/60000 (11%)]\tLoss: 0.430021\n","Train Epoch: 2 [7040/60000 (12%)]\tLoss: 0.187761\n","Train Epoch: 2 [7680/60000 (13%)]\tLoss: 0.238727\n","Train Epoch: 2 [8320/60000 (14%)]\tLoss: 0.208112\n","Train Epoch: 2 [8960/60000 (15%)]\tLoss: 0.139427\n","Train Epoch: 2 [9600/60000 (16%)]\tLoss: 0.139031\n","Train Epoch: 2 [10240/60000 (17%)]\tLoss: 0.285748\n","Train Epoch: 2 [10880/60000 (18%)]\tLoss: 0.137164\n","Train Epoch: 2 [11520/60000 (19%)]\tLoss: 0.213065\n","Train Epoch: 2 [12160/60000 (20%)]\tLoss: 0.206633\n","Train Epoch: 2 [12800/60000 (21%)]\tLoss: 0.302592\n","Train Epoch: 2 [13440/60000 (22%)]\tLoss: 0.114460\n","Train Epoch: 2 [14080/60000 (23%)]\tLoss: 0.106351\n","Train Epoch: 2 [14720/60000 (25%)]\tLoss: 0.116124\n","Train Epoch: 2 [15360/60000 (26%)]\tLoss: 0.057333\n","Train Epoch: 2 [16000/60000 (27%)]\tLoss: 0.240288\n","Train Epoch: 2 [16640/60000 (28%)]\tLoss: 0.163225\n","Train Epoch: 2 [17280/60000 (29%)]\tLoss: 0.137646\n","Train Epoch: 2 [17920/60000 (30%)]\tLoss: 0.237491\n","Train Epoch: 2 [18560/60000 (31%)]\tLoss: 0.143913\n","Train Epoch: 2 [19200/60000 (32%)]\tLoss: 0.116964\n","Train Epoch: 2 [19840/60000 (33%)]\tLoss: 0.152819\n","Train Epoch: 2 [20480/60000 (34%)]\tLoss: 0.137491\n","Train Epoch: 2 [21120/60000 (35%)]\tLoss: 0.112696\n","Train Epoch: 2 [21760/60000 (36%)]\tLoss: 0.066887\n","Train Epoch: 2 [22400/60000 (37%)]\tLoss: 0.058237\n","Train Epoch: 2 [23040/60000 (38%)]\tLoss: 0.138319\n","Train Epoch: 2 [23680/60000 (39%)]\tLoss: 0.163975\n","Train Epoch: 2 [24320/60000 (41%)]\tLoss: 0.142663\n","Train Epoch: 2 [24960/60000 (42%)]\tLoss: 0.394549\n","Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.357047\n","Train Epoch: 2 [26240/60000 (44%)]\tLoss: 0.097383\n","Train Epoch: 2 [26880/60000 (45%)]\tLoss: 0.157672\n","Train Epoch: 2 [27520/60000 (46%)]\tLoss: 0.144575\n","Train Epoch: 2 [28160/60000 (47%)]\tLoss: 0.212024\n","Train Epoch: 2 [28800/60000 (48%)]\tLoss: 0.030672\n","Train Epoch: 2 [29440/60000 (49%)]\tLoss: 0.226930\n","Train Epoch: 2 [30080/60000 (50%)]\tLoss: 0.141244\n","Train Epoch: 2 [30720/60000 (51%)]\tLoss: 0.029569\n","Train Epoch: 2 [31360/60000 (52%)]\tLoss: 0.241121\n","Train Epoch: 2 [32000/60000 (53%)]\tLoss: 0.103163\n","Train Epoch: 2 [32640/60000 (54%)]\tLoss: 0.164587\n","Train Epoch: 2 [33280/60000 (55%)]\tLoss: 0.155826\n","Train Epoch: 2 [33920/60000 (57%)]\tLoss: 0.114702\n","Train Epoch: 2 [34560/60000 (58%)]\tLoss: 0.133255\n","Train Epoch: 2 [35200/60000 (59%)]\tLoss: 0.065432\n","Train Epoch: 2 [35840/60000 (60%)]\tLoss: 0.050516\n","Train Epoch: 2 [36480/60000 (61%)]\tLoss: 0.070055\n","Train Epoch: 2 [37120/60000 (62%)]\tLoss: 0.080821\n","Train Epoch: 2 [37760/60000 (63%)]\tLoss: 0.059031\n","Train Epoch: 2 [38400/60000 (64%)]\tLoss: 0.084125\n","Train Epoch: 2 [39040/60000 (65%)]\tLoss: 0.099697\n","Train Epoch: 2 [39680/60000 (66%)]\tLoss: 0.159398\n","Train Epoch: 2 [40320/60000 (67%)]\tLoss: 0.312969\n","Train Epoch: 2 [40960/60000 (68%)]\tLoss: 0.188966\n","Train Epoch: 2 [41600/60000 (69%)]\tLoss: 0.176765\n","Train Epoch: 2 [42240/60000 (70%)]\tLoss: 0.070908\n","Train Epoch: 2 [42880/60000 (71%)]\tLoss: 0.072420\n","Train Epoch: 2 [43520/60000 (72%)]\tLoss: 0.081735\n","Train Epoch: 2 [44160/60000 (74%)]\tLoss: 0.119557\n","Train Epoch: 2 [44800/60000 (75%)]\tLoss: 0.168438\n","Train Epoch: 2 [45440/60000 (76%)]\tLoss: 0.178975\n","Train Epoch: 2 [46080/60000 (77%)]\tLoss: 0.096817\n","Train Epoch: 2 [46720/60000 (78%)]\tLoss: 0.222678\n","Train Epoch: 2 [47360/60000 (79%)]\tLoss: 0.137492\n","Train Epoch: 2 [48000/60000 (80%)]\tLoss: 0.066321\n","Train Epoch: 2 [48640/60000 (81%)]\tLoss: 0.179125\n","Train Epoch: 2 [49280/60000 (82%)]\tLoss: 0.221779\n","Train Epoch: 2 [49920/60000 (83%)]\tLoss: 0.076782\n","Train Epoch: 2 [50560/60000 (84%)]\tLoss: 0.242762\n","Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.116180\n","Train Epoch: 2 [51840/60000 (86%)]\tLoss: 0.156056\n","Train Epoch: 2 [52480/60000 (87%)]\tLoss: 0.156455\n","Train Epoch: 2 [53120/60000 (88%)]\tLoss: 0.154493\n","Train Epoch: 2 [53760/60000 (90%)]\tLoss: 0.167383\n","Train Epoch: 2 [54400/60000 (91%)]\tLoss: 0.091337\n","Train Epoch: 2 [55040/60000 (92%)]\tLoss: 0.071718\n","Train Epoch: 2 [55680/60000 (93%)]\tLoss: 0.153093\n","Train Epoch: 2 [56320/60000 (94%)]\tLoss: 0.059545\n","Train Epoch: 2 [56960/60000 (95%)]\tLoss: 0.067212\n","Train Epoch: 2 [57600/60000 (96%)]\tLoss: 0.064805\n","Train Epoch: 2 [58240/60000 (97%)]\tLoss: 0.075404\n","Train Epoch: 2 [58880/60000 (98%)]\tLoss: 0.276411\n","Train Epoch: 2 [59520/60000 (99%)]\tLoss: 0.143042\n","\n","Test set: Average loss: 0.1097, Accuracy: 9675/10000 (97%)\n","\n","Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.279287\n","Train Epoch: 3 [640/60000 (1%)]\tLoss: 0.151386\n","Train Epoch: 3 [1280/60000 (2%)]\tLoss: 0.240140\n","Train Epoch: 3 [1920/60000 (3%)]\tLoss: 0.126971\n","Train Epoch: 3 [2560/60000 (4%)]\tLoss: 0.163389\n","Train Epoch: 3 [3200/60000 (5%)]\tLoss: 0.216354\n","Train Epoch: 3 [3840/60000 (6%)]\tLoss: 0.083683\n","Train Epoch: 3 [4480/60000 (7%)]\tLoss: 0.104455\n","Train Epoch: 3 [5120/60000 (9%)]\tLoss: 0.097301\n","Train Epoch: 3 [5760/60000 (10%)]\tLoss: 0.114270\n","Train Epoch: 3 [6400/60000 (11%)]\tLoss: 0.058782\n","Train Epoch: 3 [7040/60000 (12%)]\tLoss: 0.078543\n","Train Epoch: 3 [7680/60000 (13%)]\tLoss: 0.112123\n","Train Epoch: 3 [8320/60000 (14%)]\tLoss: 0.190449\n","Train Epoch: 3 [8960/60000 (15%)]\tLoss: 0.195774\n","Train Epoch: 3 [9600/60000 (16%)]\tLoss: 0.080848\n","Train Epoch: 3 [10240/60000 (17%)]\tLoss: 0.095517\n","Train Epoch: 3 [10880/60000 (18%)]\tLoss: 0.115512\n","Train Epoch: 3 [11520/60000 (19%)]\tLoss: 0.094411\n","Train Epoch: 3 [12160/60000 (20%)]\tLoss: 0.086004\n","Train Epoch: 3 [12800/60000 (21%)]\tLoss: 0.114754\n","Train Epoch: 3 [13440/60000 (22%)]\tLoss: 0.091391\n","Train Epoch: 3 [14080/60000 (23%)]\tLoss: 0.043570\n","Train Epoch: 3 [14720/60000 (25%)]\tLoss: 0.280677\n","Train Epoch: 3 [15360/60000 (26%)]\tLoss: 0.074316\n","Train Epoch: 3 [16000/60000 (27%)]\tLoss: 0.285275\n","Train Epoch: 3 [16640/60000 (28%)]\tLoss: 0.062609\n","Train Epoch: 3 [17280/60000 (29%)]\tLoss: 0.184553\n","Train Epoch: 3 [17920/60000 (30%)]\tLoss: 0.196213\n","Train Epoch: 3 [18560/60000 (31%)]\tLoss: 0.063665\n","Train Epoch: 3 [19200/60000 (32%)]\tLoss: 0.065336\n","Train Epoch: 3 [19840/60000 (33%)]\tLoss: 0.027220\n","Train Epoch: 3 [20480/60000 (34%)]\tLoss: 0.094020\n","Train Epoch: 3 [21120/60000 (35%)]\tLoss: 0.030203\n","Train Epoch: 3 [21760/60000 (36%)]\tLoss: 0.147956\n","Train Epoch: 3 [22400/60000 (37%)]\tLoss: 0.030611\n","Train Epoch: 3 [23040/60000 (38%)]\tLoss: 0.180677\n","Train Epoch: 3 [23680/60000 (39%)]\tLoss: 0.108756\n","Train Epoch: 3 [24320/60000 (41%)]\tLoss: 0.097346\n","Train Epoch: 3 [24960/60000 (42%)]\tLoss: 0.094644\n","Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.070948\n","Train Epoch: 3 [26240/60000 (44%)]\tLoss: 0.066944\n","Train Epoch: 3 [26880/60000 (45%)]\tLoss: 0.101363\n","Train Epoch: 3 [27520/60000 (46%)]\tLoss: 0.111808\n","Train Epoch: 3 [28160/60000 (47%)]\tLoss: 0.037803\n","Train Epoch: 3 [28800/60000 (48%)]\tLoss: 0.133368\n","Train Epoch: 3 [29440/60000 (49%)]\tLoss: 0.111165\n","Train Epoch: 3 [30080/60000 (50%)]\tLoss: 0.022798\n","Train Epoch: 3 [30720/60000 (51%)]\tLoss: 0.038211\n","Train Epoch: 3 [31360/60000 (52%)]\tLoss: 0.126180\n","Train Epoch: 3 [32000/60000 (53%)]\tLoss: 0.131277\n","Train Epoch: 3 [32640/60000 (54%)]\tLoss: 0.021910\n","Train Epoch: 3 [33280/60000 (55%)]\tLoss: 0.061551\n","Train Epoch: 3 [33920/60000 (57%)]\tLoss: 0.106254\n","Train Epoch: 3 [34560/60000 (58%)]\tLoss: 0.226288\n","Train Epoch: 3 [35200/60000 (59%)]\tLoss: 0.098813\n","Train Epoch: 3 [35840/60000 (60%)]\tLoss: 0.368432\n","Train Epoch: 3 [36480/60000 (61%)]\tLoss: 0.028984\n","Train Epoch: 3 [37120/60000 (62%)]\tLoss: 0.068840\n","Train Epoch: 3 [37760/60000 (63%)]\tLoss: 0.030652\n","Train Epoch: 3 [38400/60000 (64%)]\tLoss: 0.149351\n","Train Epoch: 3 [39040/60000 (65%)]\tLoss: 0.088004\n","Train Epoch: 3 [39680/60000 (66%)]\tLoss: 0.114255\n","Train Epoch: 3 [40320/60000 (67%)]\tLoss: 0.069936\n","Train Epoch: 3 [40960/60000 (68%)]\tLoss: 0.204824\n","Train Epoch: 3 [41600/60000 (69%)]\tLoss: 0.204106\n","Train Epoch: 3 [42240/60000 (70%)]\tLoss: 0.218536\n","Train Epoch: 3 [42880/60000 (71%)]\tLoss: 0.161797\n","Train Epoch: 3 [43520/60000 (72%)]\tLoss: 0.252149\n","Train Epoch: 3 [44160/60000 (74%)]\tLoss: 0.141282\n","Train Epoch: 3 [44800/60000 (75%)]\tLoss: 0.185160\n","Train Epoch: 3 [45440/60000 (76%)]\tLoss: 0.107604\n","Train Epoch: 3 [46080/60000 (77%)]\tLoss: 0.065679\n","Train Epoch: 3 [46720/60000 (78%)]\tLoss: 0.085489\n","Train Epoch: 3 [47360/60000 (79%)]\tLoss: 0.076792\n","Train Epoch: 3 [48000/60000 (80%)]\tLoss: 0.113037\n","Train Epoch: 3 [48640/60000 (81%)]\tLoss: 0.201147\n","Train Epoch: 3 [49280/60000 (82%)]\tLoss: 0.132667\n","Train Epoch: 3 [49920/60000 (83%)]\tLoss: 0.188540\n","Train Epoch: 3 [50560/60000 (84%)]\tLoss: 0.043435\n","Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.129248\n","Train Epoch: 3 [51840/60000 (86%)]\tLoss: 0.111016\n","Train Epoch: 3 [52480/60000 (87%)]\tLoss: 0.076773\n","Train Epoch: 3 [53120/60000 (88%)]\tLoss: 0.158153\n","Train Epoch: 3 [53760/60000 (90%)]\tLoss: 0.104576\n","Train Epoch: 3 [54400/60000 (91%)]\tLoss: 0.066525\n","Train Epoch: 3 [55040/60000 (92%)]\tLoss: 0.071951\n","Train Epoch: 3 [55680/60000 (93%)]\tLoss: 0.133271\n","Train Epoch: 3 [56320/60000 (94%)]\tLoss: 0.219276\n","Train Epoch: 3 [56960/60000 (95%)]\tLoss: 0.123717\n","Train Epoch: 3 [57600/60000 (96%)]\tLoss: 0.067002\n","Train Epoch: 3 [58240/60000 (97%)]\tLoss: 0.128004\n","Train Epoch: 3 [58880/60000 (98%)]\tLoss: 0.063086\n","Train Epoch: 3 [59520/60000 (99%)]\tLoss: 0.046166\n","\n","Test set: Average loss: 0.0897, Accuracy: 9733/10000 (97%)\n","\n","Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.093918\n","Train Epoch: 4 [640/60000 (1%)]\tLoss: 0.058811\n","Train Epoch: 4 [1280/60000 (2%)]\tLoss: 0.068787\n","Train Epoch: 4 [1920/60000 (3%)]\tLoss: 0.068830\n","Train Epoch: 4 [2560/60000 (4%)]\tLoss: 0.039370\n","Train Epoch: 4 [3200/60000 (5%)]\tLoss: 0.160360\n","Train Epoch: 4 [3840/60000 (6%)]\tLoss: 0.090309\n","Train Epoch: 4 [4480/60000 (7%)]\tLoss: 0.095974\n","Train Epoch: 4 [5120/60000 (9%)]\tLoss: 0.060971\n","Train Epoch: 4 [5760/60000 (10%)]\tLoss: 0.072961\n","Train Epoch: 4 [6400/60000 (11%)]\tLoss: 0.080801\n","Train Epoch: 4 [7040/60000 (12%)]\tLoss: 0.044849\n","Train Epoch: 4 [7680/60000 (13%)]\tLoss: 0.140490\n","Train Epoch: 4 [8320/60000 (14%)]\tLoss: 0.064624\n","Train Epoch: 4 [8960/60000 (15%)]\tLoss: 0.123358\n","Train Epoch: 4 [9600/60000 (16%)]\tLoss: 0.088494\n","Train Epoch: 4 [10240/60000 (17%)]\tLoss: 0.067440\n","Train Epoch: 4 [10880/60000 (18%)]\tLoss: 0.129630\n","Train Epoch: 4 [11520/60000 (19%)]\tLoss: 0.290064\n","Train Epoch: 4 [12160/60000 (20%)]\tLoss: 0.065546\n","Train Epoch: 4 [12800/60000 (21%)]\tLoss: 0.130422\n","Train Epoch: 4 [13440/60000 (22%)]\tLoss: 0.097893\n","Train Epoch: 4 [14080/60000 (23%)]\tLoss: 0.141866\n","Train Epoch: 4 [14720/60000 (25%)]\tLoss: 0.055362\n","Train Epoch: 4 [15360/60000 (26%)]\tLoss: 0.220460\n","Train Epoch: 4 [16000/60000 (27%)]\tLoss: 0.152207\n","Train Epoch: 4 [16640/60000 (28%)]\tLoss: 0.066396\n","Train Epoch: 4 [17280/60000 (29%)]\tLoss: 0.055575\n","Train Epoch: 4 [17920/60000 (30%)]\tLoss: 0.048589\n","Train Epoch: 4 [18560/60000 (31%)]\tLoss: 0.082854\n","Train Epoch: 4 [19200/60000 (32%)]\tLoss: 0.039744\n","Train Epoch: 4 [19840/60000 (33%)]\tLoss: 0.128191\n","Train Epoch: 4 [20480/60000 (34%)]\tLoss: 0.048941\n","Train Epoch: 4 [21120/60000 (35%)]\tLoss: 0.223699\n","Train Epoch: 4 [21760/60000 (36%)]\tLoss: 0.096532\n","Train Epoch: 4 [22400/60000 (37%)]\tLoss: 0.099808\n","Train Epoch: 4 [23040/60000 (38%)]\tLoss: 0.034608\n","Train Epoch: 4 [23680/60000 (39%)]\tLoss: 0.116083\n","Train Epoch: 4 [24320/60000 (41%)]\tLoss: 0.074307\n","Train Epoch: 4 [24960/60000 (42%)]\tLoss: 0.145321\n","Train Epoch: 4 [25600/60000 (43%)]\tLoss: 0.069350\n","Train Epoch: 4 [26240/60000 (44%)]\tLoss: 0.196896\n","Train Epoch: 4 [26880/60000 (45%)]\tLoss: 0.024684\n","Train Epoch: 4 [27520/60000 (46%)]\tLoss: 0.078877\n","Train Epoch: 4 [28160/60000 (47%)]\tLoss: 0.152150\n","Train Epoch: 4 [28800/60000 (48%)]\tLoss: 0.180867\n","Train Epoch: 4 [29440/60000 (49%)]\tLoss: 0.048009\n","Train Epoch: 4 [30080/60000 (50%)]\tLoss: 0.053340\n","Train Epoch: 4 [30720/60000 (51%)]\tLoss: 0.175575\n","Train Epoch: 4 [31360/60000 (52%)]\tLoss: 0.180591\n","Train Epoch: 4 [32000/60000 (53%)]\tLoss: 0.098672\n","Train Epoch: 4 [32640/60000 (54%)]\tLoss: 0.069336\n","Train Epoch: 4 [33280/60000 (55%)]\tLoss: 0.135268\n","Train Epoch: 4 [33920/60000 (57%)]\tLoss: 0.183470\n","Train Epoch: 4 [34560/60000 (58%)]\tLoss: 0.114046\n","Train Epoch: 4 [35200/60000 (59%)]\tLoss: 0.030014\n","Train Epoch: 4 [35840/60000 (60%)]\tLoss: 0.050093\n","Train Epoch: 4 [36480/60000 (61%)]\tLoss: 0.025536\n","Train Epoch: 4 [37120/60000 (62%)]\tLoss: 0.151813\n","Train Epoch: 4 [37760/60000 (63%)]\tLoss: 0.092760\n","Train Epoch: 4 [38400/60000 (64%)]\tLoss: 0.077963\n","Train Epoch: 4 [39040/60000 (65%)]\tLoss: 0.103524\n","Train Epoch: 4 [39680/60000 (66%)]\tLoss: 0.169213\n","Train Epoch: 4 [40320/60000 (67%)]\tLoss: 0.047390\n","Train Epoch: 4 [40960/60000 (68%)]\tLoss: 0.068263\n","Train Epoch: 4 [41600/60000 (69%)]\tLoss: 0.114268\n","Train Epoch: 4 [42240/60000 (70%)]\tLoss: 0.068146\n","Train Epoch: 4 [42880/60000 (71%)]\tLoss: 0.108505\n","Train Epoch: 4 [43520/60000 (72%)]\tLoss: 0.069549\n","Train Epoch: 4 [44160/60000 (74%)]\tLoss: 0.128094\n","Train Epoch: 4 [44800/60000 (75%)]\tLoss: 0.107813\n","Train Epoch: 4 [45440/60000 (76%)]\tLoss: 0.076056\n","Train Epoch: 4 [46080/60000 (77%)]\tLoss: 0.034039\n","Train Epoch: 4 [46720/60000 (78%)]\tLoss: 0.053364\n","Train Epoch: 4 [47360/60000 (79%)]\tLoss: 0.037560\n","Train Epoch: 4 [48000/60000 (80%)]\tLoss: 0.039038\n","Train Epoch: 4 [48640/60000 (81%)]\tLoss: 0.085382\n","Train Epoch: 4 [49280/60000 (82%)]\tLoss: 0.073095\n","Train Epoch: 4 [49920/60000 (83%)]\tLoss: 0.169346\n","Train Epoch: 4 [50560/60000 (84%)]\tLoss: 0.108508\n","Train Epoch: 4 [51200/60000 (85%)]\tLoss: 0.279198\n","Train Epoch: 4 [51840/60000 (86%)]\tLoss: 0.060418\n","Train Epoch: 4 [52480/60000 (87%)]\tLoss: 0.025818\n","Train Epoch: 4 [53120/60000 (88%)]\tLoss: 0.029112\n","Train Epoch: 4 [53760/60000 (90%)]\tLoss: 0.140272\n","Train Epoch: 4 [54400/60000 (91%)]\tLoss: 0.082550\n","Train Epoch: 4 [55040/60000 (92%)]\tLoss: 0.176839\n","Train Epoch: 4 [55680/60000 (93%)]\tLoss: 0.052421\n","Train Epoch: 4 [56320/60000 (94%)]\tLoss: 0.099306\n","Train Epoch: 4 [56960/60000 (95%)]\tLoss: 0.019411\n","Train Epoch: 4 [57600/60000 (96%)]\tLoss: 0.057232\n","Train Epoch: 4 [58240/60000 (97%)]\tLoss: 0.063996\n","Train Epoch: 4 [58880/60000 (98%)]\tLoss: 0.046023\n","Train Epoch: 4 [59520/60000 (99%)]\tLoss: 0.090290\n","\n","Test set: Average loss: 0.0782, Accuracy: 9758/10000 (98%)\n","\n","Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.123057\n","Train Epoch: 5 [640/60000 (1%)]\tLoss: 0.091485\n","Train Epoch: 5 [1280/60000 (2%)]\tLoss: 0.038750\n","Train Epoch: 5 [1920/60000 (3%)]\tLoss: 0.095279\n","Train Epoch: 5 [2560/60000 (4%)]\tLoss: 0.055159\n","Train Epoch: 5 [3200/60000 (5%)]\tLoss: 0.061817\n","Train Epoch: 5 [3840/60000 (6%)]\tLoss: 0.077172\n","Train Epoch: 5 [4480/60000 (7%)]\tLoss: 0.071562\n","Train Epoch: 5 [5120/60000 (9%)]\tLoss: 0.123775\n","Train Epoch: 5 [5760/60000 (10%)]\tLoss: 0.028661\n","Train Epoch: 5 [6400/60000 (11%)]\tLoss: 0.022170\n","Train Epoch: 5 [7040/60000 (12%)]\tLoss: 0.092635\n","Train Epoch: 5 [7680/60000 (13%)]\tLoss: 0.157056\n","Train Epoch: 5 [8320/60000 (14%)]\tLoss: 0.109885\n","Train Epoch: 5 [8960/60000 (15%)]\tLoss: 0.036720\n","Train Epoch: 5 [9600/60000 (16%)]\tLoss: 0.067317\n","Train Epoch: 5 [10240/60000 (17%)]\tLoss: 0.195340\n","Train Epoch: 5 [10880/60000 (18%)]\tLoss: 0.085029\n","Train Epoch: 5 [11520/60000 (19%)]\tLoss: 0.171206\n","Train Epoch: 5 [12160/60000 (20%)]\tLoss: 0.109351\n","Train Epoch: 5 [12800/60000 (21%)]\tLoss: 0.119086\n","Train Epoch: 5 [13440/60000 (22%)]\tLoss: 0.049313\n","Train Epoch: 5 [14080/60000 (23%)]\tLoss: 0.100514\n","Train Epoch: 5 [14720/60000 (25%)]\tLoss: 0.045849\n","Train Epoch: 5 [15360/60000 (26%)]\tLoss: 0.083178\n","Train Epoch: 5 [16000/60000 (27%)]\tLoss: 0.114393\n","Train Epoch: 5 [16640/60000 (28%)]\tLoss: 0.133807\n","Train Epoch: 5 [17280/60000 (29%)]\tLoss: 0.018379\n","Train Epoch: 5 [17920/60000 (30%)]\tLoss: 0.052906\n","Train Epoch: 5 [18560/60000 (31%)]\tLoss: 0.063193\n","Train Epoch: 5 [19200/60000 (32%)]\tLoss: 0.041343\n","Train Epoch: 5 [19840/60000 (33%)]\tLoss: 0.089684\n","Train Epoch: 5 [20480/60000 (34%)]\tLoss: 0.182253\n","Train Epoch: 5 [21120/60000 (35%)]\tLoss: 0.057988\n","Train Epoch: 5 [21760/60000 (36%)]\tLoss: 0.096035\n","Train Epoch: 5 [22400/60000 (37%)]\tLoss: 0.079918\n","Train Epoch: 5 [23040/60000 (38%)]\tLoss: 0.067602\n","Train Epoch: 5 [23680/60000 (39%)]\tLoss: 0.052104\n","Train Epoch: 5 [24320/60000 (41%)]\tLoss: 0.113752\n","Train Epoch: 5 [24960/60000 (42%)]\tLoss: 0.062337\n","Train Epoch: 5 [25600/60000 (43%)]\tLoss: 0.050777\n","Train Epoch: 5 [26240/60000 (44%)]\tLoss: 0.173294\n","Train Epoch: 5 [26880/60000 (45%)]\tLoss: 0.125836\n","Train Epoch: 5 [27520/60000 (46%)]\tLoss: 0.055263\n","Train Epoch: 5 [28160/60000 (47%)]\tLoss: 0.070503\n","Train Epoch: 5 [28800/60000 (48%)]\tLoss: 0.039797\n","Train Epoch: 5 [29440/60000 (49%)]\tLoss: 0.029848\n","Train Epoch: 5 [30080/60000 (50%)]\tLoss: 0.033558\n","Train Epoch: 5 [30720/60000 (51%)]\tLoss: 0.119694\n","Train Epoch: 5 [31360/60000 (52%)]\tLoss: 0.083414\n","Train Epoch: 5 [32000/60000 (53%)]\tLoss: 0.025683\n","Train Epoch: 5 [32640/60000 (54%)]\tLoss: 0.152665\n","Train Epoch: 5 [33280/60000 (55%)]\tLoss: 0.084605\n","Train Epoch: 5 [33920/60000 (57%)]\tLoss: 0.162494\n","Train Epoch: 5 [34560/60000 (58%)]\tLoss: 0.033064\n","Train Epoch: 5 [35200/60000 (59%)]\tLoss: 0.263035\n","Train Epoch: 5 [35840/60000 (60%)]\tLoss: 0.117018\n","Train Epoch: 5 [36480/60000 (61%)]\tLoss: 0.076671\n","Train Epoch: 5 [37120/60000 (62%)]\tLoss: 0.188886\n","Train Epoch: 5 [37760/60000 (63%)]\tLoss: 0.232041\n","Train Epoch: 5 [38400/60000 (64%)]\tLoss: 0.172845\n","Train Epoch: 5 [39040/60000 (65%)]\tLoss: 0.083785\n","Train Epoch: 5 [39680/60000 (66%)]\tLoss: 0.053971\n","Train Epoch: 5 [40320/60000 (67%)]\tLoss: 0.011600\n","Train Epoch: 5 [40960/60000 (68%)]\tLoss: 0.079664\n","Train Epoch: 5 [41600/60000 (69%)]\tLoss: 0.075925\n","Train Epoch: 5 [42240/60000 (70%)]\tLoss: 0.147860\n","Train Epoch: 5 [42880/60000 (71%)]\tLoss: 0.352741\n","Train Epoch: 5 [43520/60000 (72%)]\tLoss: 0.086860\n","Train Epoch: 5 [44160/60000 (74%)]\tLoss: 0.102908\n","Train Epoch: 5 [44800/60000 (75%)]\tLoss: 0.161174\n","Train Epoch: 5 [45440/60000 (76%)]\tLoss: 0.089496\n","Train Epoch: 5 [46080/60000 (77%)]\tLoss: 0.016057\n","Train Epoch: 5 [46720/60000 (78%)]\tLoss: 0.017853\n","Train Epoch: 5 [47360/60000 (79%)]\tLoss: 0.116890\n","Train Epoch: 5 [48000/60000 (80%)]\tLoss: 0.029127\n","Train Epoch: 5 [48640/60000 (81%)]\tLoss: 0.076864\n","Train Epoch: 5 [49280/60000 (82%)]\tLoss: 0.041620\n","Train Epoch: 5 [49920/60000 (83%)]\tLoss: 0.072837\n","Train Epoch: 5 [50560/60000 (84%)]\tLoss: 0.038783\n","Train Epoch: 5 [51200/60000 (85%)]\tLoss: 0.055480\n","Train Epoch: 5 [51840/60000 (86%)]\tLoss: 0.039860\n","Train Epoch: 5 [52480/60000 (87%)]\tLoss: 0.022253\n","Train Epoch: 5 [53120/60000 (88%)]\tLoss: 0.067187\n","Train Epoch: 5 [53760/60000 (90%)]\tLoss: 0.012386\n","Train Epoch: 5 [54400/60000 (91%)]\tLoss: 0.018178\n","Train Epoch: 5 [55040/60000 (92%)]\tLoss: 0.105049\n","Train Epoch: 5 [55680/60000 (93%)]\tLoss: 0.100480\n","Train Epoch: 5 [56320/60000 (94%)]\tLoss: 0.046648\n","Train Epoch: 5 [56960/60000 (95%)]\tLoss: 0.092567\n","Train Epoch: 5 [57600/60000 (96%)]\tLoss: 0.066248\n","Train Epoch: 5 [58240/60000 (97%)]\tLoss: 0.146060\n","Train Epoch: 5 [58880/60000 (98%)]\tLoss: 0.082471\n","Train Epoch: 5 [59520/60000 (99%)]\tLoss: 0.175378\n","\n","Test set: Average loss: 0.0713, Accuracy: 9779/10000 (98%)\n","\n","Train Epoch: 6 [0/60000 (0%)]\tLoss: 0.143624\n","Train Epoch: 6 [640/60000 (1%)]\tLoss: 0.092140\n","Train Epoch: 6 [1280/60000 (2%)]\tLoss: 0.052217\n","Train Epoch: 6 [1920/60000 (3%)]\tLoss: 0.085495\n","Train Epoch: 6 [2560/60000 (4%)]\tLoss: 0.121218\n","Train Epoch: 6 [3200/60000 (5%)]\tLoss: 0.026589\n","Train Epoch: 6 [3840/60000 (6%)]\tLoss: 0.094014\n","Train Epoch: 6 [4480/60000 (7%)]\tLoss: 0.050173\n","Train Epoch: 6 [5120/60000 (9%)]\tLoss: 0.078352\n","Train Epoch: 6 [5760/60000 (10%)]\tLoss: 0.112726\n","Train Epoch: 6 [6400/60000 (11%)]\tLoss: 0.029107\n","Train Epoch: 6 [7040/60000 (12%)]\tLoss: 0.159337\n","Train Epoch: 6 [7680/60000 (13%)]\tLoss: 0.040421\n","Train Epoch: 6 [8320/60000 (14%)]\tLoss: 0.010656\n","Train Epoch: 6 [8960/60000 (15%)]\tLoss: 0.131272\n","Train Epoch: 6 [9600/60000 (16%)]\tLoss: 0.116097\n","Train Epoch: 6 [10240/60000 (17%)]\tLoss: 0.101909\n","Train Epoch: 6 [10880/60000 (18%)]\tLoss: 0.074132\n","Train Epoch: 6 [11520/60000 (19%)]\tLoss: 0.141720\n","Train Epoch: 6 [12160/60000 (20%)]\tLoss: 0.070449\n","Train Epoch: 6 [12800/60000 (21%)]\tLoss: 0.017120\n","Train Epoch: 6 [13440/60000 (22%)]\tLoss: 0.096290\n","Train Epoch: 6 [14080/60000 (23%)]\tLoss: 0.046068\n","Train Epoch: 6 [14720/60000 (25%)]\tLoss: 0.076214\n","Train Epoch: 6 [15360/60000 (26%)]\tLoss: 0.100794\n","Train Epoch: 6 [16000/60000 (27%)]\tLoss: 0.196372\n","Train Epoch: 6 [16640/60000 (28%)]\tLoss: 0.059232\n","Train Epoch: 6 [17280/60000 (29%)]\tLoss: 0.051229\n","Train Epoch: 6 [17920/60000 (30%)]\tLoss: 0.104371\n","Train Epoch: 6 [18560/60000 (31%)]\tLoss: 0.051347\n","Train Epoch: 6 [19200/60000 (32%)]\tLoss: 0.022142\n","Train Epoch: 6 [19840/60000 (33%)]\tLoss: 0.161872\n","Train Epoch: 6 [20480/60000 (34%)]\tLoss: 0.040291\n","Train Epoch: 6 [21120/60000 (35%)]\tLoss: 0.029400\n","Train Epoch: 6 [21760/60000 (36%)]\tLoss: 0.054956\n","Train Epoch: 6 [22400/60000 (37%)]\tLoss: 0.066509\n","Train Epoch: 6 [23040/60000 (38%)]\tLoss: 0.147359\n","Train Epoch: 6 [23680/60000 (39%)]\tLoss: 0.123891\n","Train Epoch: 6 [24320/60000 (41%)]\tLoss: 0.072847\n","Train Epoch: 6 [24960/60000 (42%)]\tLoss: 0.045332\n","Train Epoch: 6 [25600/60000 (43%)]\tLoss: 0.049625\n","Train Epoch: 6 [26240/60000 (44%)]\tLoss: 0.008553\n","Train Epoch: 6 [26880/60000 (45%)]\tLoss: 0.115551\n","Train Epoch: 6 [27520/60000 (46%)]\tLoss: 0.073535\n","Train Epoch: 6 [28160/60000 (47%)]\tLoss: 0.051345\n","Train Epoch: 6 [28800/60000 (48%)]\tLoss: 0.022181\n","Train Epoch: 6 [29440/60000 (49%)]\tLoss: 0.073401\n","Train Epoch: 6 [30080/60000 (50%)]\tLoss: 0.198606\n","Train Epoch: 6 [30720/60000 (51%)]\tLoss: 0.113041\n","Train Epoch: 6 [31360/60000 (52%)]\tLoss: 0.026662\n","Train Epoch: 6 [32000/60000 (53%)]\tLoss: 0.029697\n","Train Epoch: 6 [32640/60000 (54%)]\tLoss: 0.043156\n","Train Epoch: 6 [33280/60000 (55%)]\tLoss: 0.058873\n","Train Epoch: 6 [33920/60000 (57%)]\tLoss: 0.021511\n","Train Epoch: 6 [34560/60000 (58%)]\tLoss: 0.083771\n","Train Epoch: 6 [35200/60000 (59%)]\tLoss: 0.079888\n","Train Epoch: 6 [35840/60000 (60%)]\tLoss: 0.071933\n","Train Epoch: 6 [36480/60000 (61%)]\tLoss: 0.021050\n","Train Epoch: 6 [37120/60000 (62%)]\tLoss: 0.090606\n","Train Epoch: 6 [37760/60000 (63%)]\tLoss: 0.018534\n","Train Epoch: 6 [38400/60000 (64%)]\tLoss: 0.059235\n","Train Epoch: 6 [39040/60000 (65%)]\tLoss: 0.061693\n","Train Epoch: 6 [39680/60000 (66%)]\tLoss: 0.042649\n","Train Epoch: 6 [40320/60000 (67%)]\tLoss: 0.023223\n","Train Epoch: 6 [40960/60000 (68%)]\tLoss: 0.191758\n","Train Epoch: 6 [41600/60000 (69%)]\tLoss: 0.152806\n","Train Epoch: 6 [42240/60000 (70%)]\tLoss: 0.090488\n","Train Epoch: 6 [42880/60000 (71%)]\tLoss: 0.052228\n","Train Epoch: 6 [43520/60000 (72%)]\tLoss: 0.013245\n","Train Epoch: 6 [44160/60000 (74%)]\tLoss: 0.081894\n","Train Epoch: 6 [44800/60000 (75%)]\tLoss: 0.034659\n","Train Epoch: 6 [45440/60000 (76%)]\tLoss: 0.139200\n","Train Epoch: 6 [46080/60000 (77%)]\tLoss: 0.076329\n","Train Epoch: 6 [46720/60000 (78%)]\tLoss: 0.179394\n","Train Epoch: 6 [47360/60000 (79%)]\tLoss: 0.058641\n","Train Epoch: 6 [48000/60000 (80%)]\tLoss: 0.027237\n","Train Epoch: 6 [48640/60000 (81%)]\tLoss: 0.129135\n","Train Epoch: 6 [49280/60000 (82%)]\tLoss: 0.054038\n","Train Epoch: 6 [49920/60000 (83%)]\tLoss: 0.182551\n","Train Epoch: 6 [50560/60000 (84%)]\tLoss: 0.081722\n","Train Epoch: 6 [51200/60000 (85%)]\tLoss: 0.111568\n","Train Epoch: 6 [51840/60000 (86%)]\tLoss: 0.113677\n","Train Epoch: 6 [52480/60000 (87%)]\tLoss: 0.054940\n","Train Epoch: 6 [53120/60000 (88%)]\tLoss: 0.054269\n","Train Epoch: 6 [53760/60000 (90%)]\tLoss: 0.076103\n","Train Epoch: 6 [54400/60000 (91%)]\tLoss: 0.035910\n","Train Epoch: 6 [55040/60000 (92%)]\tLoss: 0.049744\n","Train Epoch: 6 [55680/60000 (93%)]\tLoss: 0.030261\n","Train Epoch: 6 [56320/60000 (94%)]\tLoss: 0.029426\n","Train Epoch: 6 [56960/60000 (95%)]\tLoss: 0.029113\n","Train Epoch: 6 [57600/60000 (96%)]\tLoss: 0.039213\n","Train Epoch: 6 [58240/60000 (97%)]\tLoss: 0.068291\n","Train Epoch: 6 [58880/60000 (98%)]\tLoss: 0.008392\n","Train Epoch: 6 [59520/60000 (99%)]\tLoss: 0.025863\n","\n","Test set: Average loss: 0.0683, Accuracy: 9787/10000 (98%)\n","\n","Train Epoch: 7 [0/60000 (0%)]\tLoss: 0.040439\n","Train Epoch: 7 [640/60000 (1%)]\tLoss: 0.041476\n","Train Epoch: 7 [1280/60000 (2%)]\tLoss: 0.080624\n","Train Epoch: 7 [1920/60000 (3%)]\tLoss: 0.086962\n","Train Epoch: 7 [2560/60000 (4%)]\tLoss: 0.038438\n","Train Epoch: 7 [3200/60000 (5%)]\tLoss: 0.051324\n","Train Epoch: 7 [3840/60000 (6%)]\tLoss: 0.089680\n","Train Epoch: 7 [4480/60000 (7%)]\tLoss: 0.106039\n","Train Epoch: 7 [5120/60000 (9%)]\tLoss: 0.033214\n","Train Epoch: 7 [5760/60000 (10%)]\tLoss: 0.172875\n","Train Epoch: 7 [6400/60000 (11%)]\tLoss: 0.051788\n","Train Epoch: 7 [7040/60000 (12%)]\tLoss: 0.007203\n","Train Epoch: 7 [7680/60000 (13%)]\tLoss: 0.078233\n","Train Epoch: 7 [8320/60000 (14%)]\tLoss: 0.141770\n","Train Epoch: 7 [8960/60000 (15%)]\tLoss: 0.065308\n","Train Epoch: 7 [9600/60000 (16%)]\tLoss: 0.015910\n","Train Epoch: 7 [10240/60000 (17%)]\tLoss: 0.022752\n","Train Epoch: 7 [10880/60000 (18%)]\tLoss: 0.019028\n","Train Epoch: 7 [11520/60000 (19%)]\tLoss: 0.070968\n","Train Epoch: 7 [12160/60000 (20%)]\tLoss: 0.050681\n","Train Epoch: 7 [12800/60000 (21%)]\tLoss: 0.057492\n","Train Epoch: 7 [13440/60000 (22%)]\tLoss: 0.138174\n","Train Epoch: 7 [14080/60000 (23%)]\tLoss: 0.118550\n","Train Epoch: 7 [14720/60000 (25%)]\tLoss: 0.064005\n","Train Epoch: 7 [15360/60000 (26%)]\tLoss: 0.119730\n","Train Epoch: 7 [16000/60000 (27%)]\tLoss: 0.058989\n","Train Epoch: 7 [16640/60000 (28%)]\tLoss: 0.047326\n","Train Epoch: 7 [17280/60000 (29%)]\tLoss: 0.033688\n","Train Epoch: 7 [17920/60000 (30%)]\tLoss: 0.127874\n","Train Epoch: 7 [18560/60000 (31%)]\tLoss: 0.131454\n","Train Epoch: 7 [19200/60000 (32%)]\tLoss: 0.056359\n","Train Epoch: 7 [19840/60000 (33%)]\tLoss: 0.080083\n","Train Epoch: 7 [20480/60000 (34%)]\tLoss: 0.216344\n","Train Epoch: 7 [21120/60000 (35%)]\tLoss: 0.015438\n","Train Epoch: 7 [21760/60000 (36%)]\tLoss: 0.135263\n","Train Epoch: 7 [22400/60000 (37%)]\tLoss: 0.136430\n","Train Epoch: 7 [23040/60000 (38%)]\tLoss: 0.070806\n","Train Epoch: 7 [23680/60000 (39%)]\tLoss: 0.140177\n","Train Epoch: 7 [24320/60000 (41%)]\tLoss: 0.043520\n","Train Epoch: 7 [24960/60000 (42%)]\tLoss: 0.054830\n","Train Epoch: 7 [25600/60000 (43%)]\tLoss: 0.044611\n","Train Epoch: 7 [26240/60000 (44%)]\tLoss: 0.049225\n","Train Epoch: 7 [26880/60000 (45%)]\tLoss: 0.102979\n","Train Epoch: 7 [27520/60000 (46%)]\tLoss: 0.030515\n","Train Epoch: 7 [28160/60000 (47%)]\tLoss: 0.052029\n","Train Epoch: 7 [28800/60000 (48%)]\tLoss: 0.061742\n","Train Epoch: 7 [29440/60000 (49%)]\tLoss: 0.024803\n","Train Epoch: 7 [30080/60000 (50%)]\tLoss: 0.044605\n","Train Epoch: 7 [30720/60000 (51%)]\tLoss: 0.137537\n","Train Epoch: 7 [31360/60000 (52%)]\tLoss: 0.060652\n","Train Epoch: 7 [32000/60000 (53%)]\tLoss: 0.028470\n","Train Epoch: 7 [32640/60000 (54%)]\tLoss: 0.072434\n","Train Epoch: 7 [33280/60000 (55%)]\tLoss: 0.095346\n","Train Epoch: 7 [33920/60000 (57%)]\tLoss: 0.020566\n","Train Epoch: 7 [34560/60000 (58%)]\tLoss: 0.135850\n","Train Epoch: 7 [35200/60000 (59%)]\tLoss: 0.048929\n","Train Epoch: 7 [35840/60000 (60%)]\tLoss: 0.180502\n","Train Epoch: 7 [36480/60000 (61%)]\tLoss: 0.018929\n","Train Epoch: 7 [37120/60000 (62%)]\tLoss: 0.050082\n","Train Epoch: 7 [37760/60000 (63%)]\tLoss: 0.035723\n","Train Epoch: 7 [38400/60000 (64%)]\tLoss: 0.028180\n","Train Epoch: 7 [39040/60000 (65%)]\tLoss: 0.032134\n","Train Epoch: 7 [39680/60000 (66%)]\tLoss: 0.041375\n","Train Epoch: 7 [40320/60000 (67%)]\tLoss: 0.084175\n","Train Epoch: 7 [40960/60000 (68%)]\tLoss: 0.063929\n","Train Epoch: 7 [41600/60000 (69%)]\tLoss: 0.032549\n","Train Epoch: 7 [42240/60000 (70%)]\tLoss: 0.069349\n","Train Epoch: 7 [42880/60000 (71%)]\tLoss: 0.166430\n","Train Epoch: 7 [43520/60000 (72%)]\tLoss: 0.074929\n","Train Epoch: 7 [44160/60000 (74%)]\tLoss: 0.033747\n","Train Epoch: 7 [44800/60000 (75%)]\tLoss: 0.022842\n","Train Epoch: 7 [45440/60000 (76%)]\tLoss: 0.034559\n","Train Epoch: 7 [46080/60000 (77%)]\tLoss: 0.043220\n","Train Epoch: 7 [46720/60000 (78%)]\tLoss: 0.067521\n","Train Epoch: 7 [47360/60000 (79%)]\tLoss: 0.065631\n","Train Epoch: 7 [48000/60000 (80%)]\tLoss: 0.074457\n","Train Epoch: 7 [48640/60000 (81%)]\tLoss: 0.139710\n","Train Epoch: 7 [49280/60000 (82%)]\tLoss: 0.088806\n","Train Epoch: 7 [49920/60000 (83%)]\tLoss: 0.077597\n","Train Epoch: 7 [50560/60000 (84%)]\tLoss: 0.065537\n","Train Epoch: 7 [51200/60000 (85%)]\tLoss: 0.056749\n","Train Epoch: 7 [51840/60000 (86%)]\tLoss: 0.031376\n","Train Epoch: 7 [52480/60000 (87%)]\tLoss: 0.087861\n","Train Epoch: 7 [53120/60000 (88%)]\tLoss: 0.142046\n","Train Epoch: 7 [53760/60000 (90%)]\tLoss: 0.033809\n","Train Epoch: 7 [54400/60000 (91%)]\tLoss: 0.107050\n","Train Epoch: 7 [55040/60000 (92%)]\tLoss: 0.102748\n","Train Epoch: 7 [55680/60000 (93%)]\tLoss: 0.016113\n","Train Epoch: 7 [56320/60000 (94%)]\tLoss: 0.073633\n","Train Epoch: 7 [56960/60000 (95%)]\tLoss: 0.082848\n","Train Epoch: 7 [57600/60000 (96%)]\tLoss: 0.215375\n","Train Epoch: 7 [58240/60000 (97%)]\tLoss: 0.188476\n","Train Epoch: 7 [58880/60000 (98%)]\tLoss: 0.096700\n","Train Epoch: 7 [59520/60000 (99%)]\tLoss: 0.086860\n","\n","Test set: Average loss: 0.0619, Accuracy: 9798/10000 (98%)\n","\n","Train Epoch: 8 [0/60000 (0%)]\tLoss: 0.090989\n","Train Epoch: 8 [640/60000 (1%)]\tLoss: 0.008470\n","Train Epoch: 8 [1280/60000 (2%)]\tLoss: 0.019627\n","Train Epoch: 8 [1920/60000 (3%)]\tLoss: 0.024061\n","Train Epoch: 8 [2560/60000 (4%)]\tLoss: 0.028409\n","Train Epoch: 8 [3200/60000 (5%)]\tLoss: 0.014908\n","Train Epoch: 8 [3840/60000 (6%)]\tLoss: 0.072696\n","Train Epoch: 8 [4480/60000 (7%)]\tLoss: 0.103281\n","Train Epoch: 8 [5120/60000 (9%)]\tLoss: 0.022239\n","Train Epoch: 8 [5760/60000 (10%)]\tLoss: 0.052838\n","Train Epoch: 8 [6400/60000 (11%)]\tLoss: 0.034033\n","Train Epoch: 8 [7040/60000 (12%)]\tLoss: 0.043952\n","Train Epoch: 8 [7680/60000 (13%)]\tLoss: 0.042832\n","Train Epoch: 8 [8320/60000 (14%)]\tLoss: 0.019438\n","Train Epoch: 8 [8960/60000 (15%)]\tLoss: 0.283164\n","Train Epoch: 8 [9600/60000 (16%)]\tLoss: 0.029202\n","Train Epoch: 8 [10240/60000 (17%)]\tLoss: 0.132719\n","Train Epoch: 8 [10880/60000 (18%)]\tLoss: 0.188950\n","Train Epoch: 8 [11520/60000 (19%)]\tLoss: 0.077518\n","Train Epoch: 8 [12160/60000 (20%)]\tLoss: 0.035193\n","Train Epoch: 8 [12800/60000 (21%)]\tLoss: 0.118636\n","Train Epoch: 8 [13440/60000 (22%)]\tLoss: 0.069076\n","Train Epoch: 8 [14080/60000 (23%)]\tLoss: 0.115621\n","Train Epoch: 8 [14720/60000 (25%)]\tLoss: 0.081844\n","Train Epoch: 8 [15360/60000 (26%)]\tLoss: 0.042333\n","Train Epoch: 8 [16000/60000 (27%)]\tLoss: 0.066640\n","Train Epoch: 8 [16640/60000 (28%)]\tLoss: 0.096716\n","Train Epoch: 8 [17280/60000 (29%)]\tLoss: 0.137289\n","Train Epoch: 8 [17920/60000 (30%)]\tLoss: 0.028153\n","Train Epoch: 8 [18560/60000 (31%)]\tLoss: 0.051837\n","Train Epoch: 8 [19200/60000 (32%)]\tLoss: 0.116348\n","Train Epoch: 8 [19840/60000 (33%)]\tLoss: 0.107834\n","Train Epoch: 8 [20480/60000 (34%)]\tLoss: 0.070137\n","Train Epoch: 8 [21120/60000 (35%)]\tLoss: 0.058599\n","Train Epoch: 8 [21760/60000 (36%)]\tLoss: 0.089028\n","Train Epoch: 8 [22400/60000 (37%)]\tLoss: 0.007532\n","Train Epoch: 8 [23040/60000 (38%)]\tLoss: 0.056466\n","Train Epoch: 8 [23680/60000 (39%)]\tLoss: 0.058082\n","Train Epoch: 8 [24320/60000 (41%)]\tLoss: 0.051021\n","Train Epoch: 8 [24960/60000 (42%)]\tLoss: 0.079891\n","Train Epoch: 8 [25600/60000 (43%)]\tLoss: 0.029178\n","Train Epoch: 8 [26240/60000 (44%)]\tLoss: 0.093749\n","Train Epoch: 8 [26880/60000 (45%)]\tLoss: 0.104057\n","Train Epoch: 8 [27520/60000 (46%)]\tLoss: 0.032692\n","Train Epoch: 8 [28160/60000 (47%)]\tLoss: 0.081681\n","Train Epoch: 8 [28800/60000 (48%)]\tLoss: 0.022375\n","Train Epoch: 8 [29440/60000 (49%)]\tLoss: 0.039535\n","Train Epoch: 8 [30080/60000 (50%)]\tLoss: 0.061739\n","Train Epoch: 8 [30720/60000 (51%)]\tLoss: 0.144855\n","Train Epoch: 8 [31360/60000 (52%)]\tLoss: 0.154467\n","Train Epoch: 8 [32000/60000 (53%)]\tLoss: 0.022960\n","Train Epoch: 8 [32640/60000 (54%)]\tLoss: 0.030062\n","Train Epoch: 8 [33280/60000 (55%)]\tLoss: 0.021711\n","Train Epoch: 8 [33920/60000 (57%)]\tLoss: 0.020199\n","Train Epoch: 8 [34560/60000 (58%)]\tLoss: 0.161566\n","Train Epoch: 8 [35200/60000 (59%)]\tLoss: 0.091358\n","Train Epoch: 8 [35840/60000 (60%)]\tLoss: 0.033618\n","Train Epoch: 8 [36480/60000 (61%)]\tLoss: 0.017889\n","Train Epoch: 8 [37120/60000 (62%)]\tLoss: 0.035265\n","Train Epoch: 8 [37760/60000 (63%)]\tLoss: 0.078381\n","Train Epoch: 8 [38400/60000 (64%)]\tLoss: 0.079663\n","Train Epoch: 8 [39040/60000 (65%)]\tLoss: 0.077321\n","Train Epoch: 8 [39680/60000 (66%)]\tLoss: 0.013657\n","Train Epoch: 8 [40320/60000 (67%)]\tLoss: 0.066169\n","Train Epoch: 8 [40960/60000 (68%)]\tLoss: 0.051776\n","Train Epoch: 8 [41600/60000 (69%)]\tLoss: 0.065484\n","Train Epoch: 8 [42240/60000 (70%)]\tLoss: 0.018657\n","Train Epoch: 8 [42880/60000 (71%)]\tLoss: 0.016982\n","Train Epoch: 8 [43520/60000 (72%)]\tLoss: 0.066006\n","Train Epoch: 8 [44160/60000 (74%)]\tLoss: 0.183632\n","Train Epoch: 8 [44800/60000 (75%)]\tLoss: 0.017892\n","Train Epoch: 8 [45440/60000 (76%)]\tLoss: 0.114879\n","Train Epoch: 8 [46080/60000 (77%)]\tLoss: 0.133889\n","Train Epoch: 8 [46720/60000 (78%)]\tLoss: 0.083526\n","Train Epoch: 8 [47360/60000 (79%)]\tLoss: 0.014153\n","Train Epoch: 8 [48000/60000 (80%)]\tLoss: 0.005534\n","Train Epoch: 8 [48640/60000 (81%)]\tLoss: 0.139073\n","Train Epoch: 8 [49280/60000 (82%)]\tLoss: 0.072176\n","Train Epoch: 8 [49920/60000 (83%)]\tLoss: 0.045919\n","Train Epoch: 8 [50560/60000 (84%)]\tLoss: 0.010424\n","Train Epoch: 8 [51200/60000 (85%)]\tLoss: 0.061715\n","Train Epoch: 8 [51840/60000 (86%)]\tLoss: 0.042998\n","Train Epoch: 8 [52480/60000 (87%)]\tLoss: 0.004538\n","Train Epoch: 8 [53120/60000 (88%)]\tLoss: 0.016535\n","Train Epoch: 8 [53760/60000 (90%)]\tLoss: 0.036886\n","Train Epoch: 8 [54400/60000 (91%)]\tLoss: 0.196557\n","Train Epoch: 8 [55040/60000 (92%)]\tLoss: 0.171675\n","Train Epoch: 8 [55680/60000 (93%)]\tLoss: 0.054367\n","Train Epoch: 8 [56320/60000 (94%)]\tLoss: 0.021303\n","Train Epoch: 8 [56960/60000 (95%)]\tLoss: 0.068968\n","Train Epoch: 8 [57600/60000 (96%)]\tLoss: 0.130293\n","Train Epoch: 8 [58240/60000 (97%)]\tLoss: 0.039244\n","Train Epoch: 8 [58880/60000 (98%)]\tLoss: 0.024936\n","Train Epoch: 8 [59520/60000 (99%)]\tLoss: 0.013477\n","\n","Test set: Average loss: 0.0517, Accuracy: 9829/10000 (98%)\n","\n","Train Epoch: 9 [0/60000 (0%)]\tLoss: 0.013349\n","Train Epoch: 9 [640/60000 (1%)]\tLoss: 0.045616\n","Train Epoch: 9 [1280/60000 (2%)]\tLoss: 0.010423\n","Train Epoch: 9 [1920/60000 (3%)]\tLoss: 0.023725\n","Train Epoch: 9 [2560/60000 (4%)]\tLoss: 0.118275\n","Train Epoch: 9 [3200/60000 (5%)]\tLoss: 0.035484\n","Train Epoch: 9 [3840/60000 (6%)]\tLoss: 0.011323\n","Train Epoch: 9 [4480/60000 (7%)]\tLoss: 0.072761\n","Train Epoch: 9 [5120/60000 (9%)]\tLoss: 0.217697\n","Train Epoch: 9 [5760/60000 (10%)]\tLoss: 0.053505\n","Train Epoch: 9 [6400/60000 (11%)]\tLoss: 0.097454\n","Train Epoch: 9 [7040/60000 (12%)]\tLoss: 0.021630\n","Train Epoch: 9 [7680/60000 (13%)]\tLoss: 0.088723\n","Train Epoch: 9 [8320/60000 (14%)]\tLoss: 0.048940\n","Train Epoch: 9 [8960/60000 (15%)]\tLoss: 0.131828\n","Train Epoch: 9 [9600/60000 (16%)]\tLoss: 0.028067\n","Train Epoch: 9 [10240/60000 (17%)]\tLoss: 0.045998\n","Train Epoch: 9 [10880/60000 (18%)]\tLoss: 0.060839\n","Train Epoch: 9 [11520/60000 (19%)]\tLoss: 0.158306\n","Train Epoch: 9 [12160/60000 (20%)]\tLoss: 0.077579\n","Train Epoch: 9 [12800/60000 (21%)]\tLoss: 0.044701\n","Train Epoch: 9 [13440/60000 (22%)]\tLoss: 0.057212\n","Train Epoch: 9 [14080/60000 (23%)]\tLoss: 0.064404\n","Train Epoch: 9 [14720/60000 (25%)]\tLoss: 0.027552\n","Train Epoch: 9 [15360/60000 (26%)]\tLoss: 0.056525\n","Train Epoch: 9 [16000/60000 (27%)]\tLoss: 0.090116\n","Train Epoch: 9 [16640/60000 (28%)]\tLoss: 0.040886\n","Train Epoch: 9 [17280/60000 (29%)]\tLoss: 0.062012\n","Train Epoch: 9 [17920/60000 (30%)]\tLoss: 0.135984\n","Train Epoch: 9 [18560/60000 (31%)]\tLoss: 0.057879\n","Train Epoch: 9 [19200/60000 (32%)]\tLoss: 0.033305\n","Train Epoch: 9 [19840/60000 (33%)]\tLoss: 0.006071\n","Train Epoch: 9 [20480/60000 (34%)]\tLoss: 0.074139\n","Train Epoch: 9 [21120/60000 (35%)]\tLoss: 0.028711\n","Train Epoch: 9 [21760/60000 (36%)]\tLoss: 0.130342\n","Train Epoch: 9 [22400/60000 (37%)]\tLoss: 0.012664\n","Train Epoch: 9 [23040/60000 (38%)]\tLoss: 0.068998\n","Train Epoch: 9 [23680/60000 (39%)]\tLoss: 0.059335\n","Train Epoch: 9 [24320/60000 (41%)]\tLoss: 0.182616\n","Train Epoch: 9 [24960/60000 (42%)]\tLoss: 0.041096\n","Train Epoch: 9 [25600/60000 (43%)]\tLoss: 0.050557\n","Train Epoch: 9 [26240/60000 (44%)]\tLoss: 0.041334\n","Train Epoch: 9 [26880/60000 (45%)]\tLoss: 0.057146\n","Train Epoch: 9 [27520/60000 (46%)]\tLoss: 0.091474\n","Train Epoch: 9 [28160/60000 (47%)]\tLoss: 0.012643\n","Train Epoch: 9 [28800/60000 (48%)]\tLoss: 0.029657\n","Train Epoch: 9 [29440/60000 (49%)]\tLoss: 0.025615\n","Train Epoch: 9 [30080/60000 (50%)]\tLoss: 0.048118\n","Train Epoch: 9 [30720/60000 (51%)]\tLoss: 0.058921\n","Train Epoch: 9 [31360/60000 (52%)]\tLoss: 0.093164\n","Train Epoch: 9 [32000/60000 (53%)]\tLoss: 0.047369\n","Train Epoch: 9 [32640/60000 (54%)]\tLoss: 0.092881\n","Train Epoch: 9 [33280/60000 (55%)]\tLoss: 0.071850\n","Train Epoch: 9 [33920/60000 (57%)]\tLoss: 0.217938\n","Train Epoch: 9 [34560/60000 (58%)]\tLoss: 0.059143\n","Train Epoch: 9 [35200/60000 (59%)]\tLoss: 0.030295\n","Train Epoch: 9 [35840/60000 (60%)]\tLoss: 0.060688\n","Train Epoch: 9 [36480/60000 (61%)]\tLoss: 0.051281\n","Train Epoch: 9 [37120/60000 (62%)]\tLoss: 0.022938\n","Train Epoch: 9 [37760/60000 (63%)]\tLoss: 0.056893\n","Train Epoch: 9 [38400/60000 (64%)]\tLoss: 0.071075\n","Train Epoch: 9 [39040/60000 (65%)]\tLoss: 0.027225\n","Train Epoch: 9 [39680/60000 (66%)]\tLoss: 0.019818\n","Train Epoch: 9 [40320/60000 (67%)]\tLoss: 0.114242\n","Train Epoch: 9 [40960/60000 (68%)]\tLoss: 0.127202\n","Train Epoch: 9 [41600/60000 (69%)]\tLoss: 0.033226\n","Train Epoch: 9 [42240/60000 (70%)]\tLoss: 0.028151\n","Train Epoch: 9 [42880/60000 (71%)]\tLoss: 0.064612\n","Train Epoch: 9 [43520/60000 (72%)]\tLoss: 0.008050\n","Train Epoch: 9 [44160/60000 (74%)]\tLoss: 0.075138\n","Train Epoch: 9 [44800/60000 (75%)]\tLoss: 0.074545\n","Train Epoch: 9 [45440/60000 (76%)]\tLoss: 0.093080\n","Train Epoch: 9 [46080/60000 (77%)]\tLoss: 0.076389\n","Train Epoch: 9 [46720/60000 (78%)]\tLoss: 0.021357\n","Train Epoch: 9 [47360/60000 (79%)]\tLoss: 0.008993\n","Train Epoch: 9 [48000/60000 (80%)]\tLoss: 0.096017\n","Train Epoch: 9 [48640/60000 (81%)]\tLoss: 0.058316\n","Train Epoch: 9 [49280/60000 (82%)]\tLoss: 0.039997\n","Train Epoch: 9 [49920/60000 (83%)]\tLoss: 0.034946\n","Train Epoch: 9 [50560/60000 (84%)]\tLoss: 0.010148\n","Train Epoch: 9 [51200/60000 (85%)]\tLoss: 0.015658\n","Train Epoch: 9 [51840/60000 (86%)]\tLoss: 0.055397\n","Train Epoch: 9 [52480/60000 (87%)]\tLoss: 0.029527\n","Train Epoch: 9 [53120/60000 (88%)]\tLoss: 0.054938\n","Train Epoch: 9 [53760/60000 (90%)]\tLoss: 0.045598\n","Train Epoch: 9 [54400/60000 (91%)]\tLoss: 0.075018\n","Train Epoch: 9 [55040/60000 (92%)]\tLoss: 0.020592\n","Train Epoch: 9 [55680/60000 (93%)]\tLoss: 0.030133\n","Train Epoch: 9 [56320/60000 (94%)]\tLoss: 0.023864\n","Train Epoch: 9 [56960/60000 (95%)]\tLoss: 0.012125\n","Train Epoch: 9 [57600/60000 (96%)]\tLoss: 0.039742\n","Train Epoch: 9 [58240/60000 (97%)]\tLoss: 0.023395\n","Train Epoch: 9 [58880/60000 (98%)]\tLoss: 0.086071\n","Train Epoch: 9 [59520/60000 (99%)]\tLoss: 0.019391\n","\n","Test set: Average loss: 0.0537, Accuracy: 9818/10000 (98%)\n","\n"]}]}]}