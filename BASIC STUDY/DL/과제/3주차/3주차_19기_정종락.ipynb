{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sgAYo4nrw2F4"
   },
   "source": [
    "# < 3주차 과제 1 : MLP 마음대로 다뤄보기>\n",
    "- dataset을 임의로 선정해서 직접 분석 해보기(제공한 코드를 활용해서 해보기)\n",
    "- activation functions 중 relu사용시 함수 직접 정의\n",
    "- lr, optimizer 등 바꿔보기\n",
    "- hidden layer/neuron 수를 바꾸기\n",
    "- 전처리도 추가\n",
    "- 모든 시도를 올려주세요!\n",
    "- 제일 높은 acc를 보인 시도를 명시해주세요!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "fX437IL6qbI-"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "from sklearn.datasets import load_wine\n",
    "from torch.utils.data import  TensorDataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oxkFzBDNmWNk"
   },
   "source": [
    "## 아래 데이터셋 중 원하는 데이터셋 하나를 선택하여, 코드 과제 진행하기!\n",
    "- 1) load_digits() <br>\n",
    "- 2) load_wine()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 `load_wine()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "FywYbfsKtjcR"
   },
   "outputs": [],
   "source": [
    "# 데이터셋 종류 :\n",
    "data = load_wine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "C2P0hqZ9yBGm"
   },
   "outputs": [],
   "source": [
    "input = data.data\n",
    "output = data.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(178, 13)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "SggpQfSPt85C"
   },
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "torch.manual_seed(777)\n",
    "if device == \"cuda\":\n",
    "  torch.cuda.manual_seed_all(777)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "bLMzf-2ntYeX"
   },
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(input, output, test_size = 0.3, random_state = 42, stratify= data.target, shuffle = True)\n",
    "\n",
    "x_train = torch.FloatTensor(x_train).to(device)\n",
    "y_train = torch.LongTensor(y_train).to(device)\n",
    "x_test = torch.FloatTensor(x_test)\n",
    "y_test = torch.LongTensor(y_test)\n",
    "\n",
    "# 데이터를 tensor로 바꿔주고 gpu 연산이 가능해지도록 gpu에 옮김\n",
    "# label 값을 왜 long 에 옮겨놓는가? loss function이 다르기 때문"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "umEdiTZkrVqS",
    "outputId": "335674e0-2dd8-449e-b295-90b42c49c21c",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.1620e+01, 1.8180e+01, 7.6380e+01, 4.0880e+02, 1.1750e-01, 1.4830e-01,\n",
      "        1.0200e-01, 5.5640e-02, 1.9570e-01, 7.2550e-02, 4.1010e-01, 1.7400e+00,\n",
      "        3.0270e+00, 2.7850e+01, 1.4590e-02, 3.2060e-02, 4.9610e-02, 1.8410e-02,\n",
      "        1.8070e-02, 5.2170e-03, 1.3360e+01, 2.5400e+01, 8.8140e+01, 5.2810e+02,\n",
      "        1.7800e-01, 2.8780e-01, 3.1860e-01, 1.4160e-01, 2.6600e-01, 9.2700e-02],\n",
      "       device='cuda:0')\n",
      "tensor(1, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(x_train[0]) # input 13개 (속성이 13개)\n",
    "print(y_train[0]) # y의 class는 3개 - 0, 1, 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "combmxzmYFyn"
   },
   "source": [
    "파이토치에서는 데이터셋을 좀 더 쉽게 다룰 수 있도록 유용한 도구로서 torch.utils.data.Dataset과 torch.utils.data.DataLoader를 제공합니다. 이를 사용하면 미니 배치 학습, 데이터 셔플(shuffle), 병렬 처리까지 간단히 수행할 수 있습니다. 기본적인 사용 방법은 Dataset을 정의하고, 이를 DataLoader에 전달하는 것\n",
    "- init : class 에서 객체가 생성되면 바로 실행되는 함수\n",
    "- len : observation 수를 정의하는 함수\n",
    "- getitem : iteration 마다 해당하는 데이터를 돌려주는 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "y38TlgXoqV5Z"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "class CustomDataset(Dataset):\n",
    "  def __init__(self):\n",
    "    self.x_data = x_train\n",
    "    self.y_data = [[y] for y in y_train]\n",
    "#  데이터셋의 전처리를 해주는 부분\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.x_data)\n",
    "#  데이터셋의 길이. 즉, 총 샘플의 수를 적어주는 부분\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    x = torch.FloatTensor(self.x_data[idx]).to(device)\n",
    "    y = torch.LongTensor(self.y_data[idx]).to(device)\n",
    "#  데이터셋에서 특정 1개의 샘플을 가져오는 함수\n",
    "\n",
    "    return x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "x8VHwnuFqino"
   },
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "\n",
    "dataset = CustomDataset()\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "C6V7a4tyq6Jc"
   },
   "outputs": [],
   "source": [
    "# input_layer와 hidden_layer의 노드 개수를 어떻게 바꿔줘야할까?\n",
    "# hidden layer/neuron 수를 바꾸기\n",
    "\n",
    "model = nn.Sequential(\n",
    "          nn.Linear(13,213, bias=True), # 13개...\n",
    "          nn.Sigmoid(),\n",
    "          nn.Linear(213,15, bias=True),\n",
    "          nn.Sigmoid(),\n",
    "          nn.Linear(15,3, bias=True), # 3개..\n",
    "          nn.Softmax()\n",
    "          ).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "07uV8RY7Yr_5"
   },
   "source": [
    "class로 구현 가능\n",
    "- init : 초기 생성 함수\n",
    "- foward : 순전파(입력값 => 예측값 의 과정)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "a0zLstbMqxEZ"
   },
   "outputs": [],
   "source": [
    "class Model(torch.nn.Module):\n",
    "\n",
    "  def __init__(self):\n",
    "    super(Model, self).__init__()\n",
    "    self.layer1 = nn.Sequential(\n",
    "          nn.Linear(30,398, bias=True), # input_layer = 30, hidden_layer1 = 398\n",
    "          nn.Sigmoid(),\n",
    "        nn.BatchNorm1d(398)\n",
    "    )\n",
    "  # activation function 이용\n",
    "  #   nn.ReLU()\n",
    "  #   nn.tanH()\n",
    "  #   https://pytorch.org/docs/stable/nn.html 그 외에도 여기서 확인 가능함\n",
    "  #   파라미터가 필요하지 않다는 것이 특징\n",
    "\n",
    "  # batch normazliation 1d, 파라미터 값으로 vector의 길이를 전해줌\n",
    "  # 추후에 이미지를 다루게 된다면, 그때는 batch normalization 2d를 이용하게 됨\n",
    "  # 그때는 파라미터 값으로 채널, 가로, 세로 길이를 전달해주게 됨\n",
    "\n",
    "    self.layer2 = nn.Sequential(\n",
    "          nn.Linear(398,15, bias=True), # hidden_layer1 = 398, hidden_layer2 = 15\n",
    "        nn.Sigmoid()\n",
    "    )\n",
    "    self.layer3 = nn.Sequential(\n",
    "          nn.Linear(15,10, bias=True), # hidden_layer1 = 15, hidden_layer2 = 10\n",
    "        nn.Sigmoid()\n",
    "    )\n",
    "    self.layer4 = nn.Sequential(\n",
    "        nn.Linear(10, 5, bias=True), # hidden_layer3 = 10, output_layer = 5\n",
    "        nn.Softmax()\n",
    "    )\n",
    "\n",
    "  def forward(self,x):\n",
    "    output = self.layer1(x)\n",
    "    output = self.layer2(output)\n",
    "    output = self.layer3(output)\n",
    "    output = self.layer4(output)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "kqcqqkECrSGK"
   },
   "outputs": [],
   "source": [
    "def init_weights(layer):\n",
    "    if isinstance(layer, nn.Linear):\n",
    "        torch.nn.init.xavier_uniform(layer.weight)\n",
    "        layer.bias.data.fill_(0.01)\n",
    "\n",
    "        #xavier사용\n",
    "        # Layer의 weight를 어떤 분포를 가지도록 초기화시켜줌+global minimum찾기 위해서"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oMDUBFg6rUpw",
    "outputId": "d77885a4-8bfa-4af9-bfaa-36c76dba40ee"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/f1/_jhcgxqn2mq116n84m1p85t00000gn/T/ipykernel_7004/2101624760.py:3: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
      "  torch.nn.init.xavier_uniform(layer.weight)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Model(\n",
       "  (layer1): Sequential(\n",
       "    (0): Linear(in_features=30, out_features=398, bias=True)\n",
       "    (1): Sigmoid()\n",
       "    (2): BatchNorm1d(398, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): Linear(in_features=398, out_features=15, bias=True)\n",
       "    (1): Sigmoid()\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): Linear(in_features=15, out_features=10, bias=True)\n",
       "    (1): Sigmoid()\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): Linear(in_features=10, out_features=5, bias=True)\n",
       "    (1): Softmax(dim=None)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Model().to(device)\n",
    "model.apply(init_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OwZt5CetrYFb",
    "outputId": "fe01a0ba-90b9-4120-8f48-2f6710ed17c8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model(\n",
      "  (layer1): Sequential(\n",
      "    (0): Linear(in_features=30, out_features=398, bias=True)\n",
      "    (1): Sigmoid()\n",
      "    (2): BatchNorm1d(398, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): Linear(in_features=398, out_features=15, bias=True)\n",
      "    (1): Sigmoid()\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): Linear(in_features=15, out_features=10, bias=True)\n",
      "    (1): Sigmoid()\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): Linear(in_features=10, out_features=5, bias=True)\n",
      "    (1): Softmax(dim=None)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "AYFp-eTErh7b"
   },
   "outputs": [],
   "source": [
    "loss_fn  = nn.CrossEntropyLoss().to(device)\n",
    "\n",
    "# 여러가지 optimizer 시도해보기\n",
    "# lr 바꿔보기\n",
    "\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr= 0.01)\n",
    "\n",
    "# https://pytorch.org/docs/stable/optim.html#module-torch.optim 페이지 참조\n",
    "# optimizer = optim.SGD(model.parameters(), lr=0.1, momentum=0.9)\n",
    "# sgd 등등"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "90QxHvlIrjS7",
    "outputId": "9325edf8-d0c9-4dd9-c674-eae81a6405c7"
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (124x13 and 30x398)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[35], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m100\u001b[39m):\n\u001b[1;32m      4\u001b[0m   optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m----> 5\u001b[0m   hypothesis \u001b[38;5;241m=\u001b[39m model(x_train)\n\u001b[1;32m      7\u001b[0m   \u001b[38;5;66;03m# 비용 함수\u001b[39;00m\n\u001b[1;32m      8\u001b[0m   cost \u001b[38;5;241m=\u001b[39m loss_fn(hypothesis, y_train)\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[30], line 34\u001b[0m, in \u001b[0;36mModel.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m,x):\n\u001b[0;32m---> 34\u001b[0m   output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer1(x)\n\u001b[1;32m     35\u001b[0m   output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer2(output)\n\u001b[1;32m     36\u001b[0m   output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer3(output)\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.11/site-packages/torch/nn/modules/container.py:215\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    214\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 215\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m module(\u001b[38;5;28minput\u001b[39m)\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.11/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mlinear(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (124x13 and 30x398)"
     ]
    }
   ],
   "source": [
    "losses = []\n",
    "for epoch in range(100):\n",
    "\n",
    "  optimizer.zero_grad()\n",
    "  hypothesis = model(x_train)\n",
    "\n",
    "  # 비용 함수\n",
    "  cost = loss_fn(hypothesis, y_train)\n",
    "  cost.backward()\n",
    "  optimizer.step()\n",
    "  losses.append(cost.item())\n",
    "\n",
    "  if epoch % 10 == 0:\n",
    "    print(epoch, cost.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    },
    "id": "81ASYrW7roFM",
    "outputId": "330a40ca-8185-4c2d-d248-257a5c44b16f"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(losses)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "h4kJzpLErqhZ",
    "outputId": "9b336fde-36f4-470d-9a2f-dc21055e0353"
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (54x13 and 30x398)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[37], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m      2\u001b[0m   model \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m   y_pred \u001b[38;5;241m=\u001b[39m model(x_test)\n\u001b[1;32m      4\u001b[0m   y_pred \u001b[38;5;241m=\u001b[39m y_pred\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[1;32m      5\u001b[0m   predicted \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margmax(y_pred, axis \u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[30], line 34\u001b[0m, in \u001b[0;36mModel.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m,x):\n\u001b[0;32m---> 34\u001b[0m   output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer1(x)\n\u001b[1;32m     35\u001b[0m   output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer2(output)\n\u001b[1;32m     36\u001b[0m   output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer3(output)\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.11/site-packages/torch/nn/modules/container.py:215\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    214\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 215\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m module(\u001b[38;5;28minput\u001b[39m)\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.11/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mlinear(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (54x13 and 30x398)"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "  model = model.to('cpu')\n",
    "  y_pred = model(x_test)\n",
    "  y_pred = y_pred.detach().numpy()\n",
    "  predicted = np.argmax(y_pred, axis =1)\n",
    "  accuracy = (accuracy_score(predicted, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vyIKhs3Nr6Ay",
    "outputId": "6d73d644-c68b-4a84-defc-6f57192ef91a",
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'y_pred' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[38], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel의 output은 :  \u001b[39m\u001b[38;5;132;01m{\u001b[39;00my_pred[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124margmax를 한 후의 output은 \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpredicted[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy는 \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maccuracy\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'y_pred' is not defined"
     ]
    }
   ],
   "source": [
    "print(f'model의 output은 :  {y_pred[0]}')\n",
    "print(f'argmax를 한 후의 output은 {predicted[0]}')\n",
    "print(f'accuracy는 {accuracy}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 아직 모델에 대해 제대로 이해하지 못해 과제 수행 불가.. 다시 공부 필요........."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\\\\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 원본코드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "FywYbfsKtjcR"
   },
   "outputs": [],
   "source": [
    "# 데이터셋 종류 :\n",
    "data = load_breast_cancer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "C2P0hqZ9yBGm"
   },
   "outputs": [],
   "source": [
    "input = data.data\n",
    "output = data.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "SggpQfSPt85C"
   },
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "torch.manual_seed(777)\n",
    "if device == \"cuda\":\n",
    "  torch.cuda.manual_seed_all(777)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "bLMzf-2ntYeX"
   },
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(input, output, test_size = 0.3, random_state = 42, stratify= data.target, shuffle = True)\n",
    "\n",
    "x_train = torch.FloatTensor(x_train).to(device)\n",
    "y_train = torch.LongTensor(y_train).to(device)\n",
    "x_test = torch.FloatTensor(x_test)\n",
    "y_test = torch.LongTensor(y_test)\n",
    "\n",
    "# 데이터를 tensor로 바꿔주고 gpu 연산이 가능해지도록 gpu에 옮김\n",
    "# label 값을 왜 long 에 옮겨놓는가? loss function이 다르기 때문"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "umEdiTZkrVqS",
    "outputId": "335674e0-2dd8-449e-b295-90b42c49c21c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.1620e+01, 1.8180e+01, 7.6380e+01, 4.0880e+02, 1.1750e-01, 1.4830e-01,\n",
      "        1.0200e-01, 5.5640e-02, 1.9570e-01, 7.2550e-02, 4.1010e-01, 1.7400e+00,\n",
      "        3.0270e+00, 2.7850e+01, 1.4590e-02, 3.2060e-02, 4.9610e-02, 1.8410e-02,\n",
      "        1.8070e-02, 5.2170e-03, 1.3360e+01, 2.5400e+01, 8.8140e+01, 5.2810e+02,\n",
      "        1.7800e-01, 2.8780e-01, 3.1860e-01, 1.4160e-01, 2.6600e-01, 9.2700e-02])\n",
      "tensor(1)\n"
     ]
    }
   ],
   "source": [
    "print(x_train[0])\n",
    "print(y_train[0])\n",
    "\n",
    "#input 30개 (속성이 30개)\n",
    "#y의 class는 2개 (양성과 음성)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "combmxzmYFyn"
   },
   "source": [
    "파이토치에서는 데이터셋을 좀 더 쉽게 다룰 수 있도록 유용한 도구로서 torch.utils.data.Dataset과 torch.utils.data.DataLoader를 제공합니다. 이를 사용하면 미니 배치 학습, 데이터 셔플(shuffle), 병렬 처리까지 간단히 수행할 수 있습니다. 기본적인 사용 방법은 Dataset을 정의하고, 이를 DataLoader에 전달하는 것\n",
    "- init : class 에서 객체가 생성되면 바로 실행되는 함수\n",
    "- len : observation 수를 정의하는 함수\n",
    "- getitem : iteration 마다 해당하는 데이터를 돌려주는 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y38TlgXoqV5Z"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "class CustomDataset(Dataset):\n",
    "  def __init__(self):\n",
    "    self.x_data = x_train\n",
    "    self.y_data = [[y] for y in y_train]\n",
    "#  데이터셋의 전처리를 해주는 부분\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.x_data)\n",
    "#  데이터셋의 길이. 즉, 총 샘플의 수를 적어주는 부분\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    x = torch.FloatTensor(self.x_data[idx]).to(device)\n",
    "    y = torch.LongTensor(self.y_data[idx]).to(device)\n",
    "#  데이터셋에서 특정 1개의 샘플을 가져오는 함수\n",
    "\n",
    "    return x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x8VHwnuFqino"
   },
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "\n",
    "dataset = CustomDataset()\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C6V7a4tyq6Jc"
   },
   "outputs": [],
   "source": [
    "# input_layer와 hidden_layer의 노드 개수를 어떻게 바꿔줘야할까?\n",
    "# hidden layer/neuron 수를 바꾸기\n",
    "\n",
    "model = nn.Sequential(\n",
    "          nn.Linear(30,398, bias=True),\n",
    "          nn.Sigmoid(),\n",
    "          nn.Linear(398,15, bias=True),\n",
    "          nn.Sigmoid(),\n",
    "          nn.Linear(15,5, bias=True),\n",
    "          nn.Softmax()\n",
    "          ).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "07uV8RY7Yr_5"
   },
   "source": [
    "class로 구현 가능\n",
    "- init : 초기 생성 함수\n",
    "- foward : 순전파(입력값 => 예측값 의 과정)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a0zLstbMqxEZ"
   },
   "outputs": [],
   "source": [
    "class Model(torch.nn.Module):\n",
    "\n",
    "  def __init__(self):\n",
    "    super(Model, self).__init__()\n",
    "    self.layer1 = nn.Sequential(\n",
    "          nn.Linear(30,398, bias=True), # input_layer = 30, hidden_layer1 = 398\n",
    "          nn.Sigmoid(),\n",
    "        nn.BatchNorm1d(398)\n",
    "    )\n",
    "  # activation function 이용\n",
    "  #   nn.ReLU()\n",
    "  #   nn.tanH()\n",
    "  #   https://pytorch.org/docs/stable/nn.html 그 외에도 여기서 확인 가능함\n",
    "  #   파라미터가 필요하지 않다는 것이 특징\n",
    "\n",
    "  # batch normazliation 1d, 파라미터 값으로 vector의 길이를 전해줌\n",
    "  # 추후에 이미지를 다루게 된다면, 그때는 batch normalization 2d를 이용하게 됨\n",
    "  # 그때는 파라미터 값으로 채널, 가로, 세로 길이를 전달해주게 됨\n",
    "\n",
    "    self.layer2 = nn.Sequential(\n",
    "          nn.Linear(398,15, bias=True), # hidden_layer1 = 398, hidden_layer2 = 15\n",
    "        nn.Sigmoid()\n",
    "    )\n",
    "    self.layer3 = nn.Sequential(\n",
    "          nn.Linear(15,10, bias=True), # hidden_layer1 = 15, hidden_layer2 = 10\n",
    "        nn.Sigmoid()\n",
    "    )\n",
    "    self.layer4 = nn.Sequential(\n",
    "        nn.Linear(10, 5, bias=True), # hidden_layer3 = 10, output_layer = 5\n",
    "        nn.Softmax()\n",
    "    )\n",
    "\n",
    "  def forward(self,x):\n",
    "    output = self.layer1(x)\n",
    "    output = self.layer2(output)\n",
    "    output = self.layer3(output)\n",
    "    output = self.layer4(output)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kqcqqkECrSGK"
   },
   "outputs": [],
   "source": [
    "def init_weights(layer):\n",
    "    if isinstance(layer, nn.Linear):\n",
    "        torch.nn.init.xavier_uniform(layer.weight)\n",
    "        layer.bias.data.fill_(0.01)\n",
    "\n",
    "        #xavier사용\n",
    "        # Layer의 weight를 어떤 분포를 가지도록 초기화시켜줌+global minimum찾기 위해서"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oMDUBFg6rUpw",
    "outputId": "d77885a4-8bfa-4af9-bfaa-36c76dba40ee"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:3: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Model(\n",
       "  (layer1): Sequential(\n",
       "    (0): Linear(in_features=30, out_features=398, bias=True)\n",
       "    (1): Sigmoid()\n",
       "    (2): BatchNorm1d(398, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): Linear(in_features=398, out_features=15, bias=True)\n",
       "    (1): Sigmoid()\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): Linear(in_features=15, out_features=10, bias=True)\n",
       "    (1): Sigmoid()\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): Linear(in_features=10, out_features=5, bias=True)\n",
       "    (1): Softmax(dim=None)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Model().to(device)\n",
    "model.apply(init_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OwZt5CetrYFb",
    "outputId": "fe01a0ba-90b9-4120-8f48-2f6710ed17c8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model(\n",
      "  (layer1): Sequential(\n",
      "    (0): Linear(in_features=30, out_features=398, bias=True)\n",
      "    (1): Sigmoid()\n",
      "    (2): BatchNorm1d(398, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): Linear(in_features=398, out_features=15, bias=True)\n",
      "    (1): Sigmoid()\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): Linear(in_features=15, out_features=10, bias=True)\n",
      "    (1): Sigmoid()\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): Linear(in_features=10, out_features=5, bias=True)\n",
      "    (1): Softmax(dim=None)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AYFp-eTErh7b"
   },
   "outputs": [],
   "source": [
    "loss_fn  = nn.CrossEntropyLoss().to(device)\n",
    "\n",
    "# 여러가지 optimizer 시도해보기\n",
    "# lr 바꿔보기\n",
    "\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr= 0.01)\n",
    "\n",
    "# https://pytorch.org/docs/stable/optim.html#module-torch.optim 페이지 참조\n",
    "# optimizer = optim.SGD(model.parameters(), lr=0.1, momentum=0.9)\n",
    "# sgd 등등"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "90QxHvlIrjS7",
    "outputId": "9325edf8-d0c9-4dd9-c674-eae81a6405c7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1.534985899925232\n",
      "10 1.2731497287750244\n",
      "20 1.1242152452468872\n",
      "30 1.0470906496047974\n",
      "40 1.0018973350524902\n",
      "50 0.9915533661842346\n",
      "60 0.9826542139053345\n",
      "70 0.9711191654205322\n",
      "80 0.9783244729042053\n",
      "90 0.9682612419128418\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/container.py:141: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  input = module(input)\n"
     ]
    }
   ],
   "source": [
    "losses = []\n",
    "for epoch in range(100):\n",
    "\n",
    "  optimizer.zero_grad()\n",
    "  hypothesis = model(x_train)\n",
    "\n",
    "  # 비용 함수\n",
    "  cost = loss_fn(hypothesis, y_train)\n",
    "  cost.backward()\n",
    "  optimizer.step()\n",
    "  losses.append(cost.item())\n",
    "\n",
    "  if epoch % 10 == 0:\n",
    "    print(epoch, cost.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    },
    "id": "81ASYrW7roFM",
    "outputId": "330a40ca-8185-4c2d-d248-257a5c44b16f"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(losses)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "h4kJzpLErqhZ",
    "outputId": "9b336fde-36f4-470d-9a2f-dc21055e0353"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/container.py:141: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  input = module(input)\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "  model = model.to('cpu')\n",
    "  y_pred = model(x_test)\n",
    "  y_pred = y_pred.detach().numpy()\n",
    "  predicted = np.argmax(y_pred, axis =1)\n",
    "  accuracy = (accuracy_score(predicted, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vyIKhs3Nr6Ay",
    "outputId": "6d73d644-c68b-4a84-defc-6f57192ef91a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model의 output은 :  [4.3696738e-03 9.9337065e-01 1.5067915e-03 2.7046577e-04 4.8240164e-04]\n",
      "argmax를 한 후의 output은 1\n",
      "accuracy는 0.9239766081871345\n"
     ]
    }
   ],
   "source": [
    "print(f'model의 output은 :  {y_pred[0]}')\n",
    "print(f'argmax를 한 후의 output은 {predicted[0]}')\n",
    "print(f'accuracy는 {accuracy}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\\\\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3RzRM7xThZV_"
   },
   "source": [
    "# < 3주차 과제 2 : CNN 맛보기>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 12912,
     "status": "ok",
     "timestamp": 1706602528420,
     "user": {
      "displayName": "정종락",
      "userId": "03058914916042464511"
     },
     "user_tz": -540
    },
    "id": "56xqgtLxhZw6"
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1780,
     "status": "ok",
     "timestamp": 1706602530193,
     "user": {
      "displayName": "정종락",
      "userId": "03058914916042464511"
     },
     "user_tz": -540
    },
    "id": "TzkF2bFNhcQ2",
    "outputId": "75b1f179-aac0-4163-d01f-0fed3c107d8b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9912422/9912422 [00:00<00:00, 89509072.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28881/28881 [00:00<00:00, 105980484.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1648877/1648877 [00:00<00:00, 32149590.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4542/4542 [00:00<00:00, 19242958.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Training settings\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "# MNIST Dataset\n",
    "train_dataset = datasets.MNIST(root='./data/',\n",
    "                               train=True,\n",
    "                               transform=transforms.ToTensor(),\n",
    "                               download=True)\n",
    "\n",
    "test_dataset = datasets.MNIST(root='./data/',\n",
    "                              train=False,\n",
    "                              transform=transforms.ToTensor())\n",
    "\n",
    "# Data Loader (Input Pipeline)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                           batch_size=batch_size,\n",
    "                                           shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
    "                                          batch_size=batch_size,\n",
    "                                          shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 448,
     "status": "ok",
     "timestamp": 1706602685877,
     "user": {
      "displayName": "정종락",
      "userId": "03058914916042464511"
     },
     "user_tz": -540
    },
    "id": "tLCSvgganBrH"
   },
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "  def __init__(self):\n",
    "    super(Net, self).__init__()\n",
    "    self.conv1 = nn.Conv2d(1, 10, kernel_size = 5)\n",
    "    self.conv2 = nn.Conv2d(10, 20, kernel_size = 5)\n",
    "    self.mp = nn.MaxPool2d(2)\n",
    "    self.fc = nn.Linear(320 , 10) ### : 알맞는 input은? 4 x 4 x 20 = 320\n",
    "\n",
    "  def forward(self, x):\n",
    "    in_size = x.size(0)\n",
    "    x = F.relu(self.mp(self.conv1(x)))\n",
    "    x = F.relu(self.mp(self.conv2(x)))\n",
    "    x = x.view(in_size, -1)\n",
    "    x = self.fc(x)\n",
    "    return F.log_softmax(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1706602687805,
     "user": {
      "displayName": "정종락",
      "userId": "03058914916042464511"
     },
     "user_tz": -540
    },
    "id": "lkYZ4pUdnUHc"
   },
   "outputs": [],
   "source": [
    "model = Net()\n",
    "optimizer = optim.SGD(model.parameters(), lr = 0.01, momentum = 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 527,
     "status": "ok",
     "timestamp": 1706602690422,
     "user": {
      "displayName": "정종락",
      "userId": "03058914916042464511"
     },
     "user_tz": -540
    },
    "id": "IzUrEM3EnXJb"
   },
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "  model.train()\n",
    "  for batch_idx, (data, target) in enumerate(train_loader):\n",
    "    data, target = Variable(data), Variable(target)\n",
    "    optimizer.zero_grad()\n",
    "    output = model(data)\n",
    "    loss = F.nll_loss(output, target)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if batch_idx % 10 == 0:\n",
    "      print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "          epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "          100. * batch_idx / len(train_loader), loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 503,
     "status": "ok",
     "timestamp": 1706602696192,
     "user": {
      "displayName": "정종락",
      "userId": "03058914916042464511"
     },
     "user_tz": -540
    },
    "id": "EFi0gYJGn2aa"
   },
   "outputs": [],
   "source": [
    "def test():\n",
    "    model.eval() # model.eval() 의 기능은?\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    for data, target in test_loader:\n",
    "        data, target = Variable(data, volatile=True), Variable(target)\n",
    "        output = model(data)\n",
    "        test_loss += F.nll_loss(output, target, size_average=False).data # nll_loss?? / cross entropy loss와의 관계 확인!\n",
    "        pred = output.data.max(1, keepdim=True)[1]\n",
    "        correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 213098,
     "status": "ok",
     "timestamp": 1706602911383,
     "user": {
      "displayName": "정종락",
      "userId": "03058914916042464511"
     },
     "user_tz": -540
    },
    "id": "zSvSZb_Bn4Nx",
    "outputId": "b77c9953-402c-4ac3-cd73-269aab6f0f10"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-4-e0319cbb53d2>:15: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return F.log_softmax(x)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.295671\n",
      "Train Epoch: 1 [640/60000 (1%)]\tLoss: 2.293139\n",
      "Train Epoch: 1 [1280/60000 (2%)]\tLoss: 2.302048\n",
      "Train Epoch: 1 [1920/60000 (3%)]\tLoss: 2.288136\n",
      "Train Epoch: 1 [2560/60000 (4%)]\tLoss: 2.275724\n",
      "Train Epoch: 1 [3200/60000 (5%)]\tLoss: 2.273154\n",
      "Train Epoch: 1 [3840/60000 (6%)]\tLoss: 2.258561\n",
      "Train Epoch: 1 [4480/60000 (7%)]\tLoss: 2.246036\n",
      "Train Epoch: 1 [5120/60000 (9%)]\tLoss: 2.219257\n",
      "Train Epoch: 1 [5760/60000 (10%)]\tLoss: 2.201262\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 2.145302\n",
      "Train Epoch: 1 [7040/60000 (12%)]\tLoss: 2.071592\n",
      "Train Epoch: 1 [7680/60000 (13%)]\tLoss: 1.994969\n",
      "Train Epoch: 1 [8320/60000 (14%)]\tLoss: 1.904625\n",
      "Train Epoch: 1 [8960/60000 (15%)]\tLoss: 1.635999\n",
      "Train Epoch: 1 [9600/60000 (16%)]\tLoss: 1.374738\n",
      "Train Epoch: 1 [10240/60000 (17%)]\tLoss: 1.147677\n",
      "Train Epoch: 1 [10880/60000 (18%)]\tLoss: 1.028661\n",
      "Train Epoch: 1 [11520/60000 (19%)]\tLoss: 0.805112\n",
      "Train Epoch: 1 [12160/60000 (20%)]\tLoss: 0.571743\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.645890\n",
      "Train Epoch: 1 [13440/60000 (22%)]\tLoss: 0.618786\n",
      "Train Epoch: 1 [14080/60000 (23%)]\tLoss: 0.583435\n",
      "Train Epoch: 1 [14720/60000 (25%)]\tLoss: 0.594046\n",
      "Train Epoch: 1 [15360/60000 (26%)]\tLoss: 0.342952\n",
      "Train Epoch: 1 [16000/60000 (27%)]\tLoss: 0.472800\n",
      "Train Epoch: 1 [16640/60000 (28%)]\tLoss: 0.505125\n",
      "Train Epoch: 1 [17280/60000 (29%)]\tLoss: 0.572673\n",
      "Train Epoch: 1 [17920/60000 (30%)]\tLoss: 0.552379\n",
      "Train Epoch: 1 [18560/60000 (31%)]\tLoss: 0.386157\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.365871\n",
      "Train Epoch: 1 [19840/60000 (33%)]\tLoss: 0.280228\n",
      "Train Epoch: 1 [20480/60000 (34%)]\tLoss: 0.633660\n",
      "Train Epoch: 1 [21120/60000 (35%)]\tLoss: 0.402107\n",
      "Train Epoch: 1 [21760/60000 (36%)]\tLoss: 0.434169\n",
      "Train Epoch: 1 [22400/60000 (37%)]\tLoss: 0.437734\n",
      "Train Epoch: 1 [23040/60000 (38%)]\tLoss: 0.482257\n",
      "Train Epoch: 1 [23680/60000 (39%)]\tLoss: 0.514713\n",
      "Train Epoch: 1 [24320/60000 (41%)]\tLoss: 0.219623\n",
      "Train Epoch: 1 [24960/60000 (42%)]\tLoss: 0.253009\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.280758\n",
      "Train Epoch: 1 [26240/60000 (44%)]\tLoss: 0.245061\n",
      "Train Epoch: 1 [26880/60000 (45%)]\tLoss: 0.344477\n",
      "Train Epoch: 1 [27520/60000 (46%)]\tLoss: 0.231800\n",
      "Train Epoch: 1 [28160/60000 (47%)]\tLoss: 0.248258\n",
      "Train Epoch: 1 [28800/60000 (48%)]\tLoss: 0.375381\n",
      "Train Epoch: 1 [29440/60000 (49%)]\tLoss: 0.371595\n",
      "Train Epoch: 1 [30080/60000 (50%)]\tLoss: 0.239153\n",
      "Train Epoch: 1 [30720/60000 (51%)]\tLoss: 0.228855\n",
      "Train Epoch: 1 [31360/60000 (52%)]\tLoss: 0.229829\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.445364\n",
      "Train Epoch: 1 [32640/60000 (54%)]\tLoss: 0.380768\n",
      "Train Epoch: 1 [33280/60000 (55%)]\tLoss: 0.194533\n",
      "Train Epoch: 1 [33920/60000 (57%)]\tLoss: 0.198695\n",
      "Train Epoch: 1 [34560/60000 (58%)]\tLoss: 0.215585\n",
      "Train Epoch: 1 [35200/60000 (59%)]\tLoss: 0.308075\n",
      "Train Epoch: 1 [35840/60000 (60%)]\tLoss: 0.299196\n",
      "Train Epoch: 1 [36480/60000 (61%)]\tLoss: 0.357346\n",
      "Train Epoch: 1 [37120/60000 (62%)]\tLoss: 0.334828\n",
      "Train Epoch: 1 [37760/60000 (63%)]\tLoss: 0.193083\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.227706\n",
      "Train Epoch: 1 [39040/60000 (65%)]\tLoss: 0.466600\n",
      "Train Epoch: 1 [39680/60000 (66%)]\tLoss: 0.127768\n",
      "Train Epoch: 1 [40320/60000 (67%)]\tLoss: 0.152430\n",
      "Train Epoch: 1 [40960/60000 (68%)]\tLoss: 0.260932\n",
      "Train Epoch: 1 [41600/60000 (69%)]\tLoss: 0.266575\n",
      "Train Epoch: 1 [42240/60000 (70%)]\tLoss: 0.249406\n",
      "Train Epoch: 1 [42880/60000 (71%)]\tLoss: 0.141596\n",
      "Train Epoch: 1 [43520/60000 (72%)]\tLoss: 0.157626\n",
      "Train Epoch: 1 [44160/60000 (74%)]\tLoss: 0.324245\n",
      "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.200041\n",
      "Train Epoch: 1 [45440/60000 (76%)]\tLoss: 0.272213\n",
      "Train Epoch: 1 [46080/60000 (77%)]\tLoss: 0.357780\n",
      "Train Epoch: 1 [46720/60000 (78%)]\tLoss: 0.184177\n",
      "Train Epoch: 1 [47360/60000 (79%)]\tLoss: 0.227344\n",
      "Train Epoch: 1 [48000/60000 (80%)]\tLoss: 0.188007\n",
      "Train Epoch: 1 [48640/60000 (81%)]\tLoss: 0.166917\n",
      "Train Epoch: 1 [49280/60000 (82%)]\tLoss: 0.365461\n",
      "Train Epoch: 1 [49920/60000 (83%)]\tLoss: 0.205529\n",
      "Train Epoch: 1 [50560/60000 (84%)]\tLoss: 0.259574\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.232600\n",
      "Train Epoch: 1 [51840/60000 (86%)]\tLoss: 0.230165\n",
      "Train Epoch: 1 [52480/60000 (87%)]\tLoss: 0.098505\n",
      "Train Epoch: 1 [53120/60000 (88%)]\tLoss: 0.167993\n",
      "Train Epoch: 1 [53760/60000 (90%)]\tLoss: 0.297836\n",
      "Train Epoch: 1 [54400/60000 (91%)]\tLoss: 0.344986\n",
      "Train Epoch: 1 [55040/60000 (92%)]\tLoss: 0.269443\n",
      "Train Epoch: 1 [55680/60000 (93%)]\tLoss: 0.138522\n",
      "Train Epoch: 1 [56320/60000 (94%)]\tLoss: 0.114453\n",
      "Train Epoch: 1 [56960/60000 (95%)]\tLoss: 0.277171\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.244093\n",
      "Train Epoch: 1 [58240/60000 (97%)]\tLoss: 0.317093\n",
      "Train Epoch: 1 [58880/60000 (98%)]\tLoss: 0.205252\n",
      "Train Epoch: 1 [59520/60000 (99%)]\tLoss: 0.222419\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-7-f52337105c2a>:6: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "  data, target = Variable(data, volatile=True), Variable(target)\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.1795, Accuracy: 9476/10000 (95%)\n",
      "\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.131244\n",
      "Train Epoch: 2 [640/60000 (1%)]\tLoss: 0.128978\n",
      "Train Epoch: 2 [1280/60000 (2%)]\tLoss: 0.162977\n",
      "Train Epoch: 2 [1920/60000 (3%)]\tLoss: 0.202203\n",
      "Train Epoch: 2 [2560/60000 (4%)]\tLoss: 0.177695\n",
      "Train Epoch: 2 [3200/60000 (5%)]\tLoss: 0.167991\n",
      "Train Epoch: 2 [3840/60000 (6%)]\tLoss: 0.214114\n",
      "Train Epoch: 2 [4480/60000 (7%)]\tLoss: 0.385808\n",
      "Train Epoch: 2 [5120/60000 (9%)]\tLoss: 0.199583\n",
      "Train Epoch: 2 [5760/60000 (10%)]\tLoss: 0.079814\n",
      "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 0.180072\n",
      "Train Epoch: 2 [7040/60000 (12%)]\tLoss: 0.267522\n",
      "Train Epoch: 2 [7680/60000 (13%)]\tLoss: 0.216321\n",
      "Train Epoch: 2 [8320/60000 (14%)]\tLoss: 0.154196\n",
      "Train Epoch: 2 [8960/60000 (15%)]\tLoss: 0.103402\n",
      "Train Epoch: 2 [9600/60000 (16%)]\tLoss: 0.277876\n",
      "Train Epoch: 2 [10240/60000 (17%)]\tLoss: 0.332586\n",
      "Train Epoch: 2 [10880/60000 (18%)]\tLoss: 0.186848\n",
      "Train Epoch: 2 [11520/60000 (19%)]\tLoss: 0.114889\n",
      "Train Epoch: 2 [12160/60000 (20%)]\tLoss: 0.209987\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 0.168469\n",
      "Train Epoch: 2 [13440/60000 (22%)]\tLoss: 0.155473\n",
      "Train Epoch: 2 [14080/60000 (23%)]\tLoss: 0.126735\n",
      "Train Epoch: 2 [14720/60000 (25%)]\tLoss: 0.131995\n",
      "Train Epoch: 2 [15360/60000 (26%)]\tLoss: 0.280377\n",
      "Train Epoch: 2 [16000/60000 (27%)]\tLoss: 0.159534\n",
      "Train Epoch: 2 [16640/60000 (28%)]\tLoss: 0.149383\n",
      "Train Epoch: 2 [17280/60000 (29%)]\tLoss: 0.256156\n",
      "Train Epoch: 2 [17920/60000 (30%)]\tLoss: 0.098165\n",
      "Train Epoch: 2 [18560/60000 (31%)]\tLoss: 0.098044\n",
      "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 0.165477\n",
      "Train Epoch: 2 [19840/60000 (33%)]\tLoss: 0.452149\n",
      "Train Epoch: 2 [20480/60000 (34%)]\tLoss: 0.095815\n",
      "Train Epoch: 2 [21120/60000 (35%)]\tLoss: 0.061242\n",
      "Train Epoch: 2 [21760/60000 (36%)]\tLoss: 0.190939\n",
      "Train Epoch: 2 [22400/60000 (37%)]\tLoss: 0.113338\n",
      "Train Epoch: 2 [23040/60000 (38%)]\tLoss: 0.160448\n",
      "Train Epoch: 2 [23680/60000 (39%)]\tLoss: 0.315400\n",
      "Train Epoch: 2 [24320/60000 (41%)]\tLoss: 0.104373\n",
      "Train Epoch: 2 [24960/60000 (42%)]\tLoss: 0.296315\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.179275\n",
      "Train Epoch: 2 [26240/60000 (44%)]\tLoss: 0.253913\n",
      "Train Epoch: 2 [26880/60000 (45%)]\tLoss: 0.170910\n",
      "Train Epoch: 2 [27520/60000 (46%)]\tLoss: 0.076110\n",
      "Train Epoch: 2 [28160/60000 (47%)]\tLoss: 0.079267\n",
      "Train Epoch: 2 [28800/60000 (48%)]\tLoss: 0.087479\n",
      "Train Epoch: 2 [29440/60000 (49%)]\tLoss: 0.058850\n",
      "Train Epoch: 2 [30080/60000 (50%)]\tLoss: 0.268535\n",
      "Train Epoch: 2 [30720/60000 (51%)]\tLoss: 0.065362\n",
      "Train Epoch: 2 [31360/60000 (52%)]\tLoss: 0.188612\n",
      "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 0.180399\n",
      "Train Epoch: 2 [32640/60000 (54%)]\tLoss: 0.304707\n",
      "Train Epoch: 2 [33280/60000 (55%)]\tLoss: 0.199394\n",
      "Train Epoch: 2 [33920/60000 (57%)]\tLoss: 0.176882\n",
      "Train Epoch: 2 [34560/60000 (58%)]\tLoss: 0.120926\n",
      "Train Epoch: 2 [35200/60000 (59%)]\tLoss: 0.164996\n",
      "Train Epoch: 2 [35840/60000 (60%)]\tLoss: 0.284006\n",
      "Train Epoch: 2 [36480/60000 (61%)]\tLoss: 0.182550\n",
      "Train Epoch: 2 [37120/60000 (62%)]\tLoss: 0.143456\n",
      "Train Epoch: 2 [37760/60000 (63%)]\tLoss: 0.226081\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 0.303594\n",
      "Train Epoch: 2 [39040/60000 (65%)]\tLoss: 0.087447\n",
      "Train Epoch: 2 [39680/60000 (66%)]\tLoss: 0.085144\n",
      "Train Epoch: 2 [40320/60000 (67%)]\tLoss: 0.128999\n",
      "Train Epoch: 2 [40960/60000 (68%)]\tLoss: 0.155497\n",
      "Train Epoch: 2 [41600/60000 (69%)]\tLoss: 0.081616\n",
      "Train Epoch: 2 [42240/60000 (70%)]\tLoss: 0.229749\n",
      "Train Epoch: 2 [42880/60000 (71%)]\tLoss: 0.252384\n",
      "Train Epoch: 2 [43520/60000 (72%)]\tLoss: 0.120570\n",
      "Train Epoch: 2 [44160/60000 (74%)]\tLoss: 0.261146\n",
      "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 0.268045\n",
      "Train Epoch: 2 [45440/60000 (76%)]\tLoss: 0.155159\n",
      "Train Epoch: 2 [46080/60000 (77%)]\tLoss: 0.079405\n",
      "Train Epoch: 2 [46720/60000 (78%)]\tLoss: 0.046414\n",
      "Train Epoch: 2 [47360/60000 (79%)]\tLoss: 0.140235\n",
      "Train Epoch: 2 [48000/60000 (80%)]\tLoss: 0.177069\n",
      "Train Epoch: 2 [48640/60000 (81%)]\tLoss: 0.059784\n",
      "Train Epoch: 2 [49280/60000 (82%)]\tLoss: 0.277946\n",
      "Train Epoch: 2 [49920/60000 (83%)]\tLoss: 0.110770\n",
      "Train Epoch: 2 [50560/60000 (84%)]\tLoss: 0.052880\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.037163\n",
      "Train Epoch: 2 [51840/60000 (86%)]\tLoss: 0.170561\n",
      "Train Epoch: 2 [52480/60000 (87%)]\tLoss: 0.107950\n",
      "Train Epoch: 2 [53120/60000 (88%)]\tLoss: 0.089759\n",
      "Train Epoch: 2 [53760/60000 (90%)]\tLoss: 0.082155\n",
      "Train Epoch: 2 [54400/60000 (91%)]\tLoss: 0.130826\n",
      "Train Epoch: 2 [55040/60000 (92%)]\tLoss: 0.130477\n",
      "Train Epoch: 2 [55680/60000 (93%)]\tLoss: 0.135752\n",
      "Train Epoch: 2 [56320/60000 (94%)]\tLoss: 0.028503\n",
      "Train Epoch: 2 [56960/60000 (95%)]\tLoss: 0.027434\n",
      "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 0.089319\n",
      "Train Epoch: 2 [58240/60000 (97%)]\tLoss: 0.120367\n",
      "Train Epoch: 2 [58880/60000 (98%)]\tLoss: 0.063254\n",
      "Train Epoch: 2 [59520/60000 (99%)]\tLoss: 0.083786\n",
      "\n",
      "Test set: Average loss: 0.1228, Accuracy: 9645/10000 (96%)\n",
      "\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.171277\n",
      "Train Epoch: 3 [640/60000 (1%)]\tLoss: 0.116764\n",
      "Train Epoch: 3 [1280/60000 (2%)]\tLoss: 0.050344\n",
      "Train Epoch: 3 [1920/60000 (3%)]\tLoss: 0.155292\n",
      "Train Epoch: 3 [2560/60000 (4%)]\tLoss: 0.191951\n",
      "Train Epoch: 3 [3200/60000 (5%)]\tLoss: 0.049252\n",
      "Train Epoch: 3 [3840/60000 (6%)]\tLoss: 0.113839\n",
      "Train Epoch: 3 [4480/60000 (7%)]\tLoss: 0.146076\n",
      "Train Epoch: 3 [5120/60000 (9%)]\tLoss: 0.022428\n",
      "Train Epoch: 3 [5760/60000 (10%)]\tLoss: 0.044369\n",
      "Train Epoch: 3 [6400/60000 (11%)]\tLoss: 0.214729\n",
      "Train Epoch: 3 [7040/60000 (12%)]\tLoss: 0.113031\n",
      "Train Epoch: 3 [7680/60000 (13%)]\tLoss: 0.119818\n",
      "Train Epoch: 3 [8320/60000 (14%)]\tLoss: 0.285808\n",
      "Train Epoch: 3 [8960/60000 (15%)]\tLoss: 0.048572\n",
      "Train Epoch: 3 [9600/60000 (16%)]\tLoss: 0.104160\n",
      "Train Epoch: 3 [10240/60000 (17%)]\tLoss: 0.048433\n",
      "Train Epoch: 3 [10880/60000 (18%)]\tLoss: 0.080406\n",
      "Train Epoch: 3 [11520/60000 (19%)]\tLoss: 0.090039\n",
      "Train Epoch: 3 [12160/60000 (20%)]\tLoss: 0.162750\n",
      "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 0.146489\n",
      "Train Epoch: 3 [13440/60000 (22%)]\tLoss: 0.040500\n",
      "Train Epoch: 3 [14080/60000 (23%)]\tLoss: 0.286264\n",
      "Train Epoch: 3 [14720/60000 (25%)]\tLoss: 0.175466\n",
      "Train Epoch: 3 [15360/60000 (26%)]\tLoss: 0.051287\n",
      "Train Epoch: 3 [16000/60000 (27%)]\tLoss: 0.046822\n",
      "Train Epoch: 3 [16640/60000 (28%)]\tLoss: 0.121609\n",
      "Train Epoch: 3 [17280/60000 (29%)]\tLoss: 0.173924\n",
      "Train Epoch: 3 [17920/60000 (30%)]\tLoss: 0.197124\n",
      "Train Epoch: 3 [18560/60000 (31%)]\tLoss: 0.184630\n",
      "Train Epoch: 3 [19200/60000 (32%)]\tLoss: 0.047958\n",
      "Train Epoch: 3 [19840/60000 (33%)]\tLoss: 0.047582\n",
      "Train Epoch: 3 [20480/60000 (34%)]\tLoss: 0.069082\n",
      "Train Epoch: 3 [21120/60000 (35%)]\tLoss: 0.081593\n",
      "Train Epoch: 3 [21760/60000 (36%)]\tLoss: 0.076997\n",
      "Train Epoch: 3 [22400/60000 (37%)]\tLoss: 0.099131\n",
      "Train Epoch: 3 [23040/60000 (38%)]\tLoss: 0.132593\n",
      "Train Epoch: 3 [23680/60000 (39%)]\tLoss: 0.092475\n",
      "Train Epoch: 3 [24320/60000 (41%)]\tLoss: 0.038153\n",
      "Train Epoch: 3 [24960/60000 (42%)]\tLoss: 0.068328\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.209243\n",
      "Train Epoch: 3 [26240/60000 (44%)]\tLoss: 0.226155\n",
      "Train Epoch: 3 [26880/60000 (45%)]\tLoss: 0.091994\n",
      "Train Epoch: 3 [27520/60000 (46%)]\tLoss: 0.151806\n",
      "Train Epoch: 3 [28160/60000 (47%)]\tLoss: 0.083212\n",
      "Train Epoch: 3 [28800/60000 (48%)]\tLoss: 0.065769\n",
      "Train Epoch: 3 [29440/60000 (49%)]\tLoss: 0.113514\n",
      "Train Epoch: 3 [30080/60000 (50%)]\tLoss: 0.033972\n",
      "Train Epoch: 3 [30720/60000 (51%)]\tLoss: 0.114078\n",
      "Train Epoch: 3 [31360/60000 (52%)]\tLoss: 0.136594\n",
      "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 0.163473\n",
      "Train Epoch: 3 [32640/60000 (54%)]\tLoss: 0.072554\n",
      "Train Epoch: 3 [33280/60000 (55%)]\tLoss: 0.093141\n",
      "Train Epoch: 3 [33920/60000 (57%)]\tLoss: 0.135874\n",
      "Train Epoch: 3 [34560/60000 (58%)]\tLoss: 0.136980\n",
      "Train Epoch: 3 [35200/60000 (59%)]\tLoss: 0.087779\n",
      "Train Epoch: 3 [35840/60000 (60%)]\tLoss: 0.096402\n",
      "Train Epoch: 3 [36480/60000 (61%)]\tLoss: 0.083224\n",
      "Train Epoch: 3 [37120/60000 (62%)]\tLoss: 0.087775\n",
      "Train Epoch: 3 [37760/60000 (63%)]\tLoss: 0.137614\n",
      "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 0.125423\n",
      "Train Epoch: 3 [39040/60000 (65%)]\tLoss: 0.129340\n",
      "Train Epoch: 3 [39680/60000 (66%)]\tLoss: 0.089058\n",
      "Train Epoch: 3 [40320/60000 (67%)]\tLoss: 0.083999\n",
      "Train Epoch: 3 [40960/60000 (68%)]\tLoss: 0.074832\n",
      "Train Epoch: 3 [41600/60000 (69%)]\tLoss: 0.081233\n",
      "Train Epoch: 3 [42240/60000 (70%)]\tLoss: 0.138824\n",
      "Train Epoch: 3 [42880/60000 (71%)]\tLoss: 0.093564\n",
      "Train Epoch: 3 [43520/60000 (72%)]\tLoss: 0.045594\n",
      "Train Epoch: 3 [44160/60000 (74%)]\tLoss: 0.043086\n",
      "Train Epoch: 3 [44800/60000 (75%)]\tLoss: 0.079786\n",
      "Train Epoch: 3 [45440/60000 (76%)]\tLoss: 0.072877\n",
      "Train Epoch: 3 [46080/60000 (77%)]\tLoss: 0.144211\n",
      "Train Epoch: 3 [46720/60000 (78%)]\tLoss: 0.120786\n",
      "Train Epoch: 3 [47360/60000 (79%)]\tLoss: 0.055585\n",
      "Train Epoch: 3 [48000/60000 (80%)]\tLoss: 0.108433\n",
      "Train Epoch: 3 [48640/60000 (81%)]\tLoss: 0.199520\n",
      "Train Epoch: 3 [49280/60000 (82%)]\tLoss: 0.034544\n",
      "Train Epoch: 3 [49920/60000 (83%)]\tLoss: 0.060929\n",
      "Train Epoch: 3 [50560/60000 (84%)]\tLoss: 0.021145\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.066065\n",
      "Train Epoch: 3 [51840/60000 (86%)]\tLoss: 0.093809\n",
      "Train Epoch: 3 [52480/60000 (87%)]\tLoss: 0.065981\n",
      "Train Epoch: 3 [53120/60000 (88%)]\tLoss: 0.049509\n",
      "Train Epoch: 3 [53760/60000 (90%)]\tLoss: 0.092648\n",
      "Train Epoch: 3 [54400/60000 (91%)]\tLoss: 0.118973\n",
      "Train Epoch: 3 [55040/60000 (92%)]\tLoss: 0.029875\n",
      "Train Epoch: 3 [55680/60000 (93%)]\tLoss: 0.148897\n",
      "Train Epoch: 3 [56320/60000 (94%)]\tLoss: 0.040930\n",
      "Train Epoch: 3 [56960/60000 (95%)]\tLoss: 0.149132\n",
      "Train Epoch: 3 [57600/60000 (96%)]\tLoss: 0.131270\n",
      "Train Epoch: 3 [58240/60000 (97%)]\tLoss: 0.072948\n",
      "Train Epoch: 3 [58880/60000 (98%)]\tLoss: 0.009752\n",
      "Train Epoch: 3 [59520/60000 (99%)]\tLoss: 0.045206\n",
      "\n",
      "Test set: Average loss: 0.0847, Accuracy: 9755/10000 (98%)\n",
      "\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.077198\n",
      "Train Epoch: 4 [640/60000 (1%)]\tLoss: 0.117837\n",
      "Train Epoch: 4 [1280/60000 (2%)]\tLoss: 0.070727\n",
      "Train Epoch: 4 [1920/60000 (3%)]\tLoss: 0.030543\n",
      "Train Epoch: 4 [2560/60000 (4%)]\tLoss: 0.099467\n",
      "Train Epoch: 4 [3200/60000 (5%)]\tLoss: 0.072891\n",
      "Train Epoch: 4 [3840/60000 (6%)]\tLoss: 0.101514\n",
      "Train Epoch: 4 [4480/60000 (7%)]\tLoss: 0.058668\n",
      "Train Epoch: 4 [5120/60000 (9%)]\tLoss: 0.067237\n",
      "Train Epoch: 4 [5760/60000 (10%)]\tLoss: 0.099664\n",
      "Train Epoch: 4 [6400/60000 (11%)]\tLoss: 0.231791\n",
      "Train Epoch: 4 [7040/60000 (12%)]\tLoss: 0.092744\n",
      "Train Epoch: 4 [7680/60000 (13%)]\tLoss: 0.051617\n",
      "Train Epoch: 4 [8320/60000 (14%)]\tLoss: 0.114205\n",
      "Train Epoch: 4 [8960/60000 (15%)]\tLoss: 0.056316\n",
      "Train Epoch: 4 [9600/60000 (16%)]\tLoss: 0.096634\n",
      "Train Epoch: 4 [10240/60000 (17%)]\tLoss: 0.090112\n",
      "Train Epoch: 4 [10880/60000 (18%)]\tLoss: 0.077575\n",
      "Train Epoch: 4 [11520/60000 (19%)]\tLoss: 0.044375\n",
      "Train Epoch: 4 [12160/60000 (20%)]\tLoss: 0.155135\n",
      "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 0.221673\n",
      "Train Epoch: 4 [13440/60000 (22%)]\tLoss: 0.114296\n",
      "Train Epoch: 4 [14080/60000 (23%)]\tLoss: 0.038682\n",
      "Train Epoch: 4 [14720/60000 (25%)]\tLoss: 0.032032\n",
      "Train Epoch: 4 [15360/60000 (26%)]\tLoss: 0.100242\n",
      "Train Epoch: 4 [16000/60000 (27%)]\tLoss: 0.062560\n",
      "Train Epoch: 4 [16640/60000 (28%)]\tLoss: 0.072872\n",
      "Train Epoch: 4 [17280/60000 (29%)]\tLoss: 0.043547\n",
      "Train Epoch: 4 [17920/60000 (30%)]\tLoss: 0.076119\n",
      "Train Epoch: 4 [18560/60000 (31%)]\tLoss: 0.070069\n",
      "Train Epoch: 4 [19200/60000 (32%)]\tLoss: 0.058139\n",
      "Train Epoch: 4 [19840/60000 (33%)]\tLoss: 0.037280\n",
      "Train Epoch: 4 [20480/60000 (34%)]\tLoss: 0.094640\n",
      "Train Epoch: 4 [21120/60000 (35%)]\tLoss: 0.156379\n",
      "Train Epoch: 4 [21760/60000 (36%)]\tLoss: 0.110843\n",
      "Train Epoch: 4 [22400/60000 (37%)]\tLoss: 0.029019\n",
      "Train Epoch: 4 [23040/60000 (38%)]\tLoss: 0.051043\n",
      "Train Epoch: 4 [23680/60000 (39%)]\tLoss: 0.025439\n",
      "Train Epoch: 4 [24320/60000 (41%)]\tLoss: 0.104135\n",
      "Train Epoch: 4 [24960/60000 (42%)]\tLoss: 0.219037\n",
      "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 0.145010\n",
      "Train Epoch: 4 [26240/60000 (44%)]\tLoss: 0.100592\n",
      "Train Epoch: 4 [26880/60000 (45%)]\tLoss: 0.025601\n",
      "Train Epoch: 4 [27520/60000 (46%)]\tLoss: 0.085069\n",
      "Train Epoch: 4 [28160/60000 (47%)]\tLoss: 0.040361\n",
      "Train Epoch: 4 [28800/60000 (48%)]\tLoss: 0.068722\n",
      "Train Epoch: 4 [29440/60000 (49%)]\tLoss: 0.128609\n",
      "Train Epoch: 4 [30080/60000 (50%)]\tLoss: 0.073473\n",
      "Train Epoch: 4 [30720/60000 (51%)]\tLoss: 0.039373\n",
      "Train Epoch: 4 [31360/60000 (52%)]\tLoss: 0.101959\n",
      "Train Epoch: 4 [32000/60000 (53%)]\tLoss: 0.078342\n",
      "Train Epoch: 4 [32640/60000 (54%)]\tLoss: 0.075641\n",
      "Train Epoch: 4 [33280/60000 (55%)]\tLoss: 0.032114\n",
      "Train Epoch: 4 [33920/60000 (57%)]\tLoss: 0.112736\n",
      "Train Epoch: 4 [34560/60000 (58%)]\tLoss: 0.193273\n",
      "Train Epoch: 4 [35200/60000 (59%)]\tLoss: 0.153838\n",
      "Train Epoch: 4 [35840/60000 (60%)]\tLoss: 0.179688\n",
      "Train Epoch: 4 [36480/60000 (61%)]\tLoss: 0.092188\n",
      "Train Epoch: 4 [37120/60000 (62%)]\tLoss: 0.079483\n",
      "Train Epoch: 4 [37760/60000 (63%)]\tLoss: 0.104278\n",
      "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 0.063964\n",
      "Train Epoch: 4 [39040/60000 (65%)]\tLoss: 0.059840\n",
      "Train Epoch: 4 [39680/60000 (66%)]\tLoss: 0.012524\n",
      "Train Epoch: 4 [40320/60000 (67%)]\tLoss: 0.047987\n",
      "Train Epoch: 4 [40960/60000 (68%)]\tLoss: 0.219317\n",
      "Train Epoch: 4 [41600/60000 (69%)]\tLoss: 0.052530\n",
      "Train Epoch: 4 [42240/60000 (70%)]\tLoss: 0.156753\n",
      "Train Epoch: 4 [42880/60000 (71%)]\tLoss: 0.030795\n",
      "Train Epoch: 4 [43520/60000 (72%)]\tLoss: 0.075205\n",
      "Train Epoch: 4 [44160/60000 (74%)]\tLoss: 0.104583\n",
      "Train Epoch: 4 [44800/60000 (75%)]\tLoss: 0.110268\n",
      "Train Epoch: 4 [45440/60000 (76%)]\tLoss: 0.039449\n",
      "Train Epoch: 4 [46080/60000 (77%)]\tLoss: 0.088243\n",
      "Train Epoch: 4 [46720/60000 (78%)]\tLoss: 0.081134\n",
      "Train Epoch: 4 [47360/60000 (79%)]\tLoss: 0.030560\n",
      "Train Epoch: 4 [48000/60000 (80%)]\tLoss: 0.086029\n",
      "Train Epoch: 4 [48640/60000 (81%)]\tLoss: 0.249632\n",
      "Train Epoch: 4 [49280/60000 (82%)]\tLoss: 0.115485\n",
      "Train Epoch: 4 [49920/60000 (83%)]\tLoss: 0.040955\n",
      "Train Epoch: 4 [50560/60000 (84%)]\tLoss: 0.060909\n",
      "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 0.067679\n",
      "Train Epoch: 4 [51840/60000 (86%)]\tLoss: 0.177301\n",
      "Train Epoch: 4 [52480/60000 (87%)]\tLoss: 0.095476\n",
      "Train Epoch: 4 [53120/60000 (88%)]\tLoss: 0.087415\n",
      "Train Epoch: 4 [53760/60000 (90%)]\tLoss: 0.125824\n",
      "Train Epoch: 4 [54400/60000 (91%)]\tLoss: 0.044214\n",
      "Train Epoch: 4 [55040/60000 (92%)]\tLoss: 0.016537\n",
      "Train Epoch: 4 [55680/60000 (93%)]\tLoss: 0.112378\n",
      "Train Epoch: 4 [56320/60000 (94%)]\tLoss: 0.103764\n",
      "Train Epoch: 4 [56960/60000 (95%)]\tLoss: 0.154449\n",
      "Train Epoch: 4 [57600/60000 (96%)]\tLoss: 0.233587\n",
      "Train Epoch: 4 [58240/60000 (97%)]\tLoss: 0.053309\n",
      "Train Epoch: 4 [58880/60000 (98%)]\tLoss: 0.110019\n",
      "Train Epoch: 4 [59520/60000 (99%)]\tLoss: 0.178104\n",
      "\n",
      "Test set: Average loss: 0.0772, Accuracy: 9775/10000 (98%)\n",
      "\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.084200\n",
      "Train Epoch: 5 [640/60000 (1%)]\tLoss: 0.041517\n",
      "Train Epoch: 5 [1280/60000 (2%)]\tLoss: 0.157677\n",
      "Train Epoch: 5 [1920/60000 (3%)]\tLoss: 0.108839\n",
      "Train Epoch: 5 [2560/60000 (4%)]\tLoss: 0.044942\n",
      "Train Epoch: 5 [3200/60000 (5%)]\tLoss: 0.054564\n",
      "Train Epoch: 5 [3840/60000 (6%)]\tLoss: 0.034461\n",
      "Train Epoch: 5 [4480/60000 (7%)]\tLoss: 0.111333\n",
      "Train Epoch: 5 [5120/60000 (9%)]\tLoss: 0.274451\n",
      "Train Epoch: 5 [5760/60000 (10%)]\tLoss: 0.059153\n",
      "Train Epoch: 5 [6400/60000 (11%)]\tLoss: 0.056768\n",
      "Train Epoch: 5 [7040/60000 (12%)]\tLoss: 0.117861\n",
      "Train Epoch: 5 [7680/60000 (13%)]\tLoss: 0.013315\n",
      "Train Epoch: 5 [8320/60000 (14%)]\tLoss: 0.046290\n",
      "Train Epoch: 5 [8960/60000 (15%)]\tLoss: 0.036183\n",
      "Train Epoch: 5 [9600/60000 (16%)]\tLoss: 0.146732\n",
      "Train Epoch: 5 [10240/60000 (17%)]\tLoss: 0.226347\n",
      "Train Epoch: 5 [10880/60000 (18%)]\tLoss: 0.058769\n",
      "Train Epoch: 5 [11520/60000 (19%)]\tLoss: 0.079751\n",
      "Train Epoch: 5 [12160/60000 (20%)]\tLoss: 0.050844\n",
      "Train Epoch: 5 [12800/60000 (21%)]\tLoss: 0.071247\n",
      "Train Epoch: 5 [13440/60000 (22%)]\tLoss: 0.060564\n",
      "Train Epoch: 5 [14080/60000 (23%)]\tLoss: 0.131285\n",
      "Train Epoch: 5 [14720/60000 (25%)]\tLoss: 0.009277\n",
      "Train Epoch: 5 [15360/60000 (26%)]\tLoss: 0.060579\n",
      "Train Epoch: 5 [16000/60000 (27%)]\tLoss: 0.068555\n",
      "Train Epoch: 5 [16640/60000 (28%)]\tLoss: 0.114525\n",
      "Train Epoch: 5 [17280/60000 (29%)]\tLoss: 0.057586\n",
      "Train Epoch: 5 [17920/60000 (30%)]\tLoss: 0.074106\n",
      "Train Epoch: 5 [18560/60000 (31%)]\tLoss: 0.009631\n",
      "Train Epoch: 5 [19200/60000 (32%)]\tLoss: 0.064614\n",
      "Train Epoch: 5 [19840/60000 (33%)]\tLoss: 0.036742\n",
      "Train Epoch: 5 [20480/60000 (34%)]\tLoss: 0.034718\n",
      "Train Epoch: 5 [21120/60000 (35%)]\tLoss: 0.205083\n",
      "Train Epoch: 5 [21760/60000 (36%)]\tLoss: 0.049773\n",
      "Train Epoch: 5 [22400/60000 (37%)]\tLoss: 0.053627\n",
      "Train Epoch: 5 [23040/60000 (38%)]\tLoss: 0.093766\n",
      "Train Epoch: 5 [23680/60000 (39%)]\tLoss: 0.167791\n",
      "Train Epoch: 5 [24320/60000 (41%)]\tLoss: 0.148156\n",
      "Train Epoch: 5 [24960/60000 (42%)]\tLoss: 0.020389\n",
      "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 0.055195\n",
      "Train Epoch: 5 [26240/60000 (44%)]\tLoss: 0.107844\n",
      "Train Epoch: 5 [26880/60000 (45%)]\tLoss: 0.083907\n",
      "Train Epoch: 5 [27520/60000 (46%)]\tLoss: 0.030437\n",
      "Train Epoch: 5 [28160/60000 (47%)]\tLoss: 0.017552\n",
      "Train Epoch: 5 [28800/60000 (48%)]\tLoss: 0.116314\n",
      "Train Epoch: 5 [29440/60000 (49%)]\tLoss: 0.117434\n",
      "Train Epoch: 5 [30080/60000 (50%)]\tLoss: 0.098879\n",
      "Train Epoch: 5 [30720/60000 (51%)]\tLoss: 0.146560\n",
      "Train Epoch: 5 [31360/60000 (52%)]\tLoss: 0.128844\n",
      "Train Epoch: 5 [32000/60000 (53%)]\tLoss: 0.020225\n",
      "Train Epoch: 5 [32640/60000 (54%)]\tLoss: 0.147552\n",
      "Train Epoch: 5 [33280/60000 (55%)]\tLoss: 0.089039\n",
      "Train Epoch: 5 [33920/60000 (57%)]\tLoss: 0.089484\n",
      "Train Epoch: 5 [34560/60000 (58%)]\tLoss: 0.112561\n",
      "Train Epoch: 5 [35200/60000 (59%)]\tLoss: 0.108480\n",
      "Train Epoch: 5 [35840/60000 (60%)]\tLoss: 0.075782\n",
      "Train Epoch: 5 [36480/60000 (61%)]\tLoss: 0.059498\n",
      "Train Epoch: 5 [37120/60000 (62%)]\tLoss: 0.124239\n",
      "Train Epoch: 5 [37760/60000 (63%)]\tLoss: 0.152802\n",
      "Train Epoch: 5 [38400/60000 (64%)]\tLoss: 0.025539\n",
      "Train Epoch: 5 [39040/60000 (65%)]\tLoss: 0.136296\n",
      "Train Epoch: 5 [39680/60000 (66%)]\tLoss: 0.047565\n",
      "Train Epoch: 5 [40320/60000 (67%)]\tLoss: 0.088629\n",
      "Train Epoch: 5 [40960/60000 (68%)]\tLoss: 0.088431\n",
      "Train Epoch: 5 [41600/60000 (69%)]\tLoss: 0.083705\n",
      "Train Epoch: 5 [42240/60000 (70%)]\tLoss: 0.070457\n",
      "Train Epoch: 5 [42880/60000 (71%)]\tLoss: 0.089871\n",
      "Train Epoch: 5 [43520/60000 (72%)]\tLoss: 0.015953\n",
      "Train Epoch: 5 [44160/60000 (74%)]\tLoss: 0.068420\n",
      "Train Epoch: 5 [44800/60000 (75%)]\tLoss: 0.026316\n",
      "Train Epoch: 5 [45440/60000 (76%)]\tLoss: 0.046558\n",
      "Train Epoch: 5 [46080/60000 (77%)]\tLoss: 0.101882\n",
      "Train Epoch: 5 [46720/60000 (78%)]\tLoss: 0.031469\n",
      "Train Epoch: 5 [47360/60000 (79%)]\tLoss: 0.050869\n",
      "Train Epoch: 5 [48000/60000 (80%)]\tLoss: 0.099609\n",
      "Train Epoch: 5 [48640/60000 (81%)]\tLoss: 0.083196\n",
      "Train Epoch: 5 [49280/60000 (82%)]\tLoss: 0.055108\n",
      "Train Epoch: 5 [49920/60000 (83%)]\tLoss: 0.091435\n",
      "Train Epoch: 5 [50560/60000 (84%)]\tLoss: 0.119692\n",
      "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 0.056496\n",
      "Train Epoch: 5 [51840/60000 (86%)]\tLoss: 0.057581\n",
      "Train Epoch: 5 [52480/60000 (87%)]\tLoss: 0.016342\n",
      "Train Epoch: 5 [53120/60000 (88%)]\tLoss: 0.074320\n",
      "Train Epoch: 5 [53760/60000 (90%)]\tLoss: 0.093061\n",
      "Train Epoch: 5 [54400/60000 (91%)]\tLoss: 0.142450\n",
      "Train Epoch: 5 [55040/60000 (92%)]\tLoss: 0.028058\n",
      "Train Epoch: 5 [55680/60000 (93%)]\tLoss: 0.077926\n",
      "Train Epoch: 5 [56320/60000 (94%)]\tLoss: 0.148257\n",
      "Train Epoch: 5 [56960/60000 (95%)]\tLoss: 0.134401\n",
      "Train Epoch: 5 [57600/60000 (96%)]\tLoss: 0.046989\n",
      "Train Epoch: 5 [58240/60000 (97%)]\tLoss: 0.060197\n",
      "Train Epoch: 5 [58880/60000 (98%)]\tLoss: 0.067203\n",
      "Train Epoch: 5 [59520/60000 (99%)]\tLoss: 0.015123\n",
      "\n",
      "Test set: Average loss: 0.0694, Accuracy: 9783/10000 (98%)\n",
      "\n",
      "Train Epoch: 6 [0/60000 (0%)]\tLoss: 0.103323\n",
      "Train Epoch: 6 [640/60000 (1%)]\tLoss: 0.055320\n",
      "Train Epoch: 6 [1280/60000 (2%)]\tLoss: 0.063396\n",
      "Train Epoch: 6 [1920/60000 (3%)]\tLoss: 0.060888\n",
      "Train Epoch: 6 [2560/60000 (4%)]\tLoss: 0.022756\n",
      "Train Epoch: 6 [3200/60000 (5%)]\tLoss: 0.039243\n",
      "Train Epoch: 6 [3840/60000 (6%)]\tLoss: 0.043597\n",
      "Train Epoch: 6 [4480/60000 (7%)]\tLoss: 0.048703\n",
      "Train Epoch: 6 [5120/60000 (9%)]\tLoss: 0.054954\n",
      "Train Epoch: 6 [5760/60000 (10%)]\tLoss: 0.076187\n",
      "Train Epoch: 6 [6400/60000 (11%)]\tLoss: 0.075089\n",
      "Train Epoch: 6 [7040/60000 (12%)]\tLoss: 0.122557\n",
      "Train Epoch: 6 [7680/60000 (13%)]\tLoss: 0.027526\n",
      "Train Epoch: 6 [8320/60000 (14%)]\tLoss: 0.036540\n",
      "Train Epoch: 6 [8960/60000 (15%)]\tLoss: 0.033255\n",
      "Train Epoch: 6 [9600/60000 (16%)]\tLoss: 0.130538\n",
      "Train Epoch: 6 [10240/60000 (17%)]\tLoss: 0.084449\n",
      "Train Epoch: 6 [10880/60000 (18%)]\tLoss: 0.245954\n",
      "Train Epoch: 6 [11520/60000 (19%)]\tLoss: 0.070633\n",
      "Train Epoch: 6 [12160/60000 (20%)]\tLoss: 0.025452\n",
      "Train Epoch: 6 [12800/60000 (21%)]\tLoss: 0.017669\n",
      "Train Epoch: 6 [13440/60000 (22%)]\tLoss: 0.084648\n",
      "Train Epoch: 6 [14080/60000 (23%)]\tLoss: 0.144759\n",
      "Train Epoch: 6 [14720/60000 (25%)]\tLoss: 0.141440\n",
      "Train Epoch: 6 [15360/60000 (26%)]\tLoss: 0.026733\n",
      "Train Epoch: 6 [16000/60000 (27%)]\tLoss: 0.115651\n",
      "Train Epoch: 6 [16640/60000 (28%)]\tLoss: 0.117902\n",
      "Train Epoch: 6 [17280/60000 (29%)]\tLoss: 0.119684\n",
      "Train Epoch: 6 [17920/60000 (30%)]\tLoss: 0.025325\n",
      "Train Epoch: 6 [18560/60000 (31%)]\tLoss: 0.018868\n",
      "Train Epoch: 6 [19200/60000 (32%)]\tLoss: 0.019721\n",
      "Train Epoch: 6 [19840/60000 (33%)]\tLoss: 0.053528\n",
      "Train Epoch: 6 [20480/60000 (34%)]\tLoss: 0.006727\n",
      "Train Epoch: 6 [21120/60000 (35%)]\tLoss: 0.090719\n",
      "Train Epoch: 6 [21760/60000 (36%)]\tLoss: 0.077950\n",
      "Train Epoch: 6 [22400/60000 (37%)]\tLoss: 0.007431\n",
      "Train Epoch: 6 [23040/60000 (38%)]\tLoss: 0.046786\n",
      "Train Epoch: 6 [23680/60000 (39%)]\tLoss: 0.167239\n",
      "Train Epoch: 6 [24320/60000 (41%)]\tLoss: 0.064247\n",
      "Train Epoch: 6 [24960/60000 (42%)]\tLoss: 0.035819\n",
      "Train Epoch: 6 [25600/60000 (43%)]\tLoss: 0.111852\n",
      "Train Epoch: 6 [26240/60000 (44%)]\tLoss: 0.022335\n",
      "Train Epoch: 6 [26880/60000 (45%)]\tLoss: 0.122457\n",
      "Train Epoch: 6 [27520/60000 (46%)]\tLoss: 0.021880\n",
      "Train Epoch: 6 [28160/60000 (47%)]\tLoss: 0.100279\n",
      "Train Epoch: 6 [28800/60000 (48%)]\tLoss: 0.082852\n",
      "Train Epoch: 6 [29440/60000 (49%)]\tLoss: 0.029848\n",
      "Train Epoch: 6 [30080/60000 (50%)]\tLoss: 0.041353\n",
      "Train Epoch: 6 [30720/60000 (51%)]\tLoss: 0.023062\n",
      "Train Epoch: 6 [31360/60000 (52%)]\tLoss: 0.177617\n",
      "Train Epoch: 6 [32000/60000 (53%)]\tLoss: 0.087337\n",
      "Train Epoch: 6 [32640/60000 (54%)]\tLoss: 0.031157\n",
      "Train Epoch: 6 [33280/60000 (55%)]\tLoss: 0.101253\n",
      "Train Epoch: 6 [33920/60000 (57%)]\tLoss: 0.024920\n",
      "Train Epoch: 6 [34560/60000 (58%)]\tLoss: 0.011886\n",
      "Train Epoch: 6 [35200/60000 (59%)]\tLoss: 0.041935\n",
      "Train Epoch: 6 [35840/60000 (60%)]\tLoss: 0.066714\n",
      "Train Epoch: 6 [36480/60000 (61%)]\tLoss: 0.051356\n",
      "Train Epoch: 6 [37120/60000 (62%)]\tLoss: 0.012925\n",
      "Train Epoch: 6 [37760/60000 (63%)]\tLoss: 0.071968\n",
      "Train Epoch: 6 [38400/60000 (64%)]\tLoss: 0.116036\n",
      "Train Epoch: 6 [39040/60000 (65%)]\tLoss: 0.020091\n",
      "Train Epoch: 6 [39680/60000 (66%)]\tLoss: 0.098419\n",
      "Train Epoch: 6 [40320/60000 (67%)]\tLoss: 0.015593\n",
      "Train Epoch: 6 [40960/60000 (68%)]\tLoss: 0.039037\n",
      "Train Epoch: 6 [41600/60000 (69%)]\tLoss: 0.027980\n",
      "Train Epoch: 6 [42240/60000 (70%)]\tLoss: 0.165550\n",
      "Train Epoch: 6 [42880/60000 (71%)]\tLoss: 0.242731\n",
      "Train Epoch: 6 [43520/60000 (72%)]\tLoss: 0.045153\n",
      "Train Epoch: 6 [44160/60000 (74%)]\tLoss: 0.064276\n",
      "Train Epoch: 6 [44800/60000 (75%)]\tLoss: 0.039800\n",
      "Train Epoch: 6 [45440/60000 (76%)]\tLoss: 0.124190\n",
      "Train Epoch: 6 [46080/60000 (77%)]\tLoss: 0.022995\n",
      "Train Epoch: 6 [46720/60000 (78%)]\tLoss: 0.018020\n",
      "Train Epoch: 6 [47360/60000 (79%)]\tLoss: 0.128622\n",
      "Train Epoch: 6 [48000/60000 (80%)]\tLoss: 0.065944\n",
      "Train Epoch: 6 [48640/60000 (81%)]\tLoss: 0.033719\n",
      "Train Epoch: 6 [49280/60000 (82%)]\tLoss: 0.140948\n",
      "Train Epoch: 6 [49920/60000 (83%)]\tLoss: 0.057879\n",
      "Train Epoch: 6 [50560/60000 (84%)]\tLoss: 0.009912\n",
      "Train Epoch: 6 [51200/60000 (85%)]\tLoss: 0.063132\n",
      "Train Epoch: 6 [51840/60000 (86%)]\tLoss: 0.071320\n",
      "Train Epoch: 6 [52480/60000 (87%)]\tLoss: 0.043893\n",
      "Train Epoch: 6 [53120/60000 (88%)]\tLoss: 0.034721\n",
      "Train Epoch: 6 [53760/60000 (90%)]\tLoss: 0.069336\n",
      "Train Epoch: 6 [54400/60000 (91%)]\tLoss: 0.170848\n",
      "Train Epoch: 6 [55040/60000 (92%)]\tLoss: 0.099263\n",
      "Train Epoch: 6 [55680/60000 (93%)]\tLoss: 0.182858\n",
      "Train Epoch: 6 [56320/60000 (94%)]\tLoss: 0.046817\n",
      "Train Epoch: 6 [56960/60000 (95%)]\tLoss: 0.074400\n",
      "Train Epoch: 6 [57600/60000 (96%)]\tLoss: 0.030950\n",
      "Train Epoch: 6 [58240/60000 (97%)]\tLoss: 0.088603\n",
      "Train Epoch: 6 [58880/60000 (98%)]\tLoss: 0.071815\n",
      "Train Epoch: 6 [59520/60000 (99%)]\tLoss: 0.018593\n",
      "\n",
      "Test set: Average loss: 0.0627, Accuracy: 9814/10000 (98%)\n",
      "\n",
      "Train Epoch: 7 [0/60000 (0%)]\tLoss: 0.081121\n",
      "Train Epoch: 7 [640/60000 (1%)]\tLoss: 0.069742\n",
      "Train Epoch: 7 [1280/60000 (2%)]\tLoss: 0.157545\n",
      "Train Epoch: 7 [1920/60000 (3%)]\tLoss: 0.023560\n",
      "Train Epoch: 7 [2560/60000 (4%)]\tLoss: 0.131832\n",
      "Train Epoch: 7 [3200/60000 (5%)]\tLoss: 0.049631\n",
      "Train Epoch: 7 [3840/60000 (6%)]\tLoss: 0.023209\n",
      "Train Epoch: 7 [4480/60000 (7%)]\tLoss: 0.175292\n",
      "Train Epoch: 7 [5120/60000 (9%)]\tLoss: 0.065454\n",
      "Train Epoch: 7 [5760/60000 (10%)]\tLoss: 0.017191\n",
      "Train Epoch: 7 [6400/60000 (11%)]\tLoss: 0.013219\n",
      "Train Epoch: 7 [7040/60000 (12%)]\tLoss: 0.004293\n",
      "Train Epoch: 7 [7680/60000 (13%)]\tLoss: 0.038547\n",
      "Train Epoch: 7 [8320/60000 (14%)]\tLoss: 0.076725\n",
      "Train Epoch: 7 [8960/60000 (15%)]\tLoss: 0.039402\n",
      "Train Epoch: 7 [9600/60000 (16%)]\tLoss: 0.075954\n",
      "Train Epoch: 7 [10240/60000 (17%)]\tLoss: 0.158441\n",
      "Train Epoch: 7 [10880/60000 (18%)]\tLoss: 0.105049\n",
      "Train Epoch: 7 [11520/60000 (19%)]\tLoss: 0.081062\n",
      "Train Epoch: 7 [12160/60000 (20%)]\tLoss: 0.064491\n",
      "Train Epoch: 7 [12800/60000 (21%)]\tLoss: 0.013527\n",
      "Train Epoch: 7 [13440/60000 (22%)]\tLoss: 0.006821\n",
      "Train Epoch: 7 [14080/60000 (23%)]\tLoss: 0.028198\n",
      "Train Epoch: 7 [14720/60000 (25%)]\tLoss: 0.058731\n",
      "Train Epoch: 7 [15360/60000 (26%)]\tLoss: 0.046744\n",
      "Train Epoch: 7 [16000/60000 (27%)]\tLoss: 0.024879\n",
      "Train Epoch: 7 [16640/60000 (28%)]\tLoss: 0.119753\n",
      "Train Epoch: 7 [17280/60000 (29%)]\tLoss: 0.053359\n",
      "Train Epoch: 7 [17920/60000 (30%)]\tLoss: 0.073322\n",
      "Train Epoch: 7 [18560/60000 (31%)]\tLoss: 0.050524\n",
      "Train Epoch: 7 [19200/60000 (32%)]\tLoss: 0.042768\n",
      "Train Epoch: 7 [19840/60000 (33%)]\tLoss: 0.109860\n",
      "Train Epoch: 7 [20480/60000 (34%)]\tLoss: 0.012207\n",
      "Train Epoch: 7 [21120/60000 (35%)]\tLoss: 0.018155\n",
      "Train Epoch: 7 [21760/60000 (36%)]\tLoss: 0.064211\n",
      "Train Epoch: 7 [22400/60000 (37%)]\tLoss: 0.030032\n",
      "Train Epoch: 7 [23040/60000 (38%)]\tLoss: 0.027992\n",
      "Train Epoch: 7 [23680/60000 (39%)]\tLoss: 0.157476\n",
      "Train Epoch: 7 [24320/60000 (41%)]\tLoss: 0.010292\n",
      "Train Epoch: 7 [24960/60000 (42%)]\tLoss: 0.022633\n",
      "Train Epoch: 7 [25600/60000 (43%)]\tLoss: 0.105751\n",
      "Train Epoch: 7 [26240/60000 (44%)]\tLoss: 0.082023\n",
      "Train Epoch: 7 [26880/60000 (45%)]\tLoss: 0.093373\n",
      "Train Epoch: 7 [27520/60000 (46%)]\tLoss: 0.085202\n",
      "Train Epoch: 7 [28160/60000 (47%)]\tLoss: 0.009227\n",
      "Train Epoch: 7 [28800/60000 (48%)]\tLoss: 0.030071\n",
      "Train Epoch: 7 [29440/60000 (49%)]\tLoss: 0.042019\n",
      "Train Epoch: 7 [30080/60000 (50%)]\tLoss: 0.074492\n",
      "Train Epoch: 7 [30720/60000 (51%)]\tLoss: 0.077538\n",
      "Train Epoch: 7 [31360/60000 (52%)]\tLoss: 0.074123\n",
      "Train Epoch: 7 [32000/60000 (53%)]\tLoss: 0.012012\n",
      "Train Epoch: 7 [32640/60000 (54%)]\tLoss: 0.085140\n",
      "Train Epoch: 7 [33280/60000 (55%)]\tLoss: 0.043899\n",
      "Train Epoch: 7 [33920/60000 (57%)]\tLoss: 0.009339\n",
      "Train Epoch: 7 [34560/60000 (58%)]\tLoss: 0.100354\n",
      "Train Epoch: 7 [35200/60000 (59%)]\tLoss: 0.041933\n",
      "Train Epoch: 7 [35840/60000 (60%)]\tLoss: 0.032173\n",
      "Train Epoch: 7 [36480/60000 (61%)]\tLoss: 0.023167\n",
      "Train Epoch: 7 [37120/60000 (62%)]\tLoss: 0.017863\n",
      "Train Epoch: 7 [37760/60000 (63%)]\tLoss: 0.098564\n",
      "Train Epoch: 7 [38400/60000 (64%)]\tLoss: 0.172651\n",
      "Train Epoch: 7 [39040/60000 (65%)]\tLoss: 0.115571\n",
      "Train Epoch: 7 [39680/60000 (66%)]\tLoss: 0.063933\n",
      "Train Epoch: 7 [40320/60000 (67%)]\tLoss: 0.030985\n",
      "Train Epoch: 7 [40960/60000 (68%)]\tLoss: 0.082702\n",
      "Train Epoch: 7 [41600/60000 (69%)]\tLoss: 0.028940\n",
      "Train Epoch: 7 [42240/60000 (70%)]\tLoss: 0.168402\n",
      "Train Epoch: 7 [42880/60000 (71%)]\tLoss: 0.102691\n",
      "Train Epoch: 7 [43520/60000 (72%)]\tLoss: 0.020637\n",
      "Train Epoch: 7 [44160/60000 (74%)]\tLoss: 0.048134\n",
      "Train Epoch: 7 [44800/60000 (75%)]\tLoss: 0.064456\n",
      "Train Epoch: 7 [45440/60000 (76%)]\tLoss: 0.027465\n",
      "Train Epoch: 7 [46080/60000 (77%)]\tLoss: 0.018020\n",
      "Train Epoch: 7 [46720/60000 (78%)]\tLoss: 0.031812\n",
      "Train Epoch: 7 [47360/60000 (79%)]\tLoss: 0.167264\n",
      "Train Epoch: 7 [48000/60000 (80%)]\tLoss: 0.048595\n",
      "Train Epoch: 7 [48640/60000 (81%)]\tLoss: 0.026910\n",
      "Train Epoch: 7 [49280/60000 (82%)]\tLoss: 0.020791\n",
      "Train Epoch: 7 [49920/60000 (83%)]\tLoss: 0.038441\n",
      "Train Epoch: 7 [50560/60000 (84%)]\tLoss: 0.162394\n",
      "Train Epoch: 7 [51200/60000 (85%)]\tLoss: 0.112041\n",
      "Train Epoch: 7 [51840/60000 (86%)]\tLoss: 0.149575\n",
      "Train Epoch: 7 [52480/60000 (87%)]\tLoss: 0.049851\n",
      "Train Epoch: 7 [53120/60000 (88%)]\tLoss: 0.064438\n",
      "Train Epoch: 7 [53760/60000 (90%)]\tLoss: 0.029189\n",
      "Train Epoch: 7 [54400/60000 (91%)]\tLoss: 0.066425\n",
      "Train Epoch: 7 [55040/60000 (92%)]\tLoss: 0.034295\n",
      "Train Epoch: 7 [55680/60000 (93%)]\tLoss: 0.015423\n",
      "Train Epoch: 7 [56320/60000 (94%)]\tLoss: 0.063114\n",
      "Train Epoch: 7 [56960/60000 (95%)]\tLoss: 0.031271\n",
      "Train Epoch: 7 [57600/60000 (96%)]\tLoss: 0.104141\n",
      "Train Epoch: 7 [58240/60000 (97%)]\tLoss: 0.125270\n",
      "Train Epoch: 7 [58880/60000 (98%)]\tLoss: 0.091343\n",
      "Train Epoch: 7 [59520/60000 (99%)]\tLoss: 0.191901\n",
      "\n",
      "Test set: Average loss: 0.0616, Accuracy: 9807/10000 (98%)\n",
      "\n",
      "Train Epoch: 8 [0/60000 (0%)]\tLoss: 0.068169\n",
      "Train Epoch: 8 [640/60000 (1%)]\tLoss: 0.009262\n",
      "Train Epoch: 8 [1280/60000 (2%)]\tLoss: 0.007523\n",
      "Train Epoch: 8 [1920/60000 (3%)]\tLoss: 0.014840\n",
      "Train Epoch: 8 [2560/60000 (4%)]\tLoss: 0.024039\n",
      "Train Epoch: 8 [3200/60000 (5%)]\tLoss: 0.016985\n",
      "Train Epoch: 8 [3840/60000 (6%)]\tLoss: 0.074413\n",
      "Train Epoch: 8 [4480/60000 (7%)]\tLoss: 0.038366\n",
      "Train Epoch: 8 [5120/60000 (9%)]\tLoss: 0.050986\n",
      "Train Epoch: 8 [5760/60000 (10%)]\tLoss: 0.041480\n",
      "Train Epoch: 8 [6400/60000 (11%)]\tLoss: 0.013489\n",
      "Train Epoch: 8 [7040/60000 (12%)]\tLoss: 0.064367\n",
      "Train Epoch: 8 [7680/60000 (13%)]\tLoss: 0.030931\n",
      "Train Epoch: 8 [8320/60000 (14%)]\tLoss: 0.020674\n",
      "Train Epoch: 8 [8960/60000 (15%)]\tLoss: 0.167358\n",
      "Train Epoch: 8 [9600/60000 (16%)]\tLoss: 0.022376\n",
      "Train Epoch: 8 [10240/60000 (17%)]\tLoss: 0.173544\n",
      "Train Epoch: 8 [10880/60000 (18%)]\tLoss: 0.068601\n",
      "Train Epoch: 8 [11520/60000 (19%)]\tLoss: 0.045853\n",
      "Train Epoch: 8 [12160/60000 (20%)]\tLoss: 0.050607\n",
      "Train Epoch: 8 [12800/60000 (21%)]\tLoss: 0.137499\n",
      "Train Epoch: 8 [13440/60000 (22%)]\tLoss: 0.069573\n",
      "Train Epoch: 8 [14080/60000 (23%)]\tLoss: 0.045923\n",
      "Train Epoch: 8 [14720/60000 (25%)]\tLoss: 0.148768\n",
      "Train Epoch: 8 [15360/60000 (26%)]\tLoss: 0.044214\n",
      "Train Epoch: 8 [16000/60000 (27%)]\tLoss: 0.082641\n",
      "Train Epoch: 8 [16640/60000 (28%)]\tLoss: 0.056403\n",
      "Train Epoch: 8 [17280/60000 (29%)]\tLoss: 0.031720\n",
      "Train Epoch: 8 [17920/60000 (30%)]\tLoss: 0.037323\n",
      "Train Epoch: 8 [18560/60000 (31%)]\tLoss: 0.008072\n",
      "Train Epoch: 8 [19200/60000 (32%)]\tLoss: 0.140211\n",
      "Train Epoch: 8 [19840/60000 (33%)]\tLoss: 0.023878\n",
      "Train Epoch: 8 [20480/60000 (34%)]\tLoss: 0.069979\n",
      "Train Epoch: 8 [21120/60000 (35%)]\tLoss: 0.060189\n",
      "Train Epoch: 8 [21760/60000 (36%)]\tLoss: 0.189496\n",
      "Train Epoch: 8 [22400/60000 (37%)]\tLoss: 0.008511\n",
      "Train Epoch: 8 [23040/60000 (38%)]\tLoss: 0.024819\n",
      "Train Epoch: 8 [23680/60000 (39%)]\tLoss: 0.016475\n",
      "Train Epoch: 8 [24320/60000 (41%)]\tLoss: 0.025983\n",
      "Train Epoch: 8 [24960/60000 (42%)]\tLoss: 0.089133\n",
      "Train Epoch: 8 [25600/60000 (43%)]\tLoss: 0.028609\n",
      "Train Epoch: 8 [26240/60000 (44%)]\tLoss: 0.061597\n",
      "Train Epoch: 8 [26880/60000 (45%)]\tLoss: 0.014644\n",
      "Train Epoch: 8 [27520/60000 (46%)]\tLoss: 0.096750\n",
      "Train Epoch: 8 [28160/60000 (47%)]\tLoss: 0.155213\n",
      "Train Epoch: 8 [28800/60000 (48%)]\tLoss: 0.181399\n",
      "Train Epoch: 8 [29440/60000 (49%)]\tLoss: 0.039052\n",
      "Train Epoch: 8 [30080/60000 (50%)]\tLoss: 0.062198\n",
      "Train Epoch: 8 [30720/60000 (51%)]\tLoss: 0.091045\n",
      "Train Epoch: 8 [31360/60000 (52%)]\tLoss: 0.047557\n",
      "Train Epoch: 8 [32000/60000 (53%)]\tLoss: 0.049129\n",
      "Train Epoch: 8 [32640/60000 (54%)]\tLoss: 0.070232\n",
      "Train Epoch: 8 [33280/60000 (55%)]\tLoss: 0.089513\n",
      "Train Epoch: 8 [33920/60000 (57%)]\tLoss: 0.018231\n",
      "Train Epoch: 8 [34560/60000 (58%)]\tLoss: 0.087402\n",
      "Train Epoch: 8 [35200/60000 (59%)]\tLoss: 0.082032\n",
      "Train Epoch: 8 [35840/60000 (60%)]\tLoss: 0.058501\n",
      "Train Epoch: 8 [36480/60000 (61%)]\tLoss: 0.064971\n",
      "Train Epoch: 8 [37120/60000 (62%)]\tLoss: 0.026759\n",
      "Train Epoch: 8 [37760/60000 (63%)]\tLoss: 0.054297\n",
      "Train Epoch: 8 [38400/60000 (64%)]\tLoss: 0.020856\n",
      "Train Epoch: 8 [39040/60000 (65%)]\tLoss: 0.018489\n",
      "Train Epoch: 8 [39680/60000 (66%)]\tLoss: 0.090216\n",
      "Train Epoch: 8 [40320/60000 (67%)]\tLoss: 0.066505\n",
      "Train Epoch: 8 [40960/60000 (68%)]\tLoss: 0.110746\n",
      "Train Epoch: 8 [41600/60000 (69%)]\tLoss: 0.148953\n",
      "Train Epoch: 8 [42240/60000 (70%)]\tLoss: 0.065771\n",
      "Train Epoch: 8 [42880/60000 (71%)]\tLoss: 0.045700\n",
      "Train Epoch: 8 [43520/60000 (72%)]\tLoss: 0.125877\n",
      "Train Epoch: 8 [44160/60000 (74%)]\tLoss: 0.059264\n",
      "Train Epoch: 8 [44800/60000 (75%)]\tLoss: 0.029547\n",
      "Train Epoch: 8 [45440/60000 (76%)]\tLoss: 0.014660\n",
      "Train Epoch: 8 [46080/60000 (77%)]\tLoss: 0.097227\n",
      "Train Epoch: 8 [46720/60000 (78%)]\tLoss: 0.007654\n",
      "Train Epoch: 8 [47360/60000 (79%)]\tLoss: 0.012058\n",
      "Train Epoch: 8 [48000/60000 (80%)]\tLoss: 0.037459\n",
      "Train Epoch: 8 [48640/60000 (81%)]\tLoss: 0.058839\n",
      "Train Epoch: 8 [49280/60000 (82%)]\tLoss: 0.037863\n",
      "Train Epoch: 8 [49920/60000 (83%)]\tLoss: 0.036984\n",
      "Train Epoch: 8 [50560/60000 (84%)]\tLoss: 0.029259\n",
      "Train Epoch: 8 [51200/60000 (85%)]\tLoss: 0.019051\n",
      "Train Epoch: 8 [51840/60000 (86%)]\tLoss: 0.069438\n",
      "Train Epoch: 8 [52480/60000 (87%)]\tLoss: 0.007723\n",
      "Train Epoch: 8 [53120/60000 (88%)]\tLoss: 0.079845\n",
      "Train Epoch: 8 [53760/60000 (90%)]\tLoss: 0.035913\n",
      "Train Epoch: 8 [54400/60000 (91%)]\tLoss: 0.116532\n",
      "Train Epoch: 8 [55040/60000 (92%)]\tLoss: 0.064559\n",
      "Train Epoch: 8 [55680/60000 (93%)]\tLoss: 0.043017\n",
      "Train Epoch: 8 [56320/60000 (94%)]\tLoss: 0.089686\n",
      "Train Epoch: 8 [56960/60000 (95%)]\tLoss: 0.090047\n",
      "Train Epoch: 8 [57600/60000 (96%)]\tLoss: 0.094574\n",
      "Train Epoch: 8 [58240/60000 (97%)]\tLoss: 0.026934\n",
      "Train Epoch: 8 [58880/60000 (98%)]\tLoss: 0.030686\n",
      "Train Epoch: 8 [59520/60000 (99%)]\tLoss: 0.208677\n",
      "\n",
      "Test set: Average loss: 0.0524, Accuracy: 9840/10000 (98%)\n",
      "\n",
      "Train Epoch: 9 [0/60000 (0%)]\tLoss: 0.015108\n",
      "Train Epoch: 9 [640/60000 (1%)]\tLoss: 0.020380\n",
      "Train Epoch: 9 [1280/60000 (2%)]\tLoss: 0.088796\n",
      "Train Epoch: 9 [1920/60000 (3%)]\tLoss: 0.033659\n",
      "Train Epoch: 9 [2560/60000 (4%)]\tLoss: 0.042583\n",
      "Train Epoch: 9 [3200/60000 (5%)]\tLoss: 0.051292\n",
      "Train Epoch: 9 [3840/60000 (6%)]\tLoss: 0.049341\n",
      "Train Epoch: 9 [4480/60000 (7%)]\tLoss: 0.066496\n",
      "Train Epoch: 9 [5120/60000 (9%)]\tLoss: 0.027826\n",
      "Train Epoch: 9 [5760/60000 (10%)]\tLoss: 0.165204\n",
      "Train Epoch: 9 [6400/60000 (11%)]\tLoss: 0.051986\n",
      "Train Epoch: 9 [7040/60000 (12%)]\tLoss: 0.109605\n",
      "Train Epoch: 9 [7680/60000 (13%)]\tLoss: 0.101595\n",
      "Train Epoch: 9 [8320/60000 (14%)]\tLoss: 0.008146\n",
      "Train Epoch: 9 [8960/60000 (15%)]\tLoss: 0.135924\n",
      "Train Epoch: 9 [9600/60000 (16%)]\tLoss: 0.011003\n",
      "Train Epoch: 9 [10240/60000 (17%)]\tLoss: 0.036468\n",
      "Train Epoch: 9 [10880/60000 (18%)]\tLoss: 0.029492\n",
      "Train Epoch: 9 [11520/60000 (19%)]\tLoss: 0.073700\n",
      "Train Epoch: 9 [12160/60000 (20%)]\tLoss: 0.028427\n",
      "Train Epoch: 9 [12800/60000 (21%)]\tLoss: 0.022824\n",
      "Train Epoch: 9 [13440/60000 (22%)]\tLoss: 0.018612\n",
      "Train Epoch: 9 [14080/60000 (23%)]\tLoss: 0.036783\n",
      "Train Epoch: 9 [14720/60000 (25%)]\tLoss: 0.014990\n",
      "Train Epoch: 9 [15360/60000 (26%)]\tLoss: 0.003985\n",
      "Train Epoch: 9 [16000/60000 (27%)]\tLoss: 0.031270\n",
      "Train Epoch: 9 [16640/60000 (28%)]\tLoss: 0.045064\n",
      "Train Epoch: 9 [17280/60000 (29%)]\tLoss: 0.024345\n",
      "Train Epoch: 9 [17920/60000 (30%)]\tLoss: 0.015603\n",
      "Train Epoch: 9 [18560/60000 (31%)]\tLoss: 0.010011\n",
      "Train Epoch: 9 [19200/60000 (32%)]\tLoss: 0.034982\n",
      "Train Epoch: 9 [19840/60000 (33%)]\tLoss: 0.041385\n",
      "Train Epoch: 9 [20480/60000 (34%)]\tLoss: 0.043578\n",
      "Train Epoch: 9 [21120/60000 (35%)]\tLoss: 0.103286\n",
      "Train Epoch: 9 [21760/60000 (36%)]\tLoss: 0.130673\n",
      "Train Epoch: 9 [22400/60000 (37%)]\tLoss: 0.024517\n",
      "Train Epoch: 9 [23040/60000 (38%)]\tLoss: 0.026847\n",
      "Train Epoch: 9 [23680/60000 (39%)]\tLoss: 0.048802\n",
      "Train Epoch: 9 [24320/60000 (41%)]\tLoss: 0.038831\n",
      "Train Epoch: 9 [24960/60000 (42%)]\tLoss: 0.060979\n",
      "Train Epoch: 9 [25600/60000 (43%)]\tLoss: 0.066740\n",
      "Train Epoch: 9 [26240/60000 (44%)]\tLoss: 0.015012\n",
      "Train Epoch: 9 [26880/60000 (45%)]\tLoss: 0.150479\n",
      "Train Epoch: 9 [27520/60000 (46%)]\tLoss: 0.028768\n",
      "Train Epoch: 9 [28160/60000 (47%)]\tLoss: 0.025901\n",
      "Train Epoch: 9 [28800/60000 (48%)]\tLoss: 0.040261\n",
      "Train Epoch: 9 [29440/60000 (49%)]\tLoss: 0.015135\n",
      "Train Epoch: 9 [30080/60000 (50%)]\tLoss: 0.024420\n",
      "Train Epoch: 9 [30720/60000 (51%)]\tLoss: 0.172079\n",
      "Train Epoch: 9 [31360/60000 (52%)]\tLoss: 0.034427\n",
      "Train Epoch: 9 [32000/60000 (53%)]\tLoss: 0.112454\n",
      "Train Epoch: 9 [32640/60000 (54%)]\tLoss: 0.083366\n",
      "Train Epoch: 9 [33280/60000 (55%)]\tLoss: 0.022070\n",
      "Train Epoch: 9 [33920/60000 (57%)]\tLoss: 0.063841\n",
      "Train Epoch: 9 [34560/60000 (58%)]\tLoss: 0.020882\n",
      "Train Epoch: 9 [35200/60000 (59%)]\tLoss: 0.116344\n",
      "Train Epoch: 9 [35840/60000 (60%)]\tLoss: 0.163644\n",
      "Train Epoch: 9 [36480/60000 (61%)]\tLoss: 0.152294\n",
      "Train Epoch: 9 [37120/60000 (62%)]\tLoss: 0.099184\n",
      "Train Epoch: 9 [37760/60000 (63%)]\tLoss: 0.091996\n",
      "Train Epoch: 9 [38400/60000 (64%)]\tLoss: 0.026991\n",
      "Train Epoch: 9 [39040/60000 (65%)]\tLoss: 0.029864\n",
      "Train Epoch: 9 [39680/60000 (66%)]\tLoss: 0.009279\n",
      "Train Epoch: 9 [40320/60000 (67%)]\tLoss: 0.050487\n",
      "Train Epoch: 9 [40960/60000 (68%)]\tLoss: 0.038586\n",
      "Train Epoch: 9 [41600/60000 (69%)]\tLoss: 0.068628\n",
      "Train Epoch: 9 [42240/60000 (70%)]\tLoss: 0.027623\n",
      "Train Epoch: 9 [42880/60000 (71%)]\tLoss: 0.061262\n",
      "Train Epoch: 9 [43520/60000 (72%)]\tLoss: 0.066981\n",
      "Train Epoch: 9 [44160/60000 (74%)]\tLoss: 0.096382\n",
      "Train Epoch: 9 [44800/60000 (75%)]\tLoss: 0.098825\n",
      "Train Epoch: 9 [45440/60000 (76%)]\tLoss: 0.024105\n",
      "Train Epoch: 9 [46080/60000 (77%)]\tLoss: 0.150338\n",
      "Train Epoch: 9 [46720/60000 (78%)]\tLoss: 0.061632\n",
      "Train Epoch: 9 [47360/60000 (79%)]\tLoss: 0.019380\n",
      "Train Epoch: 9 [48000/60000 (80%)]\tLoss: 0.030395\n",
      "Train Epoch: 9 [48640/60000 (81%)]\tLoss: 0.027009\n",
      "Train Epoch: 9 [49280/60000 (82%)]\tLoss: 0.029632\n",
      "Train Epoch: 9 [49920/60000 (83%)]\tLoss: 0.007746\n",
      "Train Epoch: 9 [50560/60000 (84%)]\tLoss: 0.063283\n",
      "Train Epoch: 9 [51200/60000 (85%)]\tLoss: 0.054085\n",
      "Train Epoch: 9 [51840/60000 (86%)]\tLoss: 0.067539\n",
      "Train Epoch: 9 [52480/60000 (87%)]\tLoss: 0.095658\n",
      "Train Epoch: 9 [53120/60000 (88%)]\tLoss: 0.014887\n",
      "Train Epoch: 9 [53760/60000 (90%)]\tLoss: 0.043254\n",
      "Train Epoch: 9 [54400/60000 (91%)]\tLoss: 0.186175\n",
      "Train Epoch: 9 [55040/60000 (92%)]\tLoss: 0.020304\n",
      "Train Epoch: 9 [55680/60000 (93%)]\tLoss: 0.127136\n",
      "Train Epoch: 9 [56320/60000 (94%)]\tLoss: 0.136253\n",
      "Train Epoch: 9 [56960/60000 (95%)]\tLoss: 0.042175\n",
      "Train Epoch: 9 [57600/60000 (96%)]\tLoss: 0.015847\n",
      "Train Epoch: 9 [58240/60000 (97%)]\tLoss: 0.041143\n",
      "Train Epoch: 9 [58880/60000 (98%)]\tLoss: 0.016701\n",
      "Train Epoch: 9 [59520/60000 (99%)]\tLoss: 0.095916\n",
      "\n",
      "Test set: Average loss: 0.0517, Accuracy: 9834/10000 (98%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, 10):\n",
    "    train(epoch)\n",
    "    test()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
