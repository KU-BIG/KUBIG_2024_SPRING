{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# < 3주차 과제 1 : MLP 마음대로 다뤄보기>\n",
        "- dataset을 임의로 선정해서 직접 분석 해보기(제공한 코드를 활용해서 해보기)\n",
        "- activation functions 중 relu사용시 함수 직접 정의\n",
        "- lr, optimizer 등 바꿔보기\n",
        "- hidden layer/neuron 수를 바꾸기\n",
        "- 전처리도 추가\n",
        "- 모든 시도를 올려주세요!\n",
        "- 제일 높은 acc를 보인 시도를 명시해주세요!\n"
      ],
      "metadata": {
        "id": "sgAYo4nrw2F4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "fX437IL6qbI-"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn, optim\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from torch.utils.data import  TensorDataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 아래 데이터셋 중 원하는 데이터셋 하나를 선택하여, 코드 과제 진행하기!\n",
        "- 1) load_digits() <br>\n",
        "- 2) load_wine()"
      ],
      "metadata": {
        "id": "oxkFzBDNmWNk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import datasets\n",
        "\n",
        "# 데이터셋 종류 :  load_digits()\n",
        "# data = load_breast_cancer()\n",
        "data = datasets.load_digits()"
      ],
      "metadata": {
        "id": "FywYbfsKtjcR"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 데이터셋 확인"
      ],
      "metadata": {
        "id": "EkLQebbBzlIp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input = data.data\n",
        "output = data.target"
      ],
      "metadata": {
        "id": "C2P0hqZ9yBGm"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(input.shape) #(1797, 64)\n",
        "print(output.shape) #(1797,)\n",
        "print(data.images.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tlQ5yNH9z56_",
        "outputId": "ffc5b2bb-de16-46cf-afc7-3994b6e2fa61"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1797, 64)\n",
            "(1797,)\n",
            "(1797, 8, 8)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#이미지확인\n",
        "plt.imshow(input[0].reshape(8, 8), cmap='gray')\n",
        "plt.axis('off')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 406
        },
        "id": "lTbiv1Hq7qYD",
        "outputId": "1adc44b3-3c49-4315-b4d1-5766225230a4"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "torch.manual_seed(777)\n",
        "if device == \"cuda\":\n",
        "  torch.cuda.manual_seed_all(777)"
      ],
      "metadata": {
        "id": "SggpQfSPt85C"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train, x_test, y_train, y_test = train_test_split(input, output, test_size = 0.3, random_state = 42, stratify= data.target, shuffle = True)\n",
        "\n",
        "x_train = torch.FloatTensor(x_train).to(device)\n",
        "y_train = torch.LongTensor(y_train).to(device)\n",
        "x_test = torch.FloatTensor(x_test)\n",
        "y_test = torch.LongTensor(y_test)\n",
        "\n",
        "# 데이터를 tensor로 바꿔주고 gpu 연산이 가능해지도록 gpu에 옮김\n",
        "# label 값을 왜 long 에 옮겨놓는가? loss function이 다르기 때문"
      ],
      "metadata": {
        "id": "bLMzf-2ntYeX"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(x_train[0]) # 첫번째 이미지의 feature\n",
        "print(y_train[0]) # 첫번째 이미지 레이블 : 0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "umEdiTZkrVqS",
        "outputId": "527c7ad1-b248-40cd-a6a9-20e7717f79a9"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([ 0.,  0.,  0., 16., 12.,  1.,  0.,  0.,  0.,  0.,  6., 16., 14.,  7.,\n",
            "         0.,  0.,  0.,  0., 14., 15.,  1., 11.,  0.,  0.,  0.,  0., 16., 15.,\n",
            "         0., 14.,  1.,  0.,  0.,  1., 16., 10.,  0., 14.,  2.,  0.,  0.,  0.,\n",
            "        15., 13.,  3., 15.,  3.,  0.,  0.,  0.,  9., 16., 16., 15.,  0.,  0.,\n",
            "         0.,  0.,  0., 13., 16.,  8.,  0.,  0.])\n",
            "tensor(0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**파이토치에서는** 데이터셋을 좀 더 쉽게 다룰 수 있도록 유용한 도구로서 torch.utils.data.Dataset과 torch.utils.data.DataLoader를 제공합니다. 이를 사용하면 미니 배치 학습, 데이터 셔플(shuffle), 병렬 처리까지 간단히 수행할 수 있습니다. 기본적인 사용 방법은 Dataset을 정의하고, 이를 DataLoader에 전달하는 것\n",
        "- init : class 에서 객체가 생성되면 바로 실행되는 함수\n",
        "- len : observation 수를 정의하는 함수\n",
        "- getitem : iteration 마다 해당하는 데이터를 돌려주는 함수"
      ],
      "metadata": {
        "id": "combmxzmYFyn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset\n",
        "class CustomDataset(Dataset):\n",
        "  def __init__(self):\n",
        "    self.x_data = x_train\n",
        "    self.y_data = [[y] for y in y_train]\n",
        "#  데이터셋의 전처리를 해주는 부분\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.x_data)\n",
        "#  데이터셋의 길이. 즉, 총 샘플의 수를 적어주는 부분\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    x = torch.FloatTensor(self.x_data[idx]).to(device)\n",
        "    y = torch.LongTensor(self.y_data[idx]).to(device)\n",
        "#  데이터셋에서 특정 1개의 샘플을 가져오는 함수\n",
        "\n",
        "    return x,y"
      ],
      "metadata": {
        "id": "y38TlgXoqV5Z"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 256\n",
        "dataset = CustomDataset()\n",
        "dataloader = DataLoader(dataset, batch_size=batch_size)"
      ],
      "metadata": {
        "id": "x8VHwnuFqino"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# input_layer와 hidden_layer의 노드 개수를 어떻게 바꿔줘야할까?\n",
        "# hidden layer/neuron 수를 바꾸기\n",
        "\n",
        "model = nn.Sequential(\n",
        "          nn.Linear(64,128, bias=True),\n",
        "          nn.Sigmoid(),\n",
        "          nn.Linear(128,15, bias=True),\n",
        "          nn.Sigmoid(),\n",
        "          nn.Linear(15,10, bias=True),\n",
        "          nn.Softmax()\n",
        "          ).to(device)\n",
        "\n",
        "#model = nn.Sequential(\n",
        "         # nn.Linear(입력 데이터의 특성(feature)의 수, 첫_은닉층_노드수, bias=True),  # 입력층 노드 수를 64로 설정\n",
        "         # nn.Sigmoid(),\n",
        "         # nn.Linear(첫_은닉층_노드수, 두_번째_은닉층_노드수, bias=True),\n",
        "         # nn.Sigmoid(),\n",
        "         # nn.Linear(두_번째_은닉층_노드수, 출력_노드수, bias=True),\n",
        "         # nn.Softmax(dim=1)  # 클래스 확률을 계산하기 위해 차원 지정\n",
        "         # ).to(device)"
      ],
      "metadata": {
        "id": "C6V7a4tyq6Jc"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1.첫번째 모델\n",
        "##### 1) Activation function : relu\n",
        "##### 2) Optimizier : Adam\n",
        "##### 3) layer 수 : 4개\n",
        "##### 4) lr : 0.01"
      ],
      "metadata": {
        "id": "0ACVLYAD1Mmu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.1모델 정의"
      ],
      "metadata": {
        "id": "Ok0CLtZYsIDy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "class로 구현 가능\n",
        "- init : 초기 생성 함수\n",
        "- foward : 순전파(입력값 => 예측값 의 과정)"
      ],
      "metadata": {
        "id": "07uV8RY7Yr_5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#ReLU\n",
        "class ReLU(torch.nn.ReLU):\n",
        "  def forward(self, x):\n",
        "    return torch.max(torch.zeros_like(x), x)"
      ],
      "metadata": {
        "id": "C5l9ogVfUQRI"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Model(torch.nn.Module):\n",
        "\n",
        "  def __init__(self):\n",
        "    super(Model, self).__init__()\n",
        "    self.layer1 = nn.Sequential(\n",
        "          nn.Linear(64,300, bias=True), # input_layer = 64, hidden_layer1 = 300\n",
        "          #nn.Sigmoid(),\n",
        "          ReLU(),\n",
        "          nn.BatchNorm1d(300)\n",
        "    )\n",
        "  # activation function 이용\n",
        "  #   nn.ReLU()\n",
        "  #   nn.tanH()\n",
        "  #   https://pytorch.org/docs/stable/nn.html 그 외에도 여기서 확인 가능함\n",
        "  #   파라미터가 필요하지 않다는 것이 특징\n",
        "\n",
        "  # batch normazliation 1d, 파라미터 값으로 vector의 길이를 전해줌\n",
        "  # 추후에 이미지를 다루게 된다면, 그때는 batch normalization 2d를 이용하게 됨\n",
        "  # 그때는 파라미터 값으로 채널, 가로, 세로 길이를 전달해주게 됨\n",
        "\n",
        "    self.layer2 = nn.Sequential(\n",
        "          nn.Linear(300,200, bias=True), # hidden_layer1 = 398, hidden_layer2 = 200\n",
        "        #nn.Sigmoid()\n",
        "        ReLU()\n",
        "    )\n",
        "    self.layer3 = nn.Sequential(\n",
        "          nn.Linear(200,50, bias=True), # hidden_layer1 = 200, hidden_layer2 = 50\n",
        "        #nn.Sigmoid()\n",
        "        ReLU()\n",
        "    )\n",
        "    self.layer4 = nn.Sequential(\n",
        "        nn.Linear(50, 10, bias=True), # hidden_layer3 = 50, output_layer = 10\n",
        "        nn.Softmax()\n",
        "    )\n",
        "\n",
        "  def forward(self,x):\n",
        "    output = self.layer1(x)\n",
        "    output = self.layer2(output)\n",
        "    output = self.layer3(output)\n",
        "    output = self.layer4(output)\n",
        "    return output"
      ],
      "metadata": {
        "id": "a0zLstbMqxEZ"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def init_weights(layer):\n",
        "    if isinstance(layer, nn.Linear):\n",
        "        torch.nn.init.xavier_uniform_(layer.weight)\n",
        "        layer.bias.data.fill_(0.01)\n",
        "\n",
        "        #xavier사용\n",
        "        # Layer의 weight를 어떤 분포를 가지도록 초기화시켜줌+global minimum찾기 위해서"
      ],
      "metadata": {
        "id": "kqcqqkECrSGK"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Model().to(device)\n",
        "model.apply(init_weights)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oMDUBFg6rUpw",
        "outputId": "f7b72de4-8710-4044-c9e6-5fdb8aae1940"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Model(\n",
              "  (layer1): Sequential(\n",
              "    (0): Linear(in_features=64, out_features=300, bias=True)\n",
              "    (1): Sigmoid()\n",
              "    (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  )\n",
              "  (layer2): Sequential(\n",
              "    (0): Linear(in_features=300, out_features=200, bias=True)\n",
              "    (1): Sigmoid()\n",
              "  )\n",
              "  (layer3): Sequential(\n",
              "    (0): Linear(in_features=200, out_features=50, bias=True)\n",
              "    (1): Sigmoid()\n",
              "  )\n",
              "  (layer4): Sequential(\n",
              "    (0): Linear(in_features=50, out_features=10, bias=True)\n",
              "    (1): Softmax(dim=None)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OwZt5CetrYFb",
        "outputId": "e24f15c1-f711-4cf4-b691-3d3401d970da"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model(\n",
            "  (layer1): Sequential(\n",
            "    (0): Linear(in_features=64, out_features=300, bias=True)\n",
            "    (1): Sigmoid()\n",
            "    (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  )\n",
            "  (layer2): Sequential(\n",
            "    (0): Linear(in_features=300, out_features=200, bias=True)\n",
            "    (1): Sigmoid()\n",
            "  )\n",
            "  (layer3): Sequential(\n",
            "    (0): Linear(in_features=200, out_features=50, bias=True)\n",
            "    (1): Sigmoid()\n",
            "  )\n",
            "  (layer4): Sequential(\n",
            "    (0): Linear(in_features=50, out_features=10, bias=True)\n",
            "    (1): Softmax(dim=None)\n",
            "  )\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.1모델 훈련"
      ],
      "metadata": {
        "id": "psHcpVLegOro"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loss_fn  = nn.CrossEntropyLoss().to(device)\n",
        "\n",
        "# 여러가지 optimizer 시도해보기\n",
        "# lr 바꿔보기\n",
        "\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr= 0.01)\n",
        "\n",
        "# https://pytorch.org/docs/stable/optim.html#module-torch.optim 페이지 참조\n",
        "# optimizer = optim.SGD(model.parameters(), lr=0.1, momentum=0.9)\n",
        "# sgd 등등"
      ],
      "metadata": {
        "id": "AYFp-eTErh7b"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "losses = []\n",
        "for epoch in range(100):\n",
        "\n",
        "  optimizer.zero_grad()\n",
        "  hypothesis = model(x_train)\n",
        "\n",
        "  # 비용 함수\n",
        "  cost = loss_fn(hypothesis, y_train)\n",
        "  cost.backward()\n",
        "  optimizer.step()\n",
        "  losses.append(cost.item())\n",
        "\n",
        "  if epoch % 10 == 0:\n",
        "    print(epoch, cost.item())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "90QxHvlIrjS7",
        "outputId": "64a5ab08-bc87-43ed-eb37-20847e72c3d0"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1518: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 2.3054139614105225\n",
            "10 1.6414518356323242\n",
            "20 1.5602022409439087\n",
            "30 1.477461338043213\n",
            "40 1.46550452709198\n",
            "50 1.4635175466537476\n",
            "60 1.4624345302581787\n",
            "70 1.4621706008911133\n",
            "80 1.462023377418518\n",
            "90 1.4619293212890625\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(losses)\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 430
        },
        "id": "81ASYrW7roFM",
        "outputId": "2a29f9a9-e5e4-437d-a9e6-aa15d601bac9"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAiMAAAGdCAYAAADAAnMpAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAuX0lEQVR4nO3de3TU9Z3/8dd3ZpJJQpLJBXKDhHATVBBRRBEvvbAqura2W7el7EKra9c1rFC7buu6dXd/XTe22+1pa1177Gl191cQLytoWWt/FASkIggSFa0R5ZIASSCEzOR+mfn8/khmkiCBmWRmvpnk+Tjne0i+l8w7n9Oa1/l8PxfLGGMEAABgE4fdBQAAgLGNMAIAAGxFGAEAALYijAAAAFsRRgAAgK0IIwAAwFaEEQAAYCvCCAAAsJXL7gLCEQgEdPz4cWVkZMiyLLvLAQAAYTDGqKmpSUVFRXI4Bu//SIgwcvz4cRUXF9tdBgAAGILq6mpNmjRp0OsJEUYyMjIk9fwymZmZNlcDAADC4fP5VFxcHPo7PpiECCPBVzOZmZmEEQAAEsz5hlgwgBUAANiKMAIAAGxFGAEAALYijAAAAFsRRgAAgK0IIwAAwFaEEQAAYCvCCAAAsBVhBAAA2IowAgAAbEUYAQAAtiKMAAAAW43pMLJh3zH9w/p3tffIabtLAQBgzBrTYWTTH+u0dleV3iKMAABgmzEdRqbkjpMkHTrVYnMlAACMXWM7jIzvDSMnCSMAANhlTIeR0t4wcpieEQAAbDOmw0iwZ6TG2662Tr/N1QAAMDaN6TCSnZYkT2qSJHpHAACwy5gOI5Zl9b2qqSeMAABghzEdRiRpam8YOUgYAQDAFmM+jJTm0jMCAICdCCPj0yQxZgQAALuM+TAydXy6JOkQPSMAANhizIeRYM9IfXOnmtq7bK4GAICxZ8yHkYyUJI1PT5YkHa5vtbkaAADGnjEfRqS+xc8O1jfbXAkAAGMPYUT9Z9TQMwIAQLwRRiRNmdC7YR49IwAAxB1hRNKU3p6RQ6foGQEAIN4II+rbvffQyWYZY2yuBgCAsYUwor4xI772bp1uZXovAADxRBiRlJrsVKEnRRKLnwEAEG+EkV7B3hHCCAAA8UUY6RWcUcOGeQAAxBdhpFffjBrCCAAA8UQY6TUlNKOGMAIAQDwRRnoFp/cePtXC9F4AAOKIMNKrJCdNDktq7fTrRFOH3eUAADBmEEZ6JbscmpSdJokZNQAAxBNhpJ/QqxrCCAAAcUMY6WdKLj0jAADEG2Gkn9CMGsIIAABxQxjpp5QwAgBA3BFG+gn2jBxpaFUgwPReAADigTDSz8SsVDkdljq7A6prare7HAAAxgTCSD8up0MTs1IlSUdOtdpcDQAAYwNh5AyTe2fUVDUQRgAAiAfCyBmKc3rDCD0jAADEBWHkDJNz6BkBACCeCCNnKOkNI0cIIwAAxEVEYaS8vFxXXHGFMjIylJeXp9tuu02VlZXnfOYXv/iFrr32WmVnZys7O1uLFy/W7t27h1V0LJX0jhmpJowAABAXEYWRbdu2qaysTG+88YY2bdqkrq4u3XDDDWppGXyRsK1bt2rp0qV69dVXtXPnThUXF+uGG27QsWPHhl18LAR7RhpaOtXU3mVzNQAAjH6WMWbIq3udPHlSeXl52rZtm6677rqwnvH7/crOztbPfvYzLV++PKxnfD6fPB6PvF6vMjMzh1pu2C773iY1tHTqf++9RhcXeWL+eQAAjEbh/v0e1pgRr9crScrJyQn7mdbWVnV1dZ3zmY6ODvl8vgFHPDGjBgCA+BlyGAkEAlq9erUWLVqk2bNnh/3ct7/9bRUVFWnx4sWD3lNeXi6PxxM6iouLh1rmkDCjBgCA+BlyGCkrK9P+/fu1bt26sJ955JFHtG7dOq1fv14pKSmD3vfAAw/I6/WGjurq6qGWOSTMqAEAIH5cQ3lo5cqV2rhxo7Zv365JkyaF9cwPf/hDPfLII/r973+vSy655Jz3ut1uud3uoZQWFcyoAQAgfiIKI8YY/e3f/q3Wr1+vrVu3asqUKWE994Mf/EAPP/ywfve732n+/PlDKjSeQj0jjBkBACDmIgojZWVlWrt2rV588UVlZGSotrZWkuTxeJSa2rPB3PLlyzVx4kSVl5dLkr7//e/roYce0tq1a1VaWhp6Jj09Xenp6dH8XaImuD/NscY2dfsDcjlZGw4AgFiJ6K/s448/Lq/Xq0996lMqLCwMHc8880zonqqqKtXU1Ax4prOzU1/60pcGPPPDH/4wer9FlOVnpCjZ5ZA/YHS8sd3ucgAAGNUifk1zPlu3bh3w/eHDhyP5iBHB4bBUnJ2qj0+2qKqhNTSGBAAARB/vHwbRN6Nm8NVlAQDA8BFGBjE5d5wk1hoBACDWCCODYBVWAADigzAyCFZhBQAgPggjgwgOWq061RrWwF0AADA0hJFBFGf3hJGmjm41tnbZXA0AAKMXYWQQqclO5WX0LEnPHjUAAMQOYeQcgiuxMm4EAIDYIYycQ9+MGtYaAQAgVggj5zA5h7VGAACINcLIOZTk9mz+RxgBACB2CCPnUBLsGWHhMwAAYoYwcg7B/WlqfO3q6PbbXA0AAKMTYeQcxqcnKy3ZKWOko6fb7C4HAIBRiTByDpZlhXpHGDcCAEBsEEbOo4QN8wAAiCnCyHkEw8gRwggAADFBGDmPvlVYWfgMAIBYIIycR0luz/ReekYAAIgNwsh5TO43gDUQMDZXAwDA6EMYOY+J2alyOix1dAd0oqnD7nIAABh1CCPnkeR0qCgrRZJ0hA3zAACIOsJIGIIb5h1hrREAAKKOMBKGklzWGgEAIFYII2EIDmKlZwQAgOgjjIQhtNYIY0YAAIg6wkgYShgzAgBAzBBGwhAcM9LY2iVvW5fN1QAAMLoQRsKQ7nZpfHqyJAaxAgAQbYSRMIU2zGOPGgAAooowEqbJ7FEDAEBMEEbCFOwZ4TUNAADRRRgJU3B6L69pAACILsJImCazCisAADFBGAlTcK2RGl+7Orr9NlcDAMDoQRgJ0/j0ZKUlO2WMVN3QZnc5AACMGoSRMFmW1TeIlXEjAABEDWEkAqFBrIwbAQAgaggjEWCtEQAAoo8wEoG+1zSEEQAAooUwEoG+1zSMGQEAIFoIIxGY3Du9t/p0mwIBY3M1AACMDoSRCBRlpcjlsNTZHVCtr93ucgAAGBUIIxFwOR2amJ0qiUGsAABEC2EkQqw1AgBAdBFGIhQMI/SMAAAQHYSRCPXt3ksYAQAgGggjEQpumMfuvQAARAdhJEKsNQIAQHQRRiIUDCO+9m41tnbaXA0AAImPMBKhtGSXJmS4JTGIFQCAaCCMDEFpb+/IYV7VAAAwbISRIShl914AAKKGMDIEpeN7wgg9IwAADB9hZAj6ZtTQMwIAwHARRoag7zUNPSMAAAwXYWQISnp7RuqbO9XU3mVzNQAAJDbCyBBkpiQpd1yyJF7VAAAwXISRIWLcCAAA0UEYGaLguBFm1AAAMDyEkSGazCBWAACigjAyRKXjg6uw8poGAIDhIIwMET0jAABEB2FkiIL709T5OtTW6be5GgAAEhdhZIiy0pLlSU2SJFU18KoGAIChIowMA7v3AgAwfISRYWDcCAAAw0cYGYa+nhFe0wAAMFSEkWGgZwQAgOGLKIyUl5friiuuUEZGhvLy8nTbbbepsrLyvM8999xzmjVrllJSUjRnzhy9/PLLQy54JAmtNVJPzwgAAEMVURjZtm2bysrK9MYbb2jTpk3q6urSDTfcoJaWwXsGXn/9dS1dulR33nmn9u3bp9tuu0233Xab9u/fP+zi7RbsGTnubVNHN9N7AQAYCssYY4b68MmTJ5WXl6dt27bpuuuuO+s9X/7yl9XS0qKNGzeGzl111VW69NJL9fOf/zysz/H5fPJ4PPJ6vcrMzBxquVFnjNGcf/5/au7o1u/vu17T89LtLgkAgBEj3L/fwxoz4vV6JUk5OTmD3rNz504tXrx4wLkbb7xRO3fuHPSZjo4O+Xy+AcdIZFlWv917GTcCAMBQDDmMBAIBrV69WosWLdLs2bMHva+2tlb5+fkDzuXn56u2tnbQZ8rLy+XxeEJHcXHxUMuMub7dexk3AgDAUAw5jJSVlWn//v1at25dNOuRJD3wwAPyer2ho7q6OuqfES30jAAAMDyuoTy0cuVKbdy4Udu3b9ekSZPOeW9BQYHq6uoGnKurq1NBQcGgz7jdbrnd7qGUFnf0jAAAMDwR9YwYY7Ry5UqtX79eW7Zs0ZQpU877zMKFC7V58+YB5zZt2qSFCxdGVukIRc8IAADDE1HPSFlZmdauXasXX3xRGRkZoXEfHo9HqampkqTly5dr4sSJKi8vlyStWrVK119/vf7jP/5Dt9xyi9atW6c9e/boiSeeiPKvYo/S8T09I0dPt6nLH1CSk3XkAACIRER/OR9//HF5vV596lOfUmFhYeh45plnQvdUVVWppqYm9P3VV1+ttWvX6oknntDcuXP1/PPPa8OGDecc9JpI8jLcSklyyB8wOna6ze5yAABIOBH1jISzJMnWrVs/ce7222/X7bffHslHJQzLslSaO04f1Dbp8KmWUE8JAAAID+8UoqBv3AiDWAEAiBRhJAr6ZtQwiBUAgEgRRqIguEfNoXrCCAAAkSKMRMG0CT1h5OBJwggAAJEijETB1Ak9G+RVn25Vexe79wIAEAnCSBSMT09WRopLxjCIFQCASBFGosCyLE3r7R35+GSzzdUAAJBYCCNRMjU0boQwAgBAJAgjURLsGWEQKwAAkSGMRElwRg2vaQAAiAxhJEr694yEs2w+AADoQRiJkpLcNDksqamjWyebOuwuBwCAhEEYiRK3y6ninJ49aj5m3AgAAGEjjEQR03sBAIgcYSSKpo5nWXgAACJFGImiaXn0jAAAECnCSBSFekbqCSMAAISLMBJFwQ3zjp5uY8M8AADCRBiJovHpycrs3TDv8CnGjQAAEA7CSBRZlhXqHWEQKwAA4SGMRFloeu8Jxo0AABAOwkiUhXbvradnBACAcBBGooyFzwAAiAxhJMqCu/eyYR4AAOEhjERZcMO85o5unWDDPAAAzoswEmVul1MloQ3zeFUDAMD5EEZigOm9AACEjzASA8FxI/SMAABwfoSRGKBnBACA8BFGYiC4YR49IwAAnB9hJAam5fX0jBxrZMM8AADOhzASA7nj+jbMO8RKrAAAnBNhJAYsywr1jnzEHjUAAJwTYSRGZhBGAAAIC2EkRqYHwwiDWAEAOCfCSIwEw8jH9IwAAHBOhJEYmT4hQ5J0sL5F/gAb5gEAMBjCSIxMzE6V2+VQZ3dA1Q2tdpcDAMCIRRiJEafDCq3EyiBWAAAGRxiJIQaxAgBwfoSRGJpOzwgAAOdFGImhYM/IAcIIAACDIozEUP/pvcYwowYAgLMhjMRQ6fg0OR2Wmju6VefrsLscAABGJMJIDLldTk3OSZPEuBEAAAZDGImxvg3zmmyuBACAkYkwEmNM7wUA4NwIIzHG9F4AAM6NMBJjoZ6REy02VwIAwMhEGImx4JiR+uYOeVu7bK4GAICRhzASY+lulwo9KZKkj04yiBUAgDMRRuIgtBJrHeNGAAA4E2EkDqYxiBUAgEERRuJgRj7TewEAGAxhJA6Y3gsAwOAII3EQHDNyrLFNbZ1+m6sBAGBkIYzEQW66W9lpSTJG+phXNQAADEAYiZNg7whhBACAgQgjcdK3EithBACA/ggjccL0XgAAzo4wEicz8jMkSZV1rMIKAEB/hJE4mVXQE0YO17eovYsZNQAABBFG4iQvo2dGTcDwqgYAgP4II3FiWZZm9vaOfFDLqxoAAIIII3E0qyBTkvRBjc/mSgAAGDkII3EUHDfCIFYAAPoQRuKI1zQAAHwSYSSOLuid3nuyqUOnmjtsrgYAgJGBMBJH49wuleSkSZIq6R0BAEDSEMLI9u3bdeutt6qoqEiWZWnDhg3nfWbNmjWaO3eu0tLSVFhYqDvuuEOnTp0aSr0JbxavagAAGCDiMNLS0qK5c+fqscceC+v+P/zhD1q+fLnuvPNOvffee3ruuee0e/du3XXXXREXOxqEBrESRgAAkCS5In1gyZIlWrJkSdj379y5U6Wlpbr33nslSVOmTNFf//Vf6/vf/36kHz0qzAxO72VGDQAAkuIwZmThwoWqrq7Wyy+/LGOM6urq9Pzzz+vmm28e9JmOjg75fL4Bx2gxq7CnZ+TD2iYFAsbmagAAsF/Mw8iiRYu0Zs0affnLX1ZycrIKCgrk8XjO+ZqnvLxcHo8ndBQXF8e6zLgpzR0nt8uhti6/qhpa7S4HAADbxTyMvP/++1q1apUeeugh7d27V6+88ooOHz6su+++e9BnHnjgAXm93tBRXV0d6zLjxumwNCM/XRKDWAEAkIYwZiRS5eXlWrRoke6//35J0iWXXKJx48bp2muv1b/+67+qsLDwE8+43W653e5Yl2abWQWZ2n/Mp8raJt00u8DucgAAsFXMe0ZaW1vlcAz8GKfTKUkyZmyOmeib3jt6xsIAADBUEYeR5uZmVVRUqKKiQpJ06NAhVVRUqKqqSlLPK5bly5eH7r/11lv1wgsv6PHHH9fBgwf1hz/8Qffee68WLFigoqKi6PwWCWYm03sBAAiJ+DXNnj179OlPfzr0/X333SdJWrFihZ566inV1NSEgokkfe1rX1NTU5N+9rOf6Vvf+paysrL0mc98ZsxO7ZX6wsjhUy1q7/IrJclpc0UAANjHMgnwrsTn88nj8cjr9SozM9PucobNGKP5//p7nWrp1G9WXqM5kzx2lwQAQNSF+/ebvWlsYFlWqHfkj4wbAQCMcYQRmzBuBACAHoQRm1zYuyw8YQQAMNYRRmwyk+m9AABIIozY5oL8DFmWVN/cqfrmDrvLAQDANoQRm6QmOzU5J02S9EENr2oAAGMXYcRGF0/smdL7zrFGewsBAMBGhBEbXTopS5L0dnWjrXUAAGAnwoiN5hZnSZLervbaWwgAADYijNho9sRMOR2Wan3tqvW2210OAAC2IIzYKC3ZpQvye6b4VvCqBgAwRhFGbHZp76sawggAYKwijNjs0uKeGTUMYgUAjFWEEZsFB7G+c7RR/sCI30AZAICoI4zYbEZehtKSnWrp9Ovjk812lwMAQNwRRmzmdFia07v4WUVVo73FAABgA8LICHBpSZYkqeJoo611AABgB8LICBBciZWeEQDAWEQYGQGCg1gr65rU1um3txgAAOKMMDICFHpSlJfhlj9gtP84S8MDAMYWwsgIYFlWv31qGm2tBQCAeCOMjBDBlVj3EUYAAGMMYWSEuJSeEQDAGEUYGSHmTPLIsqSjp9tU39xhdzkAAMQNYWSEyExJ0rQJ6ZLoHQEAjC2EkRFkbu96I4QRAMBYQhgZQYIrsTKIFQAwlhBGRpB5wRk1VY3q8gfsLQYAgDghjIwgFxVmKjstSc0d3drH0vAAgDGCMDKCOByWrpkxQZK0/cOTNlcDAEB8EEZGmOtmjJckvXaAMAIAGBsIIyPMdRf09Iy8c8yrhpZOm6sBACD2CCMjTH5mimbmZ8gY6Q8f1dtdDgAAMUcYGYGuu6DnVQ3jRgAAYwFhZAS6NjiI9cBJGWNsrgYAgNgijIxAC6bkyO1yqM7XoQMnmu0uBwCAmCKMjEApSU5dOTVXEq9qAACjH2FkhApO8d1+gEGsAIDRjTAyQgWn+O46eErtXX6bqwEAIHYIIyPUjLx0FWSmqKM7oDcPN9hdDgAAMUMYGaEsy9K1M5jiCwAY/QgjI1jwVc32Dxk3AgAYvQgjI9g108fLsqTKuibV+drtLgcAgJggjIxg2eOSdclEjyRpa+UJm6sBACA2CCMj3A0XF0iSXnjrmM2VAAAQG4SREe6Ll02UZUm7DjWo6lSr3eUAABB1hJERrtCTqmum98yq+Z+3jtpcDQAA0UcYSQBfunySpJ4wEgiwcR4AYHQhjCSAGy8uUIbbpaOn27TrEAugAQBGF8JIAkhJcupP5xZJkp7bW21zNQAARBdhJEEEX9X89t1aNXd021wNAADRQxhJEJeVZGnqhHFq6/Lr5Xdr7C4HAICoIYwkCMuyQr0jz+9lVg0AYPQgjCSQL86bJIcl7T7UoCOnWuwuBwCAqCCMJJACT4qumdGzed7/0DsCABglCCMJpv+rmi5/wOZqAAAYPsJIgrnhonyNT3fruLddz+2hdwQAkPgIIwkmJcmpsk9PkyQ9uuWA2rv8NlcEAMDwEEYS0NIFJSr0pKjG266nd1fZXQ4AAMNCGElAKUlO3fvZGZKkx179SK2dLIIGAEhchJEE9aXLJ2lybprqmzv1X68fsbscAACGjDCSoJKcDq1e3NM78vNtH8vX3mVzRQAADA1hJIF9bu5ETc9Ll7etS7987ZDd5QAAMCSEkQTmdFi6708ukCT9cschnW7ptLkiAAAiRxhJcDddXKCLizLV3NGt77/ygd3lAAAQMcJIgnM4LP3jLRfJsqR1b1azoy8AIOEQRkaBhdNydff1PQuhfed/3tHR0602VwQAQPgII6PEfX9ygS4tzpKvvVur1lWom31rAAAJIuIwsn37dt16660qKiqSZVnasGHDeZ/p6OjQgw8+qMmTJ8vtdqu0tFS/+tWvhlIvBpHkdOjRpfOU4XZp75HT+unmA3aXBABAWCIOIy0tLZo7d64ee+yxsJ/58z//c23evFm//OUvVVlZqaefflozZ86M9KNxHsU5aXr4i3MkSY+++pF2fnzK5ooAADg/V6QPLFmyREuWLAn7/ldeeUXbtm3TwYMHlZOTI0kqLS2N9GMRps/NLdKOAyf17J6jWrVun567e6Em546zuywAAAYV8zEjL730kubPn68f/OAHmjhxoi644AL93d/9ndra2gZ9pqOjQz6fb8CB8P3z5y7WjLx0nWjq0FeeeEOH6lvsLgkAgEHFPIwcPHhQO3bs0P79+7V+/Xr9+Mc/1vPPP6977rln0GfKy8vl8XhCR3FxcazLHFXSkl1a81dXanpeumq87frKEzv18clmu8sCAOCsYh5GAoGALMvSmjVrtGDBAt1888360Y9+pP/6r/8atHfkgQcekNfrDR3V1dWxLnPUyctM0dN3XaUL8tNV5+vpIfnoRJPdZQEA8AkxDyOFhYWaOHGiPB5P6NyFF14oY4yOHj161mfcbrcyMzMHHIjchAy3nr7rKs0qyNDJ3lc27x332l0WAAADxDyMLFq0SMePH1dzc99rgg8//FAOh0OTJk2K9cePebnpbq296ypdVJip+uZOfeE/X9eTfzgkY4zdpQEAIGkIYaS5uVkVFRWqqKiQJB06dEgVFRWqqqqS1POKZfny5aH7v/rVryo3N1df//rX9f7772v79u26//77dccddyg1NTU6vwXOKWdcstbedaU+MytPnd0B/ctv3tfXn3pTJ5s67C4NAIDIw8iePXs0b948zZs3T5J03333ad68eXrooYckSTU1NaFgIknp6enatGmTGhsbNX/+fC1btky33nqrfvrTn0bpV0A4stKS9csV8/Uvn7tYyS6Htlae1JKfbNeWD+rsLg0AMMZZJgH6630+nzwej7xeL+NHoqCytkn3Pr1PlXU9A1o/NXOC/v7GWbqoiLYFAERPuH+/2ZtmDJpZkKEXVy7SX10zRS6Hpa2VJ3XLo6/pm89UqLqBTfYAAPFFz8gYd7i+RT/8f5Xa+E6NJCnZ6dCfXT5RyxeW6sJC2hoAMHTh/v0mjECS9O5Rr77/ygfa8VF96NyC0hwtv3qybry4QElOOtEAAJEhjGBI3jzcoKdeP6xX9tfKH+j5n8aEDLdumVOom+cUav7kbDkcls1VAgASAWEEw1LrbdfaXUe0dneV6ps7Q+fzMtxaMrtAiy/K1xWlOUpJctpYJQBgJCOMICo6uwN67cBJ/e+7Ndr0fp2a2rtD15JdDs2fnK1F08dr0fTxml2UKRevcwAAvQgjiLqObr9e/+iUfru/Rq8dqFeNt33A9Qy3S1dMydHCqblaOC1XFxZmyskrHQAYswgjiCljjA7Wt+j1j+q146N67fz4lHz9ek2knpVfr79ggj41c4KumzFB2eOSbaoWAGAHwgjiyh8w+mONTzs/PqXXP67Xm4dPq7mjL5w4LOnS4ix9Yd5EfX7eRGWmJNlYLQAgHggjsFWXP6C3jpzWq5UntbXyhD6obQpdS0ly6JY5RVq6oFiXT86WZfEqBwBGI8IIRpTjjW367f5ardtdpQMn+nZwvrgoU//n8xfr8sk5NlYHAIgFwghGJGOM3qpq1LrdVdr4To3auvySpKULivXtm2YpK41xJQAwWhBGMOI1tHTqkd/+Uc/uOSpJyh2XrAdvuVBfmDeRVzcAMAqwUR5GvJxxyfrBl+bqmW9cpRl56TrV0qn7nn1b33r27dDqrwCA0Y8wAttdOTVX/3vvtbr/xplyOSy9sO+YvvVsBYEEAMYIwghGhGSXQ2Wfnq6ffXWeXA5LGyqO675nK9TtD9hdGgAgxggjGFFuml2ox5ZdJpfD0osVx/XNZ98mkADAKEcYwYhz48UF+s9llynJaek3bx/X6md4ZQMAoxlhBCPSDRcX6D+XXa4kp6WN79ToJ5sP2F0SACBGCCMYsf7konz94EuXSJIe3XJAWytP2FwRACAWCCMY0b4wb5L+4qoSGSOtfqZCR0+32l0SACDKCCMY8b77pxfpkkkeNbZ26Z41b6mj2293SQCAKCKMYMRzu5x67KuXyZOapHeOevW9je/bXRIAIIoII0gIxTlp+vGXL5Uk/fqNKm3Yd8zeggAAUUMYQcL49Kw8/e1npkuSHnjhXR2oa7K5IgBANBBGkFBWL75Ai6bnqq3Lr3vWvKXWzm67SwIADBNhBAnF6bD04y/PU16GWwdONOsf1+9XAmw8DQA4B8IIEs6EDLceXTpPDkt6Yd8xPfNmtd0lAQCGgTCChHTl1Fz93Y0zJUkPvfSe3jvutbkiAMBQEUaQsO6+bpo+MytPnd0Bla15S97WLrtLAgAMAWEECcvhsPQft8/VxKxUHT7VquVP7lZTO4EEABINYQQJLXtcsn75tfnKSkvS29WNuvOpPcywAYAEQxhBwptVkKn/e8eVykhxaffhBn3jv/eqvYsl4wEgURBGMCrMmeTRU19foLRkp3Z8VK971rylzu6A3WUBAMJAGMGocfnkbP1yxRVyuxza8sEJ/dV/71F9c4fdZQEAzoMwglFl4bRc/WL5fCW7HNr+4Und9OPt2vJBnd1lAQDOgTCCUee6CyboxbJFmpmfofrmTt3x1B7944Z31dbJOBIAGIkIIxiVLizM1IsrF+nOa6ZI6tnp95afvqaX3j6uLj9jSQBgJLFMAmzs4fP55PF45PV6lZmZaXc5SDA7DtTrW89VqM7XM36k0JOiFVeXaukVJfKkJdlcHQCMXuH+/SaMYEzwtnbpydcP6ddvHFF9c6ckKS3ZqSWzC7Voeq6unjZeBZ4Um6sEgNGFMAKcRXuXXy+9fVy/2nFIH9Q2Dbg2dfw4XTk1V7MnZmpmfoZm5GfIk0rPCQAMFWEEOAdjjHYdatCrlSf0xsen9O4xrwJn+X9CkSdF0/MzNHX8OE2bME5TJ6Rr6oRxKshMkWVZ8S8cABIIYQSIgLetS7sPNWjP4QZV1jXpw9omHfe2D3p/WrJT0yaka3peuqZNGKdpE9I1ZcI4leaOU0qSM46VA8DIRRgBhsnb1qUDdU366ESzDta36ODJZh082aKqhlZ1n60bRZJlSUWeVE0ZP06Tc9NUktNzFOekqSQ3TZkpvPYBMHYQRoAY6fIHdORUqz4+2ayPTjTr435hxdd+7k36PKlJmpSdqknZqSrOTlNhVqoKPSnKz0xRoSdFeRluuZzMuAcwOhBGgDgzxuh0a1dPD0p9i6obWlXVe1Q3tIZm8ZyPJzVJ2WlJykpLVs64ZHlSk5TudikjxaWMlCRlpLg0zu1UWrJL45L7vk5Nciol2aGUJKdSk5xKItQAsFm4f79dcawJGNUsy1LOuGTljMvR/NKcT1xv6ejW0dNtOnq6VUdPt6m6oVU1vnbVenuOOl+7ugNG3rYuedu6pFOtw6rHYUlul1PuJIfcLoeSXQ4lOx1KcvZ8n9T7tctpKbn3X5fToSRHz78uh9VzzuGQM/S1JafDIafV873D6jnncFhyWpLT2XPN6ZAcliWno+cIft13rt91q/d5R9/3Z35ektPqqdVhKcnlUFLvOafDYiAxMAoQRoA4Ged2aWZBhmYWZJz1eiBg1NDaqcbWTp1u7VJDS8/X3rYuNbV39zu61NrpV0tnt1o7/Gru6FZ7l19tvUewrzNgFDo3mgVDyYBQY/WEFIfVM47HYVkKRpZgeAmdtySr9/yAr3vvsdRzXmee73ctFIcGfI4GfGbwOannOQ243u98v58fPNc/b/UPX598/pP3nXlP/7vOfG7A5/Sr5Ww1DvCJn/PJe87+GWfcc47nznbizBoHfeas95zls872q4WVdQfedNafE85nnbXycJ47856hBfQvXT5Jsyd6hvTscBFGgBHC4bA0Pt2t8enuIf8MY4w6/QG1dfrV2R1QR3dAHd1+tXf1fN3l7zs6uwPq8ht1+QPq9vc81+0PqDtgeg5/z3V/7/f+QO81v5HfGAVC53uPfucCvd/7A0aB4L8BfeLcgOtG6g4EFAj0/OsPmNDnd/XW5T/LwOGe32HEv20GRrzLJmcTRgAMn2VZPa9mXKNzenEgYNQV6AlPwQDV5Q+cEWp6ApExkjFSwPR+reC/PaGt519J/c4HAn3njXpO9v/+zOcVvC94T28mOttnhO7p/V3Mmc+GzvcFq76fZwZ8P/Ba33Ohy4M81z+ynetnn/FjQhfP+vwZD579M87yswcZrjjgdzzjyXPWeM57zv9zzl5LdD7/fPWc7bmw4/U52j+SmiRpRl56uJ8adYQRAAnD4bDkdjjl5r9cwKjCcHsAAGArwggAALAVYQQAANiKMAIAAGxFGAEAALYijAAAAFsRRgAAgK0IIwAAwFaEEQAAYCvCCAAAsBVhBAAA2IowAgAAbEUYAQAAtkqIvS+D2zj7fD6bKwEAAOEK/t0O/h0fTEKEkaamJklScXGxzZUAAIBINTU1yePxDHrdMueLKyNAIBDQ8ePHlZGRIcuyovZzfT6fiouLVV1drczMzKj9XHwSbR1ftHf80NbxQ1vHT7Ta2hijpqYmFRUVyeEYfGRIQvSMOBwOTZo0KWY/PzMzk/9hxwltHV+0d/zQ1vFDW8dPNNr6XD0iQQxgBQAAtiKMAAAAW43pMOJ2u/VP//RPcrvddpcy6tHW8UV7xw9tHT+0dfzEu60TYgArAAAYvcZ0zwgAALAfYQQAANiKMAIAAGxFGAEAALYa02HkscceU2lpqVJSUnTllVdq9+7ddpeU8MrLy3XFFVcoIyNDeXl5uu2221RZWTngnvb2dpWVlSk3N1fp6en6sz/7M9XV1dlU8ejxyCOPyLIsrV69OnSOto6eY8eO6S/+4i+Um5ur1NRUzZkzR3v27AldN8booYceUmFhoVJTU7V48WIdOHDAxooTk9/v13e/+11NmTJFqampmjZtmr73ve8N2NuEth6a7du369Zbb1VRUZEsy9KGDRsGXA+nXRsaGrRs2TJlZmYqKytLd955p5qbm4dfnBmj1q1bZ5KTk82vfvUr895775m77rrLZGVlmbq6OrtLS2g33nijefLJJ83+/ftNRUWFufnmm01JSYlpbm4O3XP33Xeb4uJis3nzZrNnzx5z1VVXmauvvtrGqhPf7t27TWlpqbnkkkvMqlWrQudp6+hoaGgwkydPNl/72tfMrl27zMGDB83vfvc789FHH4XueeSRR4zH4zEbNmwwb7/9tvnc5z5npkyZYtra2mysPPE8/PDDJjc312zcuNEcOnTIPPfccyY9Pd385Cc/Cd1DWw/Nyy+/bB588EHzwgsvGElm/fr1A66H06433XSTmTt3rnnjjTfMa6+9ZqZPn26WLl067NrGbBhZsGCBKSsrC33v9/tNUVGRKS8vt7Gq0efEiRNGktm2bZsxxpjGxkaTlJRknnvuudA9f/zjH40ks3PnTrvKTGhNTU1mxowZZtOmTeb6668PhRHaOnq+/e1vm2uuuWbQ64FAwBQUFJh///d/D51rbGw0brfbPP300/EocdS45ZZbzB133DHg3Be/+EWzbNkyYwxtHS1nhpFw2vX99983ksybb74Zuue3v/2tsSzLHDt2bFj1jMnXNJ2dndq7d68WL14cOudwOLR48WLt3LnTxspGH6/XK0nKycmRJO3du1ddXV0D2n7WrFkqKSmh7YeorKxMt9xyy4A2lWjraHrppZc0f/583X777crLy9O8efP0i1/8InT90KFDqq2tHdDWHo9HV155JW0doauvvlqbN2/Whx9+KEl6++23tWPHDi1ZskQSbR0r4bTrzp07lZWVpfnz54fuWbx4sRwOh3bt2jWsz0+IjfKirb6+Xn6/X/n5+QPO5+fn64MPPrCpqtEnEAho9erVWrRokWbPni1Jqq2tVXJysrKysgbcm5+fr9raWhuqTGzr1q3TW2+9pTfffPMT12jr6Dl48KAef/xx3XffffqHf/gHvfnmm7r33nuVnJysFStWhNrzbP9Noa0j853vfEc+n0+zZs2S0+mU3+/Xww8/rGXLlkkSbR0j4bRrbW2t8vLyBlx3uVzKyckZdtuPyTCC+CgrK9P+/fu1Y8cOu0sZlaqrq7Vq1Spt2rRJKSkpdpczqgUCAc2fP1//9m//JkmaN2+e9u/fr5///OdasWKFzdWNLs8++6zWrFmjtWvX6uKLL1ZFRYVWr16toqIi2noUG5OvacaPHy+n0/mJWQV1dXUqKCiwqarRZeXKldq4caNeffVVTZo0KXS+oKBAnZ2damxsHHA/bR+5vXv36sSJE7rsssvkcrnkcrm0bds2/fSnP5XL5VJ+fj5tHSWFhYW66KKLBpy78MILVVVVJUmh9uS/KcN3//336zvf+Y6+8pWvaM6cOfrLv/xLffOb31R5ebkk2jpWwmnXgoICnThxYsD17u5uNTQ0DLvtx2QYSU5O1uWXX67NmzeHzgUCAW3evFkLFy60sbLEZ4zRypUrtX79em3ZskVTpkwZcP3yyy9XUlLSgLavrKxUVVUVbR+hz372s3r33XdVUVEROubPn69ly5aFvqato2PRokWfmKL+4YcfavLkyZKkKVOmqKCgYEBb+3w+7dq1i7aOUGtrqxyOgX+anE6nAoGAJNo6VsJp14ULF6qxsVF79+4N3bNlyxYFAgFdeeWVwytgWMNfE9i6deuM2+02Tz31lHn//ffNN77xDZOVlWVqa2vtLi2h/c3f/I3xeDxm69atpqamJnS0traG7rn77rtNSUmJ2bJli9mzZ49ZuHChWbhwoY1Vjx79Z9MYQ1tHy+7du43L5TIPP/ywOXDggFmzZo1JS0szv/71r0P3PPLIIyYrK8u8+OKL5p133jGf//znmW46BCtWrDATJ04MTe194YUXzPjx483f//3fh+6hrYemqanJ7Nu3z+zbt89IMj/60Y/Mvn37zJEjR4wx4bXrTTfdZObNm2d27dplduzYYWbMmMHU3uF69NFHTUlJiUlOTjYLFiwwb7zxht0lJTxJZz2efPLJ0D1tbW3mnnvuMdnZ2SYtLc184QtfMDU1NfYVPYqcGUZo6+j5zW9+Y2bPnm3cbreZNWuWeeKJJwZcDwQC5rvf/a7Jz883brfbfPaznzWVlZU2VZu4fD6fWbVqlSkpKTEpKSlm6tSp5sEHHzQdHR2he2jroXn11VfP+t/nFStWGGPCa9dTp06ZpUuXmvT0dJOZmWm+/vWvm6ampmHXZhnTb1k7AACAOBuTY0YAAMDIQRgBAAC2IowAAABbEUYAAICtCCMAAMBWhBEAAGArwggAALAVYQQAANiKMAIAAGxFGAEAALYijAAAAFsRRgAAgK3+P5q+emwfvbWBAAAAAElFTkSuQmCC\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "  model = model.to('cpu')\n",
        "  y_pred = model(x_test)\n",
        "  y_pred = y_pred.detach().numpy()\n",
        "  predicted = np.argmax(y_pred, axis =1)\n",
        "  accuracy = (accuracy_score(predicted, y_test))"
      ],
      "metadata": {
        "id": "h4kJzpLErqhZ"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'model의 output은 :  {y_pred[0]}')\n",
        "print(f'argmax를 한 후의 output은 {predicted[0]}')\n",
        "print(f'accuracy는 {accuracy}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vyIKhs3Nr6Ay",
        "outputId": "6d73d644-c68b-4a84-defc-6f57192ef91a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model의 output은 :  [4.3696738e-03 9.9337065e-01 1.5067915e-03 2.7046577e-04 4.8240164e-04]\n",
            "argmax를 한 후의 output은 1\n",
            "accuracy는 0.9239766081871345\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2.두번째 모델\n",
        "##### 1) Activation function : relu\n",
        "##### 2) Optimizier : SGD\n",
        "##### 3) layer 수 : 4개\n",
        "##### 4) lr : 0.1"
      ],
      "metadata": {
        "id": "8sQ2joYhVPyo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.2모델 정의"
      ],
      "metadata": {
        "id": "owk6xe8pVPyw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "class로 구현 가능\n",
        "- init : 초기 생성 함수\n",
        "- foward : 순전파(입력값 => 예측값 의 과정)"
      ],
      "metadata": {
        "id": "Wz8R9Ll8VPyw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#ReLU\n",
        "class ReLU(torch.nn.ReLU):\n",
        "  def forward(self, x):\n",
        "    return torch.max(torch.zeros_like(x), x)"
      ],
      "metadata": {
        "id": "CYCrVrj0VPyw"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Model(torch.nn.Module):\n",
        "\n",
        "  def __init__(self):\n",
        "    super(Model, self).__init__()\n",
        "    self.layer1 = nn.Sequential(\n",
        "          nn.Linear(64,300, bias=True), # input_layer = 64, hidden_layer1 = 300\n",
        "          #nn.Sigmoid(),\n",
        "          ReLU(),\n",
        "          nn.BatchNorm1d(300)\n",
        "    )\n",
        "  # activation function 이용\n",
        "  #   nn.ReLU()\n",
        "  #   nn.tanH()\n",
        "  #   https://pytorch.org/docs/stable/nn.html 그 외에도 여기서 확인 가능함\n",
        "  #   파라미터가 필요하지 않다는 것이 특징\n",
        "\n",
        "  # batch normazliation 1d, 파라미터 값으로 vector의 길이를 전해줌\n",
        "  # 추후에 이미지를 다루게 된다면, 그때는 batch normalization 2d를 이용하게 됨\n",
        "  # 그때는 파라미터 값으로 채널, 가로, 세로 길이를 전달해주게 됨\n",
        "\n",
        "    self.layer2 = nn.Sequential(\n",
        "          nn.Linear(300,200, bias=True), # hidden_layer1 = 398, hidden_layer2 = 200\n",
        "        #nn.Sigmoid()\n",
        "        ReLU()\n",
        "    )\n",
        "    self.layer3 = nn.Sequential(\n",
        "          nn.Linear(200,50, bias=True), # hidden_layer1 = 200, hidden_layer2 = 50\n",
        "        #nn.Sigmoid()\n",
        "        ReLU()\n",
        "    )\n",
        "    self.layer4 = nn.Sequential(\n",
        "        nn.Linear(50, 10, bias=True), # hidden_layer3 = 50, output_layer = 10\n",
        "        nn.Softmax()\n",
        "    )\n",
        "\n",
        "  def forward(self,x):\n",
        "    output = self.layer1(x)\n",
        "    output = self.layer2(output)\n",
        "    output = self.layer3(output)\n",
        "    output = self.layer4(output)\n",
        "    return output"
      ],
      "metadata": {
        "id": "7SGdO0L_VPyx"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def init_weights(layer):\n",
        "    if isinstance(layer, nn.Linear):\n",
        "        torch.nn.init.xavier_uniform_(layer.weight)\n",
        "        layer.bias.data.fill_(0.01)\n",
        "\n",
        "        #xavier사용\n",
        "        # Layer의 weight를 어떤 분포를 가지도록 초기화시켜줌+global minimum찾기 위해서"
      ],
      "metadata": {
        "id": "KcGVjEtvVPyx"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Model().to(device)\n",
        "model.apply(init_weights)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6609eae2-a1e0-48cf-b950-82ef666a1f08",
        "id": "CD_hX3c-VPyx"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Model(\n",
              "  (layer1): Sequential(\n",
              "    (0): Linear(in_features=64, out_features=300, bias=True)\n",
              "    (1): ReLU()\n",
              "    (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  )\n",
              "  (layer2): Sequential(\n",
              "    (0): Linear(in_features=300, out_features=200, bias=True)\n",
              "    (1): ReLU()\n",
              "  )\n",
              "  (layer3): Sequential(\n",
              "    (0): Linear(in_features=200, out_features=50, bias=True)\n",
              "    (1): ReLU()\n",
              "  )\n",
              "  (layer4): Sequential(\n",
              "    (0): Linear(in_features=50, out_features=10, bias=True)\n",
              "    (1): Softmax(dim=None)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a4373498-59e5-42d2-ab94-2613469ee4e2",
        "id": "rOu17KMkVPyx"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model(\n",
            "  (layer1): Sequential(\n",
            "    (0): Linear(in_features=64, out_features=300, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  )\n",
            "  (layer2): Sequential(\n",
            "    (0): Linear(in_features=300, out_features=200, bias=True)\n",
            "    (1): ReLU()\n",
            "  )\n",
            "  (layer3): Sequential(\n",
            "    (0): Linear(in_features=200, out_features=50, bias=True)\n",
            "    (1): ReLU()\n",
            "  )\n",
            "  (layer4): Sequential(\n",
            "    (0): Linear(in_features=50, out_features=10, bias=True)\n",
            "    (1): Softmax(dim=None)\n",
            "  )\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.2모델 훈련"
      ],
      "metadata": {
        "id": "Ni0EpAJ7VPyy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loss_fn  = nn.CrossEntropyLoss().to(device)\n",
        "\n",
        "# 여러가지 optimizer 시도해보기\n",
        "# lr 바꿔보기\n",
        "\n",
        "\n",
        "#optimizer = optim.Adam(model.parameters(), lr= 0.01)\n",
        "\n",
        "# https://pytorch.org/docs/stable/optim.html#module-torch.optim 페이지 참조\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.1, momentum=0.9)\n",
        "# sgd 등등"
      ],
      "metadata": {
        "id": "Ih7iE-N_VPyy"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "losses = []\n",
        "for epoch in range(100):\n",
        "\n",
        "  optimizer.zero_grad()\n",
        "  hypothesis = model(x_train)\n",
        "\n",
        "  # 비용 함수\n",
        "  cost = loss_fn(hypothesis, y_train)\n",
        "  cost.backward()\n",
        "  optimizer.step()\n",
        "  losses.append(cost.item())\n",
        "\n",
        "  if epoch % 10 == 0:\n",
        "    print(epoch, cost.item())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a05eb896-1a5e-400a-e83d-cf27162afb49",
        "id": "ThZ35BVSVPyy"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1518: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 2.322160243988037\n",
            "10 1.8344483375549316\n",
            "20 1.5800814628601074\n",
            "30 1.5044708251953125\n",
            "40 1.4812170267105103\n",
            "50 1.4729200601577759\n",
            "60 1.4694788455963135\n",
            "70 1.4675347805023193\n",
            "80 1.4664713144302368\n",
            "90 1.4657554626464844\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(losses)\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 430
        },
        "outputId": "4bf5df5a-75ec-486a-b1fb-6e417bb8275f",
        "id": "nWtp2zFlVPyy"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "  model = model.to('cpu')\n",
        "  y_pred = model(x_test)\n",
        "  y_pred = y_pred.detach().numpy()\n",
        "  predicted = np.argmax(y_pred, axis =1)\n",
        "  accuracy = (accuracy_score(predicted, y_test))"
      ],
      "metadata": {
        "id": "3w4alJMPVPyz"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'model의 output은 :  {y_pred[0]}')\n",
        "print(f'argmax를 한 후의 output은 {predicted[0]}')\n",
        "print(f'accuracy는 {accuracy}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8dea2015-a441-4138-8985-eda9ce98b29c",
        "id": "5i8mNGOdVPyz"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model의 output은 :  [4.0917487e-09 9.9883479e-01 6.9145399e-06 5.9836989e-06 3.0534504e-06\n",
            " 1.5437616e-05 8.8067063e-06 1.7407635e-09 1.1250229e-03 5.6536479e-08]\n",
            "argmax를 한 후의 output은 1\n",
            "accuracy는 0.975925925925926\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3.세번째 모델\n",
        "##### 1) Activation function : Sigmoid\n",
        "##### 2) Optimizier : Adam\n",
        "##### 3) layer 수 : 5개\n",
        "##### 4) lr : 0.05"
      ],
      "metadata": {
        "id": "GAti8Bo-WrzX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.3모델 정의"
      ],
      "metadata": {
        "id": "XgsigUS6Wrzg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "class로 구현 가능\n",
        "- init : 초기 생성 함수\n",
        "- foward : 순전파(입력값 => 예측값 의 과정)"
      ],
      "metadata": {
        "id": "L_uRWq9TWrzg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Model(torch.nn.Module):\n",
        "\n",
        "  def __init__(self):\n",
        "    super(Model, self).__init__()\n",
        "    self.layer1 = nn.Sequential(\n",
        "          nn.Linear(64,300, bias=True), # input_layer = 64, hidden_layer1 = 300\n",
        "          nn.Sigmoid(),\n",
        "          #ReLU(),\n",
        "          nn.BatchNorm1d(300)\n",
        "    )\n",
        "  # activation function 이용\n",
        "  #   nn.ReLU()\n",
        "  #   nn.tanH()\n",
        "  #   https://pytorch.org/docs/stable/nn.html 그 외에도 여기서 확인 가능함\n",
        "  #   파라미터가 필요하지 않다는 것이 특징\n",
        "\n",
        "  # batch normazliation 1d, 파라미터 값으로 vector의 길이를 전해줌\n",
        "  # 추후에 이미지를 다루게 된다면, 그때는 batch normalization 2d를 이용하게 됨\n",
        "  # 그때는 파라미터 값으로 채널, 가로, 세로 길이를 전달해주게 됨\n",
        "\n",
        "    self.layer2 = nn.Sequential(\n",
        "          nn.Linear(300,200, bias=True), # hidden_layer1 = 398, hidden_layer2 = 200\n",
        "        nn.Sigmoid()\n",
        "        #ReLU()\n",
        "    )\n",
        "    self.layer3 = nn.Sequential(\n",
        "          nn.Linear(200,100, bias=True), # hidden_layer2 = 200, hidden_layer2 = 50\n",
        "        nn.Sigmoid()\n",
        "        #ReLU()\n",
        "    )\n",
        "    self.layer4 = nn.Sequential(\n",
        "          nn.Linear(100,50, bias=True), # hidden_layer3 = 100, hidden_layer4 = 50\n",
        "        nn.Sigmoid()\n",
        "        #ReLU()\n",
        "    )\n",
        "    self.layer5 = nn.Sequential(\n",
        "        nn.Linear(50, 10, bias=True), # hidden_layer4 = 50, output_layer = 10\n",
        "        nn.Softmax()\n",
        "    )\n",
        "\n",
        "  def forward(self,x):\n",
        "    output = self.layer1(x)\n",
        "    output = self.layer2(output)\n",
        "    output = self.layer3(output)\n",
        "    output = self.layer4(output)\n",
        "    output = self.layer5(output)\n",
        "    return output"
      ],
      "metadata": {
        "id": "vpyFIn3qWrzg"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def init_weights(layer):\n",
        "    if isinstance(layer, nn.Linear):\n",
        "        torch.nn.init.xavier_uniform_(layer.weight)\n",
        "        layer.bias.data.fill_(0.01)\n",
        "\n",
        "        #xavier사용\n",
        "        # Layer의 weight를 어떤 분포를 가지도록 초기화시켜줌+global minimum찾기 위해서"
      ],
      "metadata": {
        "id": "210R_6TjWrzg"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Model().to(device)\n",
        "model.apply(init_weights)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4a61ca5e-eec4-4b88-9d32-3fdcff2dc3c6",
        "id": "U8AhEWa3Wrzh"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Model(\n",
              "  (layer1): Sequential(\n",
              "    (0): Linear(in_features=64, out_features=300, bias=True)\n",
              "    (1): Sigmoid()\n",
              "    (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  )\n",
              "  (layer2): Sequential(\n",
              "    (0): Linear(in_features=300, out_features=200, bias=True)\n",
              "    (1): Sigmoid()\n",
              "  )\n",
              "  (layer3): Sequential(\n",
              "    (0): Linear(in_features=200, out_features=100, bias=True)\n",
              "    (1): Sigmoid()\n",
              "  )\n",
              "  (layer4): Sequential(\n",
              "    (0): Linear(in_features=100, out_features=50, bias=True)\n",
              "    (1): Sigmoid()\n",
              "  )\n",
              "  (layer5): Sequential(\n",
              "    (0): Linear(in_features=50, out_features=10, bias=True)\n",
              "    (1): Softmax(dim=None)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.3모델 훈련"
      ],
      "metadata": {
        "id": "DNYG-6hsWrzh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loss_fn  = nn.CrossEntropyLoss().to(device)\n",
        "\n",
        "# 여러가지 optimizer 시도해보기\n",
        "# lr 바꿔보기\n",
        "\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr= 0.05)\n",
        "\n",
        "# https://pytorch.org/docs/stable/optim.html#module-torch.optim 페이지 참조\n",
        "#optimizer = optim.SGD(model.parameters(), lr=0.1, momentum=0.9)\n",
        "# sgd 등등"
      ],
      "metadata": {
        "id": "ILZ2AKy9Wrzh"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "losses = []\n",
        "for epoch in range(100):\n",
        "\n",
        "  optimizer.zero_grad()\n",
        "  hypothesis = model(x_train)\n",
        "\n",
        "  # 비용 함수\n",
        "  cost = loss_fn(hypothesis, y_train)\n",
        "  cost.backward()\n",
        "  optimizer.step()\n",
        "  losses.append(cost.item())\n",
        "\n",
        "  if epoch % 10 == 0:\n",
        "    print(epoch, cost.item())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e4893607-3b96-48b2-da80-19ac7e492cf5",
        "id": "ZHEvG7g3Wrzh"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1518: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 2.304457426071167\n",
            "10 1.8552782535552979\n",
            "20 1.7323659658432007\n",
            "30 1.5780961513519287\n",
            "40 1.5665569305419922\n",
            "50 1.5630398988723755\n",
            "60 1.5633296966552734\n",
            "70 1.5618292093276978\n",
            "80 1.561964988708496\n",
            "90 1.5621623992919922\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(losses)\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 430
        },
        "outputId": "c992e449-2e37-42f9-d158-54bb7f4cbe49",
        "id": "jNzptuqYWrzh"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "  model = model.to('cpu')\n",
        "  y_pred = model(x_test)\n",
        "  y_pred = y_pred.detach().numpy()\n",
        "  predicted = np.argmax(y_pred, axis =1)\n",
        "  accuracy = (accuracy_score(predicted, y_test))"
      ],
      "metadata": {
        "id": "_otZpU5zWrzi"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'model의 output은 :  {y_pred[0]}')\n",
        "print(f'argmax를 한 후의 output은 {predicted[0]}')\n",
        "print(f'accuracy는 {accuracy}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2dbe7b34-e0fd-4759-c913-d738469bac59",
        "id": "6CuI9GPpWrzi"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model의 output은 :  [2.8432281e-09 9.9991214e-01 4.4956041e-06 2.9786650e-06 7.3196927e-05\n",
            " 7.5175313e-07 6.0896564e-06 3.2383107e-07 8.9383292e-09 1.0477003e-09]\n",
            "argmax를 한 후의 output은 1\n",
            "accuracy는 0.8814814814814815\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3.네번째 모델\n",
        "##### 1) Activation function : Sigmoid\n",
        "##### 2) Optimizier : SGD\n",
        "##### 3) layer 수 : 4개\n",
        "##### 4) lr : 0.2"
      ],
      "metadata": {
        "id": "7Iso_5KmXkpt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.3모델 정의"
      ],
      "metadata": {
        "id": "k98l4s0wXkp1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "class로 구현 가능\n",
        "- init : 초기 생성 함수\n",
        "- foward : 순전파(입력값 => 예측값 의 과정)"
      ],
      "metadata": {
        "id": "ZMEC3ZVCXkp1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Model(torch.nn.Module):\n",
        "\n",
        "  def __init__(self):\n",
        "    super(Model, self).__init__()\n",
        "    self.layer1 = nn.Sequential(\n",
        "          nn.Linear(64,300, bias=True), # input_layer = 64, hidden_layer1 = 300\n",
        "          nn.Sigmoid(),\n",
        "          #ReLU(),\n",
        "          nn.BatchNorm1d(300)\n",
        "    )\n",
        "  # activation function 이용\n",
        "  #   nn.ReLU()\n",
        "  #   nn.tanH()\n",
        "  #   https://pytorch.org/docs/stable/nn.html 그 외에도 여기서 확인 가능함\n",
        "  #   파라미터가 필요하지 않다는 것이 특징\n",
        "\n",
        "  # batch normazliation 1d, 파라미터 값으로 vector의 길이를 전해줌\n",
        "  # 추후에 이미지를 다루게 된다면, 그때는 batch normalization 2d를 이용하게 됨\n",
        "  # 그때는 파라미터 값으로 채널, 가로, 세로 길이를 전달해주게 됨\n",
        "\n",
        "    self.layer2 = nn.Sequential(\n",
        "          nn.Linear(300,200, bias=True), # hidden_layer1 = 398, hidden_layer2 = 200\n",
        "        nn.Sigmoid()\n",
        "        #ReLU()\n",
        "    )\n",
        "    self.layer3 = nn.Sequential(\n",
        "          nn.Linear(200,100, bias=True), # hidden_layer2 = 200, hidden_layer2 = 50\n",
        "        nn.Sigmoid()\n",
        "        #ReLU()\n",
        "    )\n",
        "\n",
        "    self.layer4 = nn.Sequential(\n",
        "        nn.Linear(100, 10, bias=True), # hidden_layer4 = 50, output_layer = 10\n",
        "        nn.Softmax()\n",
        "    )\n",
        "\n",
        "  def forward(self,x):\n",
        "    output = self.layer1(x)\n",
        "    output = self.layer2(output)\n",
        "    output = self.layer3(output)\n",
        "    output = self.layer4(output)\n",
        "    return output"
      ],
      "metadata": {
        "id": "D71rev6CXkp2"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def init_weights(layer):\n",
        "    if isinstance(layer, nn.Linear):\n",
        "        torch.nn.init.xavier_uniform_(layer.weight)\n",
        "        layer.bias.data.fill_(0.01)\n",
        "\n",
        "        #xavier사용\n",
        "        # Layer의 weight를 어떤 분포를 가지도록 초기화시켜줌+global minimum찾기 위해서"
      ],
      "metadata": {
        "id": "A5G13PTZXkp2"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Model().to(device)\n",
        "model.apply(init_weights)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3a9f5dfd-e26d-443a-efb6-1f684086476e",
        "id": "deG0kJ3DXkp2"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Model(\n",
              "  (layer1): Sequential(\n",
              "    (0): Linear(in_features=64, out_features=300, bias=True)\n",
              "    (1): Sigmoid()\n",
              "    (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  )\n",
              "  (layer2): Sequential(\n",
              "    (0): Linear(in_features=300, out_features=200, bias=True)\n",
              "    (1): Sigmoid()\n",
              "  )\n",
              "  (layer3): Sequential(\n",
              "    (0): Linear(in_features=200, out_features=100, bias=True)\n",
              "    (1): Sigmoid()\n",
              "  )\n",
              "  (layer4): Sequential(\n",
              "    (0): Linear(in_features=100, out_features=10, bias=True)\n",
              "    (1): Softmax(dim=None)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.3모델 훈련"
      ],
      "metadata": {
        "id": "678IdqelXkp3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loss_fn  = nn.CrossEntropyLoss().to(device)\n",
        "\n",
        "# 여러가지 optimizer 시도해보기\n",
        "# lr 바꿔보기\n",
        "\n",
        "\n",
        "#optimizer = optim.Adam(model.parameters(), lr= 0.05)\n",
        "\n",
        "# https://pytorch.org/docs/stable/optim.html#module-torch.optim 페이지 참조\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.2, momentum=0.9)\n",
        "# sgd 등등"
      ],
      "metadata": {
        "id": "Al1meAXpXkp3"
      },
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "losses = []\n",
        "for epoch in range(100):\n",
        "\n",
        "  optimizer.zero_grad()\n",
        "  hypothesis = model(x_train)\n",
        "\n",
        "  # 비용 함수\n",
        "  cost = loss_fn(hypothesis, y_train)\n",
        "  cost.backward()\n",
        "  optimizer.step()\n",
        "  losses.append(cost.item())\n",
        "\n",
        "  if epoch % 10 == 0:\n",
        "    print(epoch, cost.item())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b3d5cd9d-f0e6-492b-b85a-a0ef34f7fa95",
        "id": "21UyJV_dXkp3"
      },
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1518: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 1.579978346824646\n",
            "10 1.5494998693466187\n",
            "20 1.5143201351165771\n",
            "30 1.4942295551300049\n",
            "40 1.4836945533752441\n",
            "50 1.4774929285049438\n",
            "60 1.4738132953643799\n",
            "70 1.4714299440383911\n",
            "80 1.469846248626709\n",
            "90 1.4685195684432983\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(losses)\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 430
        },
        "outputId": "c18f4026-94f8-44ad-ba62-93a4de8aa0a2",
        "id": "GADX_xwwXkp3"
      },
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "  model = model.to('cpu')\n",
        "  y_pred = model(x_test)\n",
        "  y_pred = y_pred.detach().numpy()\n",
        "  predicted = np.argmax(y_pred, axis =1)\n",
        "  accuracy = (accuracy_score(predicted, y_test))"
      ],
      "metadata": {
        "id": "RLV0k5NVXkp3"
      },
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'model의 output은 :  {y_pred[0]}')\n",
        "print(f'argmax를 한 후의 output은 {predicted[0]}')\n",
        "print(f'accuracy는 {accuracy}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1ba68bc3-77a1-4e7e-b72c-1ba31cac74b1",
        "id": "K2keWngxXkp3"
      },
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model의 output은 :  [1.3947553e-06 9.9753714e-01 7.4281459e-05 1.7567746e-06 3.9705930e-05\n",
            " 1.5317857e-05 1.0099827e-03 7.2198382e-06 1.3116567e-03 1.5901923e-06]\n",
            "argmax를 한 후의 output은 1\n",
            "accuracy는 0.9777777777777777\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 최종 결과\n",
        "모델1[RELU + ADAM]\n",
        "\n",
        "accuracy는 0.9239766081871345\n",
        "\n",
        "----\n",
        "모델2[RELU + SGD]\n",
        "\n",
        "accuracy는 0.975925925925926\n",
        "\n",
        "----\n",
        "\n",
        "모델3[Sigmoid + ADAM]\n",
        "\n",
        "accuracy는 0.8814814814814815\n",
        "\n",
        "----\n",
        "\n",
        "모델4[Sigmoid + SGD]\n",
        "\n",
        "accuracy는 0.9777777777777777"
      ],
      "metadata": {
        "id": "7HK0yQTvVqwQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# < 3주차 과제 2 : CNN 맛보기>"
      ],
      "metadata": {
        "id": "3RzRM7xThZV_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from __future__ import print_function\n",
        "import argparse\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.autograd import Variable"
      ],
      "metadata": {
        "id": "56xqgtLxhZw6"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training settings\n",
        "\n",
        "batch_size = 64\n",
        "\n",
        "# MNIST Dataset\n",
        "train_dataset = datasets.MNIST(root='./data/',\n",
        "                               train=True,\n",
        "                               transform=transforms.ToTensor(),\n",
        "                               download=True)\n",
        "\n",
        "test_dataset = datasets.MNIST(root='./data/',\n",
        "                              train=False,\n",
        "                              transform=transforms.ToTensor())\n",
        "\n",
        "# Data Loader (Input Pipeline)\n",
        "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
        "                                           batch_size=batch_size,\n",
        "                                           shuffle=True)\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
        "                                          batch_size=batch_size,\n",
        "                                          shuffle=False)"
      ],
      "metadata": {
        "id": "TzkF2bFNhcQ2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ebbf3070-e389-4556-fcd6-986cee80a328"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9912422/9912422 [00:00<00:00, 175753460.03it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28881/28881 [00:00<00:00, 45849997.66it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1648877/1648877 [00:00<00:00, 47140519.92it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4542/4542 [00:00<00:00, 20528587.03it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Net(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(Net, self).__init__()\n",
        "    self.conv1 = nn.Conv2d(1, 10, kernel_size = 5)\n",
        "    self.conv2 = nn.Conv2d(10, 20, kernel_size = 5)\n",
        "    self.mp = nn.MaxPool2d(2)\n",
        "    self.fc = nn.Linear(320 , 10)\n",
        "\n",
        "  def forward(self, x):\n",
        "    in_size = x.size(0)\n",
        "    x = F.relu(self.mp(self.conv1(x)))\n",
        "    x = F.relu(self.mp(self.conv2(x)))\n",
        "    x = x.view(in_size, -1)\n",
        "    x = self.fc(x)\n",
        "    return F.log_softmax(x)"
      ],
      "metadata": {
        "id": "tLCSvgganBrH"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Net()\n",
        "optimizer = optim.SGD(model.parameters(), lr = 0.01, momentum = 0.5)"
      ],
      "metadata": {
        "id": "lkYZ4pUdnUHc"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(epoch):\n",
        "  model.train()\n",
        "  for batch_idx, (data, target) in enumerate(train_loader):\n",
        "    data, target = Variable(data), Variable(target)\n",
        "    optimizer.zero_grad()\n",
        "    output = model(data)\n",
        "    loss = F.nll_loss(output, target)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    if batch_idx % 10 == 0:\n",
        "      print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "          epoch, batch_idx * len(data), len(train_loader.dataset),\n",
        "          100. * batch_idx / len(train_loader), loss.item()))"
      ],
      "metadata": {
        "id": "IzUrEM3EnXJb"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test():\n",
        "    model.eval() #model.eval() 의 기능은?\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    for data, target in test_loader:\n",
        "        data, target = Variable(data, volatile=True), Variable(target)\n",
        "        output = model(data)\n",
        "        test_loss += F.nll_loss(output, target, size_average=False).data # nll_loss?? / cross entropy loss와의 관계 확인!\n",
        "        pred = output.data.max(1, keepdim=True)[1]\n",
        "        correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
        "\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "        test_loss, correct, len(test_loader.dataset),\n",
        "        100. * correct / len(test_loader.dataset)))"
      ],
      "metadata": {
        "id": "EFi0gYJGn2aa"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(1, 10):\n",
        "    train(epoch)\n",
        "    test()"
      ],
      "metadata": {
        "id": "zSvSZb_Bn4Nx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "92f6ed71-84aa-47b1-f52a-e06fbc1e3f07"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-7-006ecbc6c563>:15: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return F.log_softmax(x)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.302332\n",
            "Train Epoch: 1 [640/60000 (1%)]\tLoss: 2.298229\n",
            "Train Epoch: 1 [1280/60000 (2%)]\tLoss: 2.301676\n",
            "Train Epoch: 1 [1920/60000 (3%)]\tLoss: 2.262216\n",
            "Train Epoch: 1 [2560/60000 (4%)]\tLoss: 2.265425\n",
            "Train Epoch: 1 [3200/60000 (5%)]\tLoss: 2.208381\n",
            "Train Epoch: 1 [3840/60000 (6%)]\tLoss: 2.187509\n",
            "Train Epoch: 1 [4480/60000 (7%)]\tLoss: 2.167179\n",
            "Train Epoch: 1 [5120/60000 (9%)]\tLoss: 2.114434\n",
            "Train Epoch: 1 [5760/60000 (10%)]\tLoss: 2.051374\n",
            "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 1.918115\n",
            "Train Epoch: 1 [7040/60000 (12%)]\tLoss: 1.760977\n",
            "Train Epoch: 1 [7680/60000 (13%)]\tLoss: 1.562535\n",
            "Train Epoch: 1 [8320/60000 (14%)]\tLoss: 1.353705\n",
            "Train Epoch: 1 [8960/60000 (15%)]\tLoss: 1.031303\n",
            "Train Epoch: 1 [9600/60000 (16%)]\tLoss: 0.982493\n",
            "Train Epoch: 1 [10240/60000 (17%)]\tLoss: 0.683455\n",
            "Train Epoch: 1 [10880/60000 (18%)]\tLoss: 0.646926\n",
            "Train Epoch: 1 [11520/60000 (19%)]\tLoss: 0.673146\n",
            "Train Epoch: 1 [12160/60000 (20%)]\tLoss: 0.734329\n",
            "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.445926\n",
            "Train Epoch: 1 [13440/60000 (22%)]\tLoss: 0.507036\n",
            "Train Epoch: 1 [14080/60000 (23%)]\tLoss: 0.466528\n",
            "Train Epoch: 1 [14720/60000 (25%)]\tLoss: 0.476599\n",
            "Train Epoch: 1 [15360/60000 (26%)]\tLoss: 0.613837\n",
            "Train Epoch: 1 [16000/60000 (27%)]\tLoss: 0.311390\n",
            "Train Epoch: 1 [16640/60000 (28%)]\tLoss: 0.455175\n",
            "Train Epoch: 1 [17280/60000 (29%)]\tLoss: 0.482060\n",
            "Train Epoch: 1 [17920/60000 (30%)]\tLoss: 0.467303\n",
            "Train Epoch: 1 [18560/60000 (31%)]\tLoss: 0.554587\n",
            "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.427109\n",
            "Train Epoch: 1 [19840/60000 (33%)]\tLoss: 0.272963\n",
            "Train Epoch: 1 [20480/60000 (34%)]\tLoss: 0.296342\n",
            "Train Epoch: 1 [21120/60000 (35%)]\tLoss: 0.403911\n",
            "Train Epoch: 1 [21760/60000 (36%)]\tLoss: 0.381528\n",
            "Train Epoch: 1 [22400/60000 (37%)]\tLoss: 0.291131\n",
            "Train Epoch: 1 [23040/60000 (38%)]\tLoss: 0.215068\n",
            "Train Epoch: 1 [23680/60000 (39%)]\tLoss: 0.363191\n",
            "Train Epoch: 1 [24320/60000 (41%)]\tLoss: 0.319822\n",
            "Train Epoch: 1 [24960/60000 (42%)]\tLoss: 0.270424\n",
            "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.503828\n",
            "Train Epoch: 1 [26240/60000 (44%)]\tLoss: 0.412424\n",
            "Train Epoch: 1 [26880/60000 (45%)]\tLoss: 0.260332\n",
            "Train Epoch: 1 [27520/60000 (46%)]\tLoss: 0.436771\n",
            "Train Epoch: 1 [28160/60000 (47%)]\tLoss: 0.305672\n",
            "Train Epoch: 1 [28800/60000 (48%)]\tLoss: 0.177745\n",
            "Train Epoch: 1 [29440/60000 (49%)]\tLoss: 0.160495\n",
            "Train Epoch: 1 [30080/60000 (50%)]\tLoss: 0.304626\n",
            "Train Epoch: 1 [30720/60000 (51%)]\tLoss: 0.250503\n",
            "Train Epoch: 1 [31360/60000 (52%)]\tLoss: 0.095222\n",
            "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.337812\n",
            "Train Epoch: 1 [32640/60000 (54%)]\tLoss: 0.203387\n",
            "Train Epoch: 1 [33280/60000 (55%)]\tLoss: 0.216131\n",
            "Train Epoch: 1 [33920/60000 (57%)]\tLoss: 0.208711\n",
            "Train Epoch: 1 [34560/60000 (58%)]\tLoss: 0.419756\n",
            "Train Epoch: 1 [35200/60000 (59%)]\tLoss: 0.144232\n",
            "Train Epoch: 1 [35840/60000 (60%)]\tLoss: 0.213816\n",
            "Train Epoch: 1 [36480/60000 (61%)]\tLoss: 0.128845\n",
            "Train Epoch: 1 [37120/60000 (62%)]\tLoss: 0.133732\n",
            "Train Epoch: 1 [37760/60000 (63%)]\tLoss: 0.146163\n",
            "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.195010\n",
            "Train Epoch: 1 [39040/60000 (65%)]\tLoss: 0.146514\n",
            "Train Epoch: 1 [39680/60000 (66%)]\tLoss: 0.127239\n",
            "Train Epoch: 1 [40320/60000 (67%)]\tLoss: 0.126332\n",
            "Train Epoch: 1 [40960/60000 (68%)]\tLoss: 0.083662\n",
            "Train Epoch: 1 [41600/60000 (69%)]\tLoss: 0.334653\n",
            "Train Epoch: 1 [42240/60000 (70%)]\tLoss: 0.200982\n",
            "Train Epoch: 1 [42880/60000 (71%)]\tLoss: 0.205119\n",
            "Train Epoch: 1 [43520/60000 (72%)]\tLoss: 0.153895\n",
            "Train Epoch: 1 [44160/60000 (74%)]\tLoss: 0.126913\n",
            "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.163321\n",
            "Train Epoch: 1 [45440/60000 (76%)]\tLoss: 0.180999\n",
            "Train Epoch: 1 [46080/60000 (77%)]\tLoss: 0.256524\n",
            "Train Epoch: 1 [46720/60000 (78%)]\tLoss: 0.131576\n",
            "Train Epoch: 1 [47360/60000 (79%)]\tLoss: 0.230732\n",
            "Train Epoch: 1 [48000/60000 (80%)]\tLoss: 0.403513\n",
            "Train Epoch: 1 [48640/60000 (81%)]\tLoss: 0.146778\n",
            "Train Epoch: 1 [49280/60000 (82%)]\tLoss: 0.122118\n",
            "Train Epoch: 1 [49920/60000 (83%)]\tLoss: 0.256554\n",
            "Train Epoch: 1 [50560/60000 (84%)]\tLoss: 0.271236\n",
            "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.240420\n",
            "Train Epoch: 1 [51840/60000 (86%)]\tLoss: 0.265123\n",
            "Train Epoch: 1 [52480/60000 (87%)]\tLoss: 0.152717\n",
            "Train Epoch: 1 [53120/60000 (88%)]\tLoss: 0.436251\n",
            "Train Epoch: 1 [53760/60000 (90%)]\tLoss: 0.190447\n",
            "Train Epoch: 1 [54400/60000 (91%)]\tLoss: 0.232781\n",
            "Train Epoch: 1 [55040/60000 (92%)]\tLoss: 0.128786\n",
            "Train Epoch: 1 [55680/60000 (93%)]\tLoss: 0.176770\n",
            "Train Epoch: 1 [56320/60000 (94%)]\tLoss: 0.175832\n",
            "Train Epoch: 1 [56960/60000 (95%)]\tLoss: 0.138598\n",
            "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.071361\n",
            "Train Epoch: 1 [58240/60000 (97%)]\tLoss: 0.142993\n",
            "Train Epoch: 1 [58880/60000 (98%)]\tLoss: 0.234366\n",
            "Train Epoch: 1 [59520/60000 (99%)]\tLoss: 0.209756\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-10-f52337105c2a>:6: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
            "  data, target = Variable(data, volatile=True), Variable(target)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
            "  warnings.warn(warning.format(ret))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.1864, Accuracy: 9447/10000 (94%)\n",
            "\n",
            "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.173064\n",
            "Train Epoch: 2 [640/60000 (1%)]\tLoss: 0.030447\n",
            "Train Epoch: 2 [1280/60000 (2%)]\tLoss: 0.113774\n",
            "Train Epoch: 2 [1920/60000 (3%)]\tLoss: 0.104233\n",
            "Train Epoch: 2 [2560/60000 (4%)]\tLoss: 0.156383\n",
            "Train Epoch: 2 [3200/60000 (5%)]\tLoss: 0.134238\n",
            "Train Epoch: 2 [3840/60000 (6%)]\tLoss: 0.162788\n",
            "Train Epoch: 2 [4480/60000 (7%)]\tLoss: 0.289906\n",
            "Train Epoch: 2 [5120/60000 (9%)]\tLoss: 0.164436\n",
            "Train Epoch: 2 [5760/60000 (10%)]\tLoss: 0.126052\n",
            "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 0.145450\n",
            "Train Epoch: 2 [7040/60000 (12%)]\tLoss: 0.173237\n",
            "Train Epoch: 2 [7680/60000 (13%)]\tLoss: 0.173301\n",
            "Train Epoch: 2 [8320/60000 (14%)]\tLoss: 0.256671\n",
            "Train Epoch: 2 [8960/60000 (15%)]\tLoss: 0.125322\n",
            "Train Epoch: 2 [9600/60000 (16%)]\tLoss: 0.111044\n",
            "Train Epoch: 2 [10240/60000 (17%)]\tLoss: 0.146597\n",
            "Train Epoch: 2 [10880/60000 (18%)]\tLoss: 0.178909\n",
            "Train Epoch: 2 [11520/60000 (19%)]\tLoss: 0.192429\n",
            "Train Epoch: 2 [12160/60000 (20%)]\tLoss: 0.226078\n",
            "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 0.222415\n",
            "Train Epoch: 2 [13440/60000 (22%)]\tLoss: 0.295885\n",
            "Train Epoch: 2 [14080/60000 (23%)]\tLoss: 0.136700\n",
            "Train Epoch: 2 [14720/60000 (25%)]\tLoss: 0.157140\n",
            "Train Epoch: 2 [15360/60000 (26%)]\tLoss: 0.077989\n",
            "Train Epoch: 2 [16000/60000 (27%)]\tLoss: 0.136002\n",
            "Train Epoch: 2 [16640/60000 (28%)]\tLoss: 0.275008\n",
            "Train Epoch: 2 [17280/60000 (29%)]\tLoss: 0.282741\n",
            "Train Epoch: 2 [17920/60000 (30%)]\tLoss: 0.112671\n",
            "Train Epoch: 2 [18560/60000 (31%)]\tLoss: 0.117363\n",
            "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 0.135170\n",
            "Train Epoch: 2 [19840/60000 (33%)]\tLoss: 0.267277\n",
            "Train Epoch: 2 [20480/60000 (34%)]\tLoss: 0.209524\n",
            "Train Epoch: 2 [21120/60000 (35%)]\tLoss: 0.101900\n",
            "Train Epoch: 2 [21760/60000 (36%)]\tLoss: 0.074099\n",
            "Train Epoch: 2 [22400/60000 (37%)]\tLoss: 0.182266\n",
            "Train Epoch: 2 [23040/60000 (38%)]\tLoss: 0.192082\n",
            "Train Epoch: 2 [23680/60000 (39%)]\tLoss: 0.114172\n",
            "Train Epoch: 2 [24320/60000 (41%)]\tLoss: 0.092134\n",
            "Train Epoch: 2 [24960/60000 (42%)]\tLoss: 0.137204\n",
            "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.076882\n",
            "Train Epoch: 2 [26240/60000 (44%)]\tLoss: 0.165739\n",
            "Train Epoch: 2 [26880/60000 (45%)]\tLoss: 0.192528\n",
            "Train Epoch: 2 [27520/60000 (46%)]\tLoss: 0.028776\n",
            "Train Epoch: 2 [28160/60000 (47%)]\tLoss: 0.152591\n",
            "Train Epoch: 2 [28800/60000 (48%)]\tLoss: 0.093463\n",
            "Train Epoch: 2 [29440/60000 (49%)]\tLoss: 0.083424\n",
            "Train Epoch: 2 [30080/60000 (50%)]\tLoss: 0.139215\n",
            "Train Epoch: 2 [30720/60000 (51%)]\tLoss: 0.127336\n",
            "Train Epoch: 2 [31360/60000 (52%)]\tLoss: 0.054709\n",
            "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 0.168695\n",
            "Train Epoch: 2 [32640/60000 (54%)]\tLoss: 0.242910\n",
            "Train Epoch: 2 [33280/60000 (55%)]\tLoss: 0.233712\n",
            "Train Epoch: 2 [33920/60000 (57%)]\tLoss: 0.058918\n",
            "Train Epoch: 2 [34560/60000 (58%)]\tLoss: 0.347066\n",
            "Train Epoch: 2 [35200/60000 (59%)]\tLoss: 0.111013\n",
            "Train Epoch: 2 [35840/60000 (60%)]\tLoss: 0.220711\n",
            "Train Epoch: 2 [36480/60000 (61%)]\tLoss: 0.193008\n",
            "Train Epoch: 2 [37120/60000 (62%)]\tLoss: 0.186006\n",
            "Train Epoch: 2 [37760/60000 (63%)]\tLoss: 0.369755\n",
            "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 0.054476\n",
            "Train Epoch: 2 [39040/60000 (65%)]\tLoss: 0.074395\n",
            "Train Epoch: 2 [39680/60000 (66%)]\tLoss: 0.174371\n",
            "Train Epoch: 2 [40320/60000 (67%)]\tLoss: 0.038786\n",
            "Train Epoch: 2 [40960/60000 (68%)]\tLoss: 0.172752\n",
            "Train Epoch: 2 [41600/60000 (69%)]\tLoss: 0.238927\n",
            "Train Epoch: 2 [42240/60000 (70%)]\tLoss: 0.082962\n",
            "Train Epoch: 2 [42880/60000 (71%)]\tLoss: 0.032855\n",
            "Train Epoch: 2 [43520/60000 (72%)]\tLoss: 0.247954\n",
            "Train Epoch: 2 [44160/60000 (74%)]\tLoss: 0.038067\n",
            "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 0.224274\n",
            "Train Epoch: 2 [45440/60000 (76%)]\tLoss: 0.078602\n",
            "Train Epoch: 2 [46080/60000 (77%)]\tLoss: 0.103118\n",
            "Train Epoch: 2 [46720/60000 (78%)]\tLoss: 0.098607\n",
            "Train Epoch: 2 [47360/60000 (79%)]\tLoss: 0.120065\n",
            "Train Epoch: 2 [48000/60000 (80%)]\tLoss: 0.183571\n",
            "Train Epoch: 2 [48640/60000 (81%)]\tLoss: 0.181581\n",
            "Train Epoch: 2 [49280/60000 (82%)]\tLoss: 0.133141\n",
            "Train Epoch: 2 [49920/60000 (83%)]\tLoss: 0.044648\n",
            "Train Epoch: 2 [50560/60000 (84%)]\tLoss: 0.152483\n",
            "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.255512\n",
            "Train Epoch: 2 [51840/60000 (86%)]\tLoss: 0.088418\n",
            "Train Epoch: 2 [52480/60000 (87%)]\tLoss: 0.045880\n",
            "Train Epoch: 2 [53120/60000 (88%)]\tLoss: 0.111811\n",
            "Train Epoch: 2 [53760/60000 (90%)]\tLoss: 0.157050\n",
            "Train Epoch: 2 [54400/60000 (91%)]\tLoss: 0.115328\n",
            "Train Epoch: 2 [55040/60000 (92%)]\tLoss: 0.074696\n",
            "Train Epoch: 2 [55680/60000 (93%)]\tLoss: 0.203757\n",
            "Train Epoch: 2 [56320/60000 (94%)]\tLoss: 0.155952\n",
            "Train Epoch: 2 [56960/60000 (95%)]\tLoss: 0.098613\n",
            "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 0.159689\n",
            "Train Epoch: 2 [58240/60000 (97%)]\tLoss: 0.049768\n",
            "Train Epoch: 2 [58880/60000 (98%)]\tLoss: 0.103592\n",
            "Train Epoch: 2 [59520/60000 (99%)]\tLoss: 0.172219\n",
            "\n",
            "Test set: Average loss: 0.1378, Accuracy: 9570/10000 (96%)\n",
            "\n",
            "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.157498\n",
            "Train Epoch: 3 [640/60000 (1%)]\tLoss: 0.044778\n",
            "Train Epoch: 3 [1280/60000 (2%)]\tLoss: 0.028091\n",
            "Train Epoch: 3 [1920/60000 (3%)]\tLoss: 0.089566\n",
            "Train Epoch: 3 [2560/60000 (4%)]\tLoss: 0.097561\n",
            "Train Epoch: 3 [3200/60000 (5%)]\tLoss: 0.225639\n",
            "Train Epoch: 3 [3840/60000 (6%)]\tLoss: 0.450939\n",
            "Train Epoch: 3 [4480/60000 (7%)]\tLoss: 0.103176\n",
            "Train Epoch: 3 [5120/60000 (9%)]\tLoss: 0.110261\n",
            "Train Epoch: 3 [5760/60000 (10%)]\tLoss: 0.084612\n",
            "Train Epoch: 3 [6400/60000 (11%)]\tLoss: 0.082899\n",
            "Train Epoch: 3 [7040/60000 (12%)]\tLoss: 0.086912\n",
            "Train Epoch: 3 [7680/60000 (13%)]\tLoss: 0.045637\n",
            "Train Epoch: 3 [8320/60000 (14%)]\tLoss: 0.128868\n",
            "Train Epoch: 3 [8960/60000 (15%)]\tLoss: 0.088176\n",
            "Train Epoch: 3 [9600/60000 (16%)]\tLoss: 0.075170\n",
            "Train Epoch: 3 [10240/60000 (17%)]\tLoss: 0.152497\n",
            "Train Epoch: 3 [10880/60000 (18%)]\tLoss: 0.216820\n",
            "Train Epoch: 3 [11520/60000 (19%)]\tLoss: 0.101185\n",
            "Train Epoch: 3 [12160/60000 (20%)]\tLoss: 0.056714\n",
            "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 0.035382\n",
            "Train Epoch: 3 [13440/60000 (22%)]\tLoss: 0.110359\n",
            "Train Epoch: 3 [14080/60000 (23%)]\tLoss: 0.032515\n",
            "Train Epoch: 3 [14720/60000 (25%)]\tLoss: 0.060686\n",
            "Train Epoch: 3 [15360/60000 (26%)]\tLoss: 0.086435\n",
            "Train Epoch: 3 [16000/60000 (27%)]\tLoss: 0.187767\n",
            "Train Epoch: 3 [16640/60000 (28%)]\tLoss: 0.070730\n",
            "Train Epoch: 3 [17280/60000 (29%)]\tLoss: 0.081541\n",
            "Train Epoch: 3 [17920/60000 (30%)]\tLoss: 0.105333\n",
            "Train Epoch: 3 [18560/60000 (31%)]\tLoss: 0.046767\n",
            "Train Epoch: 3 [19200/60000 (32%)]\tLoss: 0.079598\n",
            "Train Epoch: 3 [19840/60000 (33%)]\tLoss: 0.152373\n",
            "Train Epoch: 3 [20480/60000 (34%)]\tLoss: 0.086455\n",
            "Train Epoch: 3 [21120/60000 (35%)]\tLoss: 0.156253\n",
            "Train Epoch: 3 [21760/60000 (36%)]\tLoss: 0.114759\n",
            "Train Epoch: 3 [22400/60000 (37%)]\tLoss: 0.026383\n",
            "Train Epoch: 3 [23040/60000 (38%)]\tLoss: 0.093027\n",
            "Train Epoch: 3 [23680/60000 (39%)]\tLoss: 0.086208\n",
            "Train Epoch: 3 [24320/60000 (41%)]\tLoss: 0.111208\n",
            "Train Epoch: 3 [24960/60000 (42%)]\tLoss: 0.084877\n",
            "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.435258\n",
            "Train Epoch: 3 [26240/60000 (44%)]\tLoss: 0.118636\n",
            "Train Epoch: 3 [26880/60000 (45%)]\tLoss: 0.090411\n",
            "Train Epoch: 3 [27520/60000 (46%)]\tLoss: 0.157764\n",
            "Train Epoch: 3 [28160/60000 (47%)]\tLoss: 0.106150\n",
            "Train Epoch: 3 [28800/60000 (48%)]\tLoss: 0.148992\n",
            "Train Epoch: 3 [29440/60000 (49%)]\tLoss: 0.072556\n",
            "Train Epoch: 3 [30080/60000 (50%)]\tLoss: 0.209984\n",
            "Train Epoch: 3 [30720/60000 (51%)]\tLoss: 0.099591\n",
            "Train Epoch: 3 [31360/60000 (52%)]\tLoss: 0.178176\n",
            "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 0.126514\n",
            "Train Epoch: 3 [32640/60000 (54%)]\tLoss: 0.230703\n",
            "Train Epoch: 3 [33280/60000 (55%)]\tLoss: 0.269098\n",
            "Train Epoch: 3 [33920/60000 (57%)]\tLoss: 0.117059\n",
            "Train Epoch: 3 [34560/60000 (58%)]\tLoss: 0.098362\n",
            "Train Epoch: 3 [35200/60000 (59%)]\tLoss: 0.250059\n",
            "Train Epoch: 3 [35840/60000 (60%)]\tLoss: 0.023378\n",
            "Train Epoch: 3 [36480/60000 (61%)]\tLoss: 0.058081\n",
            "Train Epoch: 3 [37120/60000 (62%)]\tLoss: 0.108954\n",
            "Train Epoch: 3 [37760/60000 (63%)]\tLoss: 0.066411\n",
            "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 0.035049\n",
            "Train Epoch: 3 [39040/60000 (65%)]\tLoss: 0.213279\n",
            "Train Epoch: 3 [39680/60000 (66%)]\tLoss: 0.063532\n",
            "Train Epoch: 3 [40320/60000 (67%)]\tLoss: 0.068824\n",
            "Train Epoch: 3 [40960/60000 (68%)]\tLoss: 0.102634\n",
            "Train Epoch: 3 [41600/60000 (69%)]\tLoss: 0.115198\n",
            "Train Epoch: 3 [42240/60000 (70%)]\tLoss: 0.201856\n",
            "Train Epoch: 3 [42880/60000 (71%)]\tLoss: 0.110897\n",
            "Train Epoch: 3 [43520/60000 (72%)]\tLoss: 0.180842\n",
            "Train Epoch: 3 [44160/60000 (74%)]\tLoss: 0.023275\n",
            "Train Epoch: 3 [44800/60000 (75%)]\tLoss: 0.059975\n",
            "Train Epoch: 3 [45440/60000 (76%)]\tLoss: 0.160721\n",
            "Train Epoch: 3 [46080/60000 (77%)]\tLoss: 0.040384\n",
            "Train Epoch: 3 [46720/60000 (78%)]\tLoss: 0.114300\n",
            "Train Epoch: 3 [47360/60000 (79%)]\tLoss: 0.182975\n",
            "Train Epoch: 3 [48000/60000 (80%)]\tLoss: 0.057114\n",
            "Train Epoch: 3 [48640/60000 (81%)]\tLoss: 0.055122\n",
            "Train Epoch: 3 [49280/60000 (82%)]\tLoss: 0.141467\n",
            "Train Epoch: 3 [49920/60000 (83%)]\tLoss: 0.131566\n",
            "Train Epoch: 3 [50560/60000 (84%)]\tLoss: 0.084983\n",
            "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.219999\n",
            "Train Epoch: 3 [51840/60000 (86%)]\tLoss: 0.067123\n",
            "Train Epoch: 3 [52480/60000 (87%)]\tLoss: 0.053354\n",
            "Train Epoch: 3 [53120/60000 (88%)]\tLoss: 0.036950\n",
            "Train Epoch: 3 [53760/60000 (90%)]\tLoss: 0.074767\n",
            "Train Epoch: 3 [54400/60000 (91%)]\tLoss: 0.185587\n",
            "Train Epoch: 3 [55040/60000 (92%)]\tLoss: 0.107849\n",
            "Train Epoch: 3 [55680/60000 (93%)]\tLoss: 0.027876\n",
            "Train Epoch: 3 [56320/60000 (94%)]\tLoss: 0.020682\n",
            "Train Epoch: 3 [56960/60000 (95%)]\tLoss: 0.233103\n",
            "Train Epoch: 3 [57600/60000 (96%)]\tLoss: 0.138719\n",
            "Train Epoch: 3 [58240/60000 (97%)]\tLoss: 0.159390\n",
            "Train Epoch: 3 [58880/60000 (98%)]\tLoss: 0.048767\n",
            "Train Epoch: 3 [59520/60000 (99%)]\tLoss: 0.162387\n",
            "\n",
            "Test set: Average loss: 0.0860, Accuracy: 9727/10000 (97%)\n",
            "\n",
            "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.168822\n",
            "Train Epoch: 4 [640/60000 (1%)]\tLoss: 0.037730\n",
            "Train Epoch: 4 [1280/60000 (2%)]\tLoss: 0.115246\n",
            "Train Epoch: 4 [1920/60000 (3%)]\tLoss: 0.095509\n",
            "Train Epoch: 4 [2560/60000 (4%)]\tLoss: 0.059558\n",
            "Train Epoch: 4 [3200/60000 (5%)]\tLoss: 0.112372\n",
            "Train Epoch: 4 [3840/60000 (6%)]\tLoss: 0.047415\n",
            "Train Epoch: 4 [4480/60000 (7%)]\tLoss: 0.040645\n",
            "Train Epoch: 4 [5120/60000 (9%)]\tLoss: 0.108824\n",
            "Train Epoch: 4 [5760/60000 (10%)]\tLoss: 0.151617\n",
            "Train Epoch: 4 [6400/60000 (11%)]\tLoss: 0.057594\n",
            "Train Epoch: 4 [7040/60000 (12%)]\tLoss: 0.054908\n",
            "Train Epoch: 4 [7680/60000 (13%)]\tLoss: 0.145963\n",
            "Train Epoch: 4 [8320/60000 (14%)]\tLoss: 0.066243\n",
            "Train Epoch: 4 [8960/60000 (15%)]\tLoss: 0.071303\n",
            "Train Epoch: 4 [9600/60000 (16%)]\tLoss: 0.175025\n",
            "Train Epoch: 4 [10240/60000 (17%)]\tLoss: 0.041600\n",
            "Train Epoch: 4 [10880/60000 (18%)]\tLoss: 0.042556\n",
            "Train Epoch: 4 [11520/60000 (19%)]\tLoss: 0.092095\n",
            "Train Epoch: 4 [12160/60000 (20%)]\tLoss: 0.043289\n",
            "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 0.043951\n",
            "Train Epoch: 4 [13440/60000 (22%)]\tLoss: 0.029840\n",
            "Train Epoch: 4 [14080/60000 (23%)]\tLoss: 0.129716\n",
            "Train Epoch: 4 [14720/60000 (25%)]\tLoss: 0.089756\n",
            "Train Epoch: 4 [15360/60000 (26%)]\tLoss: 0.116983\n",
            "Train Epoch: 4 [16000/60000 (27%)]\tLoss: 0.090115\n",
            "Train Epoch: 4 [16640/60000 (28%)]\tLoss: 0.027893\n",
            "Train Epoch: 4 [17280/60000 (29%)]\tLoss: 0.118236\n",
            "Train Epoch: 4 [17920/60000 (30%)]\tLoss: 0.161083\n",
            "Train Epoch: 4 [18560/60000 (31%)]\tLoss: 0.088651\n",
            "Train Epoch: 4 [19200/60000 (32%)]\tLoss: 0.124257\n",
            "Train Epoch: 4 [19840/60000 (33%)]\tLoss: 0.112528\n",
            "Train Epoch: 4 [20480/60000 (34%)]\tLoss: 0.111446\n",
            "Train Epoch: 4 [21120/60000 (35%)]\tLoss: 0.092414\n",
            "Train Epoch: 4 [21760/60000 (36%)]\tLoss: 0.051345\n",
            "Train Epoch: 4 [22400/60000 (37%)]\tLoss: 0.062554\n",
            "Train Epoch: 4 [23040/60000 (38%)]\tLoss: 0.157890\n",
            "Train Epoch: 4 [23680/60000 (39%)]\tLoss: 0.020484\n",
            "Train Epoch: 4 [24320/60000 (41%)]\tLoss: 0.041001\n",
            "Train Epoch: 4 [24960/60000 (42%)]\tLoss: 0.067049\n",
            "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 0.033089\n",
            "Train Epoch: 4 [26240/60000 (44%)]\tLoss: 0.086715\n",
            "Train Epoch: 4 [26880/60000 (45%)]\tLoss: 0.059131\n",
            "Train Epoch: 4 [27520/60000 (46%)]\tLoss: 0.057669\n",
            "Train Epoch: 4 [28160/60000 (47%)]\tLoss: 0.028164\n",
            "Train Epoch: 4 [28800/60000 (48%)]\tLoss: 0.082729\n",
            "Train Epoch: 4 [29440/60000 (49%)]\tLoss: 0.036622\n",
            "Train Epoch: 4 [30080/60000 (50%)]\tLoss: 0.099328\n",
            "Train Epoch: 4 [30720/60000 (51%)]\tLoss: 0.052347\n",
            "Train Epoch: 4 [31360/60000 (52%)]\tLoss: 0.196634\n",
            "Train Epoch: 4 [32000/60000 (53%)]\tLoss: 0.093056\n",
            "Train Epoch: 4 [32640/60000 (54%)]\tLoss: 0.046766\n",
            "Train Epoch: 4 [33280/60000 (55%)]\tLoss: 0.184466\n",
            "Train Epoch: 4 [33920/60000 (57%)]\tLoss: 0.039319\n",
            "Train Epoch: 4 [34560/60000 (58%)]\tLoss: 0.057328\n",
            "Train Epoch: 4 [35200/60000 (59%)]\tLoss: 0.125174\n",
            "Train Epoch: 4 [35840/60000 (60%)]\tLoss: 0.064746\n",
            "Train Epoch: 4 [36480/60000 (61%)]\tLoss: 0.033867\n",
            "Train Epoch: 4 [37120/60000 (62%)]\tLoss: 0.032354\n",
            "Train Epoch: 4 [37760/60000 (63%)]\tLoss: 0.046937\n",
            "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 0.067161\n",
            "Train Epoch: 4 [39040/60000 (65%)]\tLoss: 0.030291\n",
            "Train Epoch: 4 [39680/60000 (66%)]\tLoss: 0.137378\n",
            "Train Epoch: 4 [40320/60000 (67%)]\tLoss: 0.134861\n",
            "Train Epoch: 4 [40960/60000 (68%)]\tLoss: 0.040241\n",
            "Train Epoch: 4 [41600/60000 (69%)]\tLoss: 0.024585\n",
            "Train Epoch: 4 [42240/60000 (70%)]\tLoss: 0.029211\n",
            "Train Epoch: 4 [42880/60000 (71%)]\tLoss: 0.036657\n",
            "Train Epoch: 4 [43520/60000 (72%)]\tLoss: 0.085964\n",
            "Train Epoch: 4 [44160/60000 (74%)]\tLoss: 0.104920\n",
            "Train Epoch: 4 [44800/60000 (75%)]\tLoss: 0.121007\n",
            "Train Epoch: 4 [45440/60000 (76%)]\tLoss: 0.211392\n",
            "Train Epoch: 4 [46080/60000 (77%)]\tLoss: 0.053144\n",
            "Train Epoch: 4 [46720/60000 (78%)]\tLoss: 0.068158\n",
            "Train Epoch: 4 [47360/60000 (79%)]\tLoss: 0.056298\n",
            "Train Epoch: 4 [48000/60000 (80%)]\tLoss: 0.019314\n",
            "Train Epoch: 4 [48640/60000 (81%)]\tLoss: 0.088635\n",
            "Train Epoch: 4 [49280/60000 (82%)]\tLoss: 0.092620\n",
            "Train Epoch: 4 [49920/60000 (83%)]\tLoss: 0.082750\n",
            "Train Epoch: 4 [50560/60000 (84%)]\tLoss: 0.017809\n",
            "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 0.045012\n",
            "Train Epoch: 4 [51840/60000 (86%)]\tLoss: 0.039570\n",
            "Train Epoch: 4 [52480/60000 (87%)]\tLoss: 0.135402\n",
            "Train Epoch: 4 [53120/60000 (88%)]\tLoss: 0.079316\n",
            "Train Epoch: 4 [53760/60000 (90%)]\tLoss: 0.089507\n",
            "Train Epoch: 4 [54400/60000 (91%)]\tLoss: 0.130325\n",
            "Train Epoch: 4 [55040/60000 (92%)]\tLoss: 0.068390\n",
            "Train Epoch: 4 [55680/60000 (93%)]\tLoss: 0.098264\n",
            "Train Epoch: 4 [56320/60000 (94%)]\tLoss: 0.064248\n",
            "Train Epoch: 4 [56960/60000 (95%)]\tLoss: 0.008748\n",
            "Train Epoch: 4 [57600/60000 (96%)]\tLoss: 0.083343\n",
            "Train Epoch: 4 [58240/60000 (97%)]\tLoss: 0.168952\n",
            "Train Epoch: 4 [58880/60000 (98%)]\tLoss: 0.093992\n",
            "Train Epoch: 4 [59520/60000 (99%)]\tLoss: 0.039406\n",
            "\n",
            "Test set: Average loss: 0.0718, Accuracy: 9769/10000 (98%)\n",
            "\n",
            "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.238422\n",
            "Train Epoch: 5 [640/60000 (1%)]\tLoss: 0.055056\n",
            "Train Epoch: 5 [1280/60000 (2%)]\tLoss: 0.029730\n",
            "Train Epoch: 5 [1920/60000 (3%)]\tLoss: 0.031657\n",
            "Train Epoch: 5 [2560/60000 (4%)]\tLoss: 0.028101\n",
            "Train Epoch: 5 [3200/60000 (5%)]\tLoss: 0.120028\n",
            "Train Epoch: 5 [3840/60000 (6%)]\tLoss: 0.047707\n",
            "Train Epoch: 5 [4480/60000 (7%)]\tLoss: 0.048971\n",
            "Train Epoch: 5 [5120/60000 (9%)]\tLoss: 0.096373\n",
            "Train Epoch: 5 [5760/60000 (10%)]\tLoss: 0.068513\n",
            "Train Epoch: 5 [6400/60000 (11%)]\tLoss: 0.076657\n",
            "Train Epoch: 5 [7040/60000 (12%)]\tLoss: 0.060082\n",
            "Train Epoch: 5 [7680/60000 (13%)]\tLoss: 0.133058\n",
            "Train Epoch: 5 [8320/60000 (14%)]\tLoss: 0.157767\n",
            "Train Epoch: 5 [8960/60000 (15%)]\tLoss: 0.191749\n",
            "Train Epoch: 5 [9600/60000 (16%)]\tLoss: 0.110351\n",
            "Train Epoch: 5 [10240/60000 (17%)]\tLoss: 0.006066\n",
            "Train Epoch: 5 [10880/60000 (18%)]\tLoss: 0.068007\n",
            "Train Epoch: 5 [11520/60000 (19%)]\tLoss: 0.035553\n",
            "Train Epoch: 5 [12160/60000 (20%)]\tLoss: 0.025609\n",
            "Train Epoch: 5 [12800/60000 (21%)]\tLoss: 0.053128\n",
            "Train Epoch: 5 [13440/60000 (22%)]\tLoss: 0.181377\n",
            "Train Epoch: 5 [14080/60000 (23%)]\tLoss: 0.006243\n",
            "Train Epoch: 5 [14720/60000 (25%)]\tLoss: 0.214228\n",
            "Train Epoch: 5 [15360/60000 (26%)]\tLoss: 0.038991\n",
            "Train Epoch: 5 [16000/60000 (27%)]\tLoss: 0.040370\n",
            "Train Epoch: 5 [16640/60000 (28%)]\tLoss: 0.043049\n",
            "Train Epoch: 5 [17280/60000 (29%)]\tLoss: 0.036772\n",
            "Train Epoch: 5 [17920/60000 (30%)]\tLoss: 0.119471\n",
            "Train Epoch: 5 [18560/60000 (31%)]\tLoss: 0.040490\n",
            "Train Epoch: 5 [19200/60000 (32%)]\tLoss: 0.110692\n",
            "Train Epoch: 5 [19840/60000 (33%)]\tLoss: 0.050987\n",
            "Train Epoch: 5 [20480/60000 (34%)]\tLoss: 0.191326\n",
            "Train Epoch: 5 [21120/60000 (35%)]\tLoss: 0.040237\n",
            "Train Epoch: 5 [21760/60000 (36%)]\tLoss: 0.014197\n",
            "Train Epoch: 5 [22400/60000 (37%)]\tLoss: 0.064567\n",
            "Train Epoch: 5 [23040/60000 (38%)]\tLoss: 0.099611\n",
            "Train Epoch: 5 [23680/60000 (39%)]\tLoss: 0.029665\n",
            "Train Epoch: 5 [24320/60000 (41%)]\tLoss: 0.232540\n",
            "Train Epoch: 5 [24960/60000 (42%)]\tLoss: 0.121470\n",
            "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 0.022110\n",
            "Train Epoch: 5 [26240/60000 (44%)]\tLoss: 0.094517\n",
            "Train Epoch: 5 [26880/60000 (45%)]\tLoss: 0.012508\n",
            "Train Epoch: 5 [27520/60000 (46%)]\tLoss: 0.038658\n",
            "Train Epoch: 5 [28160/60000 (47%)]\tLoss: 0.038520\n",
            "Train Epoch: 5 [28800/60000 (48%)]\tLoss: 0.085433\n",
            "Train Epoch: 5 [29440/60000 (49%)]\tLoss: 0.200251\n",
            "Train Epoch: 5 [30080/60000 (50%)]\tLoss: 0.017882\n",
            "Train Epoch: 5 [30720/60000 (51%)]\tLoss: 0.041936\n",
            "Train Epoch: 5 [31360/60000 (52%)]\tLoss: 0.186652\n",
            "Train Epoch: 5 [32000/60000 (53%)]\tLoss: 0.013951\n",
            "Train Epoch: 5 [32640/60000 (54%)]\tLoss: 0.104655\n",
            "Train Epoch: 5 [33280/60000 (55%)]\tLoss: 0.106097\n",
            "Train Epoch: 5 [33920/60000 (57%)]\tLoss: 0.007380\n",
            "Train Epoch: 5 [34560/60000 (58%)]\tLoss: 0.071253\n",
            "Train Epoch: 5 [35200/60000 (59%)]\tLoss: 0.129781\n",
            "Train Epoch: 5 [35840/60000 (60%)]\tLoss: 0.023799\n",
            "Train Epoch: 5 [36480/60000 (61%)]\tLoss: 0.143555\n",
            "Train Epoch: 5 [37120/60000 (62%)]\tLoss: 0.050657\n",
            "Train Epoch: 5 [37760/60000 (63%)]\tLoss: 0.229986\n",
            "Train Epoch: 5 [38400/60000 (64%)]\tLoss: 0.027627\n",
            "Train Epoch: 5 [39040/60000 (65%)]\tLoss: 0.098478\n",
            "Train Epoch: 5 [39680/60000 (66%)]\tLoss: 0.076560\n",
            "Train Epoch: 5 [40320/60000 (67%)]\tLoss: 0.021429\n",
            "Train Epoch: 5 [40960/60000 (68%)]\tLoss: 0.061354\n",
            "Train Epoch: 5 [41600/60000 (69%)]\tLoss: 0.115106\n",
            "Train Epoch: 5 [42240/60000 (70%)]\tLoss: 0.028895\n",
            "Train Epoch: 5 [42880/60000 (71%)]\tLoss: 0.066856\n",
            "Train Epoch: 5 [43520/60000 (72%)]\tLoss: 0.016275\n",
            "Train Epoch: 5 [44160/60000 (74%)]\tLoss: 0.049521\n",
            "Train Epoch: 5 [44800/60000 (75%)]\tLoss: 0.021296\n",
            "Train Epoch: 5 [45440/60000 (76%)]\tLoss: 0.069881\n",
            "Train Epoch: 5 [46080/60000 (77%)]\tLoss: 0.205745\n",
            "Train Epoch: 5 [46720/60000 (78%)]\tLoss: 0.032572\n",
            "Train Epoch: 5 [47360/60000 (79%)]\tLoss: 0.075084\n",
            "Train Epoch: 5 [48000/60000 (80%)]\tLoss: 0.081455\n",
            "Train Epoch: 5 [48640/60000 (81%)]\tLoss: 0.043303\n",
            "Train Epoch: 5 [49280/60000 (82%)]\tLoss: 0.046441\n",
            "Train Epoch: 5 [49920/60000 (83%)]\tLoss: 0.095083\n",
            "Train Epoch: 5 [50560/60000 (84%)]\tLoss: 0.021958\n",
            "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 0.069338\n",
            "Train Epoch: 5 [51840/60000 (86%)]\tLoss: 0.039502\n",
            "Train Epoch: 5 [52480/60000 (87%)]\tLoss: 0.064674\n",
            "Train Epoch: 5 [53120/60000 (88%)]\tLoss: 0.031895\n",
            "Train Epoch: 5 [53760/60000 (90%)]\tLoss: 0.040372\n",
            "Train Epoch: 5 [54400/60000 (91%)]\tLoss: 0.182619\n",
            "Train Epoch: 5 [55040/60000 (92%)]\tLoss: 0.131138\n",
            "Train Epoch: 5 [55680/60000 (93%)]\tLoss: 0.080242\n",
            "Train Epoch: 5 [56320/60000 (94%)]\tLoss: 0.111009\n",
            "Train Epoch: 5 [56960/60000 (95%)]\tLoss: 0.466931\n",
            "Train Epoch: 5 [57600/60000 (96%)]\tLoss: 0.124308\n",
            "Train Epoch: 5 [58240/60000 (97%)]\tLoss: 0.029636\n",
            "Train Epoch: 5 [58880/60000 (98%)]\tLoss: 0.080842\n",
            "Train Epoch: 5 [59520/60000 (99%)]\tLoss: 0.052449\n",
            "\n",
            "Test set: Average loss: 0.0734, Accuracy: 9765/10000 (98%)\n",
            "\n",
            "Train Epoch: 6 [0/60000 (0%)]\tLoss: 0.030772\n",
            "Train Epoch: 6 [640/60000 (1%)]\tLoss: 0.047118\n",
            "Train Epoch: 6 [1280/60000 (2%)]\tLoss: 0.072002\n",
            "Train Epoch: 6 [1920/60000 (3%)]\tLoss: 0.056024\n",
            "Train Epoch: 6 [2560/60000 (4%)]\tLoss: 0.053212\n",
            "Train Epoch: 6 [3200/60000 (5%)]\tLoss: 0.005098\n",
            "Train Epoch: 6 [3840/60000 (6%)]\tLoss: 0.104589\n",
            "Train Epoch: 6 [4480/60000 (7%)]\tLoss: 0.046894\n",
            "Train Epoch: 6 [5120/60000 (9%)]\tLoss: 0.035612\n",
            "Train Epoch: 6 [5760/60000 (10%)]\tLoss: 0.075183\n",
            "Train Epoch: 6 [6400/60000 (11%)]\tLoss: 0.037276\n",
            "Train Epoch: 6 [7040/60000 (12%)]\tLoss: 0.079748\n",
            "Train Epoch: 6 [7680/60000 (13%)]\tLoss: 0.129053\n",
            "Train Epoch: 6 [8320/60000 (14%)]\tLoss: 0.036853\n",
            "Train Epoch: 6 [8960/60000 (15%)]\tLoss: 0.029565\n",
            "Train Epoch: 6 [9600/60000 (16%)]\tLoss: 0.057272\n",
            "Train Epoch: 6 [10240/60000 (17%)]\tLoss: 0.097789\n",
            "Train Epoch: 6 [10880/60000 (18%)]\tLoss: 0.035155\n",
            "Train Epoch: 6 [11520/60000 (19%)]\tLoss: 0.073086\n",
            "Train Epoch: 6 [12160/60000 (20%)]\tLoss: 0.210404\n",
            "Train Epoch: 6 [12800/60000 (21%)]\tLoss: 0.093650\n",
            "Train Epoch: 6 [13440/60000 (22%)]\tLoss: 0.024862\n",
            "Train Epoch: 6 [14080/60000 (23%)]\tLoss: 0.128671\n",
            "Train Epoch: 6 [14720/60000 (25%)]\tLoss: 0.082851\n",
            "Train Epoch: 6 [15360/60000 (26%)]\tLoss: 0.067613\n",
            "Train Epoch: 6 [16000/60000 (27%)]\tLoss: 0.048973\n",
            "Train Epoch: 6 [16640/60000 (28%)]\tLoss: 0.033667\n",
            "Train Epoch: 6 [17280/60000 (29%)]\tLoss: 0.010809\n",
            "Train Epoch: 6 [17920/60000 (30%)]\tLoss: 0.092140\n",
            "Train Epoch: 6 [18560/60000 (31%)]\tLoss: 0.049465\n",
            "Train Epoch: 6 [19200/60000 (32%)]\tLoss: 0.030409\n",
            "Train Epoch: 6 [19840/60000 (33%)]\tLoss: 0.036655\n",
            "Train Epoch: 6 [20480/60000 (34%)]\tLoss: 0.070272\n",
            "Train Epoch: 6 [21120/60000 (35%)]\tLoss: 0.042107\n",
            "Train Epoch: 6 [21760/60000 (36%)]\tLoss: 0.062887\n",
            "Train Epoch: 6 [22400/60000 (37%)]\tLoss: 0.028675\n",
            "Train Epoch: 6 [23040/60000 (38%)]\tLoss: 0.042676\n",
            "Train Epoch: 6 [23680/60000 (39%)]\tLoss: 0.024633\n",
            "Train Epoch: 6 [24320/60000 (41%)]\tLoss: 0.034643\n",
            "Train Epoch: 6 [24960/60000 (42%)]\tLoss: 0.068753\n",
            "Train Epoch: 6 [25600/60000 (43%)]\tLoss: 0.033153\n",
            "Train Epoch: 6 [26240/60000 (44%)]\tLoss: 0.031244\n",
            "Train Epoch: 6 [26880/60000 (45%)]\tLoss: 0.018485\n",
            "Train Epoch: 6 [27520/60000 (46%)]\tLoss: 0.026355\n",
            "Train Epoch: 6 [28160/60000 (47%)]\tLoss: 0.039915\n",
            "Train Epoch: 6 [28800/60000 (48%)]\tLoss: 0.019634\n",
            "Train Epoch: 6 [29440/60000 (49%)]\tLoss: 0.181608\n",
            "Train Epoch: 6 [30080/60000 (50%)]\tLoss: 0.250092\n",
            "Train Epoch: 6 [30720/60000 (51%)]\tLoss: 0.123417\n",
            "Train Epoch: 6 [31360/60000 (52%)]\tLoss: 0.089790\n",
            "Train Epoch: 6 [32000/60000 (53%)]\tLoss: 0.039473\n",
            "Train Epoch: 6 [32640/60000 (54%)]\tLoss: 0.155136\n",
            "Train Epoch: 6 [33280/60000 (55%)]\tLoss: 0.131392\n",
            "Train Epoch: 6 [33920/60000 (57%)]\tLoss: 0.010330\n",
            "Train Epoch: 6 [34560/60000 (58%)]\tLoss: 0.061474\n",
            "Train Epoch: 6 [35200/60000 (59%)]\tLoss: 0.009180\n",
            "Train Epoch: 6 [35840/60000 (60%)]\tLoss: 0.119389\n",
            "Train Epoch: 6 [36480/60000 (61%)]\tLoss: 0.141153\n",
            "Train Epoch: 6 [37120/60000 (62%)]\tLoss: 0.019424\n",
            "Train Epoch: 6 [37760/60000 (63%)]\tLoss: 0.036496\n",
            "Train Epoch: 6 [38400/60000 (64%)]\tLoss: 0.023134\n",
            "Train Epoch: 6 [39040/60000 (65%)]\tLoss: 0.047308\n",
            "Train Epoch: 6 [39680/60000 (66%)]\tLoss: 0.046240\n",
            "Train Epoch: 6 [40320/60000 (67%)]\tLoss: 0.055766\n",
            "Train Epoch: 6 [40960/60000 (68%)]\tLoss: 0.154195\n",
            "Train Epoch: 6 [41600/60000 (69%)]\tLoss: 0.092611\n",
            "Train Epoch: 6 [42240/60000 (70%)]\tLoss: 0.125797\n",
            "Train Epoch: 6 [42880/60000 (71%)]\tLoss: 0.030806\n",
            "Train Epoch: 6 [43520/60000 (72%)]\tLoss: 0.090459\n",
            "Train Epoch: 6 [44160/60000 (74%)]\tLoss: 0.034829\n",
            "Train Epoch: 6 [44800/60000 (75%)]\tLoss: 0.140489\n",
            "Train Epoch: 6 [45440/60000 (76%)]\tLoss: 0.099459\n",
            "Train Epoch: 6 [46080/60000 (77%)]\tLoss: 0.079793\n",
            "Train Epoch: 6 [46720/60000 (78%)]\tLoss: 0.019237\n",
            "Train Epoch: 6 [47360/60000 (79%)]\tLoss: 0.079816\n",
            "Train Epoch: 6 [48000/60000 (80%)]\tLoss: 0.040622\n",
            "Train Epoch: 6 [48640/60000 (81%)]\tLoss: 0.057045\n",
            "Train Epoch: 6 [49280/60000 (82%)]\tLoss: 0.147157\n",
            "Train Epoch: 6 [49920/60000 (83%)]\tLoss: 0.096699\n",
            "Train Epoch: 6 [50560/60000 (84%)]\tLoss: 0.137361\n",
            "Train Epoch: 6 [51200/60000 (85%)]\tLoss: 0.094584\n",
            "Train Epoch: 6 [51840/60000 (86%)]\tLoss: 0.063779\n",
            "Train Epoch: 6 [52480/60000 (87%)]\tLoss: 0.021110\n",
            "Train Epoch: 6 [53120/60000 (88%)]\tLoss: 0.057804\n",
            "Train Epoch: 6 [53760/60000 (90%)]\tLoss: 0.049202\n",
            "Train Epoch: 6 [54400/60000 (91%)]\tLoss: 0.233846\n",
            "Train Epoch: 6 [55040/60000 (92%)]\tLoss: 0.114240\n",
            "Train Epoch: 6 [55680/60000 (93%)]\tLoss: 0.026268\n",
            "Train Epoch: 6 [56320/60000 (94%)]\tLoss: 0.049192\n",
            "Train Epoch: 6 [56960/60000 (95%)]\tLoss: 0.020381\n",
            "Train Epoch: 6 [57600/60000 (96%)]\tLoss: 0.031941\n",
            "Train Epoch: 6 [58240/60000 (97%)]\tLoss: 0.012854\n",
            "Train Epoch: 6 [58880/60000 (98%)]\tLoss: 0.084447\n",
            "Train Epoch: 6 [59520/60000 (99%)]\tLoss: 0.064615\n",
            "\n",
            "Test set: Average loss: 0.0648, Accuracy: 9793/10000 (98%)\n",
            "\n",
            "Train Epoch: 7 [0/60000 (0%)]\tLoss: 0.032704\n",
            "Train Epoch: 7 [640/60000 (1%)]\tLoss: 0.041778\n",
            "Train Epoch: 7 [1280/60000 (2%)]\tLoss: 0.055231\n",
            "Train Epoch: 7 [1920/60000 (3%)]\tLoss: 0.080751\n",
            "Train Epoch: 7 [2560/60000 (4%)]\tLoss: 0.011508\n",
            "Train Epoch: 7 [3200/60000 (5%)]\tLoss: 0.101958\n",
            "Train Epoch: 7 [3840/60000 (6%)]\tLoss: 0.048289\n",
            "Train Epoch: 7 [4480/60000 (7%)]\tLoss: 0.026528\n",
            "Train Epoch: 7 [5120/60000 (9%)]\tLoss: 0.041576\n",
            "Train Epoch: 7 [5760/60000 (10%)]\tLoss: 0.048789\n",
            "Train Epoch: 7 [6400/60000 (11%)]\tLoss: 0.007448\n",
            "Train Epoch: 7 [7040/60000 (12%)]\tLoss: 0.037675\n",
            "Train Epoch: 7 [7680/60000 (13%)]\tLoss: 0.110061\n",
            "Train Epoch: 7 [8320/60000 (14%)]\tLoss: 0.033682\n",
            "Train Epoch: 7 [8960/60000 (15%)]\tLoss: 0.059127\n",
            "Train Epoch: 7 [9600/60000 (16%)]\tLoss: 0.037317\n",
            "Train Epoch: 7 [10240/60000 (17%)]\tLoss: 0.091056\n",
            "Train Epoch: 7 [10880/60000 (18%)]\tLoss: 0.032086\n",
            "Train Epoch: 7 [11520/60000 (19%)]\tLoss: 0.091500\n",
            "Train Epoch: 7 [12160/60000 (20%)]\tLoss: 0.055792\n",
            "Train Epoch: 7 [12800/60000 (21%)]\tLoss: 0.049369\n",
            "Train Epoch: 7 [13440/60000 (22%)]\tLoss: 0.034150\n",
            "Train Epoch: 7 [14080/60000 (23%)]\tLoss: 0.086959\n",
            "Train Epoch: 7 [14720/60000 (25%)]\tLoss: 0.014754\n",
            "Train Epoch: 7 [15360/60000 (26%)]\tLoss: 0.141089\n",
            "Train Epoch: 7 [16000/60000 (27%)]\tLoss: 0.022201\n",
            "Train Epoch: 7 [16640/60000 (28%)]\tLoss: 0.033259\n",
            "Train Epoch: 7 [17280/60000 (29%)]\tLoss: 0.085190\n",
            "Train Epoch: 7 [17920/60000 (30%)]\tLoss: 0.080035\n",
            "Train Epoch: 7 [18560/60000 (31%)]\tLoss: 0.356058\n",
            "Train Epoch: 7 [19200/60000 (32%)]\tLoss: 0.025688\n",
            "Train Epoch: 7 [19840/60000 (33%)]\tLoss: 0.115132\n",
            "Train Epoch: 7 [20480/60000 (34%)]\tLoss: 0.093419\n",
            "Train Epoch: 7 [21120/60000 (35%)]\tLoss: 0.050892\n",
            "Train Epoch: 7 [21760/60000 (36%)]\tLoss: 0.055937\n",
            "Train Epoch: 7 [22400/60000 (37%)]\tLoss: 0.040263\n",
            "Train Epoch: 7 [23040/60000 (38%)]\tLoss: 0.058996\n",
            "Train Epoch: 7 [23680/60000 (39%)]\tLoss: 0.067616\n",
            "Train Epoch: 7 [24320/60000 (41%)]\tLoss: 0.060851\n",
            "Train Epoch: 7 [24960/60000 (42%)]\tLoss: 0.088541\n",
            "Train Epoch: 7 [25600/60000 (43%)]\tLoss: 0.070896\n",
            "Train Epoch: 7 [26240/60000 (44%)]\tLoss: 0.028837\n",
            "Train Epoch: 7 [26880/60000 (45%)]\tLoss: 0.112802\n",
            "Train Epoch: 7 [27520/60000 (46%)]\tLoss: 0.132585\n",
            "Train Epoch: 7 [28160/60000 (47%)]\tLoss: 0.048404\n",
            "Train Epoch: 7 [28800/60000 (48%)]\tLoss: 0.158516\n",
            "Train Epoch: 7 [29440/60000 (49%)]\tLoss: 0.027469\n",
            "Train Epoch: 7 [30080/60000 (50%)]\tLoss: 0.082446\n",
            "Train Epoch: 7 [30720/60000 (51%)]\tLoss: 0.021720\n",
            "Train Epoch: 7 [31360/60000 (52%)]\tLoss: 0.097069\n",
            "Train Epoch: 7 [32000/60000 (53%)]\tLoss: 0.040246\n",
            "Train Epoch: 7 [32640/60000 (54%)]\tLoss: 0.082269\n",
            "Train Epoch: 7 [33280/60000 (55%)]\tLoss: 0.047769\n",
            "Train Epoch: 7 [33920/60000 (57%)]\tLoss: 0.045597\n",
            "Train Epoch: 7 [34560/60000 (58%)]\tLoss: 0.058399\n",
            "Train Epoch: 7 [35200/60000 (59%)]\tLoss: 0.049094\n",
            "Train Epoch: 7 [35840/60000 (60%)]\tLoss: 0.012884\n",
            "Train Epoch: 7 [36480/60000 (61%)]\tLoss: 0.120557\n",
            "Train Epoch: 7 [37120/60000 (62%)]\tLoss: 0.113745\n",
            "Train Epoch: 7 [37760/60000 (63%)]\tLoss: 0.094940\n",
            "Train Epoch: 7 [38400/60000 (64%)]\tLoss: 0.133492\n",
            "Train Epoch: 7 [39040/60000 (65%)]\tLoss: 0.097987\n",
            "Train Epoch: 7 [39680/60000 (66%)]\tLoss: 0.021823\n",
            "Train Epoch: 7 [40320/60000 (67%)]\tLoss: 0.029201\n",
            "Train Epoch: 7 [40960/60000 (68%)]\tLoss: 0.034635\n",
            "Train Epoch: 7 [41600/60000 (69%)]\tLoss: 0.042779\n",
            "Train Epoch: 7 [42240/60000 (70%)]\tLoss: 0.035880\n",
            "Train Epoch: 7 [42880/60000 (71%)]\tLoss: 0.056221\n",
            "Train Epoch: 7 [43520/60000 (72%)]\tLoss: 0.019982\n",
            "Train Epoch: 7 [44160/60000 (74%)]\tLoss: 0.108458\n",
            "Train Epoch: 7 [44800/60000 (75%)]\tLoss: 0.060489\n",
            "Train Epoch: 7 [45440/60000 (76%)]\tLoss: 0.133887\n",
            "Train Epoch: 7 [46080/60000 (77%)]\tLoss: 0.029248\n",
            "Train Epoch: 7 [46720/60000 (78%)]\tLoss: 0.059048\n",
            "Train Epoch: 7 [47360/60000 (79%)]\tLoss: 0.031227\n",
            "Train Epoch: 7 [48000/60000 (80%)]\tLoss: 0.023410\n",
            "Train Epoch: 7 [48640/60000 (81%)]\tLoss: 0.015869\n",
            "Train Epoch: 7 [49280/60000 (82%)]\tLoss: 0.053054\n",
            "Train Epoch: 7 [49920/60000 (83%)]\tLoss: 0.052791\n",
            "Train Epoch: 7 [50560/60000 (84%)]\tLoss: 0.040692\n",
            "Train Epoch: 7 [51200/60000 (85%)]\tLoss: 0.008709\n",
            "Train Epoch: 7 [51840/60000 (86%)]\tLoss: 0.046884\n",
            "Train Epoch: 7 [52480/60000 (87%)]\tLoss: 0.053231\n",
            "Train Epoch: 7 [53120/60000 (88%)]\tLoss: 0.024348\n",
            "Train Epoch: 7 [53760/60000 (90%)]\tLoss: 0.077478\n",
            "Train Epoch: 7 [54400/60000 (91%)]\tLoss: 0.114323\n",
            "Train Epoch: 7 [55040/60000 (92%)]\tLoss: 0.037816\n",
            "Train Epoch: 7 [55680/60000 (93%)]\tLoss: 0.152172\n",
            "Train Epoch: 7 [56320/60000 (94%)]\tLoss: 0.051951\n",
            "Train Epoch: 7 [56960/60000 (95%)]\tLoss: 0.011282\n",
            "Train Epoch: 7 [57600/60000 (96%)]\tLoss: 0.048542\n",
            "Train Epoch: 7 [58240/60000 (97%)]\tLoss: 0.061363\n",
            "Train Epoch: 7 [58880/60000 (98%)]\tLoss: 0.068004\n",
            "Train Epoch: 7 [59520/60000 (99%)]\tLoss: 0.006632\n",
            "\n",
            "Test set: Average loss: 0.0687, Accuracy: 9782/10000 (98%)\n",
            "\n",
            "Train Epoch: 8 [0/60000 (0%)]\tLoss: 0.016794\n",
            "Train Epoch: 8 [640/60000 (1%)]\tLoss: 0.094861\n",
            "Train Epoch: 8 [1280/60000 (2%)]\tLoss: 0.021087\n",
            "Train Epoch: 8 [1920/60000 (3%)]\tLoss: 0.163609\n",
            "Train Epoch: 8 [2560/60000 (4%)]\tLoss: 0.022120\n",
            "Train Epoch: 8 [3200/60000 (5%)]\tLoss: 0.196318\n",
            "Train Epoch: 8 [3840/60000 (6%)]\tLoss: 0.067735\n",
            "Train Epoch: 8 [4480/60000 (7%)]\tLoss: 0.064410\n",
            "Train Epoch: 8 [5120/60000 (9%)]\tLoss: 0.011829\n",
            "Train Epoch: 8 [5760/60000 (10%)]\tLoss: 0.140687\n",
            "Train Epoch: 8 [6400/60000 (11%)]\tLoss: 0.131492\n",
            "Train Epoch: 8 [7040/60000 (12%)]\tLoss: 0.035306\n",
            "Train Epoch: 8 [7680/60000 (13%)]\tLoss: 0.047945\n",
            "Train Epoch: 8 [8320/60000 (14%)]\tLoss: 0.028245\n",
            "Train Epoch: 8 [8960/60000 (15%)]\tLoss: 0.194767\n",
            "Train Epoch: 8 [9600/60000 (16%)]\tLoss: 0.057992\n",
            "Train Epoch: 8 [10240/60000 (17%)]\tLoss: 0.095854\n",
            "Train Epoch: 8 [10880/60000 (18%)]\tLoss: 0.091323\n",
            "Train Epoch: 8 [11520/60000 (19%)]\tLoss: 0.082287\n",
            "Train Epoch: 8 [12160/60000 (20%)]\tLoss: 0.026467\n",
            "Train Epoch: 8 [12800/60000 (21%)]\tLoss: 0.166079\n",
            "Train Epoch: 8 [13440/60000 (22%)]\tLoss: 0.058783\n",
            "Train Epoch: 8 [14080/60000 (23%)]\tLoss: 0.050196\n",
            "Train Epoch: 8 [14720/60000 (25%)]\tLoss: 0.052853\n",
            "Train Epoch: 8 [15360/60000 (26%)]\tLoss: 0.026463\n",
            "Train Epoch: 8 [16000/60000 (27%)]\tLoss: 0.007266\n",
            "Train Epoch: 8 [16640/60000 (28%)]\tLoss: 0.047057\n",
            "Train Epoch: 8 [17280/60000 (29%)]\tLoss: 0.082907\n",
            "Train Epoch: 8 [17920/60000 (30%)]\tLoss: 0.014460\n",
            "Train Epoch: 8 [18560/60000 (31%)]\tLoss: 0.094930\n",
            "Train Epoch: 8 [19200/60000 (32%)]\tLoss: 0.071342\n",
            "Train Epoch: 8 [19840/60000 (33%)]\tLoss: 0.144675\n",
            "Train Epoch: 8 [20480/60000 (34%)]\tLoss: 0.136438\n",
            "Train Epoch: 8 [21120/60000 (35%)]\tLoss: 0.012929\n",
            "Train Epoch: 8 [21760/60000 (36%)]\tLoss: 0.032332\n",
            "Train Epoch: 8 [22400/60000 (37%)]\tLoss: 0.105628\n",
            "Train Epoch: 8 [23040/60000 (38%)]\tLoss: 0.033102\n",
            "Train Epoch: 8 [23680/60000 (39%)]\tLoss: 0.092660\n",
            "Train Epoch: 8 [24320/60000 (41%)]\tLoss: 0.011204\n",
            "Train Epoch: 8 [24960/60000 (42%)]\tLoss: 0.051930\n",
            "Train Epoch: 8 [25600/60000 (43%)]\tLoss: 0.054716\n",
            "Train Epoch: 8 [26240/60000 (44%)]\tLoss: 0.120004\n",
            "Train Epoch: 8 [26880/60000 (45%)]\tLoss: 0.091254\n",
            "Train Epoch: 8 [27520/60000 (46%)]\tLoss: 0.081527\n",
            "Train Epoch: 8 [28160/60000 (47%)]\tLoss: 0.102887\n",
            "Train Epoch: 8 [28800/60000 (48%)]\tLoss: 0.029285\n",
            "Train Epoch: 8 [29440/60000 (49%)]\tLoss: 0.059323\n",
            "Train Epoch: 8 [30080/60000 (50%)]\tLoss: 0.040456\n",
            "Train Epoch: 8 [30720/60000 (51%)]\tLoss: 0.035098\n",
            "Train Epoch: 8 [31360/60000 (52%)]\tLoss: 0.072148\n",
            "Train Epoch: 8 [32000/60000 (53%)]\tLoss: 0.058653\n",
            "Train Epoch: 8 [32640/60000 (54%)]\tLoss: 0.010502\n",
            "Train Epoch: 8 [33280/60000 (55%)]\tLoss: 0.031941\n",
            "Train Epoch: 8 [33920/60000 (57%)]\tLoss: 0.097893\n",
            "Train Epoch: 8 [34560/60000 (58%)]\tLoss: 0.021788\n",
            "Train Epoch: 8 [35200/60000 (59%)]\tLoss: 0.045006\n",
            "Train Epoch: 8 [35840/60000 (60%)]\tLoss: 0.144693\n",
            "Train Epoch: 8 [36480/60000 (61%)]\tLoss: 0.020092\n",
            "Train Epoch: 8 [37120/60000 (62%)]\tLoss: 0.011054\n",
            "Train Epoch: 8 [37760/60000 (63%)]\tLoss: 0.013661\n",
            "Train Epoch: 8 [38400/60000 (64%)]\tLoss: 0.020483\n",
            "Train Epoch: 8 [39040/60000 (65%)]\tLoss: 0.082993\n",
            "Train Epoch: 8 [39680/60000 (66%)]\tLoss: 0.069746\n",
            "Train Epoch: 8 [40320/60000 (67%)]\tLoss: 0.029159\n",
            "Train Epoch: 8 [40960/60000 (68%)]\tLoss: 0.063597\n",
            "Train Epoch: 8 [41600/60000 (69%)]\tLoss: 0.009078\n",
            "Train Epoch: 8 [42240/60000 (70%)]\tLoss: 0.055658\n",
            "Train Epoch: 8 [42880/60000 (71%)]\tLoss: 0.018294\n",
            "Train Epoch: 8 [43520/60000 (72%)]\tLoss: 0.018857\n",
            "Train Epoch: 8 [44160/60000 (74%)]\tLoss: 0.216415\n",
            "Train Epoch: 8 [44800/60000 (75%)]\tLoss: 0.031886\n",
            "Train Epoch: 8 [45440/60000 (76%)]\tLoss: 0.073854\n",
            "Train Epoch: 8 [46080/60000 (77%)]\tLoss: 0.042900\n",
            "Train Epoch: 8 [46720/60000 (78%)]\tLoss: 0.041821\n",
            "Train Epoch: 8 [47360/60000 (79%)]\tLoss: 0.049712\n",
            "Train Epoch: 8 [48000/60000 (80%)]\tLoss: 0.092966\n",
            "Train Epoch: 8 [48640/60000 (81%)]\tLoss: 0.039979\n",
            "Train Epoch: 8 [49280/60000 (82%)]\tLoss: 0.049257\n",
            "Train Epoch: 8 [49920/60000 (83%)]\tLoss: 0.139388\n",
            "Train Epoch: 8 [50560/60000 (84%)]\tLoss: 0.139659\n",
            "Train Epoch: 8 [51200/60000 (85%)]\tLoss: 0.048042\n",
            "Train Epoch: 8 [51840/60000 (86%)]\tLoss: 0.090934\n",
            "Train Epoch: 8 [52480/60000 (87%)]\tLoss: 0.059411\n",
            "Train Epoch: 8 [53120/60000 (88%)]\tLoss: 0.039280\n",
            "Train Epoch: 8 [53760/60000 (90%)]\tLoss: 0.009059\n",
            "Train Epoch: 8 [54400/60000 (91%)]\tLoss: 0.047928\n",
            "Train Epoch: 8 [55040/60000 (92%)]\tLoss: 0.033041\n",
            "Train Epoch: 8 [55680/60000 (93%)]\tLoss: 0.004757\n",
            "Train Epoch: 8 [56320/60000 (94%)]\tLoss: 0.017648\n",
            "Train Epoch: 8 [56960/60000 (95%)]\tLoss: 0.019795\n",
            "Train Epoch: 8 [57600/60000 (96%)]\tLoss: 0.066640\n",
            "Train Epoch: 8 [58240/60000 (97%)]\tLoss: 0.043510\n",
            "Train Epoch: 8 [58880/60000 (98%)]\tLoss: 0.079562\n",
            "Train Epoch: 8 [59520/60000 (99%)]\tLoss: 0.069945\n",
            "\n",
            "Test set: Average loss: 0.0567, Accuracy: 9820/10000 (98%)\n",
            "\n",
            "Train Epoch: 9 [0/60000 (0%)]\tLoss: 0.030601\n",
            "Train Epoch: 9 [640/60000 (1%)]\tLoss: 0.097770\n",
            "Train Epoch: 9 [1280/60000 (2%)]\tLoss: 0.054204\n",
            "Train Epoch: 9 [1920/60000 (3%)]\tLoss: 0.051038\n",
            "Train Epoch: 9 [2560/60000 (4%)]\tLoss: 0.020012\n",
            "Train Epoch: 9 [3200/60000 (5%)]\tLoss: 0.070380\n",
            "Train Epoch: 9 [3840/60000 (6%)]\tLoss: 0.022992\n",
            "Train Epoch: 9 [4480/60000 (7%)]\tLoss: 0.095636\n",
            "Train Epoch: 9 [5120/60000 (9%)]\tLoss: 0.122260\n",
            "Train Epoch: 9 [5760/60000 (10%)]\tLoss: 0.079070\n",
            "Train Epoch: 9 [6400/60000 (11%)]\tLoss: 0.027502\n",
            "Train Epoch: 9 [7040/60000 (12%)]\tLoss: 0.041376\n",
            "Train Epoch: 9 [7680/60000 (13%)]\tLoss: 0.042485\n",
            "Train Epoch: 9 [8320/60000 (14%)]\tLoss: 0.044824\n",
            "Train Epoch: 9 [8960/60000 (15%)]\tLoss: 0.030561\n",
            "Train Epoch: 9 [9600/60000 (16%)]\tLoss: 0.006989\n",
            "Train Epoch: 9 [10240/60000 (17%)]\tLoss: 0.019085\n",
            "Train Epoch: 9 [10880/60000 (18%)]\tLoss: 0.141832\n",
            "Train Epoch: 9 [11520/60000 (19%)]\tLoss: 0.015358\n",
            "Train Epoch: 9 [12160/60000 (20%)]\tLoss: 0.051371\n",
            "Train Epoch: 9 [12800/60000 (21%)]\tLoss: 0.036944\n",
            "Train Epoch: 9 [13440/60000 (22%)]\tLoss: 0.092487\n",
            "Train Epoch: 9 [14080/60000 (23%)]\tLoss: 0.094229\n",
            "Train Epoch: 9 [14720/60000 (25%)]\tLoss: 0.038335\n",
            "Train Epoch: 9 [15360/60000 (26%)]\tLoss: 0.043873\n",
            "Train Epoch: 9 [16000/60000 (27%)]\tLoss: 0.019065\n",
            "Train Epoch: 9 [16640/60000 (28%)]\tLoss: 0.020044\n",
            "Train Epoch: 9 [17280/60000 (29%)]\tLoss: 0.043743\n",
            "Train Epoch: 9 [17920/60000 (30%)]\tLoss: 0.018020\n",
            "Train Epoch: 9 [18560/60000 (31%)]\tLoss: 0.061998\n",
            "Train Epoch: 9 [19200/60000 (32%)]\tLoss: 0.122298\n",
            "Train Epoch: 9 [19840/60000 (33%)]\tLoss: 0.052997\n",
            "Train Epoch: 9 [20480/60000 (34%)]\tLoss: 0.034222\n",
            "Train Epoch: 9 [21120/60000 (35%)]\tLoss: 0.050330\n",
            "Train Epoch: 9 [21760/60000 (36%)]\tLoss: 0.007260\n",
            "Train Epoch: 9 [22400/60000 (37%)]\tLoss: 0.020566\n",
            "Train Epoch: 9 [23040/60000 (38%)]\tLoss: 0.019989\n",
            "Train Epoch: 9 [23680/60000 (39%)]\tLoss: 0.122053\n",
            "Train Epoch: 9 [24320/60000 (41%)]\tLoss: 0.028519\n",
            "Train Epoch: 9 [24960/60000 (42%)]\tLoss: 0.023926\n",
            "Train Epoch: 9 [25600/60000 (43%)]\tLoss: 0.013563\n",
            "Train Epoch: 9 [26240/60000 (44%)]\tLoss: 0.013637\n",
            "Train Epoch: 9 [26880/60000 (45%)]\tLoss: 0.036845\n",
            "Train Epoch: 9 [27520/60000 (46%)]\tLoss: 0.009838\n",
            "Train Epoch: 9 [28160/60000 (47%)]\tLoss: 0.016975\n",
            "Train Epoch: 9 [28800/60000 (48%)]\tLoss: 0.018467\n",
            "Train Epoch: 9 [29440/60000 (49%)]\tLoss: 0.165981\n",
            "Train Epoch: 9 [30080/60000 (50%)]\tLoss: 0.063433\n",
            "Train Epoch: 9 [30720/60000 (51%)]\tLoss: 0.056773\n",
            "Train Epoch: 9 [31360/60000 (52%)]\tLoss: 0.015553\n",
            "Train Epoch: 9 [32000/60000 (53%)]\tLoss: 0.104743\n",
            "Train Epoch: 9 [32640/60000 (54%)]\tLoss: 0.117250\n",
            "Train Epoch: 9 [33280/60000 (55%)]\tLoss: 0.057803\n",
            "Train Epoch: 9 [33920/60000 (57%)]\tLoss: 0.041570\n",
            "Train Epoch: 9 [34560/60000 (58%)]\tLoss: 0.038963\n",
            "Train Epoch: 9 [35200/60000 (59%)]\tLoss: 0.008817\n",
            "Train Epoch: 9 [35840/60000 (60%)]\tLoss: 0.022109\n",
            "Train Epoch: 9 [36480/60000 (61%)]\tLoss: 0.033438\n",
            "Train Epoch: 9 [37120/60000 (62%)]\tLoss: 0.021197\n",
            "Train Epoch: 9 [37760/60000 (63%)]\tLoss: 0.035909\n",
            "Train Epoch: 9 [38400/60000 (64%)]\tLoss: 0.039373\n",
            "Train Epoch: 9 [39040/60000 (65%)]\tLoss: 0.033333\n",
            "Train Epoch: 9 [39680/60000 (66%)]\tLoss: 0.006818\n",
            "Train Epoch: 9 [40320/60000 (67%)]\tLoss: 0.140222\n",
            "Train Epoch: 9 [40960/60000 (68%)]\tLoss: 0.013779\n",
            "Train Epoch: 9 [41600/60000 (69%)]\tLoss: 0.040981\n",
            "Train Epoch: 9 [42240/60000 (70%)]\tLoss: 0.023075\n",
            "Train Epoch: 9 [42880/60000 (71%)]\tLoss: 0.032732\n",
            "Train Epoch: 9 [43520/60000 (72%)]\tLoss: 0.117002\n",
            "Train Epoch: 9 [44160/60000 (74%)]\tLoss: 0.004057\n",
            "Train Epoch: 9 [44800/60000 (75%)]\tLoss: 0.008150\n",
            "Train Epoch: 9 [45440/60000 (76%)]\tLoss: 0.063277\n",
            "Train Epoch: 9 [46080/60000 (77%)]\tLoss: 0.032353\n",
            "Train Epoch: 9 [46720/60000 (78%)]\tLoss: 0.012359\n",
            "Train Epoch: 9 [47360/60000 (79%)]\tLoss: 0.070694\n",
            "Train Epoch: 9 [48000/60000 (80%)]\tLoss: 0.050242\n",
            "Train Epoch: 9 [48640/60000 (81%)]\tLoss: 0.068368\n",
            "Train Epoch: 9 [49280/60000 (82%)]\tLoss: 0.052743\n",
            "Train Epoch: 9 [49920/60000 (83%)]\tLoss: 0.024226\n",
            "Train Epoch: 9 [50560/60000 (84%)]\tLoss: 0.102990\n",
            "Train Epoch: 9 [51200/60000 (85%)]\tLoss: 0.235192\n",
            "Train Epoch: 9 [51840/60000 (86%)]\tLoss: 0.055315\n",
            "Train Epoch: 9 [52480/60000 (87%)]\tLoss: 0.154739\n",
            "Train Epoch: 9 [53120/60000 (88%)]\tLoss: 0.050985\n",
            "Train Epoch: 9 [53760/60000 (90%)]\tLoss: 0.048231\n",
            "Train Epoch: 9 [54400/60000 (91%)]\tLoss: 0.581440\n",
            "Train Epoch: 9 [55040/60000 (92%)]\tLoss: 0.094402\n",
            "Train Epoch: 9 [55680/60000 (93%)]\tLoss: 0.046928\n",
            "Train Epoch: 9 [56320/60000 (94%)]\tLoss: 0.073964\n",
            "Train Epoch: 9 [56960/60000 (95%)]\tLoss: 0.077274\n",
            "Train Epoch: 9 [57600/60000 (96%)]\tLoss: 0.021262\n",
            "Train Epoch: 9 [58240/60000 (97%)]\tLoss: 0.109298\n",
            "Train Epoch: 9 [58880/60000 (98%)]\tLoss: 0.039363\n",
            "Train Epoch: 9 [59520/60000 (99%)]\tLoss: 0.048133\n",
            "\n",
            "Test set: Average loss: 0.0708, Accuracy: 9764/10000 (98%)\n",
            "\n"
          ]
        }
      ]
    }
  ]
}