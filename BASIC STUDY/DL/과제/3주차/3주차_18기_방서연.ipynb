{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# < 3주차 과제 1 : MLP 마음대로 다뤄보기>\n",
        "- dataset을 임의로 선정해서 직접 분석 해보기(제공한 코드를 활용해서 해보기)\n",
        "- activation functions 중 relu사용시 함수 직접 정의\n",
        "- lr, optimizer 등 바꿔보기\n",
        "- hidden layer/neuron 수를 바꾸기\n",
        "- 전처리도 추가\n",
        "- 모든 시도를 올려주세요!\n",
        "- 제일 높은 acc를 보인 시도를 명시해주세요!\n"
      ],
      "metadata": {
        "id": "sgAYo4nrw2F4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "fX437IL6qbI-"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn, optim\n",
        "from sklearn.datasets import load_digits\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from torch.utils.data import  TensorDataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data1 = load_breast_cancer()"
      ],
      "metadata": {
        "id": "Qu5tIUwxbr3W"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input1 = data1.data\n",
        "output1 = data1.target"
      ],
      "metadata": {
        "id": "CdRPuDtnbus7"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input1.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3h3UFvyQbxz7",
        "outputId": "282a8446-bb88-4700-bfd4-b37213b4e4ec"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(569, 30)"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output1.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5y6Cf4PwbzN-",
        "outputId": "0c5fc7e0-98db-4ebc-bed5-cd5bb2ebf9a5"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(569,)"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 아래 데이터셋 중 원하는 데이터셋 하나를 선택하여, 코드 과제 진행하기!\n",
        "- 1) load_digits() <br>\n",
        "- 2) load_wine()"
      ],
      "metadata": {
        "id": "oxkFzBDNmWNk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = load_digits()"
      ],
      "metadata": {
        "id": "FohgMR-dUh3x"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input = data.data\n",
        "output = data.target"
      ],
      "metadata": {
        "id": "C2P0hqZ9yBGm"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TyJ_lGWDbIFJ",
        "outputId": "c4ad87ff-3e9c-4fb8-cf8a-dc83630820e1"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1797, 64)"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5XEUzyxRbJP_",
        "outputId": "e4abc787-a61c-4d1b-df5b-2e81913fcac2"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1797,)"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "torch.manual_seed(777)\n",
        "if device == \"cuda\":\n",
        "  torch.cuda.manual_seed_all(777)"
      ],
      "metadata": {
        "id": "SggpQfSPt85C"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train, x_test, y_train, y_test = train_test_split(input, output, test_size = 0.3, random_state = 42, stratify= data.target, shuffle = True)\n",
        "\n",
        "x_train = torch.FloatTensor(x_train).to(device)\n",
        "y_train = torch.LongTensor(y_train).to(device)\n",
        "x_test = torch.FloatTensor(x_test)\n",
        "y_test = torch.LongTensor(y_test)\n",
        "\n",
        "# 데이터를 tensor로 바꿔주고 gpu 연산이 가능해지도록 gpu에 옮김\n",
        "# label 값을 왜 long 에 옮겨놓는가? loss function이 다르기 때문"
      ],
      "metadata": {
        "id": "bLMzf-2ntYeX"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(x_train[0])\n",
        "print(y_train[0])\n",
        "\n",
        "#input 30개 (속성이 30개)\n",
        "#y의 class는 2개 (양성과 음성)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "umEdiTZkrVqS",
        "outputId": "e71db1e4-601f-4e34-dc54-1058f5f52911"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([ 0.,  0.,  0., 16., 12.,  1.,  0.,  0.,  0.,  0.,  6., 16., 14.,  7.,\n",
            "         0.,  0.,  0.,  0., 14., 15.,  1., 11.,  0.,  0.,  0.,  0., 16., 15.,\n",
            "         0., 14.,  1.,  0.,  0.,  1., 16., 10.,  0., 14.,  2.,  0.,  0.,  0.,\n",
            "        15., 13.,  3., 15.,  3.,  0.,  0.,  0.,  9., 16., 16., 15.,  0.,  0.,\n",
            "         0.,  0.,  0., 13., 16.,  8.,  0.,  0.], device='cuda:0')\n",
            "tensor(0, device='cuda:0')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "파이토치에서는 데이터셋을 좀 더 쉽게 다룰 수 있도록 유용한 도구로서 torch.utils.data.Dataset과 torch.utils.data.DataLoader를 제공합니다. 이를 사용하면 미니 배치 학습, 데이터 셔플(shuffle), 병렬 처리까지 간단히 수행할 수 있습니다. 기본적인 사용 방법은 Dataset을 정의하고, 이를 DataLoader에 전달하는 것\n",
        "- init : class 에서 객체가 생성되면 바로 실행되는 함수\n",
        "- len : observation 수를 정의하는 함수\n",
        "- getitem : iteration 마다 해당하는 데이터를 돌려주는 함수"
      ],
      "metadata": {
        "id": "combmxzmYFyn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://wikidocs.net/57165"
      ],
      "metadata": {
        "id": "lJjz6CLOWIhy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset\n",
        "\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "class CustomDataset(Dataset):\n",
        "  def __init__(self):\n",
        "    self.x_data = x_train\n",
        "    self.y_data = [[y] for y in y_train]\n",
        "#  데이터셋의 전처리를 해주는 부분\n",
        "    #self.scaler = StandardScaler()\n",
        "    #self.scaler = MinMaxScaler() # 하나는 주석처리\n",
        "    #self.scaler = scaler\n",
        "    #self.x_data = self.scaler.fit_transform(self.x_data)\n",
        "\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.x_data)\n",
        "#  데이터셋의 길이. 즉, 총 샘플의 수를 적어주는 부분\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    x = torch.FloatTensor(self.x_data[idx]).to(device)\n",
        "    y = torch.LongTensor(self.y_data[idx]).to(device)\n",
        "#  데이터셋에서 특정 1개의 샘플을 가져오는 함수\n",
        "\n",
        "    return x,y"
      ],
      "metadata": {
        "id": "y38TlgXoqV5Z"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 8\n",
        "\n",
        "scaler = StandardScaler()\n",
        "# scaler = MinMaxScaler()  # Uncomment if you want to use MinMaxScaler\n",
        "\n",
        "#dataset = CustomDataset(x_train, y_train, scaler)\n",
        "dataset = CustomDataset()\n",
        "dataloader = DataLoader(dataset, batch_size=batch_size)"
      ],
      "metadata": {
        "id": "x8VHwnuFqino"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# input_layer와 hidden_layer의 노드 개수를 어떻게 바꿔줘야할까?\n",
        "# hidden layer/neuron 수를 바꾸기\n",
        "\n",
        "model = nn.Sequential(\n",
        "          nn.Linear(30,398, bias=True),\n",
        "          nn.Sigmoid(),\n",
        "          nn.Linear(398,15, bias=True),\n",
        "          nn.Sigmoid(),\n",
        "          nn.Linear(15,5, bias=True),\n",
        "          nn.Softmax()\n",
        "          ).to(device)"
      ],
      "metadata": {
        "id": "C6V7a4tyq6Jc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = nn.Sequential(\n",
        "          nn.Linear(64,200, bias=True),\n",
        "          nn.Sigmoid(),\n",
        "          nn.Linear(200,100, bias=True),\n",
        "          nn.Sigmoid(),\n",
        "          nn.Linear(100,5, bias=True),\n",
        "          nn.Softmax()\n",
        "          ).to(device)"
      ],
      "metadata": {
        "id": "qSiG3L4oXSJa"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "class로 구현 가능\n",
        "- init : 초기 생성 함수\n",
        "- foward : 순전파(입력값 => 예측값 의 과정)"
      ],
      "metadata": {
        "id": "07uV8RY7Yr_5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Model(torch.nn.Module):\n",
        "\n",
        "  def __init__(self):\n",
        "    super(Model, self).__init__()\n",
        "    self.layer1 = nn.Sequential(\n",
        "          nn.Linear(64,1257, bias=True), # input_layer = 30, hidden_layer1 = 398\n",
        "          nn.Sigmoid(),\n",
        "        nn.BatchNorm1d(1257)\n",
        "    )\n",
        "  # activation function 이용\n",
        "  #   nn.ReLU()\n",
        "  #   nn.tanH()\n",
        "  #   https://pytorch.org/docs/stable/nn.html 그 외에도 여기서 확인 가능함\n",
        "  #   파라미터가 필요하지 않다는 것이 특징\n",
        "\n",
        "  # batch normazliation 1d, 파라미터 값으로 vector의 길이를 전해줌\n",
        "  # 추후에 이미지를 다루게 된다면, 그때는 batch normalization 2d를 이용하게 됨\n",
        "  # 그때는 파라미터 값으로 채널, 가로, 세로 길이를 전달해주게 됨\n",
        "\n",
        "    self.layer2 = nn.Sequential(\n",
        "          nn.Linear(1257,15, bias=True), # hidden_layer1 = 398, hidden_layer2 = 15\n",
        "        nn.Sigmoid()\n",
        "    )\n",
        "    self.layer3 = nn.Sequential(\n",
        "          nn.Linear(15,10, bias=True), # hidden_layer1 = 15, hidden_layer2 = 10\n",
        "        nn.Sigmoid()\n",
        "    )\n",
        "    self.layer4 = nn.Sequential(\n",
        "        nn.Linear(10, 5, bias=True), # hidden_layer3 = 10, output_layer = 5\n",
        "        nn.Softmax()\n",
        "    )\n",
        "\n",
        "  def forward(self,x):\n",
        "    output = self.layer1(x)\n",
        "    output = self.layer2(output)\n",
        "    output = self.layer3(output)\n",
        "    output = self.layer4(output)\n",
        "    return output"
      ],
      "metadata": {
        "id": "a0zLstbMqxEZ"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def init_weights(layer):\n",
        "    if isinstance(layer, nn.Linear):\n",
        "        torch.nn.init.xavier_uniform(layer.weight)\n",
        "        layer.bias.data.fill_(0.01)\n",
        "\n",
        "        #xavier사용\n",
        "        # Layer의 weight를 어떤 분포를 가지도록 초기화시켜줌+global minimum찾기 위해서"
      ],
      "metadata": {
        "id": "kqcqqkECrSGK"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Model().to(device)\n",
        "model.apply(init_weights)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oMDUBFg6rUpw",
        "outputId": "09203074-4d33-47fd-e465-803b268b2986"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-21-6196a47a4462>:3: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "  torch.nn.init.xavier_uniform(layer.weight)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Model(\n",
              "  (layer1): Sequential(\n",
              "    (0): Linear(in_features=64, out_features=1257, bias=True)\n",
              "    (1): Sigmoid()\n",
              "    (2): BatchNorm1d(1257, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  )\n",
              "  (layer2): Sequential(\n",
              "    (0): Linear(in_features=1257, out_features=15, bias=True)\n",
              "    (1): Sigmoid()\n",
              "  )\n",
              "  (layer3): Sequential(\n",
              "    (0): Linear(in_features=15, out_features=10, bias=True)\n",
              "    (1): Sigmoid()\n",
              "  )\n",
              "  (layer4): Sequential(\n",
              "    (0): Linear(in_features=10, out_features=5, bias=True)\n",
              "    (1): Softmax(dim=None)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OwZt5CetrYFb",
        "outputId": "fe01a0ba-90b9-4120-8f48-2f6710ed17c8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model(\n",
            "  (layer1): Sequential(\n",
            "    (0): Linear(in_features=30, out_features=398, bias=True)\n",
            "    (1): Sigmoid()\n",
            "    (2): BatchNorm1d(398, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  )\n",
            "  (layer2): Sequential(\n",
            "    (0): Linear(in_features=398, out_features=15, bias=True)\n",
            "    (1): Sigmoid()\n",
            "  )\n",
            "  (layer3): Sequential(\n",
            "    (0): Linear(in_features=15, out_features=10, bias=True)\n",
            "    (1): Sigmoid()\n",
            "  )\n",
            "  (layer4): Sequential(\n",
            "    (0): Linear(in_features=10, out_features=5, bias=True)\n",
            "    (1): Softmax(dim=None)\n",
            "  )\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4RRiQ1ujYpmJ",
        "outputId": "30febe15-c2ad-4823-9d36-4fbb3f0be835"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model(\n",
            "  (layer1): Sequential(\n",
            "    (0): Linear(in_features=30, out_features=60, bias=True)\n",
            "    (1): Sigmoid()\n",
            "    (2): BatchNorm1d(60, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  )\n",
            "  (layer2): Sequential(\n",
            "    (0): Linear(in_features=60, out_features=15, bias=True)\n",
            "    (1): Sigmoid()\n",
            "  )\n",
            "  (layer3): Sequential(\n",
            "    (0): Linear(in_features=15, out_features=10, bias=True)\n",
            "    (1): Sigmoid()\n",
            "  )\n",
            "  (layer4): Sequential(\n",
            "    (0): Linear(in_features=10, out_features=5, bias=True)\n",
            "    (1): Softmax(dim=None)\n",
            "  )\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss_fn  = nn.CrossEntropyLoss().to(device)\n",
        "\n",
        "# 여러가지 optimizer 시도해보기\n",
        "# lr 바꿔보기\n",
        "\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr= 0.01)\n",
        "\n",
        "# https://pytorch.org/docs/stable/optim.html#module-torch.optim 페이지 참조\n",
        "# optimizer = optim.SGD(model.parameters(), lr=0.1, momentum=0.9)\n",
        "# sgd 등등\n",
        "\n",
        "#optimizer = optim.RMSprop(model.parameters(), lr=0.01)"
      ],
      "metadata": {
        "id": "AYFp-eTErh7b"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "losses = []\n",
        "for epoch in range(100):\n",
        "\n",
        "  optimizer.zero_grad()\n",
        "  hypothesis = model(x_train)\n",
        "\n",
        "  # 비용 함수\n",
        "  cost = loss_fn(hypothesis, y_train)\n",
        "  cost.backward()\n",
        "  optimizer.step()\n",
        "  losses.append(cost.item())\n",
        "\n",
        "  if epoch % 10 == 0:\n",
        "    print(epoch, cost.item())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 393
        },
        "id": "90QxHvlIrjS7",
        "outputId": "acffae70-289a-49cb-e6c8-952c1663a868"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "CUDA error: CUBLAS_STATUS_EXECUTION_FAILED when calling cublasLtMatmul with transpose_mat1 1 transpose_mat2 0 m 1257 n 1257 k 64 mat1_ld 64 mat2_ld 64 result_ld 1257 abcType 0 computeType 68 scaleType 0",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-26-de979bc2cd9d>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m   \u001b[0mhypothesis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m   \u001b[0;31m# 비용 함수\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-20-c7a231fd41bf>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    213\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 215\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    216\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: CUBLAS_STATUS_EXECUTION_FAILED when calling cublasLtMatmul with transpose_mat1 1 transpose_mat2 0 m 1257 n 1257 k 64 mat1_ld 64 mat2_ld 64 result_ld 1257 abcType 0 computeType 68 scaleType 0"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(x_train.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UnLZlwYlZ8qm",
        "outputId": "9a10eed4-46b5-48d4-ef2a-fad04d50cc94"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1257, 64])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(losses)\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 430
        },
        "id": "81ASYrW7roFM",
        "outputId": "625e4603-526d-47d6-a61c-7381d3f6382a"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "  model = model.to('cpu')\n",
        "  y_pred = model(x_test)\n",
        "  y_pred = y_pred.detach().numpy()\n",
        "  predicted = np.argmax(y_pred, axis =1)\n",
        "  accuracy = (accuracy_score(predicted, y_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h4kJzpLErqhZ",
        "outputId": "9b336fde-36f4-470d-9a2f-dc21055e0353"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/container.py:141: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  input = module(input)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'model의 output은 :  {y_pred[0]}')\n",
        "print(f'argmax를 한 후의 output은 {predicted[0]}')\n",
        "print(f'accuracy는 {accuracy}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vyIKhs3Nr6Ay",
        "outputId": "6d73d644-c68b-4a84-defc-6f57192ef91a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model의 output은 :  [4.3696738e-03 9.9337065e-01 1.5067915e-03 2.7046577e-04 4.8240164e-04]\n",
            "argmax를 한 후의 output은 1\n",
            "accuracy는 0.9239766081871345\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# < 3주차 과제 2 : CNN 맛보기>"
      ],
      "metadata": {
        "id": "3RzRM7xThZV_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from __future__ import print_function\n",
        "import argparse\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.autograd import Variable"
      ],
      "metadata": {
        "id": "56xqgtLxhZw6"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training settings\n",
        "\n",
        "batch_size = 64\n",
        "\n",
        "# MNIST Dataset\n",
        "train_dataset = datasets.MNIST(root='./data/',\n",
        "                               train=True,\n",
        "                               transform=transforms.ToTensor(),\n",
        "                               download=True)\n",
        "\n",
        "test_dataset = datasets.MNIST(root='./data/',\n",
        "                              train=False,\n",
        "                              transform=transforms.ToTensor())\n",
        "\n",
        "# Data Loader (Input Pipeline)\n",
        "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
        "                                           batch_size=batch_size,\n",
        "                                           shuffle=True)\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
        "                                          batch_size=batch_size,\n",
        "                                          shuffle=False)"
      ],
      "metadata": {
        "id": "TzkF2bFNhcQ2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "93ad0672-62b1-4eae-e2cf-7df8274215d9"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9912422/9912422 [00:00<00:00, 128641700.68it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28881/28881 [00:00<00:00, 40827668.97it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1648877/1648877 [00:00<00:00, 34749381.46it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4542/4542 [00:00<00:00, 17704952.39it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Net(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(Net, self).__init__()\n",
        "    self.conv1 = nn.Conv2d(1, 10, kernel_size = 5)\n",
        "    self.conv2 = nn.Conv2d(10, 20, kernel_size = 5)\n",
        "    self.mp = nn.MaxPool2d(2)\n",
        "    self.fc = nn.Linear(320 , 10) ### : 알맞는 input은?\n",
        "\n",
        "  def forward(self, x):\n",
        "    in_size = x.size(0)\n",
        "    x = F.relu(self.mp(self.conv1(x)))\n",
        "    x = F.relu(self.mp(self.conv2(x)))\n",
        "    x = x.view(in_size, -1)\n",
        "    x = self.fc(x)\n",
        "    return F.log_softmax(x)"
      ],
      "metadata": {
        "id": "tLCSvgganBrH"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Net()\n",
        "optimizer = optim.SGD(model.parameters(), lr = 0.01, momentum = 0.5)"
      ],
      "metadata": {
        "id": "lkYZ4pUdnUHc"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(epoch):\n",
        "  model.train()\n",
        "  for batch_idx, (data, target) in enumerate(train_loader):\n",
        "    data, target = Variable(data), Variable(target)\n",
        "    optimizer.zero_grad()\n",
        "    output = model(data)\n",
        "    loss = F.nll_loss(output, target)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    if batch_idx % 10 == 0:\n",
        "      print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "          epoch, batch_idx * len(data), len(train_loader.dataset),\n",
        "          100. * batch_idx / len(train_loader), loss.item()))"
      ],
      "metadata": {
        "id": "IzUrEM3EnXJb"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test():\n",
        "    model.eval() #model.eval() 의 기능은?\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    for data, target in test_loader:\n",
        "        data, target = Variable(data, volatile=True), Variable(target)\n",
        "        output = model(data)\n",
        "        test_loss += F.nll_loss(output, target, size_average=False).data # nll_loss?? / cross entropy loss와의 관계 확인!\n",
        "        pred = output.data.max(1, keepdim=True)[1]\n",
        "        correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
        "\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "        test_loss, correct, len(test_loader.dataset),\n",
        "        100. * correct / len(test_loader.dataset)))"
      ],
      "metadata": {
        "id": "EFi0gYJGn2aa"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(1, 10):\n",
        "    train(epoch)\n",
        "    test()"
      ],
      "metadata": {
        "id": "zSvSZb_Bn4Nx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a0f76835-db1e-4a20-8528-31d0fb992184"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-31-006ecbc6c563>:15: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return F.log_softmax(x)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.296850\n",
            "Train Epoch: 1 [640/60000 (1%)]\tLoss: 2.292770\n",
            "Train Epoch: 1 [1280/60000 (2%)]\tLoss: 2.264382\n",
            "Train Epoch: 1 [1920/60000 (3%)]\tLoss: 2.265215\n",
            "Train Epoch: 1 [2560/60000 (4%)]\tLoss: 2.243272\n",
            "Train Epoch: 1 [3200/60000 (5%)]\tLoss: 2.203503\n",
            "Train Epoch: 1 [3840/60000 (6%)]\tLoss: 2.164200\n",
            "Train Epoch: 1 [4480/60000 (7%)]\tLoss: 2.109573\n",
            "Train Epoch: 1 [5120/60000 (9%)]\tLoss: 2.076760\n",
            "Train Epoch: 1 [5760/60000 (10%)]\tLoss: 1.949383\n",
            "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 1.832283\n",
            "Train Epoch: 1 [7040/60000 (12%)]\tLoss: 1.691589\n",
            "Train Epoch: 1 [7680/60000 (13%)]\tLoss: 1.503357\n",
            "Train Epoch: 1 [8320/60000 (14%)]\tLoss: 1.253193\n",
            "Train Epoch: 1 [8960/60000 (15%)]\tLoss: 1.056212\n",
            "Train Epoch: 1 [9600/60000 (16%)]\tLoss: 0.836572\n",
            "Train Epoch: 1 [10240/60000 (17%)]\tLoss: 0.648694\n",
            "Train Epoch: 1 [10880/60000 (18%)]\tLoss: 0.743896\n",
            "Train Epoch: 1 [11520/60000 (19%)]\tLoss: 0.689470\n",
            "Train Epoch: 1 [12160/60000 (20%)]\tLoss: 0.508749\n",
            "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.512068\n",
            "Train Epoch: 1 [13440/60000 (22%)]\tLoss: 0.795956\n",
            "Train Epoch: 1 [14080/60000 (23%)]\tLoss: 0.683483\n",
            "Train Epoch: 1 [14720/60000 (25%)]\tLoss: 0.439123\n",
            "Train Epoch: 1 [15360/60000 (26%)]\tLoss: 0.640609\n",
            "Train Epoch: 1 [16000/60000 (27%)]\tLoss: 0.313575\n",
            "Train Epoch: 1 [16640/60000 (28%)]\tLoss: 0.403848\n",
            "Train Epoch: 1 [17280/60000 (29%)]\tLoss: 0.421104\n",
            "Train Epoch: 1 [17920/60000 (30%)]\tLoss: 0.460406\n",
            "Train Epoch: 1 [18560/60000 (31%)]\tLoss: 0.558470\n",
            "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.335662\n",
            "Train Epoch: 1 [19840/60000 (33%)]\tLoss: 0.359416\n",
            "Train Epoch: 1 [20480/60000 (34%)]\tLoss: 0.393250\n",
            "Train Epoch: 1 [21120/60000 (35%)]\tLoss: 0.279657\n",
            "Train Epoch: 1 [21760/60000 (36%)]\tLoss: 0.361013\n",
            "Train Epoch: 1 [22400/60000 (37%)]\tLoss: 0.401142\n",
            "Train Epoch: 1 [23040/60000 (38%)]\tLoss: 0.315094\n",
            "Train Epoch: 1 [23680/60000 (39%)]\tLoss: 0.308736\n",
            "Train Epoch: 1 [24320/60000 (41%)]\tLoss: 0.242540\n",
            "Train Epoch: 1 [24960/60000 (42%)]\tLoss: 0.228798\n",
            "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.317567\n",
            "Train Epoch: 1 [26240/60000 (44%)]\tLoss: 0.378114\n",
            "Train Epoch: 1 [26880/60000 (45%)]\tLoss: 0.232855\n",
            "Train Epoch: 1 [27520/60000 (46%)]\tLoss: 0.353012\n",
            "Train Epoch: 1 [28160/60000 (47%)]\tLoss: 0.407709\n",
            "Train Epoch: 1 [28800/60000 (48%)]\tLoss: 0.154372\n",
            "Train Epoch: 1 [29440/60000 (49%)]\tLoss: 0.466653\n",
            "Train Epoch: 1 [30080/60000 (50%)]\tLoss: 0.190220\n",
            "Train Epoch: 1 [30720/60000 (51%)]\tLoss: 0.306661\n",
            "Train Epoch: 1 [31360/60000 (52%)]\tLoss: 0.243453\n",
            "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.167940\n",
            "Train Epoch: 1 [32640/60000 (54%)]\tLoss: 0.322708\n",
            "Train Epoch: 1 [33280/60000 (55%)]\tLoss: 0.506893\n",
            "Train Epoch: 1 [33920/60000 (57%)]\tLoss: 0.380123\n",
            "Train Epoch: 1 [34560/60000 (58%)]\tLoss: 0.338088\n",
            "Train Epoch: 1 [35200/60000 (59%)]\tLoss: 0.294857\n",
            "Train Epoch: 1 [35840/60000 (60%)]\tLoss: 0.487116\n",
            "Train Epoch: 1 [36480/60000 (61%)]\tLoss: 0.121271\n",
            "Train Epoch: 1 [37120/60000 (62%)]\tLoss: 0.201526\n",
            "Train Epoch: 1 [37760/60000 (63%)]\tLoss: 0.185376\n",
            "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.231483\n",
            "Train Epoch: 1 [39040/60000 (65%)]\tLoss: 0.293414\n",
            "Train Epoch: 1 [39680/60000 (66%)]\tLoss: 0.412018\n",
            "Train Epoch: 1 [40320/60000 (67%)]\tLoss: 0.221384\n",
            "Train Epoch: 1 [40960/60000 (68%)]\tLoss: 0.144107\n",
            "Train Epoch: 1 [41600/60000 (69%)]\tLoss: 0.224165\n",
            "Train Epoch: 1 [42240/60000 (70%)]\tLoss: 0.366595\n",
            "Train Epoch: 1 [42880/60000 (71%)]\tLoss: 0.251035\n",
            "Train Epoch: 1 [43520/60000 (72%)]\tLoss: 0.102802\n",
            "Train Epoch: 1 [44160/60000 (74%)]\tLoss: 0.170099\n",
            "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.177239\n",
            "Train Epoch: 1 [45440/60000 (76%)]\tLoss: 0.269174\n",
            "Train Epoch: 1 [46080/60000 (77%)]\tLoss: 0.284241\n",
            "Train Epoch: 1 [46720/60000 (78%)]\tLoss: 0.164390\n",
            "Train Epoch: 1 [47360/60000 (79%)]\tLoss: 0.288239\n",
            "Train Epoch: 1 [48000/60000 (80%)]\tLoss: 0.401214\n",
            "Train Epoch: 1 [48640/60000 (81%)]\tLoss: 0.221674\n",
            "Train Epoch: 1 [49280/60000 (82%)]\tLoss: 0.216805\n",
            "Train Epoch: 1 [49920/60000 (83%)]\tLoss: 0.312839\n",
            "Train Epoch: 1 [50560/60000 (84%)]\tLoss: 0.181447\n",
            "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.196345\n",
            "Train Epoch: 1 [51840/60000 (86%)]\tLoss: 0.110866\n",
            "Train Epoch: 1 [52480/60000 (87%)]\tLoss: 0.212694\n",
            "Train Epoch: 1 [53120/60000 (88%)]\tLoss: 0.117312\n",
            "Train Epoch: 1 [53760/60000 (90%)]\tLoss: 0.223168\n",
            "Train Epoch: 1 [54400/60000 (91%)]\tLoss: 0.154486\n",
            "Train Epoch: 1 [55040/60000 (92%)]\tLoss: 0.198809\n",
            "Train Epoch: 1 [55680/60000 (93%)]\tLoss: 0.187043\n",
            "Train Epoch: 1 [56320/60000 (94%)]\tLoss: 0.302028\n",
            "Train Epoch: 1 [56960/60000 (95%)]\tLoss: 0.208204\n",
            "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.226758\n",
            "Train Epoch: 1 [58240/60000 (97%)]\tLoss: 0.165741\n",
            "Train Epoch: 1 [58880/60000 (98%)]\tLoss: 0.195758\n",
            "Train Epoch: 1 [59520/60000 (99%)]\tLoss: 0.169209\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-34-f52337105c2a>:6: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
            "  data, target = Variable(data, volatile=True), Variable(target)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
            "  warnings.warn(warning.format(ret))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.1676, Accuracy: 9509/10000 (95%)\n",
            "\n",
            "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.260767\n",
            "Train Epoch: 2 [640/60000 (1%)]\tLoss: 0.247650\n",
            "Train Epoch: 2 [1280/60000 (2%)]\tLoss: 0.134128\n",
            "Train Epoch: 2 [1920/60000 (3%)]\tLoss: 0.196315\n",
            "Train Epoch: 2 [2560/60000 (4%)]\tLoss: 0.083859\n",
            "Train Epoch: 2 [3200/60000 (5%)]\tLoss: 0.206104\n",
            "Train Epoch: 2 [3840/60000 (6%)]\tLoss: 0.257733\n",
            "Train Epoch: 2 [4480/60000 (7%)]\tLoss: 0.142071\n",
            "Train Epoch: 2 [5120/60000 (9%)]\tLoss: 0.122771\n",
            "Train Epoch: 2 [5760/60000 (10%)]\tLoss: 0.109826\n",
            "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 0.084878\n",
            "Train Epoch: 2 [7040/60000 (12%)]\tLoss: 0.261807\n",
            "Train Epoch: 2 [7680/60000 (13%)]\tLoss: 0.124530\n",
            "Train Epoch: 2 [8320/60000 (14%)]\tLoss: 0.177584\n",
            "Train Epoch: 2 [8960/60000 (15%)]\tLoss: 0.223157\n",
            "Train Epoch: 2 [9600/60000 (16%)]\tLoss: 0.233964\n",
            "Train Epoch: 2 [10240/60000 (17%)]\tLoss: 0.116032\n",
            "Train Epoch: 2 [10880/60000 (18%)]\tLoss: 0.246247\n",
            "Train Epoch: 2 [11520/60000 (19%)]\tLoss: 0.158162\n",
            "Train Epoch: 2 [12160/60000 (20%)]\tLoss: 0.124799\n",
            "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 0.214254\n",
            "Train Epoch: 2 [13440/60000 (22%)]\tLoss: 0.155672\n",
            "Train Epoch: 2 [14080/60000 (23%)]\tLoss: 0.203405\n",
            "Train Epoch: 2 [14720/60000 (25%)]\tLoss: 0.238094\n",
            "Train Epoch: 2 [15360/60000 (26%)]\tLoss: 0.125334\n",
            "Train Epoch: 2 [16000/60000 (27%)]\tLoss: 0.254593\n",
            "Train Epoch: 2 [16640/60000 (28%)]\tLoss: 0.247020\n",
            "Train Epoch: 2 [17280/60000 (29%)]\tLoss: 0.088378\n",
            "Train Epoch: 2 [17920/60000 (30%)]\tLoss: 0.172806\n",
            "Train Epoch: 2 [18560/60000 (31%)]\tLoss: 0.178988\n",
            "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 0.264861\n",
            "Train Epoch: 2 [19840/60000 (33%)]\tLoss: 0.127813\n",
            "Train Epoch: 2 [20480/60000 (34%)]\tLoss: 0.180485\n",
            "Train Epoch: 2 [21120/60000 (35%)]\tLoss: 0.140487\n",
            "Train Epoch: 2 [21760/60000 (36%)]\tLoss: 0.169691\n",
            "Train Epoch: 2 [22400/60000 (37%)]\tLoss: 0.105625\n",
            "Train Epoch: 2 [23040/60000 (38%)]\tLoss: 0.131385\n",
            "Train Epoch: 2 [23680/60000 (39%)]\tLoss: 0.091577\n",
            "Train Epoch: 2 [24320/60000 (41%)]\tLoss: 0.195221\n",
            "Train Epoch: 2 [24960/60000 (42%)]\tLoss: 0.077898\n",
            "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.087079\n",
            "Train Epoch: 2 [26240/60000 (44%)]\tLoss: 0.170550\n",
            "Train Epoch: 2 [26880/60000 (45%)]\tLoss: 0.151543\n",
            "Train Epoch: 2 [27520/60000 (46%)]\tLoss: 0.069989\n",
            "Train Epoch: 2 [28160/60000 (47%)]\tLoss: 0.150988\n",
            "Train Epoch: 2 [28800/60000 (48%)]\tLoss: 0.103976\n",
            "Train Epoch: 2 [29440/60000 (49%)]\tLoss: 0.233545\n",
            "Train Epoch: 2 [30080/60000 (50%)]\tLoss: 0.090763\n",
            "Train Epoch: 2 [30720/60000 (51%)]\tLoss: 0.135238\n",
            "Train Epoch: 2 [31360/60000 (52%)]\tLoss: 0.079207\n",
            "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 0.081542\n",
            "Train Epoch: 2 [32640/60000 (54%)]\tLoss: 0.069060\n",
            "Train Epoch: 2 [33280/60000 (55%)]\tLoss: 0.083421\n",
            "Train Epoch: 2 [33920/60000 (57%)]\tLoss: 0.166005\n",
            "Train Epoch: 2 [34560/60000 (58%)]\tLoss: 0.132003\n",
            "Train Epoch: 2 [35200/60000 (59%)]\tLoss: 0.155793\n",
            "Train Epoch: 2 [35840/60000 (60%)]\tLoss: 0.157366\n",
            "Train Epoch: 2 [36480/60000 (61%)]\tLoss: 0.231430\n",
            "Train Epoch: 2 [37120/60000 (62%)]\tLoss: 0.062262\n",
            "Train Epoch: 2 [37760/60000 (63%)]\tLoss: 0.081509\n",
            "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 0.320863\n",
            "Train Epoch: 2 [39040/60000 (65%)]\tLoss: 0.053519\n",
            "Train Epoch: 2 [39680/60000 (66%)]\tLoss: 0.346656\n",
            "Train Epoch: 2 [40320/60000 (67%)]\tLoss: 0.204136\n",
            "Train Epoch: 2 [40960/60000 (68%)]\tLoss: 0.370520\n",
            "Train Epoch: 2 [41600/60000 (69%)]\tLoss: 0.117673\n",
            "Train Epoch: 2 [42240/60000 (70%)]\tLoss: 0.445525\n",
            "Train Epoch: 2 [42880/60000 (71%)]\tLoss: 0.124164\n",
            "Train Epoch: 2 [43520/60000 (72%)]\tLoss: 0.128371\n",
            "Train Epoch: 2 [44160/60000 (74%)]\tLoss: 0.199293\n",
            "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 0.096741\n",
            "Train Epoch: 2 [45440/60000 (76%)]\tLoss: 0.147828\n",
            "Train Epoch: 2 [46080/60000 (77%)]\tLoss: 0.155803\n",
            "Train Epoch: 2 [46720/60000 (78%)]\tLoss: 0.157648\n",
            "Train Epoch: 2 [47360/60000 (79%)]\tLoss: 0.057631\n",
            "Train Epoch: 2 [48000/60000 (80%)]\tLoss: 0.094488\n",
            "Train Epoch: 2 [48640/60000 (81%)]\tLoss: 0.254548\n",
            "Train Epoch: 2 [49280/60000 (82%)]\tLoss: 0.111214\n",
            "Train Epoch: 2 [49920/60000 (83%)]\tLoss: 0.123198\n",
            "Train Epoch: 2 [50560/60000 (84%)]\tLoss: 0.034000\n",
            "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.100751\n",
            "Train Epoch: 2 [51840/60000 (86%)]\tLoss: 0.064253\n",
            "Train Epoch: 2 [52480/60000 (87%)]\tLoss: 0.106284\n",
            "Train Epoch: 2 [53120/60000 (88%)]\tLoss: 0.067414\n",
            "Train Epoch: 2 [53760/60000 (90%)]\tLoss: 0.080924\n",
            "Train Epoch: 2 [54400/60000 (91%)]\tLoss: 0.186419\n",
            "Train Epoch: 2 [55040/60000 (92%)]\tLoss: 0.172662\n",
            "Train Epoch: 2 [55680/60000 (93%)]\tLoss: 0.161583\n",
            "Train Epoch: 2 [56320/60000 (94%)]\tLoss: 0.212545\n",
            "Train Epoch: 2 [56960/60000 (95%)]\tLoss: 0.108049\n",
            "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 0.068879\n",
            "Train Epoch: 2 [58240/60000 (97%)]\tLoss: 0.081062\n",
            "Train Epoch: 2 [58880/60000 (98%)]\tLoss: 0.127650\n",
            "Train Epoch: 2 [59520/60000 (99%)]\tLoss: 0.100494\n",
            "\n",
            "Test set: Average loss: 0.1043, Accuracy: 9676/10000 (97%)\n",
            "\n",
            "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.059771\n",
            "Train Epoch: 3 [640/60000 (1%)]\tLoss: 0.089302\n",
            "Train Epoch: 3 [1280/60000 (2%)]\tLoss: 0.137026\n",
            "Train Epoch: 3 [1920/60000 (3%)]\tLoss: 0.153726\n",
            "Train Epoch: 3 [2560/60000 (4%)]\tLoss: 0.064144\n",
            "Train Epoch: 3 [3200/60000 (5%)]\tLoss: 0.127605\n",
            "Train Epoch: 3 [3840/60000 (6%)]\tLoss: 0.086415\n",
            "Train Epoch: 3 [4480/60000 (7%)]\tLoss: 0.154257\n",
            "Train Epoch: 3 [5120/60000 (9%)]\tLoss: 0.071100\n",
            "Train Epoch: 3 [5760/60000 (10%)]\tLoss: 0.088585\n",
            "Train Epoch: 3 [6400/60000 (11%)]\tLoss: 0.127738\n",
            "Train Epoch: 3 [7040/60000 (12%)]\tLoss: 0.127319\n",
            "Train Epoch: 3 [7680/60000 (13%)]\tLoss: 0.105851\n",
            "Train Epoch: 3 [8320/60000 (14%)]\tLoss: 0.118146\n",
            "Train Epoch: 3 [8960/60000 (15%)]\tLoss: 0.187329\n",
            "Train Epoch: 3 [9600/60000 (16%)]\tLoss: 0.078531\n",
            "Train Epoch: 3 [10240/60000 (17%)]\tLoss: 0.053272\n",
            "Train Epoch: 3 [10880/60000 (18%)]\tLoss: 0.266613\n",
            "Train Epoch: 3 [11520/60000 (19%)]\tLoss: 0.124044\n",
            "Train Epoch: 3 [12160/60000 (20%)]\tLoss: 0.115271\n",
            "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 0.204347\n",
            "Train Epoch: 3 [13440/60000 (22%)]\tLoss: 0.241782\n",
            "Train Epoch: 3 [14080/60000 (23%)]\tLoss: 0.048203\n",
            "Train Epoch: 3 [14720/60000 (25%)]\tLoss: 0.113804\n",
            "Train Epoch: 3 [15360/60000 (26%)]\tLoss: 0.064075\n",
            "Train Epoch: 3 [16000/60000 (27%)]\tLoss: 0.117930\n",
            "Train Epoch: 3 [16640/60000 (28%)]\tLoss: 0.055405\n",
            "Train Epoch: 3 [17280/60000 (29%)]\tLoss: 0.039264\n",
            "Train Epoch: 3 [17920/60000 (30%)]\tLoss: 0.045688\n",
            "Train Epoch: 3 [18560/60000 (31%)]\tLoss: 0.068056\n",
            "Train Epoch: 3 [19200/60000 (32%)]\tLoss: 0.063983\n",
            "Train Epoch: 3 [19840/60000 (33%)]\tLoss: 0.059702\n",
            "Train Epoch: 3 [20480/60000 (34%)]\tLoss: 0.074839\n",
            "Train Epoch: 3 [21120/60000 (35%)]\tLoss: 0.280152\n",
            "Train Epoch: 3 [21760/60000 (36%)]\tLoss: 0.131845\n",
            "Train Epoch: 3 [22400/60000 (37%)]\tLoss: 0.273695\n",
            "Train Epoch: 3 [23040/60000 (38%)]\tLoss: 0.052920\n",
            "Train Epoch: 3 [23680/60000 (39%)]\tLoss: 0.085033\n",
            "Train Epoch: 3 [24320/60000 (41%)]\tLoss: 0.160165\n",
            "Train Epoch: 3 [24960/60000 (42%)]\tLoss: 0.081862\n",
            "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.177543\n",
            "Train Epoch: 3 [26240/60000 (44%)]\tLoss: 0.076121\n",
            "Train Epoch: 3 [26880/60000 (45%)]\tLoss: 0.137081\n",
            "Train Epoch: 3 [27520/60000 (46%)]\tLoss: 0.154377\n",
            "Train Epoch: 3 [28160/60000 (47%)]\tLoss: 0.062509\n",
            "Train Epoch: 3 [28800/60000 (48%)]\tLoss: 0.112363\n",
            "Train Epoch: 3 [29440/60000 (49%)]\tLoss: 0.096629\n",
            "Train Epoch: 3 [30080/60000 (50%)]\tLoss: 0.019229\n",
            "Train Epoch: 3 [30720/60000 (51%)]\tLoss: 0.163903\n",
            "Train Epoch: 3 [31360/60000 (52%)]\tLoss: 0.105144\n",
            "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 0.036484\n",
            "Train Epoch: 3 [32640/60000 (54%)]\tLoss: 0.105707\n",
            "Train Epoch: 3 [33280/60000 (55%)]\tLoss: 0.046231\n",
            "Train Epoch: 3 [33920/60000 (57%)]\tLoss: 0.072357\n",
            "Train Epoch: 3 [34560/60000 (58%)]\tLoss: 0.033888\n",
            "Train Epoch: 3 [35200/60000 (59%)]\tLoss: 0.237100\n",
            "Train Epoch: 3 [35840/60000 (60%)]\tLoss: 0.108751\n",
            "Train Epoch: 3 [36480/60000 (61%)]\tLoss: 0.087016\n",
            "Train Epoch: 3 [37120/60000 (62%)]\tLoss: 0.059484\n",
            "Train Epoch: 3 [37760/60000 (63%)]\tLoss: 0.082769\n",
            "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 0.112568\n",
            "Train Epoch: 3 [39040/60000 (65%)]\tLoss: 0.062591\n",
            "Train Epoch: 3 [39680/60000 (66%)]\tLoss: 0.083457\n",
            "Train Epoch: 3 [40320/60000 (67%)]\tLoss: 0.107029\n",
            "Train Epoch: 3 [40960/60000 (68%)]\tLoss: 0.064895\n",
            "Train Epoch: 3 [41600/60000 (69%)]\tLoss: 0.081286\n",
            "Train Epoch: 3 [42240/60000 (70%)]\tLoss: 0.089418\n",
            "Train Epoch: 3 [42880/60000 (71%)]\tLoss: 0.175021\n",
            "Train Epoch: 3 [43520/60000 (72%)]\tLoss: 0.070571\n",
            "Train Epoch: 3 [44160/60000 (74%)]\tLoss: 0.041268\n",
            "Train Epoch: 3 [44800/60000 (75%)]\tLoss: 0.074664\n",
            "Train Epoch: 3 [45440/60000 (76%)]\tLoss: 0.128365\n",
            "Train Epoch: 3 [46080/60000 (77%)]\tLoss: 0.162048\n",
            "Train Epoch: 3 [46720/60000 (78%)]\tLoss: 0.135828\n",
            "Train Epoch: 3 [47360/60000 (79%)]\tLoss: 0.103378\n",
            "Train Epoch: 3 [48000/60000 (80%)]\tLoss: 0.116224\n",
            "Train Epoch: 3 [48640/60000 (81%)]\tLoss: 0.121831\n",
            "Train Epoch: 3 [49280/60000 (82%)]\tLoss: 0.162967\n",
            "Train Epoch: 3 [49920/60000 (83%)]\tLoss: 0.035450\n",
            "Train Epoch: 3 [50560/60000 (84%)]\tLoss: 0.089180\n",
            "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.118968\n",
            "Train Epoch: 3 [51840/60000 (86%)]\tLoss: 0.098937\n",
            "Train Epoch: 3 [52480/60000 (87%)]\tLoss: 0.081540\n",
            "Train Epoch: 3 [53120/60000 (88%)]\tLoss: 0.033553\n",
            "Train Epoch: 3 [53760/60000 (90%)]\tLoss: 0.153510\n",
            "Train Epoch: 3 [54400/60000 (91%)]\tLoss: 0.123494\n",
            "Train Epoch: 3 [55040/60000 (92%)]\tLoss: 0.038696\n",
            "Train Epoch: 3 [55680/60000 (93%)]\tLoss: 0.061626\n",
            "Train Epoch: 3 [56320/60000 (94%)]\tLoss: 0.098893\n",
            "Train Epoch: 3 [56960/60000 (95%)]\tLoss: 0.040955\n",
            "Train Epoch: 3 [57600/60000 (96%)]\tLoss: 0.124545\n",
            "Train Epoch: 3 [58240/60000 (97%)]\tLoss: 0.154531\n",
            "Train Epoch: 3 [58880/60000 (98%)]\tLoss: 0.077002\n",
            "Train Epoch: 3 [59520/60000 (99%)]\tLoss: 0.285004\n",
            "\n",
            "Test set: Average loss: 0.0823, Accuracy: 9754/10000 (98%)\n",
            "\n",
            "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.216446\n",
            "Train Epoch: 4 [640/60000 (1%)]\tLoss: 0.027406\n",
            "Train Epoch: 4 [1280/60000 (2%)]\tLoss: 0.188444\n",
            "Train Epoch: 4 [1920/60000 (3%)]\tLoss: 0.055002\n",
            "Train Epoch: 4 [2560/60000 (4%)]\tLoss: 0.146059\n",
            "Train Epoch: 4 [3200/60000 (5%)]\tLoss: 0.141597\n",
            "Train Epoch: 4 [3840/60000 (6%)]\tLoss: 0.053609\n",
            "Train Epoch: 4 [4480/60000 (7%)]\tLoss: 0.027409\n",
            "Train Epoch: 4 [5120/60000 (9%)]\tLoss: 0.083737\n",
            "Train Epoch: 4 [5760/60000 (10%)]\tLoss: 0.121464\n",
            "Train Epoch: 4 [6400/60000 (11%)]\tLoss: 0.031577\n",
            "Train Epoch: 4 [7040/60000 (12%)]\tLoss: 0.112827\n",
            "Train Epoch: 4 [7680/60000 (13%)]\tLoss: 0.111485\n",
            "Train Epoch: 4 [8320/60000 (14%)]\tLoss: 0.085072\n",
            "Train Epoch: 4 [8960/60000 (15%)]\tLoss: 0.174668\n",
            "Train Epoch: 4 [9600/60000 (16%)]\tLoss: 0.136041\n",
            "Train Epoch: 4 [10240/60000 (17%)]\tLoss: 0.094105\n",
            "Train Epoch: 4 [10880/60000 (18%)]\tLoss: 0.147340\n",
            "Train Epoch: 4 [11520/60000 (19%)]\tLoss: 0.152630\n",
            "Train Epoch: 4 [12160/60000 (20%)]\tLoss: 0.056501\n",
            "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 0.087363\n",
            "Train Epoch: 4 [13440/60000 (22%)]\tLoss: 0.092794\n",
            "Train Epoch: 4 [14080/60000 (23%)]\tLoss: 0.088041\n",
            "Train Epoch: 4 [14720/60000 (25%)]\tLoss: 0.079380\n",
            "Train Epoch: 4 [15360/60000 (26%)]\tLoss: 0.075379\n",
            "Train Epoch: 4 [16000/60000 (27%)]\tLoss: 0.060943\n",
            "Train Epoch: 4 [16640/60000 (28%)]\tLoss: 0.068959\n",
            "Train Epoch: 4 [17280/60000 (29%)]\tLoss: 0.115300\n",
            "Train Epoch: 4 [17920/60000 (30%)]\tLoss: 0.162874\n",
            "Train Epoch: 4 [18560/60000 (31%)]\tLoss: 0.052858\n",
            "Train Epoch: 4 [19200/60000 (32%)]\tLoss: 0.045551\n",
            "Train Epoch: 4 [19840/60000 (33%)]\tLoss: 0.110573\n",
            "Train Epoch: 4 [20480/60000 (34%)]\tLoss: 0.047006\n",
            "Train Epoch: 4 [21120/60000 (35%)]\tLoss: 0.260179\n",
            "Train Epoch: 4 [21760/60000 (36%)]\tLoss: 0.033586\n",
            "Train Epoch: 4 [22400/60000 (37%)]\tLoss: 0.115503\n",
            "Train Epoch: 4 [23040/60000 (38%)]\tLoss: 0.095767\n",
            "Train Epoch: 4 [23680/60000 (39%)]\tLoss: 0.036049\n",
            "Train Epoch: 4 [24320/60000 (41%)]\tLoss: 0.039728\n",
            "Train Epoch: 4 [24960/60000 (42%)]\tLoss: 0.029857\n",
            "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 0.061737\n",
            "Train Epoch: 4 [26240/60000 (44%)]\tLoss: 0.055981\n",
            "Train Epoch: 4 [26880/60000 (45%)]\tLoss: 0.077871\n",
            "Train Epoch: 4 [27520/60000 (46%)]\tLoss: 0.076933\n",
            "Train Epoch: 4 [28160/60000 (47%)]\tLoss: 0.009899\n",
            "Train Epoch: 4 [28800/60000 (48%)]\tLoss: 0.051879\n",
            "Train Epoch: 4 [29440/60000 (49%)]\tLoss: 0.131395\n",
            "Train Epoch: 4 [30080/60000 (50%)]\tLoss: 0.085922\n",
            "Train Epoch: 4 [30720/60000 (51%)]\tLoss: 0.049479\n",
            "Train Epoch: 4 [31360/60000 (52%)]\tLoss: 0.028915\n",
            "Train Epoch: 4 [32000/60000 (53%)]\tLoss: 0.102383\n",
            "Train Epoch: 4 [32640/60000 (54%)]\tLoss: 0.077027\n",
            "Train Epoch: 4 [33280/60000 (55%)]\tLoss: 0.192994\n",
            "Train Epoch: 4 [33920/60000 (57%)]\tLoss: 0.069452\n",
            "Train Epoch: 4 [34560/60000 (58%)]\tLoss: 0.121735\n",
            "Train Epoch: 4 [35200/60000 (59%)]\tLoss: 0.162769\n",
            "Train Epoch: 4 [35840/60000 (60%)]\tLoss: 0.079212\n",
            "Train Epoch: 4 [36480/60000 (61%)]\tLoss: 0.104996\n",
            "Train Epoch: 4 [37120/60000 (62%)]\tLoss: 0.099860\n",
            "Train Epoch: 4 [37760/60000 (63%)]\tLoss: 0.038325\n",
            "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 0.143162\n",
            "Train Epoch: 4 [39040/60000 (65%)]\tLoss: 0.155435\n",
            "Train Epoch: 4 [39680/60000 (66%)]\tLoss: 0.053457\n",
            "Train Epoch: 4 [40320/60000 (67%)]\tLoss: 0.079615\n",
            "Train Epoch: 4 [40960/60000 (68%)]\tLoss: 0.045905\n",
            "Train Epoch: 4 [41600/60000 (69%)]\tLoss: 0.089913\n",
            "Train Epoch: 4 [42240/60000 (70%)]\tLoss: 0.029416\n",
            "Train Epoch: 4 [42880/60000 (71%)]\tLoss: 0.068210\n",
            "Train Epoch: 4 [43520/60000 (72%)]\tLoss: 0.045196\n",
            "Train Epoch: 4 [44160/60000 (74%)]\tLoss: 0.020313\n",
            "Train Epoch: 4 [44800/60000 (75%)]\tLoss: 0.046063\n",
            "Train Epoch: 4 [45440/60000 (76%)]\tLoss: 0.040479\n",
            "Train Epoch: 4 [46080/60000 (77%)]\tLoss: 0.062499\n",
            "Train Epoch: 4 [46720/60000 (78%)]\tLoss: 0.065548\n",
            "Train Epoch: 4 [47360/60000 (79%)]\tLoss: 0.062718\n",
            "Train Epoch: 4 [48000/60000 (80%)]\tLoss: 0.097921\n",
            "Train Epoch: 4 [48640/60000 (81%)]\tLoss: 0.126550\n",
            "Train Epoch: 4 [49280/60000 (82%)]\tLoss: 0.171162\n",
            "Train Epoch: 4 [49920/60000 (83%)]\tLoss: 0.020849\n",
            "Train Epoch: 4 [50560/60000 (84%)]\tLoss: 0.054035\n",
            "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 0.083895\n",
            "Train Epoch: 4 [51840/60000 (86%)]\tLoss: 0.066183\n",
            "Train Epoch: 4 [52480/60000 (87%)]\tLoss: 0.217111\n",
            "Train Epoch: 4 [53120/60000 (88%)]\tLoss: 0.056794\n",
            "Train Epoch: 4 [53760/60000 (90%)]\tLoss: 0.080365\n",
            "Train Epoch: 4 [54400/60000 (91%)]\tLoss: 0.091411\n",
            "Train Epoch: 4 [55040/60000 (92%)]\tLoss: 0.060614\n",
            "Train Epoch: 4 [55680/60000 (93%)]\tLoss: 0.251953\n",
            "Train Epoch: 4 [56320/60000 (94%)]\tLoss: 0.052446\n",
            "Train Epoch: 4 [56960/60000 (95%)]\tLoss: 0.036158\n",
            "Train Epoch: 4 [57600/60000 (96%)]\tLoss: 0.058855\n",
            "Train Epoch: 4 [58240/60000 (97%)]\tLoss: 0.082028\n",
            "Train Epoch: 4 [58880/60000 (98%)]\tLoss: 0.160753\n",
            "Train Epoch: 4 [59520/60000 (99%)]\tLoss: 0.203320\n",
            "\n",
            "Test set: Average loss: 0.0743, Accuracy: 9768/10000 (98%)\n",
            "\n",
            "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.077391\n",
            "Train Epoch: 5 [640/60000 (1%)]\tLoss: 0.136267\n",
            "Train Epoch: 5 [1280/60000 (2%)]\tLoss: 0.090049\n",
            "Train Epoch: 5 [1920/60000 (3%)]\tLoss: 0.075163\n",
            "Train Epoch: 5 [2560/60000 (4%)]\tLoss: 0.171012\n",
            "Train Epoch: 5 [3200/60000 (5%)]\tLoss: 0.077976\n",
            "Train Epoch: 5 [3840/60000 (6%)]\tLoss: 0.021520\n",
            "Train Epoch: 5 [4480/60000 (7%)]\tLoss: 0.222851\n",
            "Train Epoch: 5 [5120/60000 (9%)]\tLoss: 0.107327\n",
            "Train Epoch: 5 [5760/60000 (10%)]\tLoss: 0.018900\n",
            "Train Epoch: 5 [6400/60000 (11%)]\tLoss: 0.053140\n",
            "Train Epoch: 5 [7040/60000 (12%)]\tLoss: 0.099992\n",
            "Train Epoch: 5 [7680/60000 (13%)]\tLoss: 0.164284\n",
            "Train Epoch: 5 [8320/60000 (14%)]\tLoss: 0.045534\n",
            "Train Epoch: 5 [8960/60000 (15%)]\tLoss: 0.106346\n",
            "Train Epoch: 5 [9600/60000 (16%)]\tLoss: 0.097684\n",
            "Train Epoch: 5 [10240/60000 (17%)]\tLoss: 0.111809\n",
            "Train Epoch: 5 [10880/60000 (18%)]\tLoss: 0.048333\n",
            "Train Epoch: 5 [11520/60000 (19%)]\tLoss: 0.180757\n",
            "Train Epoch: 5 [12160/60000 (20%)]\tLoss: 0.061664\n",
            "Train Epoch: 5 [12800/60000 (21%)]\tLoss: 0.050182\n",
            "Train Epoch: 5 [13440/60000 (22%)]\tLoss: 0.065766\n",
            "Train Epoch: 5 [14080/60000 (23%)]\tLoss: 0.082863\n",
            "Train Epoch: 5 [14720/60000 (25%)]\tLoss: 0.089075\n",
            "Train Epoch: 5 [15360/60000 (26%)]\tLoss: 0.155883\n",
            "Train Epoch: 5 [16000/60000 (27%)]\tLoss: 0.060890\n",
            "Train Epoch: 5 [16640/60000 (28%)]\tLoss: 0.041140\n",
            "Train Epoch: 5 [17280/60000 (29%)]\tLoss: 0.050419\n",
            "Train Epoch: 5 [17920/60000 (30%)]\tLoss: 0.005928\n",
            "Train Epoch: 5 [18560/60000 (31%)]\tLoss: 0.213816\n",
            "Train Epoch: 5 [19200/60000 (32%)]\tLoss: 0.038103\n",
            "Train Epoch: 5 [19840/60000 (33%)]\tLoss: 0.072723\n",
            "Train Epoch: 5 [20480/60000 (34%)]\tLoss: 0.017001\n",
            "Train Epoch: 5 [21120/60000 (35%)]\tLoss: 0.172995\n",
            "Train Epoch: 5 [21760/60000 (36%)]\tLoss: 0.067251\n",
            "Train Epoch: 5 [22400/60000 (37%)]\tLoss: 0.062622\n",
            "Train Epoch: 5 [23040/60000 (38%)]\tLoss: 0.040068\n",
            "Train Epoch: 5 [23680/60000 (39%)]\tLoss: 0.034639\n",
            "Train Epoch: 5 [24320/60000 (41%)]\tLoss: 0.022430\n",
            "Train Epoch: 5 [24960/60000 (42%)]\tLoss: 0.021416\n",
            "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 0.037218\n",
            "Train Epoch: 5 [26240/60000 (44%)]\tLoss: 0.091777\n",
            "Train Epoch: 5 [26880/60000 (45%)]\tLoss: 0.077222\n",
            "Train Epoch: 5 [27520/60000 (46%)]\tLoss: 0.082764\n",
            "Train Epoch: 5 [28160/60000 (47%)]\tLoss: 0.016171\n",
            "Train Epoch: 5 [28800/60000 (48%)]\tLoss: 0.089727\n",
            "Train Epoch: 5 [29440/60000 (49%)]\tLoss: 0.183966\n",
            "Train Epoch: 5 [30080/60000 (50%)]\tLoss: 0.028098\n",
            "Train Epoch: 5 [30720/60000 (51%)]\tLoss: 0.025884\n",
            "Train Epoch: 5 [31360/60000 (52%)]\tLoss: 0.075572\n",
            "Train Epoch: 5 [32000/60000 (53%)]\tLoss: 0.083958\n",
            "Train Epoch: 5 [32640/60000 (54%)]\tLoss: 0.084411\n",
            "Train Epoch: 5 [33280/60000 (55%)]\tLoss: 0.034067\n",
            "Train Epoch: 5 [33920/60000 (57%)]\tLoss: 0.040645\n",
            "Train Epoch: 5 [34560/60000 (58%)]\tLoss: 0.116550\n",
            "Train Epoch: 5 [35200/60000 (59%)]\tLoss: 0.045782\n",
            "Train Epoch: 5 [35840/60000 (60%)]\tLoss: 0.026958\n",
            "Train Epoch: 5 [36480/60000 (61%)]\tLoss: 0.086392\n",
            "Train Epoch: 5 [37120/60000 (62%)]\tLoss: 0.126083\n",
            "Train Epoch: 5 [37760/60000 (63%)]\tLoss: 0.031856\n",
            "Train Epoch: 5 [38400/60000 (64%)]\tLoss: 0.071462\n",
            "Train Epoch: 5 [39040/60000 (65%)]\tLoss: 0.158657\n",
            "Train Epoch: 5 [39680/60000 (66%)]\tLoss: 0.075373\n",
            "Train Epoch: 5 [40320/60000 (67%)]\tLoss: 0.125774\n",
            "Train Epoch: 5 [40960/60000 (68%)]\tLoss: 0.035635\n",
            "Train Epoch: 5 [41600/60000 (69%)]\tLoss: 0.042603\n",
            "Train Epoch: 5 [42240/60000 (70%)]\tLoss: 0.146430\n",
            "Train Epoch: 5 [42880/60000 (71%)]\tLoss: 0.022241\n",
            "Train Epoch: 5 [43520/60000 (72%)]\tLoss: 0.116671\n",
            "Train Epoch: 5 [44160/60000 (74%)]\tLoss: 0.074408\n",
            "Train Epoch: 5 [44800/60000 (75%)]\tLoss: 0.113566\n",
            "Train Epoch: 5 [45440/60000 (76%)]\tLoss: 0.053266\n",
            "Train Epoch: 5 [46080/60000 (77%)]\tLoss: 0.061344\n",
            "Train Epoch: 5 [46720/60000 (78%)]\tLoss: 0.066074\n",
            "Train Epoch: 5 [47360/60000 (79%)]\tLoss: 0.099674\n",
            "Train Epoch: 5 [48000/60000 (80%)]\tLoss: 0.067792\n",
            "Train Epoch: 5 [48640/60000 (81%)]\tLoss: 0.164331\n",
            "Train Epoch: 5 [49280/60000 (82%)]\tLoss: 0.122360\n",
            "Train Epoch: 5 [49920/60000 (83%)]\tLoss: 0.063866\n",
            "Train Epoch: 5 [50560/60000 (84%)]\tLoss: 0.066215\n",
            "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 0.073978\n",
            "Train Epoch: 5 [51840/60000 (86%)]\tLoss: 0.148699\n",
            "Train Epoch: 5 [52480/60000 (87%)]\tLoss: 0.089505\n",
            "Train Epoch: 5 [53120/60000 (88%)]\tLoss: 0.053582\n",
            "Train Epoch: 5 [53760/60000 (90%)]\tLoss: 0.104035\n",
            "Train Epoch: 5 [54400/60000 (91%)]\tLoss: 0.075346\n",
            "Train Epoch: 5 [55040/60000 (92%)]\tLoss: 0.017216\n",
            "Train Epoch: 5 [55680/60000 (93%)]\tLoss: 0.087836\n",
            "Train Epoch: 5 [56320/60000 (94%)]\tLoss: 0.098476\n",
            "Train Epoch: 5 [56960/60000 (95%)]\tLoss: 0.087802\n",
            "Train Epoch: 5 [57600/60000 (96%)]\tLoss: 0.105609\n",
            "Train Epoch: 5 [58240/60000 (97%)]\tLoss: 0.072230\n",
            "Train Epoch: 5 [58880/60000 (98%)]\tLoss: 0.126032\n",
            "Train Epoch: 5 [59520/60000 (99%)]\tLoss: 0.128203\n",
            "\n",
            "Test set: Average loss: 0.0641, Accuracy: 9783/10000 (98%)\n",
            "\n",
            "Train Epoch: 6 [0/60000 (0%)]\tLoss: 0.035383\n",
            "Train Epoch: 6 [640/60000 (1%)]\tLoss: 0.092421\n",
            "Train Epoch: 6 [1280/60000 (2%)]\tLoss: 0.080626\n",
            "Train Epoch: 6 [1920/60000 (3%)]\tLoss: 0.074638\n",
            "Train Epoch: 6 [2560/60000 (4%)]\tLoss: 0.016548\n",
            "Train Epoch: 6 [3200/60000 (5%)]\tLoss: 0.008911\n",
            "Train Epoch: 6 [3840/60000 (6%)]\tLoss: 0.175066\n",
            "Train Epoch: 6 [4480/60000 (7%)]\tLoss: 0.047755\n",
            "Train Epoch: 6 [5120/60000 (9%)]\tLoss: 0.026387\n",
            "Train Epoch: 6 [5760/60000 (10%)]\tLoss: 0.020478\n",
            "Train Epoch: 6 [6400/60000 (11%)]\tLoss: 0.031995\n",
            "Train Epoch: 6 [7040/60000 (12%)]\tLoss: 0.052329\n",
            "Train Epoch: 6 [7680/60000 (13%)]\tLoss: 0.028609\n",
            "Train Epoch: 6 [8320/60000 (14%)]\tLoss: 0.152523\n",
            "Train Epoch: 6 [8960/60000 (15%)]\tLoss: 0.032038\n",
            "Train Epoch: 6 [9600/60000 (16%)]\tLoss: 0.111906\n",
            "Train Epoch: 6 [10240/60000 (17%)]\tLoss: 0.017495\n",
            "Train Epoch: 6 [10880/60000 (18%)]\tLoss: 0.054564\n",
            "Train Epoch: 6 [11520/60000 (19%)]\tLoss: 0.048195\n",
            "Train Epoch: 6 [12160/60000 (20%)]\tLoss: 0.083905\n",
            "Train Epoch: 6 [12800/60000 (21%)]\tLoss: 0.059113\n",
            "Train Epoch: 6 [13440/60000 (22%)]\tLoss: 0.134248\n",
            "Train Epoch: 6 [14080/60000 (23%)]\tLoss: 0.205463\n",
            "Train Epoch: 6 [14720/60000 (25%)]\tLoss: 0.032943\n",
            "Train Epoch: 6 [15360/60000 (26%)]\tLoss: 0.022907\n",
            "Train Epoch: 6 [16000/60000 (27%)]\tLoss: 0.084625\n",
            "Train Epoch: 6 [16640/60000 (28%)]\tLoss: 0.088002\n",
            "Train Epoch: 6 [17280/60000 (29%)]\tLoss: 0.097928\n",
            "Train Epoch: 6 [17920/60000 (30%)]\tLoss: 0.056755\n",
            "Train Epoch: 6 [18560/60000 (31%)]\tLoss: 0.040136\n",
            "Train Epoch: 6 [19200/60000 (32%)]\tLoss: 0.085971\n",
            "Train Epoch: 6 [19840/60000 (33%)]\tLoss: 0.143452\n",
            "Train Epoch: 6 [20480/60000 (34%)]\tLoss: 0.041827\n",
            "Train Epoch: 6 [21120/60000 (35%)]\tLoss: 0.029965\n",
            "Train Epoch: 6 [21760/60000 (36%)]\tLoss: 0.066171\n",
            "Train Epoch: 6 [22400/60000 (37%)]\tLoss: 0.067514\n",
            "Train Epoch: 6 [23040/60000 (38%)]\tLoss: 0.174444\n",
            "Train Epoch: 6 [23680/60000 (39%)]\tLoss: 0.034379\n",
            "Train Epoch: 6 [24320/60000 (41%)]\tLoss: 0.081712\n",
            "Train Epoch: 6 [24960/60000 (42%)]\tLoss: 0.031126\n",
            "Train Epoch: 6 [25600/60000 (43%)]\tLoss: 0.048596\n",
            "Train Epoch: 6 [26240/60000 (44%)]\tLoss: 0.055810\n",
            "Train Epoch: 6 [26880/60000 (45%)]\tLoss: 0.037258\n",
            "Train Epoch: 6 [27520/60000 (46%)]\tLoss: 0.045195\n",
            "Train Epoch: 6 [28160/60000 (47%)]\tLoss: 0.037426\n",
            "Train Epoch: 6 [28800/60000 (48%)]\tLoss: 0.051072\n",
            "Train Epoch: 6 [29440/60000 (49%)]\tLoss: 0.132705\n",
            "Train Epoch: 6 [30080/60000 (50%)]\tLoss: 0.109215\n",
            "Train Epoch: 6 [30720/60000 (51%)]\tLoss: 0.042093\n",
            "Train Epoch: 6 [31360/60000 (52%)]\tLoss: 0.030249\n",
            "Train Epoch: 6 [32000/60000 (53%)]\tLoss: 0.082102\n",
            "Train Epoch: 6 [32640/60000 (54%)]\tLoss: 0.134100\n",
            "Train Epoch: 6 [33280/60000 (55%)]\tLoss: 0.066037\n",
            "Train Epoch: 6 [33920/60000 (57%)]\tLoss: 0.075663\n",
            "Train Epoch: 6 [34560/60000 (58%)]\tLoss: 0.063753\n",
            "Train Epoch: 6 [35200/60000 (59%)]\tLoss: 0.033443\n",
            "Train Epoch: 6 [35840/60000 (60%)]\tLoss: 0.128789\n",
            "Train Epoch: 6 [36480/60000 (61%)]\tLoss: 0.136456\n",
            "Train Epoch: 6 [37120/60000 (62%)]\tLoss: 0.079133\n",
            "Train Epoch: 6 [37760/60000 (63%)]\tLoss: 0.029224\n",
            "Train Epoch: 6 [38400/60000 (64%)]\tLoss: 0.072293\n",
            "Train Epoch: 6 [39040/60000 (65%)]\tLoss: 0.026863\n",
            "Train Epoch: 6 [39680/60000 (66%)]\tLoss: 0.133673\n",
            "Train Epoch: 6 [40320/60000 (67%)]\tLoss: 0.095048\n",
            "Train Epoch: 6 [40960/60000 (68%)]\tLoss: 0.151782\n",
            "Train Epoch: 6 [41600/60000 (69%)]\tLoss: 0.038530\n",
            "Train Epoch: 6 [42240/60000 (70%)]\tLoss: 0.045736\n",
            "Train Epoch: 6 [42880/60000 (71%)]\tLoss: 0.061016\n",
            "Train Epoch: 6 [43520/60000 (72%)]\tLoss: 0.077760\n",
            "Train Epoch: 6 [44160/60000 (74%)]\tLoss: 0.031646\n",
            "Train Epoch: 6 [44800/60000 (75%)]\tLoss: 0.072997\n",
            "Train Epoch: 6 [45440/60000 (76%)]\tLoss: 0.029039\n",
            "Train Epoch: 6 [46080/60000 (77%)]\tLoss: 0.056750\n",
            "Train Epoch: 6 [46720/60000 (78%)]\tLoss: 0.079002\n",
            "Train Epoch: 6 [47360/60000 (79%)]\tLoss: 0.151805\n",
            "Train Epoch: 6 [48000/60000 (80%)]\tLoss: 0.012612\n",
            "Train Epoch: 6 [48640/60000 (81%)]\tLoss: 0.066529\n",
            "Train Epoch: 6 [49280/60000 (82%)]\tLoss: 0.040742\n",
            "Train Epoch: 6 [49920/60000 (83%)]\tLoss: 0.016478\n",
            "Train Epoch: 6 [50560/60000 (84%)]\tLoss: 0.047536\n",
            "Train Epoch: 6 [51200/60000 (85%)]\tLoss: 0.013546\n",
            "Train Epoch: 6 [51840/60000 (86%)]\tLoss: 0.065908\n",
            "Train Epoch: 6 [52480/60000 (87%)]\tLoss: 0.046383\n",
            "Train Epoch: 6 [53120/60000 (88%)]\tLoss: 0.063955\n",
            "Train Epoch: 6 [53760/60000 (90%)]\tLoss: 0.029078\n",
            "Train Epoch: 6 [54400/60000 (91%)]\tLoss: 0.028342\n",
            "Train Epoch: 6 [55040/60000 (92%)]\tLoss: 0.077858\n",
            "Train Epoch: 6 [55680/60000 (93%)]\tLoss: 0.092285\n",
            "Train Epoch: 6 [56320/60000 (94%)]\tLoss: 0.019226\n",
            "Train Epoch: 6 [56960/60000 (95%)]\tLoss: 0.078191\n",
            "Train Epoch: 6 [57600/60000 (96%)]\tLoss: 0.123492\n",
            "Train Epoch: 6 [58240/60000 (97%)]\tLoss: 0.021225\n",
            "Train Epoch: 6 [58880/60000 (98%)]\tLoss: 0.011799\n",
            "Train Epoch: 6 [59520/60000 (99%)]\tLoss: 0.016908\n",
            "\n",
            "Test set: Average loss: 0.0609, Accuracy: 9804/10000 (98%)\n",
            "\n",
            "Train Epoch: 7 [0/60000 (0%)]\tLoss: 0.125004\n",
            "Train Epoch: 7 [640/60000 (1%)]\tLoss: 0.063833\n",
            "Train Epoch: 7 [1280/60000 (2%)]\tLoss: 0.007554\n",
            "Train Epoch: 7 [1920/60000 (3%)]\tLoss: 0.108922\n",
            "Train Epoch: 7 [2560/60000 (4%)]\tLoss: 0.008325\n",
            "Train Epoch: 7 [3200/60000 (5%)]\tLoss: 0.012128\n",
            "Train Epoch: 7 [3840/60000 (6%)]\tLoss: 0.094792\n",
            "Train Epoch: 7 [4480/60000 (7%)]\tLoss: 0.020421\n",
            "Train Epoch: 7 [5120/60000 (9%)]\tLoss: 0.057956\n",
            "Train Epoch: 7 [5760/60000 (10%)]\tLoss: 0.016975\n",
            "Train Epoch: 7 [6400/60000 (11%)]\tLoss: 0.025985\n",
            "Train Epoch: 7 [7040/60000 (12%)]\tLoss: 0.046713\n",
            "Train Epoch: 7 [7680/60000 (13%)]\tLoss: 0.057394\n",
            "Train Epoch: 7 [8320/60000 (14%)]\tLoss: 0.037047\n",
            "Train Epoch: 7 [8960/60000 (15%)]\tLoss: 0.032164\n",
            "Train Epoch: 7 [9600/60000 (16%)]\tLoss: 0.199980\n",
            "Train Epoch: 7 [10240/60000 (17%)]\tLoss: 0.012701\n",
            "Train Epoch: 7 [10880/60000 (18%)]\tLoss: 0.181472\n",
            "Train Epoch: 7 [11520/60000 (19%)]\tLoss: 0.070060\n",
            "Train Epoch: 7 [12160/60000 (20%)]\tLoss: 0.044015\n",
            "Train Epoch: 7 [12800/60000 (21%)]\tLoss: 0.031659\n",
            "Train Epoch: 7 [13440/60000 (22%)]\tLoss: 0.107188\n",
            "Train Epoch: 7 [14080/60000 (23%)]\tLoss: 0.146206\n",
            "Train Epoch: 7 [14720/60000 (25%)]\tLoss: 0.080256\n",
            "Train Epoch: 7 [15360/60000 (26%)]\tLoss: 0.064902\n",
            "Train Epoch: 7 [16000/60000 (27%)]\tLoss: 0.074746\n",
            "Train Epoch: 7 [16640/60000 (28%)]\tLoss: 0.040378\n",
            "Train Epoch: 7 [17280/60000 (29%)]\tLoss: 0.030912\n",
            "Train Epoch: 7 [17920/60000 (30%)]\tLoss: 0.074321\n",
            "Train Epoch: 7 [18560/60000 (31%)]\tLoss: 0.086378\n",
            "Train Epoch: 7 [19200/60000 (32%)]\tLoss: 0.029964\n",
            "Train Epoch: 7 [19840/60000 (33%)]\tLoss: 0.068061\n",
            "Train Epoch: 7 [20480/60000 (34%)]\tLoss: 0.220600\n",
            "Train Epoch: 7 [21120/60000 (35%)]\tLoss: 0.039092\n",
            "Train Epoch: 7 [21760/60000 (36%)]\tLoss: 0.090115\n",
            "Train Epoch: 7 [22400/60000 (37%)]\tLoss: 0.014566\n",
            "Train Epoch: 7 [23040/60000 (38%)]\tLoss: 0.049835\n",
            "Train Epoch: 7 [23680/60000 (39%)]\tLoss: 0.055775\n",
            "Train Epoch: 7 [24320/60000 (41%)]\tLoss: 0.052732\n",
            "Train Epoch: 7 [24960/60000 (42%)]\tLoss: 0.054152\n",
            "Train Epoch: 7 [25600/60000 (43%)]\tLoss: 0.101744\n",
            "Train Epoch: 7 [26240/60000 (44%)]\tLoss: 0.066818\n",
            "Train Epoch: 7 [26880/60000 (45%)]\tLoss: 0.016570\n",
            "Train Epoch: 7 [27520/60000 (46%)]\tLoss: 0.062466\n",
            "Train Epoch: 7 [28160/60000 (47%)]\tLoss: 0.097993\n",
            "Train Epoch: 7 [28800/60000 (48%)]\tLoss: 0.096518\n",
            "Train Epoch: 7 [29440/60000 (49%)]\tLoss: 0.132796\n",
            "Train Epoch: 7 [30080/60000 (50%)]\tLoss: 0.047048\n",
            "Train Epoch: 7 [30720/60000 (51%)]\tLoss: 0.038345\n",
            "Train Epoch: 7 [31360/60000 (52%)]\tLoss: 0.047333\n",
            "Train Epoch: 7 [32000/60000 (53%)]\tLoss: 0.082526\n",
            "Train Epoch: 7 [32640/60000 (54%)]\tLoss: 0.065675\n",
            "Train Epoch: 7 [33280/60000 (55%)]\tLoss: 0.027472\n",
            "Train Epoch: 7 [33920/60000 (57%)]\tLoss: 0.065616\n",
            "Train Epoch: 7 [34560/60000 (58%)]\tLoss: 0.008258\n",
            "Train Epoch: 7 [35200/60000 (59%)]\tLoss: 0.154754\n",
            "Train Epoch: 7 [35840/60000 (60%)]\tLoss: 0.100174\n",
            "Train Epoch: 7 [36480/60000 (61%)]\tLoss: 0.081536\n",
            "Train Epoch: 7 [37120/60000 (62%)]\tLoss: 0.045257\n",
            "Train Epoch: 7 [37760/60000 (63%)]\tLoss: 0.161163\n",
            "Train Epoch: 7 [38400/60000 (64%)]\tLoss: 0.082944\n",
            "Train Epoch: 7 [39040/60000 (65%)]\tLoss: 0.045258\n",
            "Train Epoch: 7 [39680/60000 (66%)]\tLoss: 0.041525\n",
            "Train Epoch: 7 [40320/60000 (67%)]\tLoss: 0.077871\n",
            "Train Epoch: 7 [40960/60000 (68%)]\tLoss: 0.022017\n",
            "Train Epoch: 7 [41600/60000 (69%)]\tLoss: 0.111779\n",
            "Train Epoch: 7 [42240/60000 (70%)]\tLoss: 0.003138\n",
            "Train Epoch: 7 [42880/60000 (71%)]\tLoss: 0.040563\n",
            "Train Epoch: 7 [43520/60000 (72%)]\tLoss: 0.025253\n",
            "Train Epoch: 7 [44160/60000 (74%)]\tLoss: 0.012069\n",
            "Train Epoch: 7 [44800/60000 (75%)]\tLoss: 0.078451\n",
            "Train Epoch: 7 [45440/60000 (76%)]\tLoss: 0.087421\n",
            "Train Epoch: 7 [46080/60000 (77%)]\tLoss: 0.289161\n",
            "Train Epoch: 7 [46720/60000 (78%)]\tLoss: 0.035337\n",
            "Train Epoch: 7 [47360/60000 (79%)]\tLoss: 0.008290\n",
            "Train Epoch: 7 [48000/60000 (80%)]\tLoss: 0.064534\n",
            "Train Epoch: 7 [48640/60000 (81%)]\tLoss: 0.139533\n",
            "Train Epoch: 7 [49280/60000 (82%)]\tLoss: 0.036266\n",
            "Train Epoch: 7 [49920/60000 (83%)]\tLoss: 0.028904\n",
            "Train Epoch: 7 [50560/60000 (84%)]\tLoss: 0.102497\n",
            "Train Epoch: 7 [51200/60000 (85%)]\tLoss: 0.072845\n",
            "Train Epoch: 7 [51840/60000 (86%)]\tLoss: 0.027554\n",
            "Train Epoch: 7 [52480/60000 (87%)]\tLoss: 0.016979\n",
            "Train Epoch: 7 [53120/60000 (88%)]\tLoss: 0.045166\n",
            "Train Epoch: 7 [53760/60000 (90%)]\tLoss: 0.070150\n",
            "Train Epoch: 7 [54400/60000 (91%)]\tLoss: 0.110854\n",
            "Train Epoch: 7 [55040/60000 (92%)]\tLoss: 0.025623\n",
            "Train Epoch: 7 [55680/60000 (93%)]\tLoss: 0.048981\n",
            "Train Epoch: 7 [56320/60000 (94%)]\tLoss: 0.098018\n",
            "Train Epoch: 7 [56960/60000 (95%)]\tLoss: 0.046446\n",
            "Train Epoch: 7 [57600/60000 (96%)]\tLoss: 0.026721\n",
            "Train Epoch: 7 [58240/60000 (97%)]\tLoss: 0.068387\n",
            "Train Epoch: 7 [58880/60000 (98%)]\tLoss: 0.032768\n",
            "Train Epoch: 7 [59520/60000 (99%)]\tLoss: 0.046645\n",
            "\n",
            "Test set: Average loss: 0.0518, Accuracy: 9835/10000 (98%)\n",
            "\n",
            "Train Epoch: 8 [0/60000 (0%)]\tLoss: 0.022384\n",
            "Train Epoch: 8 [640/60000 (1%)]\tLoss: 0.068103\n",
            "Train Epoch: 8 [1280/60000 (2%)]\tLoss: 0.061528\n",
            "Train Epoch: 8 [1920/60000 (3%)]\tLoss: 0.060480\n",
            "Train Epoch: 8 [2560/60000 (4%)]\tLoss: 0.014395\n",
            "Train Epoch: 8 [3200/60000 (5%)]\tLoss: 0.023374\n",
            "Train Epoch: 8 [3840/60000 (6%)]\tLoss: 0.040758\n",
            "Train Epoch: 8 [4480/60000 (7%)]\tLoss: 0.045163\n",
            "Train Epoch: 8 [5120/60000 (9%)]\tLoss: 0.041475\n",
            "Train Epoch: 8 [5760/60000 (10%)]\tLoss: 0.103351\n",
            "Train Epoch: 8 [6400/60000 (11%)]\tLoss: 0.088674\n",
            "Train Epoch: 8 [7040/60000 (12%)]\tLoss: 0.104439\n",
            "Train Epoch: 8 [7680/60000 (13%)]\tLoss: 0.013556\n",
            "Train Epoch: 8 [8320/60000 (14%)]\tLoss: 0.006231\n",
            "Train Epoch: 8 [8960/60000 (15%)]\tLoss: 0.105758\n",
            "Train Epoch: 8 [9600/60000 (16%)]\tLoss: 0.040142\n",
            "Train Epoch: 8 [10240/60000 (17%)]\tLoss: 0.038471\n",
            "Train Epoch: 8 [10880/60000 (18%)]\tLoss: 0.019134\n",
            "Train Epoch: 8 [11520/60000 (19%)]\tLoss: 0.012156\n",
            "Train Epoch: 8 [12160/60000 (20%)]\tLoss: 0.035409\n",
            "Train Epoch: 8 [12800/60000 (21%)]\tLoss: 0.010880\n",
            "Train Epoch: 8 [13440/60000 (22%)]\tLoss: 0.097286\n",
            "Train Epoch: 8 [14080/60000 (23%)]\tLoss: 0.082003\n",
            "Train Epoch: 8 [14720/60000 (25%)]\tLoss: 0.027491\n",
            "Train Epoch: 8 [15360/60000 (26%)]\tLoss: 0.044002\n",
            "Train Epoch: 8 [16000/60000 (27%)]\tLoss: 0.028840\n",
            "Train Epoch: 8 [16640/60000 (28%)]\tLoss: 0.149228\n",
            "Train Epoch: 8 [17280/60000 (29%)]\tLoss: 0.040767\n",
            "Train Epoch: 8 [17920/60000 (30%)]\tLoss: 0.015417\n",
            "Train Epoch: 8 [18560/60000 (31%)]\tLoss: 0.004448\n",
            "Train Epoch: 8 [19200/60000 (32%)]\tLoss: 0.006921\n",
            "Train Epoch: 8 [19840/60000 (33%)]\tLoss: 0.008244\n",
            "Train Epoch: 8 [20480/60000 (34%)]\tLoss: 0.088056\n",
            "Train Epoch: 8 [21120/60000 (35%)]\tLoss: 0.026098\n",
            "Train Epoch: 8 [21760/60000 (36%)]\tLoss: 0.030003\n",
            "Train Epoch: 8 [22400/60000 (37%)]\tLoss: 0.021987\n",
            "Train Epoch: 8 [23040/60000 (38%)]\tLoss: 0.052818\n",
            "Train Epoch: 8 [23680/60000 (39%)]\tLoss: 0.036573\n",
            "Train Epoch: 8 [24320/60000 (41%)]\tLoss: 0.142492\n",
            "Train Epoch: 8 [24960/60000 (42%)]\tLoss: 0.030859\n",
            "Train Epoch: 8 [25600/60000 (43%)]\tLoss: 0.095510\n",
            "Train Epoch: 8 [26240/60000 (44%)]\tLoss: 0.174073\n",
            "Train Epoch: 8 [26880/60000 (45%)]\tLoss: 0.031600\n",
            "Train Epoch: 8 [27520/60000 (46%)]\tLoss: 0.049842\n",
            "Train Epoch: 8 [28160/60000 (47%)]\tLoss: 0.157818\n",
            "Train Epoch: 8 [28800/60000 (48%)]\tLoss: 0.021840\n",
            "Train Epoch: 8 [29440/60000 (49%)]\tLoss: 0.115036\n",
            "Train Epoch: 8 [30080/60000 (50%)]\tLoss: 0.060400\n",
            "Train Epoch: 8 [30720/60000 (51%)]\tLoss: 0.050606\n",
            "Train Epoch: 8 [31360/60000 (52%)]\tLoss: 0.053370\n",
            "Train Epoch: 8 [32000/60000 (53%)]\tLoss: 0.093469\n",
            "Train Epoch: 8 [32640/60000 (54%)]\tLoss: 0.030435\n",
            "Train Epoch: 8 [33280/60000 (55%)]\tLoss: 0.026797\n",
            "Train Epoch: 8 [33920/60000 (57%)]\tLoss: 0.081905\n",
            "Train Epoch: 8 [34560/60000 (58%)]\tLoss: 0.149831\n",
            "Train Epoch: 8 [35200/60000 (59%)]\tLoss: 0.049706\n",
            "Train Epoch: 8 [35840/60000 (60%)]\tLoss: 0.076963\n",
            "Train Epoch: 8 [36480/60000 (61%)]\tLoss: 0.017046\n",
            "Train Epoch: 8 [37120/60000 (62%)]\tLoss: 0.075737\n",
            "Train Epoch: 8 [37760/60000 (63%)]\tLoss: 0.119930\n",
            "Train Epoch: 8 [38400/60000 (64%)]\tLoss: 0.030370\n",
            "Train Epoch: 8 [39040/60000 (65%)]\tLoss: 0.067112\n",
            "Train Epoch: 8 [39680/60000 (66%)]\tLoss: 0.162386\n",
            "Train Epoch: 8 [40320/60000 (67%)]\tLoss: 0.095533\n",
            "Train Epoch: 8 [40960/60000 (68%)]\tLoss: 0.045846\n",
            "Train Epoch: 8 [41600/60000 (69%)]\tLoss: 0.174818\n",
            "Train Epoch: 8 [42240/60000 (70%)]\tLoss: 0.108092\n",
            "Train Epoch: 8 [42880/60000 (71%)]\tLoss: 0.031597\n",
            "Train Epoch: 8 [43520/60000 (72%)]\tLoss: 0.009054\n",
            "Train Epoch: 8 [44160/60000 (74%)]\tLoss: 0.033085\n",
            "Train Epoch: 8 [44800/60000 (75%)]\tLoss: 0.126445\n",
            "Train Epoch: 8 [45440/60000 (76%)]\tLoss: 0.049132\n",
            "Train Epoch: 8 [46080/60000 (77%)]\tLoss: 0.111960\n",
            "Train Epoch: 8 [46720/60000 (78%)]\tLoss: 0.085670\n",
            "Train Epoch: 8 [47360/60000 (79%)]\tLoss: 0.044094\n",
            "Train Epoch: 8 [48000/60000 (80%)]\tLoss: 0.049477\n",
            "Train Epoch: 8 [48640/60000 (81%)]\tLoss: 0.077518\n",
            "Train Epoch: 8 [49280/60000 (82%)]\tLoss: 0.205935\n",
            "Train Epoch: 8 [49920/60000 (83%)]\tLoss: 0.010460\n",
            "Train Epoch: 8 [50560/60000 (84%)]\tLoss: 0.021294\n",
            "Train Epoch: 8 [51200/60000 (85%)]\tLoss: 0.045019\n",
            "Train Epoch: 8 [51840/60000 (86%)]\tLoss: 0.090655\n",
            "Train Epoch: 8 [52480/60000 (87%)]\tLoss: 0.102049\n",
            "Train Epoch: 8 [53120/60000 (88%)]\tLoss: 0.048515\n",
            "Train Epoch: 8 [53760/60000 (90%)]\tLoss: 0.029816\n",
            "Train Epoch: 8 [54400/60000 (91%)]\tLoss: 0.024829\n",
            "Train Epoch: 8 [55040/60000 (92%)]\tLoss: 0.147792\n",
            "Train Epoch: 8 [55680/60000 (93%)]\tLoss: 0.180095\n",
            "Train Epoch: 8 [56320/60000 (94%)]\tLoss: 0.030854\n",
            "Train Epoch: 8 [56960/60000 (95%)]\tLoss: 0.034081\n",
            "Train Epoch: 8 [57600/60000 (96%)]\tLoss: 0.115465\n",
            "Train Epoch: 8 [58240/60000 (97%)]\tLoss: 0.064939\n",
            "Train Epoch: 8 [58880/60000 (98%)]\tLoss: 0.046033\n",
            "Train Epoch: 8 [59520/60000 (99%)]\tLoss: 0.027036\n",
            "\n",
            "Test set: Average loss: 0.0528, Accuracy: 9829/10000 (98%)\n",
            "\n",
            "Train Epoch: 9 [0/60000 (0%)]\tLoss: 0.066411\n",
            "Train Epoch: 9 [640/60000 (1%)]\tLoss: 0.028108\n",
            "Train Epoch: 9 [1280/60000 (2%)]\tLoss: 0.079306\n",
            "Train Epoch: 9 [1920/60000 (3%)]\tLoss: 0.031761\n",
            "Train Epoch: 9 [2560/60000 (4%)]\tLoss: 0.019307\n",
            "Train Epoch: 9 [3200/60000 (5%)]\tLoss: 0.060743\n",
            "Train Epoch: 9 [3840/60000 (6%)]\tLoss: 0.051302\n",
            "Train Epoch: 9 [4480/60000 (7%)]\tLoss: 0.016373\n",
            "Train Epoch: 9 [5120/60000 (9%)]\tLoss: 0.007123\n",
            "Train Epoch: 9 [5760/60000 (10%)]\tLoss: 0.017418\n",
            "Train Epoch: 9 [6400/60000 (11%)]\tLoss: 0.085933\n",
            "Train Epoch: 9 [7040/60000 (12%)]\tLoss: 0.080440\n",
            "Train Epoch: 9 [7680/60000 (13%)]\tLoss: 0.100914\n",
            "Train Epoch: 9 [8320/60000 (14%)]\tLoss: 0.033971\n",
            "Train Epoch: 9 [8960/60000 (15%)]\tLoss: 0.042982\n",
            "Train Epoch: 9 [9600/60000 (16%)]\tLoss: 0.042822\n",
            "Train Epoch: 9 [10240/60000 (17%)]\tLoss: 0.164021\n",
            "Train Epoch: 9 [10880/60000 (18%)]\tLoss: 0.011706\n",
            "Train Epoch: 9 [11520/60000 (19%)]\tLoss: 0.174269\n",
            "Train Epoch: 9 [12160/60000 (20%)]\tLoss: 0.047161\n",
            "Train Epoch: 9 [12800/60000 (21%)]\tLoss: 0.082493\n",
            "Train Epoch: 9 [13440/60000 (22%)]\tLoss: 0.018623\n",
            "Train Epoch: 9 [14080/60000 (23%)]\tLoss: 0.208214\n",
            "Train Epoch: 9 [14720/60000 (25%)]\tLoss: 0.127963\n",
            "Train Epoch: 9 [15360/60000 (26%)]\tLoss: 0.163653\n",
            "Train Epoch: 9 [16000/60000 (27%)]\tLoss: 0.028253\n",
            "Train Epoch: 9 [16640/60000 (28%)]\tLoss: 0.060391\n",
            "Train Epoch: 9 [17280/60000 (29%)]\tLoss: 0.069441\n",
            "Train Epoch: 9 [17920/60000 (30%)]\tLoss: 0.059256\n",
            "Train Epoch: 9 [18560/60000 (31%)]\tLoss: 0.009523\n",
            "Train Epoch: 9 [19200/60000 (32%)]\tLoss: 0.060047\n",
            "Train Epoch: 9 [19840/60000 (33%)]\tLoss: 0.006462\n",
            "Train Epoch: 9 [20480/60000 (34%)]\tLoss: 0.042463\n",
            "Train Epoch: 9 [21120/60000 (35%)]\tLoss: 0.271670\n",
            "Train Epoch: 9 [21760/60000 (36%)]\tLoss: 0.046890\n",
            "Train Epoch: 9 [22400/60000 (37%)]\tLoss: 0.011221\n",
            "Train Epoch: 9 [23040/60000 (38%)]\tLoss: 0.137353\n",
            "Train Epoch: 9 [23680/60000 (39%)]\tLoss: 0.075640\n",
            "Train Epoch: 9 [24320/60000 (41%)]\tLoss: 0.012010\n",
            "Train Epoch: 9 [24960/60000 (42%)]\tLoss: 0.050346\n",
            "Train Epoch: 9 [25600/60000 (43%)]\tLoss: 0.086529\n",
            "Train Epoch: 9 [26240/60000 (44%)]\tLoss: 0.040140\n",
            "Train Epoch: 9 [26880/60000 (45%)]\tLoss: 0.069379\n",
            "Train Epoch: 9 [27520/60000 (46%)]\tLoss: 0.056024\n",
            "Train Epoch: 9 [28160/60000 (47%)]\tLoss: 0.009826\n",
            "Train Epoch: 9 [28800/60000 (48%)]\tLoss: 0.034802\n",
            "Train Epoch: 9 [29440/60000 (49%)]\tLoss: 0.035618\n",
            "Train Epoch: 9 [30080/60000 (50%)]\tLoss: 0.020710\n",
            "Train Epoch: 9 [30720/60000 (51%)]\tLoss: 0.079924\n",
            "Train Epoch: 9 [31360/60000 (52%)]\tLoss: 0.045786\n",
            "Train Epoch: 9 [32000/60000 (53%)]\tLoss: 0.057723\n",
            "Train Epoch: 9 [32640/60000 (54%)]\tLoss: 0.096668\n",
            "Train Epoch: 9 [33280/60000 (55%)]\tLoss: 0.026388\n",
            "Train Epoch: 9 [33920/60000 (57%)]\tLoss: 0.011654\n",
            "Train Epoch: 9 [34560/60000 (58%)]\tLoss: 0.076897\n",
            "Train Epoch: 9 [35200/60000 (59%)]\tLoss: 0.308924\n",
            "Train Epoch: 9 [35840/60000 (60%)]\tLoss: 0.019597\n",
            "Train Epoch: 9 [36480/60000 (61%)]\tLoss: 0.012656\n",
            "Train Epoch: 9 [37120/60000 (62%)]\tLoss: 0.001955\n",
            "Train Epoch: 9 [37760/60000 (63%)]\tLoss: 0.006369\n",
            "Train Epoch: 9 [38400/60000 (64%)]\tLoss: 0.026868\n",
            "Train Epoch: 9 [39040/60000 (65%)]\tLoss: 0.061684\n",
            "Train Epoch: 9 [39680/60000 (66%)]\tLoss: 0.057780\n",
            "Train Epoch: 9 [40320/60000 (67%)]\tLoss: 0.020851\n",
            "Train Epoch: 9 [40960/60000 (68%)]\tLoss: 0.013494\n",
            "Train Epoch: 9 [41600/60000 (69%)]\tLoss: 0.026123\n",
            "Train Epoch: 9 [42240/60000 (70%)]\tLoss: 0.048911\n",
            "Train Epoch: 9 [42880/60000 (71%)]\tLoss: 0.012029\n",
            "Train Epoch: 9 [43520/60000 (72%)]\tLoss: 0.010350\n",
            "Train Epoch: 9 [44160/60000 (74%)]\tLoss: 0.046684\n",
            "Train Epoch: 9 [44800/60000 (75%)]\tLoss: 0.026016\n",
            "Train Epoch: 9 [45440/60000 (76%)]\tLoss: 0.114862\n",
            "Train Epoch: 9 [46080/60000 (77%)]\tLoss: 0.059712\n",
            "Train Epoch: 9 [46720/60000 (78%)]\tLoss: 0.018304\n",
            "Train Epoch: 9 [47360/60000 (79%)]\tLoss: 0.053107\n",
            "Train Epoch: 9 [48000/60000 (80%)]\tLoss: 0.072689\n",
            "Train Epoch: 9 [48640/60000 (81%)]\tLoss: 0.107195\n",
            "Train Epoch: 9 [49280/60000 (82%)]\tLoss: 0.044911\n",
            "Train Epoch: 9 [49920/60000 (83%)]\tLoss: 0.066463\n",
            "Train Epoch: 9 [50560/60000 (84%)]\tLoss: 0.030838\n",
            "Train Epoch: 9 [51200/60000 (85%)]\tLoss: 0.036026\n",
            "Train Epoch: 9 [51840/60000 (86%)]\tLoss: 0.073213\n",
            "Train Epoch: 9 [52480/60000 (87%)]\tLoss: 0.038072\n",
            "Train Epoch: 9 [53120/60000 (88%)]\tLoss: 0.121879\n",
            "Train Epoch: 9 [53760/60000 (90%)]\tLoss: 0.030898\n",
            "Train Epoch: 9 [54400/60000 (91%)]\tLoss: 0.022491\n",
            "Train Epoch: 9 [55040/60000 (92%)]\tLoss: 0.030547\n",
            "Train Epoch: 9 [55680/60000 (93%)]\tLoss: 0.026087\n",
            "Train Epoch: 9 [56320/60000 (94%)]\tLoss: 0.059783\n",
            "Train Epoch: 9 [56960/60000 (95%)]\tLoss: 0.178392\n",
            "Train Epoch: 9 [57600/60000 (96%)]\tLoss: 0.014163\n",
            "Train Epoch: 9 [58240/60000 (97%)]\tLoss: 0.089982\n",
            "Train Epoch: 9 [58880/60000 (98%)]\tLoss: 0.020329\n",
            "Train Epoch: 9 [59520/60000 (99%)]\tLoss: 0.049320\n",
            "\n",
            "Test set: Average loss: 0.0489, Accuracy: 9846/10000 (98%)\n",
            "\n"
          ]
        }
      ]
    }
  ]
}