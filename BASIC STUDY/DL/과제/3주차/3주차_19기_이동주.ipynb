{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sgAYo4nrw2F4"
   },
   "source": [
    "# < 3주차 과제 1 : MLP 마음대로 다뤄보기>\n",
    "- dataset을 임의로 선정해서 직접 분석 해보기(제공한 코드를 활용해서 해보기)\n",
    "- activation functions 중 relu사용시 함수 직접 정의\n",
    "- lr, optimizer 등 바꿔보기\n",
    "- hidden layer/neuron 수를 바꾸기\n",
    "- 전처리도 추가\n",
    "- 모든 시도를 올려주세요!\n",
    "- 제일 높은 acc를 보인 시도를 명시해주세요!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "id": "fX437IL6qbI-"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "from sklearn.datasets import load_wine\n",
    "from torch.utils.data import  TensorDataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oxkFzBDNmWNk"
   },
   "source": [
    "## 아래 데이터셋 중 원하는 데이터셋 하나를 선택하여, 코드 과제 진행하기!\n",
    "- 1) load_digits() <br>\n",
    "- 2) load_wine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "id": "FywYbfsKtjcR"
   },
   "outputs": [],
   "source": [
    "\n",
    "# 데이터셋 종류 : \n",
    "data = load_wine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "id": "C2P0hqZ9yBGm"
   },
   "outputs": [],
   "source": [
    "input = data.data\n",
    "output = data.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "id": "SggpQfSPt85C"
   },
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "torch.manual_seed(777)\n",
    "if device == \"cuda\":\n",
    "  torch.cuda.manual_seed_all(777)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "id": "bLMzf-2ntYeX"
   },
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(input, output, test_size = 0.3, random_state = 42, stratify= data.target, shuffle = True)\n",
    "\n",
    "x_train = torch.FloatTensor(x_train).to(device)\n",
    "y_train = torch.LongTensor(y_train).to(device)\n",
    "x_test = torch.FloatTensor(x_test)\n",
    "y_test = torch.LongTensor(y_test)\n",
    "\n",
    "# 데이터를 tensor로 바꿔주고 gpu 연산이 가능해지도록 gpu에 옮김\n",
    "# label 값을 왜 long 에 옮겨놓는가? loss function이 다르기 때문 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "umEdiTZkrVqS",
    "outputId": "335674e0-2dd8-449e-b295-90b42c49c21c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.3750e+01, 1.7300e+00, 2.4100e+00, 1.6000e+01, 8.9000e+01, 2.6000e+00,\n",
      "        2.7600e+00, 2.9000e-01, 1.8100e+00, 5.6000e+00, 1.1500e+00, 2.9000e+00,\n",
      "        1.3200e+03])\n",
      "torch.Size([124, 13])\n",
      "tensor(0)\n",
      "torch.Size([124])\n"
     ]
    }
   ],
   "source": [
    "print(x_train[0])\n",
    "print(x_train.shape)\n",
    "\n",
    "print(y_train[0])\n",
    "print(y_train.shape)\n",
    "\n",
    "#input 13개 (속성이 13개)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "combmxzmYFyn"
   },
   "source": [
    "파이토치에서는 데이터셋을 좀 더 쉽게 다룰 수 있도록 유용한 도구로서 torch.utils.data.Dataset과 torch.utils.data.DataLoader를 제공합니다. 이를 사용하면 미니 배치 학습, 데이터 셔플(shuffle), 병렬 처리까지 간단히 수행할 수 있습니다. 기본적인 사용 방법은 Dataset을 정의하고, 이를 DataLoader에 전달하는 것\n",
    "- init : class 에서 객체가 생성되면 바로 실행되는 함수\n",
    "- len : observation 수를 정의하는 함수\n",
    "- getitem : iteration 마다 해당하는 데이터를 돌려주는 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "id": "y38TlgXoqV5Z"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "class CustomDataset(Dataset):\n",
    "  def __init__(self):\n",
    "    self.x_data = x_train\n",
    "    self.y_data = [[y] for y in y_train]\n",
    "#  데이터셋의 전처리를 해주는 부분\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.x_data)\n",
    "#  데이터셋의 길이. 즉, 총 샘플의 수를 적어주는 부분\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    x = torch.FloatTensor(self.x_data[idx]).to(device)\n",
    "    y = torch.LongTensor(self.y_data[idx]).to(device)\n",
    "#  데이터셋에서 특정 1개의 샘플을 가져오는 함수\n",
    "\n",
    "    return x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "id": "x8VHwnuFqino"
   },
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "\n",
    "dataset = CustomDataset()\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "id": "C6V7a4tyq6Jc"
   },
   "outputs": [],
   "source": [
    "# input_layer와 hidden_layer의 노드 개수를 어떻게 바꿔줘야할까? \n",
    "# hidden layer/neuron 수를 바꾸기\n",
    "\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(13, 398, bias=True), \n",
    "    nn.Sigmoid(),\n",
    "    nn.Linear(398, 15, bias=True),\n",
    "    nn.Sigmoid(),\n",
    "    nn.Linear(15,3, bias=True), \n",
    "    nn.Softmax()\n",
    ").to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "07uV8RY7Yr_5"
   },
   "source": [
    "class로 구현 가능\n",
    "- init : 초기 생성 함수\n",
    "- foward : 순전파(입력값 => 예측값 의 과정)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "id": "a0zLstbMqxEZ"
   },
   "outputs": [],
   "source": [
    "class Model(torch.nn.Module):\n",
    "\n",
    "  def __init__(self):\n",
    "    super(Model, self).__init__()\n",
    "    self.layer1 = nn.Sequential(\n",
    "          nn.Linear(13,398, bias=True), # input_layer = 13, hidden_layer1 = 398 \n",
    "          nn.Sigmoid(),\n",
    "        nn.BatchNorm1d(398)\n",
    "    )\n",
    "  # activation function 이용 \n",
    "  #   nn.ReLU()\n",
    "  #   nn.tanH()\n",
    "  #   https://pytorch.org/docs/stable/nn.html 그 외에도 여기서 확인 가능함 \n",
    "  #   파라미터가 필요하지 않다는 것이 특징\n",
    "\n",
    "  # batch normazliation 1d, 파라미터 값으로 vector의 길이를 전해줌\n",
    "  # 추후에 이미지를 다루게 된다면, 그때는 batch normalization 2d를 이용하게 됨 \n",
    "  # 그때는 파라미터 값으로 채널, 가로, 세로 길이를 전달해주게 됨 \n",
    "\n",
    "    self.layer2 = nn.Sequential(\n",
    "          nn.Linear(398,15, bias=True), # hidden_layer1 = 398, hidden_layer2 = 15\n",
    "        nn.Sigmoid()\n",
    "    )\n",
    "    self.layer3 = nn.Sequential(\n",
    "          nn.Linear(15,10, bias=True), # hidden_layer1 = 15, hidden_layer2 = 10\n",
    "        nn.Sigmoid()\n",
    "    )\n",
    "    self.layer4 = nn.Sequential(\n",
    "        nn.Linear(10, 5, bias=True), # hidden_layer3 = 10, output_layer = 5\n",
    "        nn.Softmax()\n",
    "    )\n",
    "\n",
    "  def forward(self,x):\n",
    "    output = self.layer1(x)\n",
    "    output = self.layer2(output)\n",
    "    output = self.layer3(output)\n",
    "    output = self.layer4(output)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 이전 breast cancer 데이터도 output label이 2개인데도 output layer가 5로 설정되어있고 잘 돌아가길래 이번 데이터 또한 5로 설정해둠"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "id": "kqcqqkECrSGK"
   },
   "outputs": [],
   "source": [
    "def init_weights(layer):\n",
    "    if isinstance(layer, nn.Linear):\n",
    "        torch.nn.init.xavier_uniform(layer.weight)\n",
    "        layer.bias.data.fill_(0.01)\n",
    "\n",
    "        #xavier사용\n",
    "        # Layer의 weight를 어떤 분포를 가지도록 초기화시켜줌+global minimum찾기 위해서"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oMDUBFg6rUpw",
    "outputId": "d77885a4-8bfa-4af9-bfaa-36c76dba40ee"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_27612\\2101624760.py:3: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
      "  torch.nn.init.xavier_uniform(layer.weight)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Model(\n",
       "  (layer1): Sequential(\n",
       "    (0): Linear(in_features=13, out_features=398, bias=True)\n",
       "    (1): Sigmoid()\n",
       "    (2): BatchNorm1d(398, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): Linear(in_features=398, out_features=15, bias=True)\n",
       "    (1): Sigmoid()\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): Linear(in_features=15, out_features=10, bias=True)\n",
       "    (1): Sigmoid()\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): Linear(in_features=10, out_features=5, bias=True)\n",
       "    (1): Softmax(dim=None)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Model().to(device)\n",
    "model.apply(init_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OwZt5CetrYFb",
    "outputId": "fe01a0ba-90b9-4120-8f48-2f6710ed17c8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model(\n",
      "  (layer1): Sequential(\n",
      "    (0): Linear(in_features=13, out_features=398, bias=True)\n",
      "    (1): Sigmoid()\n",
      "    (2): BatchNorm1d(398, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): Linear(in_features=398, out_features=15, bias=True)\n",
      "    (1): Sigmoid()\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): Linear(in_features=15, out_features=10, bias=True)\n",
      "    (1): Sigmoid()\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): Linear(in_features=10, out_features=5, bias=True)\n",
      "    (1): Softmax(dim=None)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "id": "AYFp-eTErh7b"
   },
   "outputs": [],
   "source": [
    "loss_fn  = nn.CrossEntropyLoss().to(device)\n",
    "\n",
    "# 여러가지 optimizer 시도해보기\n",
    "# lr 바꿔보기\n",
    "\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr= 0.01)\n",
    "\n",
    "# https://pytorch.org/docs/stable/optim.html#module-torch.optim 페이지 참조\n",
    "# optimizer = optim.SGD(model.parameters(), lr=0.1, momentum=0.9)\n",
    "# sgd 등등"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "90QxHvlIrjS7",
    "outputId": "9325edf8-d0c9-4dd9-c674-eae81a6405c7"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Windows\\System32\\myenv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return self._call_impl(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1.5799548625946045\n",
      "10 1.422746181488037\n",
      "20 1.3021668195724487\n",
      "30 1.1942405700683594\n",
      "40 1.10732901096344\n",
      "50 1.03164803981781\n",
      "60 1.0013011693954468\n",
      "70 0.9627605676651001\n",
      "80 0.9461923837661743\n",
      "90 0.9653967022895813\n"
     ]
    }
   ],
   "source": [
    "losses = []\n",
    "for epoch in range(100):\n",
    "\n",
    "  optimizer.zero_grad()\n",
    "  hypothesis = model(x_train)\n",
    "\n",
    "  # 비용 함수\n",
    "  cost = loss_fn(hypothesis, y_train)\n",
    "  cost.backward()\n",
    "  optimizer.step()\n",
    "  losses.append(cost.item())\n",
    "\n",
    "  if epoch % 10 == 0:\n",
    "    print(epoch, cost.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    },
    "id": "81ASYrW7roFM",
    "outputId": "330a40ca-8185-4c2d-d248-257a5c44b16f"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(losses)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "h4kJzpLErqhZ",
    "outputId": "9b336fde-36f4-470d-9a2f-dc21055e0353"
   },
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "  model = model.to('cpu')\n",
    "  y_pred = model(x_test)\n",
    "  y_pred = y_pred.detach().numpy()\n",
    "  predicted = np.argmax(y_pred, axis =1)\n",
    "  accuracy = (accuracy_score(predicted, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vyIKhs3Nr6Ay",
    "outputId": "6d73d644-c68b-4a84-defc-6f57192ef91a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model의 output은 :  [0.9733017  0.01560928 0.00824407 0.00105031 0.0017947 ]\n",
      "argmax를 한 후의 output은 0\n",
      "accuracy는 0.9444444444444444\n"
     ]
    }
   ],
   "source": [
    "print(f'model의 output은 :  {y_pred[0]}')\n",
    "print(f'argmax를 한 후의 output은 {predicted[0]}')\n",
    "print(f'accuracy는 {accuracy}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3RzRM7xThZV_"
   },
   "source": [
    "# < 3주차 과제 2 : CNN 맛보기>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "56xqgtLxhZw6"
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "TzkF2bFNhcQ2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./data/MNIST\\raw\\train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████| 9912422/9912422 [00:00<00:00, 15848231.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST\\raw\\train-images-idx3-ubyte.gz to ./data/MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST\\raw\\train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████| 28881/28881 [00:00<00:00, 15053522.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST\\raw\\train-labels-idx1-ubyte.gz to ./data/MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST\\raw\\t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████████████████████████████████████████████████████████████| 1648877/1648877 [00:00<00:00, 14802596.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST\\raw\\t10k-images-idx3-ubyte.gz to ./data/MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST\\raw\\t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 4542/4542 [00:00<00:00, 2396292.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST\\raw\\t10k-labels-idx1-ubyte.gz to ./data/MNIST\\raw\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Training settings\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "# MNIST Dataset\n",
    "train_dataset = datasets.MNIST(root='./data/',\n",
    "                               train=True,\n",
    "                               transform=transforms.ToTensor(),\n",
    "                               download=True)\n",
    "\n",
    "test_dataset = datasets.MNIST(root='./data/',\n",
    "                              train=False,\n",
    "                              transform=transforms.ToTensor())\n",
    "\n",
    "# Data Loader (Input Pipeline)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                           batch_size=batch_size,\n",
    "                                           shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
    "                                          batch_size=batch_size,\n",
    "                                          shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "id": "tLCSvgganBrH"
   },
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "  def __init__(self):\n",
    "    super(Net, self).__init__()\n",
    "    self.conv1 = nn.Conv2d(1, 10, kernel_size = 5) \n",
    "    self.conv2 = nn.Conv2d(10, 20, kernel_size = 5)\n",
    "    self.mp = nn.MaxPool2d(2)\n",
    "    self.fc = nn.Linear(320 , 10) # 알맞는 input은?\n",
    "\n",
    "  def forward(self, x):\n",
    "    in_size = x.size(0)\n",
    "    x = F.relu(self.mp(self.conv1(x)))\n",
    "    x = F.relu(self.mp(self.conv2(x)))\n",
    "    x = x.view(in_size, -1)  \n",
    "    x = self.fc(x)\n",
    "    return F.log_softmax(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "id": "lkYZ4pUdnUHc"
   },
   "outputs": [],
   "source": [
    "model = Net()\n",
    "optimizer = optim.SGD(model.parameters(), lr = 0.01, momentum = 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "id": "IzUrEM3EnXJb"
   },
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "  model.train()  # 훈련 모드로 설정\n",
    "  for batch_idx, (data, target) in enumerate(train_loader):\n",
    "    data, target = Variable(data), Variable(target)\n",
    "    optimizer.zero_grad()    # gradient 초기화\n",
    "    output = model(data)\n",
    "    loss = F.nll_loss(output, target)   # negative log likelihood loss\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if batch_idx % 10 == 0:\n",
    "      print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "          epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "          100. * batch_idx / len(train_loader), loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "id": "EFi0gYJGn2aa"
   },
   "outputs": [],
   "source": [
    "def test():\n",
    "    model.eval() #model.eval() 의 기능은?   # 평가 모드로 설정\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    for data, target in test_loader:\n",
    "        data, target = Variable(data, volatile=True), Variable(target)\n",
    "        output = model(data)\n",
    "        test_loss += F.nll_loss(output, target, size_average=False).data # nll_loss?? / cross entropy loss와의 관계 확인!\n",
    "        pred = output.data.max(1, keepdim=True)[1]\n",
    "        correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)    # 평균 손실\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- nll_loss: negative log likelihood loss\n",
    "- cross entropy loss와의 관계: cross entropy는 두 확률 분포 간의 거리가 최소화되도록 학습시키며, NLL은 cross entropy loss에서 정답 클래스에 해당하는 확률의 로그를 취한 것"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "id": "zSvSZb_Bn4Nx"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_27612\\2638290573.py:15: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return F.log_softmax(x)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.309309\n",
      "Train Epoch: 1 [640/60000 (1%)]\tLoss: 2.293573\n",
      "Train Epoch: 1 [1280/60000 (2%)]\tLoss: 2.296662\n",
      "Train Epoch: 1 [1920/60000 (3%)]\tLoss: 2.282233\n",
      "Train Epoch: 1 [2560/60000 (4%)]\tLoss: 2.276705\n",
      "Train Epoch: 1 [3200/60000 (5%)]\tLoss: 2.252425\n",
      "Train Epoch: 1 [3840/60000 (6%)]\tLoss: 2.267917\n",
      "Train Epoch: 1 [4480/60000 (7%)]\tLoss: 2.239310\n",
      "Train Epoch: 1 [5120/60000 (9%)]\tLoss: 2.219860\n",
      "Train Epoch: 1 [5760/60000 (10%)]\tLoss: 2.186896\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 2.142303\n",
      "Train Epoch: 1 [7040/60000 (12%)]\tLoss: 2.075975\n",
      "Train Epoch: 1 [7680/60000 (13%)]\tLoss: 2.025887\n",
      "Train Epoch: 1 [8320/60000 (14%)]\tLoss: 1.857085\n",
      "Train Epoch: 1 [8960/60000 (15%)]\tLoss: 1.670601\n",
      "Train Epoch: 1 [9600/60000 (16%)]\tLoss: 1.397461\n",
      "Train Epoch: 1 [10240/60000 (17%)]\tLoss: 1.089701\n",
      "Train Epoch: 1 [10880/60000 (18%)]\tLoss: 1.117220\n",
      "Train Epoch: 1 [11520/60000 (19%)]\tLoss: 0.964369\n",
      "Train Epoch: 1 [12160/60000 (20%)]\tLoss: 0.770526\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.726372\n",
      "Train Epoch: 1 [13440/60000 (22%)]\tLoss: 0.605205\n",
      "Train Epoch: 1 [14080/60000 (23%)]\tLoss: 0.616738\n",
      "Train Epoch: 1 [14720/60000 (25%)]\tLoss: 0.527937\n",
      "Train Epoch: 1 [15360/60000 (26%)]\tLoss: 0.458589\n",
      "Train Epoch: 1 [16000/60000 (27%)]\tLoss: 0.480716\n",
      "Train Epoch: 1 [16640/60000 (28%)]\tLoss: 0.566983\n",
      "Train Epoch: 1 [17280/60000 (29%)]\tLoss: 0.547431\n",
      "Train Epoch: 1 [17920/60000 (30%)]\tLoss: 0.478194\n",
      "Train Epoch: 1 [18560/60000 (31%)]\tLoss: 0.556999\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.657039\n",
      "Train Epoch: 1 [19840/60000 (33%)]\tLoss: 0.333448\n",
      "Train Epoch: 1 [20480/60000 (34%)]\tLoss: 0.239490\n",
      "Train Epoch: 1 [21120/60000 (35%)]\tLoss: 0.400291\n",
      "Train Epoch: 1 [21760/60000 (36%)]\tLoss: 0.747798\n",
      "Train Epoch: 1 [22400/60000 (37%)]\tLoss: 0.499100\n",
      "Train Epoch: 1 [23040/60000 (38%)]\tLoss: 0.654022\n",
      "Train Epoch: 1 [23680/60000 (39%)]\tLoss: 0.358354\n",
      "Train Epoch: 1 [24320/60000 (41%)]\tLoss: 0.237725\n",
      "Train Epoch: 1 [24960/60000 (42%)]\tLoss: 0.294242\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.575599\n",
      "Train Epoch: 1 [26240/60000 (44%)]\tLoss: 0.316030\n",
      "Train Epoch: 1 [26880/60000 (45%)]\tLoss: 0.310578\n",
      "Train Epoch: 1 [27520/60000 (46%)]\tLoss: 0.414585\n",
      "Train Epoch: 1 [28160/60000 (47%)]\tLoss: 0.393994\n",
      "Train Epoch: 1 [28800/60000 (48%)]\tLoss: 0.469305\n",
      "Train Epoch: 1 [29440/60000 (49%)]\tLoss: 0.345271\n",
      "Train Epoch: 1 [30080/60000 (50%)]\tLoss: 0.541518\n",
      "Train Epoch: 1 [30720/60000 (51%)]\tLoss: 0.298417\n",
      "Train Epoch: 1 [31360/60000 (52%)]\tLoss: 0.312328\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.284801\n",
      "Train Epoch: 1 [32640/60000 (54%)]\tLoss: 0.269022\n",
      "Train Epoch: 1 [33280/60000 (55%)]\tLoss: 0.258705\n",
      "Train Epoch: 1 [33920/60000 (57%)]\tLoss: 0.196343\n",
      "Train Epoch: 1 [34560/60000 (58%)]\tLoss: 0.331101\n",
      "Train Epoch: 1 [35200/60000 (59%)]\tLoss: 0.338074\n",
      "Train Epoch: 1 [35840/60000 (60%)]\tLoss: 0.270606\n",
      "Train Epoch: 1 [36480/60000 (61%)]\tLoss: 0.251664\n",
      "Train Epoch: 1 [37120/60000 (62%)]\tLoss: 0.260920\n",
      "Train Epoch: 1 [37760/60000 (63%)]\tLoss: 0.198183\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.490254\n",
      "Train Epoch: 1 [39040/60000 (65%)]\tLoss: 0.274386\n",
      "Train Epoch: 1 [39680/60000 (66%)]\tLoss: 0.344352\n",
      "Train Epoch: 1 [40320/60000 (67%)]\tLoss: 0.211142\n",
      "Train Epoch: 1 [40960/60000 (68%)]\tLoss: 0.204309\n",
      "Train Epoch: 1 [41600/60000 (69%)]\tLoss: 0.360587\n",
      "Train Epoch: 1 [42240/60000 (70%)]\tLoss: 0.403056\n",
      "Train Epoch: 1 [42880/60000 (71%)]\tLoss: 0.322145\n",
      "Train Epoch: 1 [43520/60000 (72%)]\tLoss: 0.274463\n",
      "Train Epoch: 1 [44160/60000 (74%)]\tLoss: 0.186740\n",
      "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.214903\n",
      "Train Epoch: 1 [45440/60000 (76%)]\tLoss: 0.413038\n",
      "Train Epoch: 1 [46080/60000 (77%)]\tLoss: 0.126830\n",
      "Train Epoch: 1 [46720/60000 (78%)]\tLoss: 0.205016\n",
      "Train Epoch: 1 [47360/60000 (79%)]\tLoss: 0.208861\n",
      "Train Epoch: 1 [48000/60000 (80%)]\tLoss: 0.172094\n",
      "Train Epoch: 1 [48640/60000 (81%)]\tLoss: 0.138193\n",
      "Train Epoch: 1 [49280/60000 (82%)]\tLoss: 0.282023\n",
      "Train Epoch: 1 [49920/60000 (83%)]\tLoss: 0.370670\n",
      "Train Epoch: 1 [50560/60000 (84%)]\tLoss: 0.210942\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.270496\n",
      "Train Epoch: 1 [51840/60000 (86%)]\tLoss: 0.303919\n",
      "Train Epoch: 1 [52480/60000 (87%)]\tLoss: 0.113731\n",
      "Train Epoch: 1 [53120/60000 (88%)]\tLoss: 0.238193\n",
      "Train Epoch: 1 [53760/60000 (90%)]\tLoss: 0.272003\n",
      "Train Epoch: 1 [54400/60000 (91%)]\tLoss: 0.433990\n",
      "Train Epoch: 1 [55040/60000 (92%)]\tLoss: 0.340532\n",
      "Train Epoch: 1 [55680/60000 (93%)]\tLoss: 0.212436\n",
      "Train Epoch: 1 [56320/60000 (94%)]\tLoss: 0.150610\n",
      "Train Epoch: 1 [56960/60000 (95%)]\tLoss: 0.286857\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.352740\n",
      "Train Epoch: 1 [58240/60000 (97%)]\tLoss: 0.191100\n",
      "Train Epoch: 1 [58880/60000 (98%)]\tLoss: 0.622074\n",
      "Train Epoch: 1 [59520/60000 (99%)]\tLoss: 0.163857\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_27612\\1412383255.py:6: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "  data, target = Variable(data, volatile=True), Variable(target)\n",
      "C:\\Windows\\System32\\myenv\\lib\\site-packages\\torch\\nn\\_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.1772, Accuracy: 9484/10000 (95%)\n",
      "\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.096937\n",
      "Train Epoch: 2 [640/60000 (1%)]\tLoss: 0.186270\n",
      "Train Epoch: 2 [1280/60000 (2%)]\tLoss: 0.324445\n",
      "Train Epoch: 2 [1920/60000 (3%)]\tLoss: 0.251303\n",
      "Train Epoch: 2 [2560/60000 (4%)]\tLoss: 0.177348\n",
      "Train Epoch: 2 [3200/60000 (5%)]\tLoss: 0.144502\n",
      "Train Epoch: 2 [3840/60000 (6%)]\tLoss: 0.136747\n",
      "Train Epoch: 2 [4480/60000 (7%)]\tLoss: 0.218501\n",
      "Train Epoch: 2 [5120/60000 (9%)]\tLoss: 0.258863\n",
      "Train Epoch: 2 [5760/60000 (10%)]\tLoss: 0.073829\n",
      "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 0.137388\n",
      "Train Epoch: 2 [7040/60000 (12%)]\tLoss: 0.170885\n",
      "Train Epoch: 2 [7680/60000 (13%)]\tLoss: 0.199225\n",
      "Train Epoch: 2 [8320/60000 (14%)]\tLoss: 0.194255\n",
      "Train Epoch: 2 [8960/60000 (15%)]\tLoss: 0.103051\n",
      "Train Epoch: 2 [9600/60000 (16%)]\tLoss: 0.257100\n",
      "Train Epoch: 2 [10240/60000 (17%)]\tLoss: 0.126356\n",
      "Train Epoch: 2 [10880/60000 (18%)]\tLoss: 0.296945\n",
      "Train Epoch: 2 [11520/60000 (19%)]\tLoss: 0.110145\n",
      "Train Epoch: 2 [12160/60000 (20%)]\tLoss: 0.118186\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 0.239631\n",
      "Train Epoch: 2 [13440/60000 (22%)]\tLoss: 0.116859\n",
      "Train Epoch: 2 [14080/60000 (23%)]\tLoss: 0.157671\n",
      "Train Epoch: 2 [14720/60000 (25%)]\tLoss: 0.271774\n",
      "Train Epoch: 2 [15360/60000 (26%)]\tLoss: 0.091086\n",
      "Train Epoch: 2 [16000/60000 (27%)]\tLoss: 0.257206\n",
      "Train Epoch: 2 [16640/60000 (28%)]\tLoss: 0.063319\n",
      "Train Epoch: 2 [17280/60000 (29%)]\tLoss: 0.091022\n",
      "Train Epoch: 2 [17920/60000 (30%)]\tLoss: 0.257353\n",
      "Train Epoch: 2 [18560/60000 (31%)]\tLoss: 0.365804\n",
      "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 0.283530\n",
      "Train Epoch: 2 [19840/60000 (33%)]\tLoss: 0.300383\n",
      "Train Epoch: 2 [20480/60000 (34%)]\tLoss: 0.222074\n",
      "Train Epoch: 2 [21120/60000 (35%)]\tLoss: 0.053408\n",
      "Train Epoch: 2 [21760/60000 (36%)]\tLoss: 0.141720\n",
      "Train Epoch: 2 [22400/60000 (37%)]\tLoss: 0.149929\n",
      "Train Epoch: 2 [23040/60000 (38%)]\tLoss: 0.201569\n",
      "Train Epoch: 2 [23680/60000 (39%)]\tLoss: 0.059729\n",
      "Train Epoch: 2 [24320/60000 (41%)]\tLoss: 0.206043\n",
      "Train Epoch: 2 [24960/60000 (42%)]\tLoss: 0.318075\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.187565\n",
      "Train Epoch: 2 [26240/60000 (44%)]\tLoss: 0.377272\n",
      "Train Epoch: 2 [26880/60000 (45%)]\tLoss: 0.132402\n",
      "Train Epoch: 2 [27520/60000 (46%)]\tLoss: 0.168519\n",
      "Train Epoch: 2 [28160/60000 (47%)]\tLoss: 0.205165\n",
      "Train Epoch: 2 [28800/60000 (48%)]\tLoss: 0.115330\n",
      "Train Epoch: 2 [29440/60000 (49%)]\tLoss: 0.051680\n",
      "Train Epoch: 2 [30080/60000 (50%)]\tLoss: 0.241762\n",
      "Train Epoch: 2 [30720/60000 (51%)]\tLoss: 0.461172\n",
      "Train Epoch: 2 [31360/60000 (52%)]\tLoss: 0.290974\n",
      "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 0.084834\n",
      "Train Epoch: 2 [32640/60000 (54%)]\tLoss: 0.215487\n",
      "Train Epoch: 2 [33280/60000 (55%)]\tLoss: 0.205326\n",
      "Train Epoch: 2 [33920/60000 (57%)]\tLoss: 0.088050\n",
      "Train Epoch: 2 [34560/60000 (58%)]\tLoss: 0.118425\n",
      "Train Epoch: 2 [35200/60000 (59%)]\tLoss: 0.105761\n",
      "Train Epoch: 2 [35840/60000 (60%)]\tLoss: 0.124633\n",
      "Train Epoch: 2 [36480/60000 (61%)]\tLoss: 0.137118\n",
      "Train Epoch: 2 [37120/60000 (62%)]\tLoss: 0.042354\n",
      "Train Epoch: 2 [37760/60000 (63%)]\tLoss: 0.118856\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 0.201564\n",
      "Train Epoch: 2 [39040/60000 (65%)]\tLoss: 0.084733\n",
      "Train Epoch: 2 [39680/60000 (66%)]\tLoss: 0.084503\n",
      "Train Epoch: 2 [40320/60000 (67%)]\tLoss: 0.097409\n",
      "Train Epoch: 2 [40960/60000 (68%)]\tLoss: 0.101341\n",
      "Train Epoch: 2 [41600/60000 (69%)]\tLoss: 0.153668\n",
      "Train Epoch: 2 [42240/60000 (70%)]\tLoss: 0.254914\n",
      "Train Epoch: 2 [42880/60000 (71%)]\tLoss: 0.301409\n",
      "Train Epoch: 2 [43520/60000 (72%)]\tLoss: 0.057941\n",
      "Train Epoch: 2 [44160/60000 (74%)]\tLoss: 0.156339\n",
      "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 0.154555\n",
      "Train Epoch: 2 [45440/60000 (76%)]\tLoss: 0.174049\n",
      "Train Epoch: 2 [46080/60000 (77%)]\tLoss: 0.235465\n",
      "Train Epoch: 2 [46720/60000 (78%)]\tLoss: 0.234422\n",
      "Train Epoch: 2 [47360/60000 (79%)]\tLoss: 0.198960\n",
      "Train Epoch: 2 [48000/60000 (80%)]\tLoss: 0.226451\n",
      "Train Epoch: 2 [48640/60000 (81%)]\tLoss: 0.057438\n",
      "Train Epoch: 2 [49280/60000 (82%)]\tLoss: 0.063341\n",
      "Train Epoch: 2 [49920/60000 (83%)]\tLoss: 0.125279\n",
      "Train Epoch: 2 [50560/60000 (84%)]\tLoss: 0.137420\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.097715\n",
      "Train Epoch: 2 [51840/60000 (86%)]\tLoss: 0.391492\n",
      "Train Epoch: 2 [52480/60000 (87%)]\tLoss: 0.052594\n",
      "Train Epoch: 2 [53120/60000 (88%)]\tLoss: 0.086388\n",
      "Train Epoch: 2 [53760/60000 (90%)]\tLoss: 0.128686\n",
      "Train Epoch: 2 [54400/60000 (91%)]\tLoss: 0.048496\n",
      "Train Epoch: 2 [55040/60000 (92%)]\tLoss: 0.076126\n",
      "Train Epoch: 2 [55680/60000 (93%)]\tLoss: 0.108162\n",
      "Train Epoch: 2 [56320/60000 (94%)]\tLoss: 0.210203\n",
      "Train Epoch: 2 [56960/60000 (95%)]\tLoss: 0.104961\n",
      "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 0.154635\n",
      "Train Epoch: 2 [58240/60000 (97%)]\tLoss: 0.092148\n",
      "Train Epoch: 2 [58880/60000 (98%)]\tLoss: 0.119643\n",
      "Train Epoch: 2 [59520/60000 (99%)]\tLoss: 0.126364\n",
      "\n",
      "Test set: Average loss: 0.1096, Accuracy: 9660/10000 (97%)\n",
      "\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.316800\n",
      "Train Epoch: 3 [640/60000 (1%)]\tLoss: 0.112155\n",
      "Train Epoch: 3 [1280/60000 (2%)]\tLoss: 0.177287\n",
      "Train Epoch: 3 [1920/60000 (3%)]\tLoss: 0.126445\n",
      "Train Epoch: 3 [2560/60000 (4%)]\tLoss: 0.042649\n",
      "Train Epoch: 3 [3200/60000 (5%)]\tLoss: 0.137082\n",
      "Train Epoch: 3 [3840/60000 (6%)]\tLoss: 0.119891\n",
      "Train Epoch: 3 [4480/60000 (7%)]\tLoss: 0.287565\n",
      "Train Epoch: 3 [5120/60000 (9%)]\tLoss: 0.169142\n",
      "Train Epoch: 3 [5760/60000 (10%)]\tLoss: 0.166173\n",
      "Train Epoch: 3 [6400/60000 (11%)]\tLoss: 0.121658\n",
      "Train Epoch: 3 [7040/60000 (12%)]\tLoss: 0.086842\n",
      "Train Epoch: 3 [7680/60000 (13%)]\tLoss: 0.286009\n",
      "Train Epoch: 3 [8320/60000 (14%)]\tLoss: 0.035979\n",
      "Train Epoch: 3 [8960/60000 (15%)]\tLoss: 0.084350\n",
      "Train Epoch: 3 [9600/60000 (16%)]\tLoss: 0.093326\n",
      "Train Epoch: 3 [10240/60000 (17%)]\tLoss: 0.291785\n",
      "Train Epoch: 3 [10880/60000 (18%)]\tLoss: 0.073005\n",
      "Train Epoch: 3 [11520/60000 (19%)]\tLoss: 0.095488\n",
      "Train Epoch: 3 [12160/60000 (20%)]\tLoss: 0.093754\n",
      "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 0.103354\n",
      "Train Epoch: 3 [13440/60000 (22%)]\tLoss: 0.161415\n",
      "Train Epoch: 3 [14080/60000 (23%)]\tLoss: 0.126549\n",
      "Train Epoch: 3 [14720/60000 (25%)]\tLoss: 0.081120\n",
      "Train Epoch: 3 [15360/60000 (26%)]\tLoss: 0.113401\n",
      "Train Epoch: 3 [16000/60000 (27%)]\tLoss: 0.069319\n",
      "Train Epoch: 3 [16640/60000 (28%)]\tLoss: 0.031521\n",
      "Train Epoch: 3 [17280/60000 (29%)]\tLoss: 0.086603\n",
      "Train Epoch: 3 [17920/60000 (30%)]\tLoss: 0.190183\n",
      "Train Epoch: 3 [18560/60000 (31%)]\tLoss: 0.064180\n",
      "Train Epoch: 3 [19200/60000 (32%)]\tLoss: 0.173285\n",
      "Train Epoch: 3 [19840/60000 (33%)]\tLoss: 0.080454\n",
      "Train Epoch: 3 [20480/60000 (34%)]\tLoss: 0.203930\n",
      "Train Epoch: 3 [21120/60000 (35%)]\tLoss: 0.128292\n",
      "Train Epoch: 3 [21760/60000 (36%)]\tLoss: 0.123646\n",
      "Train Epoch: 3 [22400/60000 (37%)]\tLoss: 0.276760\n",
      "Train Epoch: 3 [23040/60000 (38%)]\tLoss: 0.160025\n",
      "Train Epoch: 3 [23680/60000 (39%)]\tLoss: 0.298427\n",
      "Train Epoch: 3 [24320/60000 (41%)]\tLoss: 0.049330\n",
      "Train Epoch: 3 [24960/60000 (42%)]\tLoss: 0.277632\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.059135\n",
      "Train Epoch: 3 [26240/60000 (44%)]\tLoss: 0.068844\n",
      "Train Epoch: 3 [26880/60000 (45%)]\tLoss: 0.047975\n",
      "Train Epoch: 3 [27520/60000 (46%)]\tLoss: 0.141024\n",
      "Train Epoch: 3 [28160/60000 (47%)]\tLoss: 0.215359\n",
      "Train Epoch: 3 [28800/60000 (48%)]\tLoss: 0.049826\n",
      "Train Epoch: 3 [29440/60000 (49%)]\tLoss: 0.042385\n",
      "Train Epoch: 3 [30080/60000 (50%)]\tLoss: 0.171693\n",
      "Train Epoch: 3 [30720/60000 (51%)]\tLoss: 0.149093\n",
      "Train Epoch: 3 [31360/60000 (52%)]\tLoss: 0.029206\n",
      "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 0.184330\n",
      "Train Epoch: 3 [32640/60000 (54%)]\tLoss: 0.102806\n",
      "Train Epoch: 3 [33280/60000 (55%)]\tLoss: 0.079034\n",
      "Train Epoch: 3 [33920/60000 (57%)]\tLoss: 0.079250\n",
      "Train Epoch: 3 [34560/60000 (58%)]\tLoss: 0.091638\n",
      "Train Epoch: 3 [35200/60000 (59%)]\tLoss: 0.100095\n",
      "Train Epoch: 3 [35840/60000 (60%)]\tLoss: 0.290858\n",
      "Train Epoch: 3 [36480/60000 (61%)]\tLoss: 0.131326\n",
      "Train Epoch: 3 [37120/60000 (62%)]\tLoss: 0.087420\n",
      "Train Epoch: 3 [37760/60000 (63%)]\tLoss: 0.069799\n",
      "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 0.099077\n",
      "Train Epoch: 3 [39040/60000 (65%)]\tLoss: 0.143592\n",
      "Train Epoch: 3 [39680/60000 (66%)]\tLoss: 0.070414\n",
      "Train Epoch: 3 [40320/60000 (67%)]\tLoss: 0.088117\n",
      "Train Epoch: 3 [40960/60000 (68%)]\tLoss: 0.128052\n",
      "Train Epoch: 3 [41600/60000 (69%)]\tLoss: 0.138475\n",
      "Train Epoch: 3 [42240/60000 (70%)]\tLoss: 0.098664\n",
      "Train Epoch: 3 [42880/60000 (71%)]\tLoss: 0.037279\n",
      "Train Epoch: 3 [43520/60000 (72%)]\tLoss: 0.169216\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 3 [44160/60000 (74%)]\tLoss: 0.040367\n",
      "Train Epoch: 3 [44800/60000 (75%)]\tLoss: 0.075582\n",
      "Train Epoch: 3 [45440/60000 (76%)]\tLoss: 0.076652\n",
      "Train Epoch: 3 [46080/60000 (77%)]\tLoss: 0.249816\n",
      "Train Epoch: 3 [46720/60000 (78%)]\tLoss: 0.052060\n",
      "Train Epoch: 3 [47360/60000 (79%)]\tLoss: 0.051096\n",
      "Train Epoch: 3 [48000/60000 (80%)]\tLoss: 0.036776\n",
      "Train Epoch: 3 [48640/60000 (81%)]\tLoss: 0.036809\n",
      "Train Epoch: 3 [49280/60000 (82%)]\tLoss: 0.044409\n",
      "Train Epoch: 3 [49920/60000 (83%)]\tLoss: 0.162081\n",
      "Train Epoch: 3 [50560/60000 (84%)]\tLoss: 0.052684\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.084908\n",
      "Train Epoch: 3 [51840/60000 (86%)]\tLoss: 0.044190\n",
      "Train Epoch: 3 [52480/60000 (87%)]\tLoss: 0.076864\n",
      "Train Epoch: 3 [53120/60000 (88%)]\tLoss: 0.107694\n",
      "Train Epoch: 3 [53760/60000 (90%)]\tLoss: 0.282696\n",
      "Train Epoch: 3 [54400/60000 (91%)]\tLoss: 0.088295\n",
      "Train Epoch: 3 [55040/60000 (92%)]\tLoss: 0.036876\n",
      "Train Epoch: 3 [55680/60000 (93%)]\tLoss: 0.122806\n",
      "Train Epoch: 3 [56320/60000 (94%)]\tLoss: 0.085221\n",
      "Train Epoch: 3 [56960/60000 (95%)]\tLoss: 0.066436\n",
      "Train Epoch: 3 [57600/60000 (96%)]\tLoss: 0.109979\n",
      "Train Epoch: 3 [58240/60000 (97%)]\tLoss: 0.052102\n",
      "Train Epoch: 3 [58880/60000 (98%)]\tLoss: 0.020035\n",
      "Train Epoch: 3 [59520/60000 (99%)]\tLoss: 0.181601\n",
      "\n",
      "Test set: Average loss: 0.0855, Accuracy: 9736/10000 (97%)\n",
      "\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.229812\n",
      "Train Epoch: 4 [640/60000 (1%)]\tLoss: 0.046286\n",
      "Train Epoch: 4 [1280/60000 (2%)]\tLoss: 0.122463\n",
      "Train Epoch: 4 [1920/60000 (3%)]\tLoss: 0.159479\n",
      "Train Epoch: 4 [2560/60000 (4%)]\tLoss: 0.127102\n",
      "Train Epoch: 4 [3200/60000 (5%)]\tLoss: 0.257655\n",
      "Train Epoch: 4 [3840/60000 (6%)]\tLoss: 0.076195\n",
      "Train Epoch: 4 [4480/60000 (7%)]\tLoss: 0.069163\n",
      "Train Epoch: 4 [5120/60000 (9%)]\tLoss: 0.108317\n",
      "Train Epoch: 4 [5760/60000 (10%)]\tLoss: 0.030338\n",
      "Train Epoch: 4 [6400/60000 (11%)]\tLoss: 0.132359\n",
      "Train Epoch: 4 [7040/60000 (12%)]\tLoss: 0.046342\n",
      "Train Epoch: 4 [7680/60000 (13%)]\tLoss: 0.028872\n",
      "Train Epoch: 4 [8320/60000 (14%)]\tLoss: 0.170815\n",
      "Train Epoch: 4 [8960/60000 (15%)]\tLoss: 0.023544\n",
      "Train Epoch: 4 [9600/60000 (16%)]\tLoss: 0.087349\n",
      "Train Epoch: 4 [10240/60000 (17%)]\tLoss: 0.292306\n",
      "Train Epoch: 4 [10880/60000 (18%)]\tLoss: 0.022224\n",
      "Train Epoch: 4 [11520/60000 (19%)]\tLoss: 0.031095\n",
      "Train Epoch: 4 [12160/60000 (20%)]\tLoss: 0.171184\n",
      "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 0.180919\n",
      "Train Epoch: 4 [13440/60000 (22%)]\tLoss: 0.046617\n",
      "Train Epoch: 4 [14080/60000 (23%)]\tLoss: 0.057229\n",
      "Train Epoch: 4 [14720/60000 (25%)]\tLoss: 0.099503\n",
      "Train Epoch: 4 [15360/60000 (26%)]\tLoss: 0.119209\n",
      "Train Epoch: 4 [16000/60000 (27%)]\tLoss: 0.113561\n",
      "Train Epoch: 4 [16640/60000 (28%)]\tLoss: 0.071304\n",
      "Train Epoch: 4 [17280/60000 (29%)]\tLoss: 0.055310\n",
      "Train Epoch: 4 [17920/60000 (30%)]\tLoss: 0.066942\n",
      "Train Epoch: 4 [18560/60000 (31%)]\tLoss: 0.118814\n",
      "Train Epoch: 4 [19200/60000 (32%)]\tLoss: 0.134666\n",
      "Train Epoch: 4 [19840/60000 (33%)]\tLoss: 0.081096\n",
      "Train Epoch: 4 [20480/60000 (34%)]\tLoss: 0.142156\n",
      "Train Epoch: 4 [21120/60000 (35%)]\tLoss: 0.108551\n",
      "Train Epoch: 4 [21760/60000 (36%)]\tLoss: 0.063714\n",
      "Train Epoch: 4 [22400/60000 (37%)]\tLoss: 0.155036\n",
      "Train Epoch: 4 [23040/60000 (38%)]\tLoss: 0.117136\n",
      "Train Epoch: 4 [23680/60000 (39%)]\tLoss: 0.059386\n",
      "Train Epoch: 4 [24320/60000 (41%)]\tLoss: 0.077705\n",
      "Train Epoch: 4 [24960/60000 (42%)]\tLoss: 0.091862\n",
      "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 0.038347\n",
      "Train Epoch: 4 [26240/60000 (44%)]\tLoss: 0.027428\n",
      "Train Epoch: 4 [26880/60000 (45%)]\tLoss: 0.174915\n",
      "Train Epoch: 4 [27520/60000 (46%)]\tLoss: 0.067035\n",
      "Train Epoch: 4 [28160/60000 (47%)]\tLoss: 0.100286\n",
      "Train Epoch: 4 [28800/60000 (48%)]\tLoss: 0.031173\n",
      "Train Epoch: 4 [29440/60000 (49%)]\tLoss: 0.055238\n",
      "Train Epoch: 4 [30080/60000 (50%)]\tLoss: 0.048829\n",
      "Train Epoch: 4 [30720/60000 (51%)]\tLoss: 0.031009\n",
      "Train Epoch: 4 [31360/60000 (52%)]\tLoss: 0.165506\n",
      "Train Epoch: 4 [32000/60000 (53%)]\tLoss: 0.194730\n",
      "Train Epoch: 4 [32640/60000 (54%)]\tLoss: 0.198490\n",
      "Train Epoch: 4 [33280/60000 (55%)]\tLoss: 0.139004\n",
      "Train Epoch: 4 [33920/60000 (57%)]\tLoss: 0.042538\n",
      "Train Epoch: 4 [34560/60000 (58%)]\tLoss: 0.071347\n",
      "Train Epoch: 4 [35200/60000 (59%)]\tLoss: 0.137417\n",
      "Train Epoch: 4 [35840/60000 (60%)]\tLoss: 0.053154\n",
      "Train Epoch: 4 [36480/60000 (61%)]\tLoss: 0.139981\n",
      "Train Epoch: 4 [37120/60000 (62%)]\tLoss: 0.077869\n",
      "Train Epoch: 4 [37760/60000 (63%)]\tLoss: 0.036147\n",
      "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 0.048716\n",
      "Train Epoch: 4 [39040/60000 (65%)]\tLoss: 0.023248\n",
      "Train Epoch: 4 [39680/60000 (66%)]\tLoss: 0.097793\n",
      "Train Epoch: 4 [40320/60000 (67%)]\tLoss: 0.068059\n",
      "Train Epoch: 4 [40960/60000 (68%)]\tLoss: 0.042329\n",
      "Train Epoch: 4 [41600/60000 (69%)]\tLoss: 0.047183\n",
      "Train Epoch: 4 [42240/60000 (70%)]\tLoss: 0.028395\n",
      "Train Epoch: 4 [42880/60000 (71%)]\tLoss: 0.136581\n",
      "Train Epoch: 4 [43520/60000 (72%)]\tLoss: 0.059978\n",
      "Train Epoch: 4 [44160/60000 (74%)]\tLoss: 0.093350\n",
      "Train Epoch: 4 [44800/60000 (75%)]\tLoss: 0.061487\n",
      "Train Epoch: 4 [45440/60000 (76%)]\tLoss: 0.134158\n",
      "Train Epoch: 4 [46080/60000 (77%)]\tLoss: 0.197336\n",
      "Train Epoch: 4 [46720/60000 (78%)]\tLoss: 0.018434\n",
      "Train Epoch: 4 [47360/60000 (79%)]\tLoss: 0.025371\n",
      "Train Epoch: 4 [48000/60000 (80%)]\tLoss: 0.021366\n",
      "Train Epoch: 4 [48640/60000 (81%)]\tLoss: 0.198268\n",
      "Train Epoch: 4 [49280/60000 (82%)]\tLoss: 0.032208\n",
      "Train Epoch: 4 [49920/60000 (83%)]\tLoss: 0.032490\n",
      "Train Epoch: 4 [50560/60000 (84%)]\tLoss: 0.053476\n",
      "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 0.073860\n",
      "Train Epoch: 4 [51840/60000 (86%)]\tLoss: 0.030418\n",
      "Train Epoch: 4 [52480/60000 (87%)]\tLoss: 0.028404\n",
      "Train Epoch: 4 [53120/60000 (88%)]\tLoss: 0.145813\n",
      "Train Epoch: 4 [53760/60000 (90%)]\tLoss: 0.041205\n",
      "Train Epoch: 4 [54400/60000 (91%)]\tLoss: 0.044059\n",
      "Train Epoch: 4 [55040/60000 (92%)]\tLoss: 0.073665\n",
      "Train Epoch: 4 [55680/60000 (93%)]\tLoss: 0.005842\n",
      "Train Epoch: 4 [56320/60000 (94%)]\tLoss: 0.032215\n",
      "Train Epoch: 4 [56960/60000 (95%)]\tLoss: 0.112644\n",
      "Train Epoch: 4 [57600/60000 (96%)]\tLoss: 0.035524\n",
      "Train Epoch: 4 [58240/60000 (97%)]\tLoss: 0.135709\n",
      "Train Epoch: 4 [58880/60000 (98%)]\tLoss: 0.089006\n",
      "Train Epoch: 4 [59520/60000 (99%)]\tLoss: 0.102920\n",
      "\n",
      "Test set: Average loss: 0.0734, Accuracy: 9776/10000 (98%)\n",
      "\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.151765\n",
      "Train Epoch: 5 [640/60000 (1%)]\tLoss: 0.379208\n",
      "Train Epoch: 5 [1280/60000 (2%)]\tLoss: 0.035973\n",
      "Train Epoch: 5 [1920/60000 (3%)]\tLoss: 0.157598\n",
      "Train Epoch: 5 [2560/60000 (4%)]\tLoss: 0.101444\n",
      "Train Epoch: 5 [3200/60000 (5%)]\tLoss: 0.036175\n",
      "Train Epoch: 5 [3840/60000 (6%)]\tLoss: 0.022693\n",
      "Train Epoch: 5 [4480/60000 (7%)]\tLoss: 0.039274\n",
      "Train Epoch: 5 [5120/60000 (9%)]\tLoss: 0.101170\n",
      "Train Epoch: 5 [5760/60000 (10%)]\tLoss: 0.047733\n",
      "Train Epoch: 5 [6400/60000 (11%)]\tLoss: 0.064092\n",
      "Train Epoch: 5 [7040/60000 (12%)]\tLoss: 0.141388\n",
      "Train Epoch: 5 [7680/60000 (13%)]\tLoss: 0.077620\n",
      "Train Epoch: 5 [8320/60000 (14%)]\tLoss: 0.016357\n",
      "Train Epoch: 5 [8960/60000 (15%)]\tLoss: 0.114735\n",
      "Train Epoch: 5 [9600/60000 (16%)]\tLoss: 0.038374\n",
      "Train Epoch: 5 [10240/60000 (17%)]\tLoss: 0.107069\n",
      "Train Epoch: 5 [10880/60000 (18%)]\tLoss: 0.025532\n",
      "Train Epoch: 5 [11520/60000 (19%)]\tLoss: 0.128362\n",
      "Train Epoch: 5 [12160/60000 (20%)]\tLoss: 0.028429\n",
      "Train Epoch: 5 [12800/60000 (21%)]\tLoss: 0.100492\n",
      "Train Epoch: 5 [13440/60000 (22%)]\tLoss: 0.081260\n",
      "Train Epoch: 5 [14080/60000 (23%)]\tLoss: 0.056162\n",
      "Train Epoch: 5 [14720/60000 (25%)]\tLoss: 0.024037\n",
      "Train Epoch: 5 [15360/60000 (26%)]\tLoss: 0.052479\n",
      "Train Epoch: 5 [16000/60000 (27%)]\tLoss: 0.207978\n",
      "Train Epoch: 5 [16640/60000 (28%)]\tLoss: 0.230169\n",
      "Train Epoch: 5 [17280/60000 (29%)]\tLoss: 0.210616\n",
      "Train Epoch: 5 [17920/60000 (30%)]\tLoss: 0.103674\n",
      "Train Epoch: 5 [18560/60000 (31%)]\tLoss: 0.124603\n",
      "Train Epoch: 5 [19200/60000 (32%)]\tLoss: 0.065401\n",
      "Train Epoch: 5 [19840/60000 (33%)]\tLoss: 0.044292\n",
      "Train Epoch: 5 [20480/60000 (34%)]\tLoss: 0.219615\n",
      "Train Epoch: 5 [21120/60000 (35%)]\tLoss: 0.085746\n",
      "Train Epoch: 5 [21760/60000 (36%)]\tLoss: 0.214457\n",
      "Train Epoch: 5 [22400/60000 (37%)]\tLoss: 0.041080\n",
      "Train Epoch: 5 [23040/60000 (38%)]\tLoss: 0.075877\n",
      "Train Epoch: 5 [23680/60000 (39%)]\tLoss: 0.133828\n",
      "Train Epoch: 5 [24320/60000 (41%)]\tLoss: 0.148283\n",
      "Train Epoch: 5 [24960/60000 (42%)]\tLoss: 0.088464\n",
      "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 0.034927\n",
      "Train Epoch: 5 [26240/60000 (44%)]\tLoss: 0.177917\n",
      "Train Epoch: 5 [26880/60000 (45%)]\tLoss: 0.092450\n",
      "Train Epoch: 5 [27520/60000 (46%)]\tLoss: 0.023174\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 5 [28160/60000 (47%)]\tLoss: 0.167361\n",
      "Train Epoch: 5 [28800/60000 (48%)]\tLoss: 0.203687\n",
      "Train Epoch: 5 [29440/60000 (49%)]\tLoss: 0.108243\n",
      "Train Epoch: 5 [30080/60000 (50%)]\tLoss: 0.076906\n",
      "Train Epoch: 5 [30720/60000 (51%)]\tLoss: 0.041682\n",
      "Train Epoch: 5 [31360/60000 (52%)]\tLoss: 0.196346\n",
      "Train Epoch: 5 [32000/60000 (53%)]\tLoss: 0.040728\n",
      "Train Epoch: 5 [32640/60000 (54%)]\tLoss: 0.095148\n",
      "Train Epoch: 5 [33280/60000 (55%)]\tLoss: 0.087720\n",
      "Train Epoch: 5 [33920/60000 (57%)]\tLoss: 0.101088\n",
      "Train Epoch: 5 [34560/60000 (58%)]\tLoss: 0.044703\n",
      "Train Epoch: 5 [35200/60000 (59%)]\tLoss: 0.118359\n",
      "Train Epoch: 5 [35840/60000 (60%)]\tLoss: 0.016017\n",
      "Train Epoch: 5 [36480/60000 (61%)]\tLoss: 0.071791\n",
      "Train Epoch: 5 [37120/60000 (62%)]\tLoss: 0.022322\n",
      "Train Epoch: 5 [37760/60000 (63%)]\tLoss: 0.050030\n",
      "Train Epoch: 5 [38400/60000 (64%)]\tLoss: 0.085425\n",
      "Train Epoch: 5 [39040/60000 (65%)]\tLoss: 0.081714\n",
      "Train Epoch: 5 [39680/60000 (66%)]\tLoss: 0.047795\n",
      "Train Epoch: 5 [40320/60000 (67%)]\tLoss: 0.119455\n",
      "Train Epoch: 5 [40960/60000 (68%)]\tLoss: 0.026779\n",
      "Train Epoch: 5 [41600/60000 (69%)]\tLoss: 0.065802\n",
      "Train Epoch: 5 [42240/60000 (70%)]\tLoss: 0.024375\n",
      "Train Epoch: 5 [42880/60000 (71%)]\tLoss: 0.031036\n",
      "Train Epoch: 5 [43520/60000 (72%)]\tLoss: 0.063860\n",
      "Train Epoch: 5 [44160/60000 (74%)]\tLoss: 0.022569\n",
      "Train Epoch: 5 [44800/60000 (75%)]\tLoss: 0.100814\n",
      "Train Epoch: 5 [45440/60000 (76%)]\tLoss: 0.068308\n",
      "Train Epoch: 5 [46080/60000 (77%)]\tLoss: 0.257785\n",
      "Train Epoch: 5 [46720/60000 (78%)]\tLoss: 0.042734\n",
      "Train Epoch: 5 [47360/60000 (79%)]\tLoss: 0.022858\n",
      "Train Epoch: 5 [48000/60000 (80%)]\tLoss: 0.067182\n",
      "Train Epoch: 5 [48640/60000 (81%)]\tLoss: 0.016923\n",
      "Train Epoch: 5 [49280/60000 (82%)]\tLoss: 0.082443\n",
      "Train Epoch: 5 [49920/60000 (83%)]\tLoss: 0.147519\n",
      "Train Epoch: 5 [50560/60000 (84%)]\tLoss: 0.053398\n",
      "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 0.213511\n",
      "Train Epoch: 5 [51840/60000 (86%)]\tLoss: 0.057599\n",
      "Train Epoch: 5 [52480/60000 (87%)]\tLoss: 0.086462\n",
      "Train Epoch: 5 [53120/60000 (88%)]\tLoss: 0.014954\n",
      "Train Epoch: 5 [53760/60000 (90%)]\tLoss: 0.026115\n",
      "Train Epoch: 5 [54400/60000 (91%)]\tLoss: 0.014208\n",
      "Train Epoch: 5 [55040/60000 (92%)]\tLoss: 0.022985\n",
      "Train Epoch: 5 [55680/60000 (93%)]\tLoss: 0.064383\n",
      "Train Epoch: 5 [56320/60000 (94%)]\tLoss: 0.027171\n",
      "Train Epoch: 5 [56960/60000 (95%)]\tLoss: 0.032260\n",
      "Train Epoch: 5 [57600/60000 (96%)]\tLoss: 0.031739\n",
      "Train Epoch: 5 [58240/60000 (97%)]\tLoss: 0.040100\n",
      "Train Epoch: 5 [58880/60000 (98%)]\tLoss: 0.052416\n",
      "Train Epoch: 5 [59520/60000 (99%)]\tLoss: 0.064581\n",
      "\n",
      "Test set: Average loss: 0.0674, Accuracy: 9790/10000 (98%)\n",
      "\n",
      "Train Epoch: 6 [0/60000 (0%)]\tLoss: 0.026532\n",
      "Train Epoch: 6 [640/60000 (1%)]\tLoss: 0.077566\n",
      "Train Epoch: 6 [1280/60000 (2%)]\tLoss: 0.043150\n",
      "Train Epoch: 6 [1920/60000 (3%)]\tLoss: 0.069719\n",
      "Train Epoch: 6 [2560/60000 (4%)]\tLoss: 0.050837\n",
      "Train Epoch: 6 [3200/60000 (5%)]\tLoss: 0.103122\n",
      "Train Epoch: 6 [3840/60000 (6%)]\tLoss: 0.015247\n",
      "Train Epoch: 6 [4480/60000 (7%)]\tLoss: 0.021447\n",
      "Train Epoch: 6 [5120/60000 (9%)]\tLoss: 0.025426\n",
      "Train Epoch: 6 [5760/60000 (10%)]\tLoss: 0.092639\n",
      "Train Epoch: 6 [6400/60000 (11%)]\tLoss: 0.031295\n",
      "Train Epoch: 6 [7040/60000 (12%)]\tLoss: 0.167109\n",
      "Train Epoch: 6 [7680/60000 (13%)]\tLoss: 0.030392\n",
      "Train Epoch: 6 [8320/60000 (14%)]\tLoss: 0.038526\n",
      "Train Epoch: 6 [8960/60000 (15%)]\tLoss: 0.034159\n",
      "Train Epoch: 6 [9600/60000 (16%)]\tLoss: 0.102418\n",
      "Train Epoch: 6 [10240/60000 (17%)]\tLoss: 0.061811\n",
      "Train Epoch: 6 [10880/60000 (18%)]\tLoss: 0.070037\n",
      "Train Epoch: 6 [11520/60000 (19%)]\tLoss: 0.118859\n",
      "Train Epoch: 6 [12160/60000 (20%)]\tLoss: 0.042504\n",
      "Train Epoch: 6 [12800/60000 (21%)]\tLoss: 0.035894\n",
      "Train Epoch: 6 [13440/60000 (22%)]\tLoss: 0.049121\n",
      "Train Epoch: 6 [14080/60000 (23%)]\tLoss: 0.077764\n",
      "Train Epoch: 6 [14720/60000 (25%)]\tLoss: 0.114851\n",
      "Train Epoch: 6 [15360/60000 (26%)]\tLoss: 0.089252\n",
      "Train Epoch: 6 [16000/60000 (27%)]\tLoss: 0.203853\n",
      "Train Epoch: 6 [16640/60000 (28%)]\tLoss: 0.108703\n",
      "Train Epoch: 6 [17280/60000 (29%)]\tLoss: 0.075007\n",
      "Train Epoch: 6 [17920/60000 (30%)]\tLoss: 0.031859\n",
      "Train Epoch: 6 [18560/60000 (31%)]\tLoss: 0.034216\n",
      "Train Epoch: 6 [19200/60000 (32%)]\tLoss: 0.056121\n",
      "Train Epoch: 6 [19840/60000 (33%)]\tLoss: 0.106049\n",
      "Train Epoch: 6 [20480/60000 (34%)]\tLoss: 0.031815\n",
      "Train Epoch: 6 [21120/60000 (35%)]\tLoss: 0.044102\n",
      "Train Epoch: 6 [21760/60000 (36%)]\tLoss: 0.122111\n",
      "Train Epoch: 6 [22400/60000 (37%)]\tLoss: 0.068673\n",
      "Train Epoch: 6 [23040/60000 (38%)]\tLoss: 0.080942\n",
      "Train Epoch: 6 [23680/60000 (39%)]\tLoss: 0.023808\n",
      "Train Epoch: 6 [24320/60000 (41%)]\tLoss: 0.055465\n",
      "Train Epoch: 6 [24960/60000 (42%)]\tLoss: 0.035991\n",
      "Train Epoch: 6 [25600/60000 (43%)]\tLoss: 0.046888\n",
      "Train Epoch: 6 [26240/60000 (44%)]\tLoss: 0.091896\n",
      "Train Epoch: 6 [26880/60000 (45%)]\tLoss: 0.053614\n",
      "Train Epoch: 6 [27520/60000 (46%)]\tLoss: 0.109693\n",
      "Train Epoch: 6 [28160/60000 (47%)]\tLoss: 0.081563\n",
      "Train Epoch: 6 [28800/60000 (48%)]\tLoss: 0.040693\n",
      "Train Epoch: 6 [29440/60000 (49%)]\tLoss: 0.148309\n",
      "Train Epoch: 6 [30080/60000 (50%)]\tLoss: 0.140518\n",
      "Train Epoch: 6 [30720/60000 (51%)]\tLoss: 0.088203\n",
      "Train Epoch: 6 [31360/60000 (52%)]\tLoss: 0.125591\n",
      "Train Epoch: 6 [32000/60000 (53%)]\tLoss: 0.046722\n",
      "Train Epoch: 6 [32640/60000 (54%)]\tLoss: 0.051442\n",
      "Train Epoch: 6 [33280/60000 (55%)]\tLoss: 0.122486\n",
      "Train Epoch: 6 [33920/60000 (57%)]\tLoss: 0.033398\n",
      "Train Epoch: 6 [34560/60000 (58%)]\tLoss: 0.066772\n",
      "Train Epoch: 6 [35200/60000 (59%)]\tLoss: 0.086835\n",
      "Train Epoch: 6 [35840/60000 (60%)]\tLoss: 0.023291\n",
      "Train Epoch: 6 [36480/60000 (61%)]\tLoss: 0.029016\n",
      "Train Epoch: 6 [37120/60000 (62%)]\tLoss: 0.134321\n",
      "Train Epoch: 6 [37760/60000 (63%)]\tLoss: 0.125866\n",
      "Train Epoch: 6 [38400/60000 (64%)]\tLoss: 0.067481\n",
      "Train Epoch: 6 [39040/60000 (65%)]\tLoss: 0.020323\n",
      "Train Epoch: 6 [39680/60000 (66%)]\tLoss: 0.027461\n",
      "Train Epoch: 6 [40320/60000 (67%)]\tLoss: 0.038143\n",
      "Train Epoch: 6 [40960/60000 (68%)]\tLoss: 0.046365\n",
      "Train Epoch: 6 [41600/60000 (69%)]\tLoss: 0.233113\n",
      "Train Epoch: 6 [42240/60000 (70%)]\tLoss: 0.067154\n",
      "Train Epoch: 6 [42880/60000 (71%)]\tLoss: 0.154919\n",
      "Train Epoch: 6 [43520/60000 (72%)]\tLoss: 0.013534\n",
      "Train Epoch: 6 [44160/60000 (74%)]\tLoss: 0.050168\n",
      "Train Epoch: 6 [44800/60000 (75%)]\tLoss: 0.204669\n",
      "Train Epoch: 6 [45440/60000 (76%)]\tLoss: 0.093483\n",
      "Train Epoch: 6 [46080/60000 (77%)]\tLoss: 0.061180\n",
      "Train Epoch: 6 [46720/60000 (78%)]\tLoss: 0.062498\n",
      "Train Epoch: 6 [47360/60000 (79%)]\tLoss: 0.087020\n",
      "Train Epoch: 6 [48000/60000 (80%)]\tLoss: 0.148047\n",
      "Train Epoch: 6 [48640/60000 (81%)]\tLoss: 0.116040\n",
      "Train Epoch: 6 [49280/60000 (82%)]\tLoss: 0.039958\n",
      "Train Epoch: 6 [49920/60000 (83%)]\tLoss: 0.024191\n",
      "Train Epoch: 6 [50560/60000 (84%)]\tLoss: 0.141601\n",
      "Train Epoch: 6 [51200/60000 (85%)]\tLoss: 0.039679\n",
      "Train Epoch: 6 [51840/60000 (86%)]\tLoss: 0.033354\n",
      "Train Epoch: 6 [52480/60000 (87%)]\tLoss: 0.060801\n",
      "Train Epoch: 6 [53120/60000 (88%)]\tLoss: 0.049668\n",
      "Train Epoch: 6 [53760/60000 (90%)]\tLoss: 0.092160\n",
      "Train Epoch: 6 [54400/60000 (91%)]\tLoss: 0.085045\n",
      "Train Epoch: 6 [55040/60000 (92%)]\tLoss: 0.153341\n",
      "Train Epoch: 6 [55680/60000 (93%)]\tLoss: 0.045078\n",
      "Train Epoch: 6 [56320/60000 (94%)]\tLoss: 0.046136\n",
      "Train Epoch: 6 [56960/60000 (95%)]\tLoss: 0.234458\n",
      "Train Epoch: 6 [57600/60000 (96%)]\tLoss: 0.074333\n",
      "Train Epoch: 6 [58240/60000 (97%)]\tLoss: 0.110938\n",
      "Train Epoch: 6 [58880/60000 (98%)]\tLoss: 0.121289\n",
      "Train Epoch: 6 [59520/60000 (99%)]\tLoss: 0.131888\n",
      "\n",
      "Test set: Average loss: 0.0637, Accuracy: 9813/10000 (98%)\n",
      "\n",
      "Train Epoch: 7 [0/60000 (0%)]\tLoss: 0.176269\n",
      "Train Epoch: 7 [640/60000 (1%)]\tLoss: 0.137348\n",
      "Train Epoch: 7 [1280/60000 (2%)]\tLoss: 0.068934\n",
      "Train Epoch: 7 [1920/60000 (3%)]\tLoss: 0.065255\n",
      "Train Epoch: 7 [2560/60000 (4%)]\tLoss: 0.074814\n",
      "Train Epoch: 7 [3200/60000 (5%)]\tLoss: 0.024843\n",
      "Train Epoch: 7 [3840/60000 (6%)]\tLoss: 0.047278\n",
      "Train Epoch: 7 [4480/60000 (7%)]\tLoss: 0.021793\n",
      "Train Epoch: 7 [5120/60000 (9%)]\tLoss: 0.097247\n",
      "Train Epoch: 7 [5760/60000 (10%)]\tLoss: 0.038104\n",
      "Train Epoch: 7 [6400/60000 (11%)]\tLoss: 0.064232\n",
      "Train Epoch: 7 [7040/60000 (12%)]\tLoss: 0.021299\n",
      "Train Epoch: 7 [7680/60000 (13%)]\tLoss: 0.015818\n",
      "Train Epoch: 7 [8320/60000 (14%)]\tLoss: 0.055286\n",
      "Train Epoch: 7 [8960/60000 (15%)]\tLoss: 0.051463\n",
      "Train Epoch: 7 [9600/60000 (16%)]\tLoss: 0.075325\n",
      "Train Epoch: 7 [10240/60000 (17%)]\tLoss: 0.077157\n",
      "Train Epoch: 7 [10880/60000 (18%)]\tLoss: 0.040870\n",
      "Train Epoch: 7 [11520/60000 (19%)]\tLoss: 0.058726\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 7 [12160/60000 (20%)]\tLoss: 0.054959\n",
      "Train Epoch: 7 [12800/60000 (21%)]\tLoss: 0.053031\n",
      "Train Epoch: 7 [13440/60000 (22%)]\tLoss: 0.055352\n",
      "Train Epoch: 7 [14080/60000 (23%)]\tLoss: 0.041357\n",
      "Train Epoch: 7 [14720/60000 (25%)]\tLoss: 0.011941\n",
      "Train Epoch: 7 [15360/60000 (26%)]\tLoss: 0.022041\n",
      "Train Epoch: 7 [16000/60000 (27%)]\tLoss: 0.011238\n",
      "Train Epoch: 7 [16640/60000 (28%)]\tLoss: 0.073066\n",
      "Train Epoch: 7 [17280/60000 (29%)]\tLoss: 0.025230\n",
      "Train Epoch: 7 [17920/60000 (30%)]\tLoss: 0.095356\n",
      "Train Epoch: 7 [18560/60000 (31%)]\tLoss: 0.191831\n",
      "Train Epoch: 7 [19200/60000 (32%)]\tLoss: 0.142769\n",
      "Train Epoch: 7 [19840/60000 (33%)]\tLoss: 0.011154\n",
      "Train Epoch: 7 [20480/60000 (34%)]\tLoss: 0.067888\n",
      "Train Epoch: 7 [21120/60000 (35%)]\tLoss: 0.083316\n",
      "Train Epoch: 7 [21760/60000 (36%)]\tLoss: 0.072813\n",
      "Train Epoch: 7 [22400/60000 (37%)]\tLoss: 0.173466\n",
      "Train Epoch: 7 [23040/60000 (38%)]\tLoss: 0.065520\n",
      "Train Epoch: 7 [23680/60000 (39%)]\tLoss: 0.016467\n",
      "Train Epoch: 7 [24320/60000 (41%)]\tLoss: 0.034409\n",
      "Train Epoch: 7 [24960/60000 (42%)]\tLoss: 0.038685\n",
      "Train Epoch: 7 [25600/60000 (43%)]\tLoss: 0.273884\n",
      "Train Epoch: 7 [26240/60000 (44%)]\tLoss: 0.066765\n",
      "Train Epoch: 7 [26880/60000 (45%)]\tLoss: 0.067949\n",
      "Train Epoch: 7 [27520/60000 (46%)]\tLoss: 0.107532\n",
      "Train Epoch: 7 [28160/60000 (47%)]\tLoss: 0.100661\n",
      "Train Epoch: 7 [28800/60000 (48%)]\tLoss: 0.010658\n",
      "Train Epoch: 7 [29440/60000 (49%)]\tLoss: 0.029181\n",
      "Train Epoch: 7 [30080/60000 (50%)]\tLoss: 0.019175\n",
      "Train Epoch: 7 [30720/60000 (51%)]\tLoss: 0.064324\n",
      "Train Epoch: 7 [31360/60000 (52%)]\tLoss: 0.009170\n",
      "Train Epoch: 7 [32000/60000 (53%)]\tLoss: 0.031295\n",
      "Train Epoch: 7 [32640/60000 (54%)]\tLoss: 0.010918\n",
      "Train Epoch: 7 [33280/60000 (55%)]\tLoss: 0.148627\n",
      "Train Epoch: 7 [33920/60000 (57%)]\tLoss: 0.070344\n",
      "Train Epoch: 7 [34560/60000 (58%)]\tLoss: 0.080366\n",
      "Train Epoch: 7 [35200/60000 (59%)]\tLoss: 0.051583\n",
      "Train Epoch: 7 [35840/60000 (60%)]\tLoss: 0.021560\n",
      "Train Epoch: 7 [36480/60000 (61%)]\tLoss: 0.045095\n",
      "Train Epoch: 7 [37120/60000 (62%)]\tLoss: 0.027289\n",
      "Train Epoch: 7 [37760/60000 (63%)]\tLoss: 0.092695\n",
      "Train Epoch: 7 [38400/60000 (64%)]\tLoss: 0.073061\n",
      "Train Epoch: 7 [39040/60000 (65%)]\tLoss: 0.043069\n",
      "Train Epoch: 7 [39680/60000 (66%)]\tLoss: 0.037328\n",
      "Train Epoch: 7 [40320/60000 (67%)]\tLoss: 0.020932\n",
      "Train Epoch: 7 [40960/60000 (68%)]\tLoss: 0.029267\n",
      "Train Epoch: 7 [41600/60000 (69%)]\tLoss: 0.094086\n",
      "Train Epoch: 7 [42240/60000 (70%)]\tLoss: 0.126662\n",
      "Train Epoch: 7 [42880/60000 (71%)]\tLoss: 0.168294\n",
      "Train Epoch: 7 [43520/60000 (72%)]\tLoss: 0.004936\n",
      "Train Epoch: 7 [44160/60000 (74%)]\tLoss: 0.212175\n",
      "Train Epoch: 7 [44800/60000 (75%)]\tLoss: 0.106079\n",
      "Train Epoch: 7 [45440/60000 (76%)]\tLoss: 0.058790\n",
      "Train Epoch: 7 [46080/60000 (77%)]\tLoss: 0.051637\n",
      "Train Epoch: 7 [46720/60000 (78%)]\tLoss: 0.081539\n",
      "Train Epoch: 7 [47360/60000 (79%)]\tLoss: 0.044484\n",
      "Train Epoch: 7 [48000/60000 (80%)]\tLoss: 0.016723\n",
      "Train Epoch: 7 [48640/60000 (81%)]\tLoss: 0.039977\n",
      "Train Epoch: 7 [49280/60000 (82%)]\tLoss: 0.069085\n",
      "Train Epoch: 7 [49920/60000 (83%)]\tLoss: 0.024210\n",
      "Train Epoch: 7 [50560/60000 (84%)]\tLoss: 0.034833\n",
      "Train Epoch: 7 [51200/60000 (85%)]\tLoss: 0.019193\n",
      "Train Epoch: 7 [51840/60000 (86%)]\tLoss: 0.036220\n",
      "Train Epoch: 7 [52480/60000 (87%)]\tLoss: 0.023075\n",
      "Train Epoch: 7 [53120/60000 (88%)]\tLoss: 0.026439\n",
      "Train Epoch: 7 [53760/60000 (90%)]\tLoss: 0.079850\n",
      "Train Epoch: 7 [54400/60000 (91%)]\tLoss: 0.091636\n",
      "Train Epoch: 7 [55040/60000 (92%)]\tLoss: 0.060736\n",
      "Train Epoch: 7 [55680/60000 (93%)]\tLoss: 0.029572\n",
      "Train Epoch: 7 [56320/60000 (94%)]\tLoss: 0.066912\n",
      "Train Epoch: 7 [56960/60000 (95%)]\tLoss: 0.020006\n",
      "Train Epoch: 7 [57600/60000 (96%)]\tLoss: 0.109181\n",
      "Train Epoch: 7 [58240/60000 (97%)]\tLoss: 0.032636\n",
      "Train Epoch: 7 [58880/60000 (98%)]\tLoss: 0.008478\n",
      "Train Epoch: 7 [59520/60000 (99%)]\tLoss: 0.205580\n",
      "\n",
      "Test set: Average loss: 0.0570, Accuracy: 9837/10000 (98%)\n",
      "\n",
      "Train Epoch: 8 [0/60000 (0%)]\tLoss: 0.051888\n",
      "Train Epoch: 8 [640/60000 (1%)]\tLoss: 0.044688\n",
      "Train Epoch: 8 [1280/60000 (2%)]\tLoss: 0.079417\n",
      "Train Epoch: 8 [1920/60000 (3%)]\tLoss: 0.108737\n",
      "Train Epoch: 8 [2560/60000 (4%)]\tLoss: 0.026783\n",
      "Train Epoch: 8 [3200/60000 (5%)]\tLoss: 0.021963\n",
      "Train Epoch: 8 [3840/60000 (6%)]\tLoss: 0.071346\n",
      "Train Epoch: 8 [4480/60000 (7%)]\tLoss: 0.013343\n",
      "Train Epoch: 8 [5120/60000 (9%)]\tLoss: 0.044731\n",
      "Train Epoch: 8 [5760/60000 (10%)]\tLoss: 0.017586\n",
      "Train Epoch: 8 [6400/60000 (11%)]\tLoss: 0.043131\n",
      "Train Epoch: 8 [7040/60000 (12%)]\tLoss: 0.148398\n",
      "Train Epoch: 8 [7680/60000 (13%)]\tLoss: 0.031111\n",
      "Train Epoch: 8 [8320/60000 (14%)]\tLoss: 0.049070\n",
      "Train Epoch: 8 [8960/60000 (15%)]\tLoss: 0.073592\n",
      "Train Epoch: 8 [9600/60000 (16%)]\tLoss: 0.048421\n",
      "Train Epoch: 8 [10240/60000 (17%)]\tLoss: 0.025911\n",
      "Train Epoch: 8 [10880/60000 (18%)]\tLoss: 0.153157\n",
      "Train Epoch: 8 [11520/60000 (19%)]\tLoss: 0.110560\n",
      "Train Epoch: 8 [12160/60000 (20%)]\tLoss: 0.007372\n",
      "Train Epoch: 8 [12800/60000 (21%)]\tLoss: 0.014166\n",
      "Train Epoch: 8 [13440/60000 (22%)]\tLoss: 0.013238\n",
      "Train Epoch: 8 [14080/60000 (23%)]\tLoss: 0.081420\n",
      "Train Epoch: 8 [14720/60000 (25%)]\tLoss: 0.024922\n",
      "Train Epoch: 8 [15360/60000 (26%)]\tLoss: 0.086203\n",
      "Train Epoch: 8 [16000/60000 (27%)]\tLoss: 0.060528\n",
      "Train Epoch: 8 [16640/60000 (28%)]\tLoss: 0.105094\n",
      "Train Epoch: 8 [17280/60000 (29%)]\tLoss: 0.168492\n",
      "Train Epoch: 8 [17920/60000 (30%)]\tLoss: 0.018854\n",
      "Train Epoch: 8 [18560/60000 (31%)]\tLoss: 0.023086\n",
      "Train Epoch: 8 [19200/60000 (32%)]\tLoss: 0.112413\n",
      "Train Epoch: 8 [19840/60000 (33%)]\tLoss: 0.008448\n",
      "Train Epoch: 8 [20480/60000 (34%)]\tLoss: 0.034627\n",
      "Train Epoch: 8 [21120/60000 (35%)]\tLoss: 0.044450\n",
      "Train Epoch: 8 [21760/60000 (36%)]\tLoss: 0.026077\n",
      "Train Epoch: 8 [22400/60000 (37%)]\tLoss: 0.104715\n",
      "Train Epoch: 8 [23040/60000 (38%)]\tLoss: 0.094833\n",
      "Train Epoch: 8 [23680/60000 (39%)]\tLoss: 0.043792\n",
      "Train Epoch: 8 [24320/60000 (41%)]\tLoss: 0.132120\n",
      "Train Epoch: 8 [24960/60000 (42%)]\tLoss: 0.119669\n",
      "Train Epoch: 8 [25600/60000 (43%)]\tLoss: 0.056822\n",
      "Train Epoch: 8 [26240/60000 (44%)]\tLoss: 0.074454\n",
      "Train Epoch: 8 [26880/60000 (45%)]\tLoss: 0.025321\n",
      "Train Epoch: 8 [27520/60000 (46%)]\tLoss: 0.091545\n",
      "Train Epoch: 8 [28160/60000 (47%)]\tLoss: 0.011061\n",
      "Train Epoch: 8 [28800/60000 (48%)]\tLoss: 0.065979\n",
      "Train Epoch: 8 [29440/60000 (49%)]\tLoss: 0.029971\n",
      "Train Epoch: 8 [30080/60000 (50%)]\tLoss: 0.043168\n",
      "Train Epoch: 8 [30720/60000 (51%)]\tLoss: 0.010319\n",
      "Train Epoch: 8 [31360/60000 (52%)]\tLoss: 0.104777\n",
      "Train Epoch: 8 [32000/60000 (53%)]\tLoss: 0.012208\n",
      "Train Epoch: 8 [32640/60000 (54%)]\tLoss: 0.038200\n",
      "Train Epoch: 8 [33280/60000 (55%)]\tLoss: 0.172944\n",
      "Train Epoch: 8 [33920/60000 (57%)]\tLoss: 0.024774\n",
      "Train Epoch: 8 [34560/60000 (58%)]\tLoss: 0.114237\n",
      "Train Epoch: 8 [35200/60000 (59%)]\tLoss: 0.049965\n",
      "Train Epoch: 8 [35840/60000 (60%)]\tLoss: 0.009871\n",
      "Train Epoch: 8 [36480/60000 (61%)]\tLoss: 0.067605\n",
      "Train Epoch: 8 [37120/60000 (62%)]\tLoss: 0.018113\n",
      "Train Epoch: 8 [37760/60000 (63%)]\tLoss: 0.015634\n",
      "Train Epoch: 8 [38400/60000 (64%)]\tLoss: 0.096913\n",
      "Train Epoch: 8 [39040/60000 (65%)]\tLoss: 0.019540\n",
      "Train Epoch: 8 [39680/60000 (66%)]\tLoss: 0.061879\n",
      "Train Epoch: 8 [40320/60000 (67%)]\tLoss: 0.078016\n",
      "Train Epoch: 8 [40960/60000 (68%)]\tLoss: 0.082933\n",
      "Train Epoch: 8 [41600/60000 (69%)]\tLoss: 0.006174\n",
      "Train Epoch: 8 [42240/60000 (70%)]\tLoss: 0.043082\n",
      "Train Epoch: 8 [42880/60000 (71%)]\tLoss: 0.025422\n",
      "Train Epoch: 8 [43520/60000 (72%)]\tLoss: 0.047004\n",
      "Train Epoch: 8 [44160/60000 (74%)]\tLoss: 0.035759\n",
      "Train Epoch: 8 [44800/60000 (75%)]\tLoss: 0.045333\n",
      "Train Epoch: 8 [45440/60000 (76%)]\tLoss: 0.027234\n",
      "Train Epoch: 8 [46080/60000 (77%)]\tLoss: 0.096650\n",
      "Train Epoch: 8 [46720/60000 (78%)]\tLoss: 0.129106\n",
      "Train Epoch: 8 [47360/60000 (79%)]\tLoss: 0.027844\n",
      "Train Epoch: 8 [48000/60000 (80%)]\tLoss: 0.081023\n",
      "Train Epoch: 8 [48640/60000 (81%)]\tLoss: 0.022276\n",
      "Train Epoch: 8 [49280/60000 (82%)]\tLoss: 0.029792\n",
      "Train Epoch: 8 [49920/60000 (83%)]\tLoss: 0.063480\n",
      "Train Epoch: 8 [50560/60000 (84%)]\tLoss: 0.123550\n",
      "Train Epoch: 8 [51200/60000 (85%)]\tLoss: 0.052283\n",
      "Train Epoch: 8 [51840/60000 (86%)]\tLoss: 0.057099\n",
      "Train Epoch: 8 [52480/60000 (87%)]\tLoss: 0.030903\n",
      "Train Epoch: 8 [53120/60000 (88%)]\tLoss: 0.023094\n",
      "Train Epoch: 8 [53760/60000 (90%)]\tLoss: 0.012096\n",
      "Train Epoch: 8 [54400/60000 (91%)]\tLoss: 0.035232\n",
      "Train Epoch: 8 [55040/60000 (92%)]\tLoss: 0.033022\n",
      "Train Epoch: 8 [55680/60000 (93%)]\tLoss: 0.041908\n",
      "Train Epoch: 8 [56320/60000 (94%)]\tLoss: 0.015650\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 8 [56960/60000 (95%)]\tLoss: 0.022572\n",
      "Train Epoch: 8 [57600/60000 (96%)]\tLoss: 0.026741\n",
      "Train Epoch: 8 [58240/60000 (97%)]\tLoss: 0.030269\n",
      "Train Epoch: 8 [58880/60000 (98%)]\tLoss: 0.041570\n",
      "Train Epoch: 8 [59520/60000 (99%)]\tLoss: 0.073421\n",
      "\n",
      "Test set: Average loss: 0.0661, Accuracy: 9795/10000 (98%)\n",
      "\n",
      "Train Epoch: 9 [0/60000 (0%)]\tLoss: 0.004104\n",
      "Train Epoch: 9 [640/60000 (1%)]\tLoss: 0.077803\n",
      "Train Epoch: 9 [1280/60000 (2%)]\tLoss: 0.036079\n",
      "Train Epoch: 9 [1920/60000 (3%)]\tLoss: 0.099306\n",
      "Train Epoch: 9 [2560/60000 (4%)]\tLoss: 0.041084\n",
      "Train Epoch: 9 [3200/60000 (5%)]\tLoss: 0.021708\n",
      "Train Epoch: 9 [3840/60000 (6%)]\tLoss: 0.087443\n",
      "Train Epoch: 9 [4480/60000 (7%)]\tLoss: 0.043010\n",
      "Train Epoch: 9 [5120/60000 (9%)]\tLoss: 0.105656\n",
      "Train Epoch: 9 [5760/60000 (10%)]\tLoss: 0.138257\n",
      "Train Epoch: 9 [6400/60000 (11%)]\tLoss: 0.121129\n",
      "Train Epoch: 9 [7040/60000 (12%)]\tLoss: 0.073876\n",
      "Train Epoch: 9 [7680/60000 (13%)]\tLoss: 0.116029\n",
      "Train Epoch: 9 [8320/60000 (14%)]\tLoss: 0.020586\n",
      "Train Epoch: 9 [8960/60000 (15%)]\tLoss: 0.136221\n",
      "Train Epoch: 9 [9600/60000 (16%)]\tLoss: 0.031705\n",
      "Train Epoch: 9 [10240/60000 (17%)]\tLoss: 0.092159\n",
      "Train Epoch: 9 [10880/60000 (18%)]\tLoss: 0.076674\n",
      "Train Epoch: 9 [11520/60000 (19%)]\tLoss: 0.017246\n",
      "Train Epoch: 9 [12160/60000 (20%)]\tLoss: 0.053758\n",
      "Train Epoch: 9 [12800/60000 (21%)]\tLoss: 0.084437\n",
      "Train Epoch: 9 [13440/60000 (22%)]\tLoss: 0.035924\n",
      "Train Epoch: 9 [14080/60000 (23%)]\tLoss: 0.117162\n",
      "Train Epoch: 9 [14720/60000 (25%)]\tLoss: 0.053191\n",
      "Train Epoch: 9 [15360/60000 (26%)]\tLoss: 0.060064\n",
      "Train Epoch: 9 [16000/60000 (27%)]\tLoss: 0.034961\n",
      "Train Epoch: 9 [16640/60000 (28%)]\tLoss: 0.081999\n",
      "Train Epoch: 9 [17280/60000 (29%)]\tLoss: 0.078830\n",
      "Train Epoch: 9 [17920/60000 (30%)]\tLoss: 0.047145\n",
      "Train Epoch: 9 [18560/60000 (31%)]\tLoss: 0.088168\n",
      "Train Epoch: 9 [19200/60000 (32%)]\tLoss: 0.031746\n",
      "Train Epoch: 9 [19840/60000 (33%)]\tLoss: 0.099758\n",
      "Train Epoch: 9 [20480/60000 (34%)]\tLoss: 0.006166\n",
      "Train Epoch: 9 [21120/60000 (35%)]\tLoss: 0.010164\n",
      "Train Epoch: 9 [21760/60000 (36%)]\tLoss: 0.022878\n",
      "Train Epoch: 9 [22400/60000 (37%)]\tLoss: 0.056057\n",
      "Train Epoch: 9 [23040/60000 (38%)]\tLoss: 0.033006\n",
      "Train Epoch: 9 [23680/60000 (39%)]\tLoss: 0.013419\n",
      "Train Epoch: 9 [24320/60000 (41%)]\tLoss: 0.052940\n",
      "Train Epoch: 9 [24960/60000 (42%)]\tLoss: 0.046405\n",
      "Train Epoch: 9 [25600/60000 (43%)]\tLoss: 0.043646\n",
      "Train Epoch: 9 [26240/60000 (44%)]\tLoss: 0.031623\n",
      "Train Epoch: 9 [26880/60000 (45%)]\tLoss: 0.107557\n",
      "Train Epoch: 9 [27520/60000 (46%)]\tLoss: 0.146916\n",
      "Train Epoch: 9 [28160/60000 (47%)]\tLoss: 0.005476\n",
      "Train Epoch: 9 [28800/60000 (48%)]\tLoss: 0.087449\n",
      "Train Epoch: 9 [29440/60000 (49%)]\tLoss: 0.030786\n",
      "Train Epoch: 9 [30080/60000 (50%)]\tLoss: 0.005617\n",
      "Train Epoch: 9 [30720/60000 (51%)]\tLoss: 0.011197\n",
      "Train Epoch: 9 [31360/60000 (52%)]\tLoss: 0.144557\n",
      "Train Epoch: 9 [32000/60000 (53%)]\tLoss: 0.080998\n",
      "Train Epoch: 9 [32640/60000 (54%)]\tLoss: 0.078311\n",
      "Train Epoch: 9 [33280/60000 (55%)]\tLoss: 0.015798\n",
      "Train Epoch: 9 [33920/60000 (57%)]\tLoss: 0.017539\n",
      "Train Epoch: 9 [34560/60000 (58%)]\tLoss: 0.046703\n",
      "Train Epoch: 9 [35200/60000 (59%)]\tLoss: 0.093090\n",
      "Train Epoch: 9 [35840/60000 (60%)]\tLoss: 0.013055\n",
      "Train Epoch: 9 [36480/60000 (61%)]\tLoss: 0.023124\n",
      "Train Epoch: 9 [37120/60000 (62%)]\tLoss: 0.045420\n",
      "Train Epoch: 9 [37760/60000 (63%)]\tLoss: 0.087021\n",
      "Train Epoch: 9 [38400/60000 (64%)]\tLoss: 0.108326\n",
      "Train Epoch: 9 [39040/60000 (65%)]\tLoss: 0.119310\n",
      "Train Epoch: 9 [39680/60000 (66%)]\tLoss: 0.158206\n",
      "Train Epoch: 9 [40320/60000 (67%)]\tLoss: 0.091032\n",
      "Train Epoch: 9 [40960/60000 (68%)]\tLoss: 0.079991\n",
      "Train Epoch: 9 [41600/60000 (69%)]\tLoss: 0.032587\n",
      "Train Epoch: 9 [42240/60000 (70%)]\tLoss: 0.098595\n",
      "Train Epoch: 9 [42880/60000 (71%)]\tLoss: 0.068001\n",
      "Train Epoch: 9 [43520/60000 (72%)]\tLoss: 0.011630\n",
      "Train Epoch: 9 [44160/60000 (74%)]\tLoss: 0.016352\n",
      "Train Epoch: 9 [44800/60000 (75%)]\tLoss: 0.066524\n",
      "Train Epoch: 9 [45440/60000 (76%)]\tLoss: 0.037213\n",
      "Train Epoch: 9 [46080/60000 (77%)]\tLoss: 0.075597\n",
      "Train Epoch: 9 [46720/60000 (78%)]\tLoss: 0.048652\n",
      "Train Epoch: 9 [47360/60000 (79%)]\tLoss: 0.017811\n",
      "Train Epoch: 9 [48000/60000 (80%)]\tLoss: 0.156465\n",
      "Train Epoch: 9 [48640/60000 (81%)]\tLoss: 0.028355\n",
      "Train Epoch: 9 [49280/60000 (82%)]\tLoss: 0.060980\n",
      "Train Epoch: 9 [49920/60000 (83%)]\tLoss: 0.070580\n",
      "Train Epoch: 9 [50560/60000 (84%)]\tLoss: 0.034117\n",
      "Train Epoch: 9 [51200/60000 (85%)]\tLoss: 0.111745\n",
      "Train Epoch: 9 [51840/60000 (86%)]\tLoss: 0.198302\n",
      "Train Epoch: 9 [52480/60000 (87%)]\tLoss: 0.172200\n",
      "Train Epoch: 9 [53120/60000 (88%)]\tLoss: 0.013509\n",
      "Train Epoch: 9 [53760/60000 (90%)]\tLoss: 0.066568\n",
      "Train Epoch: 9 [54400/60000 (91%)]\tLoss: 0.027130\n",
      "Train Epoch: 9 [55040/60000 (92%)]\tLoss: 0.020197\n",
      "Train Epoch: 9 [55680/60000 (93%)]\tLoss: 0.163423\n",
      "Train Epoch: 9 [56320/60000 (94%)]\tLoss: 0.024456\n",
      "Train Epoch: 9 [56960/60000 (95%)]\tLoss: 0.037692\n",
      "Train Epoch: 9 [57600/60000 (96%)]\tLoss: 0.030168\n",
      "Train Epoch: 9 [58240/60000 (97%)]\tLoss: 0.027669\n",
      "Train Epoch: 9 [58880/60000 (98%)]\tLoss: 0.103347\n",
      "Train Epoch: 9 [59520/60000 (99%)]\tLoss: 0.086568\n",
      "\n",
      "Test set: Average loss: 0.0534, Accuracy: 9849/10000 (98%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, 10):\n",
    "    train(epoch)\n",
    "    test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
