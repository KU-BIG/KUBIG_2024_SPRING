{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# < 3주차 과제 1 : MLP 마음대로 다뤄보기>\n",
        "- dataset을 임의로 선정해서 직접 분석 해보기(제공한 코드를 활용해서 해보기)\n",
        "- activation functions 중 relu사용시 함수 직접 정의\n",
        "- lr, optimizer 등 바꿔보기\n",
        "- hidden layer/neuron 수를 바꾸기\n",
        "- 전처리도 추가\n",
        "- 모든 시도를 올려주세요!\n",
        "- 제일 높은 acc를 보인 시도를 명시해주세요!\n"
      ],
      "metadata": {
        "id": "sgAYo4nrw2F4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fX437IL6qbI-"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn, optim\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from torch.utils.data import  TensorDataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 아래 데이터셋 중 원하는 데이터셋 하나를 선택하여, 코드 과제 진행하기!\n",
        "- 1) load_digits() <br>\n",
        "- 2) load_wine()"
      ],
      "metadata": {
        "id": "oxkFzBDNmWNk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_wine\n",
        "\n",
        "# 데이터셋 종류 :\n",
        "data = load_wine()"
      ],
      "metadata": {
        "id": "FywYbfsKtjcR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 데이터셋의 구조 확인\n",
        "print(\"Feature names:\", data.feature_names)\n",
        "print(\"Target names:\", data.target_names)\n",
        "print(\"Data shape:\", data.data.shape)\n",
        "print(\"Target shape:\", data.target.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q45Pno6iLYxD",
        "outputId": "5200820b-d06e-496c-ef72-bdc2e288ac87"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Feature names: ['alcohol', 'malic_acid', 'ash', 'alcalinity_of_ash', 'magnesium', 'total_phenols', 'flavanoids', 'nonflavanoid_phenols', 'proanthocyanins', 'color_intensity', 'hue', 'od280/od315_of_diluted_wines', 'proline']\n",
            "Target names: ['class_0' 'class_1' 'class_2']\n",
            "Data shape: (178, 13)\n",
            "Target shape: (178,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input = data.data\n",
        "output = data.target"
      ],
      "metadata": {
        "id": "C2P0hqZ9yBGm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "torch.manual_seed(777)   # 같은 조건에서 항상 같은 난수 생성\n",
        "if device == \"cuda\":\n",
        "  torch.cuda.manual_seed_all(777)"
      ],
      "metadata": {
        "id": "SggpQfSPt85C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train, x_test, y_train, y_test = train_test_split(input, output, test_size = 0.3, random_state = 42, stratify= data.target, shuffle = True)\n",
        "\n",
        "x_train = torch.FloatTensor(x_train).to(device)\n",
        "y_train = torch.LongTensor(y_train).to(device)\n",
        "x_test = torch.FloatTensor(x_test)\n",
        "y_test = torch.LongTensor(y_test)\n",
        "\n",
        "# 데이터를 tensor로 바꿔주고 gpu 연산이 가능해지도록 gpu에 옮김\n",
        "# label 값을 왜 long 에 옮겨놓는가? loss function이 다르기 때문"
      ],
      "metadata": {
        "id": "bLMzf-2ntYeX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(x_train[0])\n",
        "print(y_train[0])\n",
        "\n",
        "#input 30개 (속성이 30개)\n",
        "#y의 class는 2개 (양성과 음성)\n",
        "\n",
        "# wine 데이터의 경우 속성 13개, class 3개"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "umEdiTZkrVqS",
        "outputId": "750b2bd0-f771-4692-839e-cd7f168446a1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([1.3750e+01, 1.7300e+00, 2.4100e+00, 1.6000e+01, 8.9000e+01, 2.6000e+00,\n",
            "        2.7600e+00, 2.9000e-01, 1.8100e+00, 5.6000e+00, 1.1500e+00, 2.9000e+00,\n",
            "        1.3200e+03], device='cuda:0')\n",
            "tensor(0, device='cuda:0')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "파이토치에서는 데이터셋을 좀 더 쉽게 다룰 수 있도록 유용한 도구로서 torch.utils.data.Dataset과 torch.utils.data.DataLoader를 제공합니다. 이를 사용하면 미니 배치 학습, 데이터 셔플(shuffle), 병렬 처리까지 간단히 수행할 수 있습니다. 기본적인 사용 방법은 Dataset을 정의하고, 이를 DataLoader에 전달하는 것\n",
        "- init : class 에서 객체가 생성되면 바로 실행되는 함수\n",
        "- len : observation 수를 정의하는 함수\n",
        "- getitem : iteration 마다 해당하는 데이터를 돌려주는 함수"
      ],
      "metadata": {
        "id": "combmxzmYFyn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset\n",
        "class CustomDataset(Dataset):\n",
        "  def __init__(self):\n",
        "    self.x_data = x_train\n",
        "    self.y_data = [[y] for y in y_train]\n",
        "#  데이터셋의 전처리를 해주는 부분\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.x_data)\n",
        "#  데이터셋의 길이. 즉, 총 샘플의 수를 적어주는 부분\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    x = torch.FloatTensor(self.x_data[idx]).to(device)\n",
        "    y = torch.LongTensor(self.y_data[idx]).to(device)\n",
        "#  데이터셋에서 특정 1개의 샘플을 가져오는 함수\n",
        "\n",
        "    return x,y"
      ],
      "metadata": {
        "id": "y38TlgXoqV5Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 8\n",
        "\n",
        "dataset = CustomDataset()\n",
        "dataloader = DataLoader(dataset, batch_size=batch_size)"
      ],
      "metadata": {
        "id": "x8VHwnuFqino"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# input_layer와 hidden_layer의 노드 개수를 어떻게 바꿔줘야할까?\n",
        "# hidden layer/neuron 수를 바꾸기\n",
        "\n",
        "# wine 데이터의 경우, 178 * 13 feature이므로 최종적으로 13 * 3으로...?\n",
        "\n",
        "model = nn.Sequential(\n",
        "          nn.Linear(13,398, bias=True),\n",
        "          nn.Sigmoid(),\n",
        "          nn.Linear(398,15, bias=True),\n",
        "          nn.Sigmoid(),\n",
        "          nn.Linear(15,3, bias=True),\n",
        "          nn.Softmax()\n",
        "          ).to(device)"
      ],
      "metadata": {
        "id": "C6V7a4tyq6Jc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "class로 구현 가능\n",
        "- init : 초기 생성 함수\n",
        "- foward : 순전파(입력값 => 예측값 의 과정)"
      ],
      "metadata": {
        "id": "07uV8RY7Yr_5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Model(torch.nn.Module):\n",
        "\n",
        "  def __init__(self):\n",
        "    super(Model, self).__init__()\n",
        "    self.layer1 = nn.Sequential(\n",
        "          nn.Linear(13,398, bias=True), # input_layer = 30(13으로 수정), hidden_layer1 = 398\n",
        "          nn.Sigmoid(),\n",
        "        nn.BatchNorm1d(398)\n",
        "    )\n",
        "  # activation function 이용\n",
        "  #   nn.ReLU()\n",
        "  #   nn.tanH()\n",
        "  #   https://pytorch.org/docs/stable/nn.html 그 외에도 여기서 확인 가능함\n",
        "  #   파라미터가 필요하지 않다는 것이 특징\n",
        "\n",
        "  # batch normazliation 1d, 파라미터 값으로 vector의 길이를 전해줌\n",
        "  # 추후에 이미지를 다루게 된다면, 그때는 batch normalization 2d를 이용하게 됨\n",
        "  # 그때는 파라미터 값으로 채널, 가로, 세로 길이를 전달해주게 됨\n",
        "\n",
        "    self.layer2 = nn.Sequential(\n",
        "          nn.Linear(398,15, bias=True), # hidden_layer1 = 398, hidden_layer2 = 15\n",
        "        nn.Sigmoid()\n",
        "    )\n",
        "    self.layer3 = nn.Sequential(\n",
        "          nn.Linear(15,10, bias=True), # hidden_layer1 = 15, hidden_layer2 = 10\n",
        "        nn.Sigmoid()\n",
        "    )\n",
        "    self.layer4 = nn.Sequential(\n",
        "        nn.Linear(10,3, bias=True), # hidden_layer3 = 10, output_layer = 5\n",
        "        nn.Softmax()\n",
        "    )\n",
        "\n",
        "  def forward(self,x):\n",
        "    output = self.layer1(x)\n",
        "    output = self.layer2(output)\n",
        "    output = self.layer3(output)\n",
        "    output = self.layer4(output)\n",
        "    return output"
      ],
      "metadata": {
        "id": "a0zLstbMqxEZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def init_weights(layer):\n",
        "    if isinstance(layer, nn.Linear):\n",
        "        torch.nn.init.xavier_uniform(layer.weight)\n",
        "        layer.bias.data.fill_(0.01)\n",
        "\n",
        "        #xavier사용\n",
        "        # Layer의 weight를 어떤 분포를 가지도록 초기화시켜줌+global minimum찾기 위해서"
      ],
      "metadata": {
        "id": "kqcqqkECrSGK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "가중치 초기화. W ~ +- sqrt((6/(n_in + n_out))) (Xavier Uniform의 경우)\n",
        "\n",
        "Lecun, Xavier, He 등의 방법이 있음.\n"
      ],
      "metadata": {
        "id": "zYLO54I7F-eK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = Model().to(device)\n",
        "model.apply(init_weights)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oMDUBFg6rUpw",
        "outputId": "4339a607-e0a1-4f3a-e321-4376495a6569"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-12-6196a47a4462>:3: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "  torch.nn.init.xavier_uniform(layer.weight)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Model(\n",
              "  (layer1): Sequential(\n",
              "    (0): Linear(in_features=13, out_features=398, bias=True)\n",
              "    (1): Sigmoid()\n",
              "    (2): BatchNorm1d(398, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  )\n",
              "  (layer2): Sequential(\n",
              "    (0): Linear(in_features=398, out_features=15, bias=True)\n",
              "    (1): Sigmoid()\n",
              "  )\n",
              "  (layer3): Sequential(\n",
              "    (0): Linear(in_features=15, out_features=10, bias=True)\n",
              "    (1): Sigmoid()\n",
              "  )\n",
              "  (layer4): Sequential(\n",
              "    (0): Linear(in_features=10, out_features=3, bias=True)\n",
              "    (1): Softmax(dim=None)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OwZt5CetrYFb",
        "outputId": "030893e5-d6ce-4ed7-c044-9efcb833e10d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model(\n",
            "  (layer1): Sequential(\n",
            "    (0): Linear(in_features=13, out_features=398, bias=True)\n",
            "    (1): Sigmoid()\n",
            "    (2): BatchNorm1d(398, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  )\n",
            "  (layer2): Sequential(\n",
            "    (0): Linear(in_features=398, out_features=15, bias=True)\n",
            "    (1): Sigmoid()\n",
            "  )\n",
            "  (layer3): Sequential(\n",
            "    (0): Linear(in_features=15, out_features=10, bias=True)\n",
            "    (1): Sigmoid()\n",
            "  )\n",
            "  (layer4): Sequential(\n",
            "    (0): Linear(in_features=10, out_features=3, bias=True)\n",
            "    (1): Softmax(dim=None)\n",
            "  )\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss_fn  = nn.CrossEntropyLoss().to(device)\n",
        "\n",
        "# 여러가지 optimizer 시도해보기\n",
        "# lr 바꿔보기\n",
        "\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr= 0.01)\n",
        "\n",
        "#optimizer = optim.SGD(model.parameters(), lr=0.1, momentum=0.9)\n",
        "\n",
        "# https://pytorch.org/docs/stable/optim.html#module-torch.optim 페이지 참조\n",
        "# optimizer = optim.SGD(model.parameters(), lr=0.1, momentum=0.9)\n",
        "# sgd 등등\n",
        "\n",
        "# 세션 때 나온 옵티마이저 종류: Adagrad, RMSProp, AdaDelta, NAG, Adam 등등\n",
        "\n",
        "# https://pytorch.org/docs/stable/optim.html#module-torch.optim 페이지 참조 (스터디 이후 참고했던 부분)\n",
        "# Adagrad일때 결과가 제일 좋았음!\n",
        "\n",
        "#optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
        "#optimizer = optim.Adamax(model.parameters(), lr= 0.01)\n",
        "#optimizer = optim.Adam(model.parameters(), lr= 0.01)\n",
        "#optimizer = optim.AdamW(model.parameters(), lr= 0.01)\n",
        "#optimizer = optim.NAdam(model.parameters(), lr= 0.01)\n",
        "\n",
        "# Adagrad(adaptive gradient) adapts different learning rates for every parameter and time step:\n",
        "# AdaGrad는 큰 기울기를 가져 학습이 많이 된 변수는 학습률을 감소시킵니다. 학습이 적게 된 다른 변수는 잘 학습되도록 학습률을 높게 설정하여 조절할 수 있다는 장점이 있습니다.\n",
        "\n",
        "# Rmsprop : Adagrad의 g_t가 무한히 커지는 것을 방지, 학습률이 0으로 수렴하는 문제 해결\n",
        "\n",
        "# Adam = Rmsprop + momentum\n"
      ],
      "metadata": {
        "id": "AYFp-eTErh7b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "losses = []\n",
        "for epoch in range(100):\n",
        "\n",
        "  optimizer.zero_grad()\n",
        "  hypothesis = model(x_train)\n",
        "\n",
        "  # 비용 함수\n",
        "  cost = loss_fn(hypothesis, y_train)\n",
        "  cost.backward()\n",
        "  optimizer.step()\n",
        "  losses.append(cost.item()) # 그래프 그리기 위해 Loss값을 losses라는 리스트에 추가, item()은 텐서에서 loss 결과값의 scaler만 추출하는 함수\n",
        "\n",
        "  if epoch % 10 == 0:\n",
        "    print(epoch, cost.item())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "90QxHvlIrjS7",
        "outputId": "99ecc56a-6a3d-4f7a-c6fa-5b3d3297c3b0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 1.084458589553833\n",
            "10 0.7436531782150269\n",
            "20 0.650433361530304\n",
            "30 0.6259011626243591\n",
            "40 0.5985153913497925\n",
            "50 0.589630126953125\n",
            "60 0.5833690166473389\n",
            "70 0.5750383734703064\n",
            "80 0.577163815498352\n",
            "90 0.5723230242729187\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(losses)\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 430
        },
        "id": "81ASYrW7roFM",
        "outputId": "0d361d9b-7378-4735-f1a0-e8f20fdf6f4f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "  model = model.to('cpu')\n",
        "  y_pred = model(x_test)\n",
        "  y_pred = y_pred.detach().numpy()\n",
        "  predicted = np.argmax(y_pred, axis =1)\n",
        "  accuracy = (accuracy_score(predicted, y_test))"
      ],
      "metadata": {
        "id": "h4kJzpLErqhZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'model의 output은 :  {y_pred[0]}')\n",
        "print(f'argmax를 한 후의 output은 {predicted[0]}')\n",
        "print(f'accuracy는 {accuracy}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vyIKhs3Nr6Ay",
        "outputId": "6ca2da8e-d544-4e6a-9e2e-04bebbbbbae9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model의 output은 :  [0.9971938  0.00141522 0.00139096]\n",
            "argmax를 한 후의 output은 0\n",
            "accuracy는 0.9629629629629629\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# < 3주차 과제 2 : CNN 맛보기>"
      ],
      "metadata": {
        "id": "3RzRM7xThZV_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from __future__ import print_function\n",
        "import argparse\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.autograd import Variable"
      ],
      "metadata": {
        "id": "56xqgtLxhZw6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training settings\n",
        "\n",
        "batch_size = 64\n",
        "\n",
        "# MNIST Dataset\n",
        "train_dataset = datasets.MNIST(root='./data/',\n",
        "                               train=True,\n",
        "                               transform=transforms.ToTensor(),\n",
        "                               download=True)\n",
        "\n",
        "test_dataset = datasets.MNIST(root='./data/',\n",
        "                              train=False,\n",
        "                              transform=transforms.ToTensor())\n",
        "\n",
        "# Data Loader (Input Pipeline)\n",
        "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
        "                                           batch_size=batch_size,\n",
        "                                           shuffle=True)\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
        "                                          batch_size=batch_size,\n",
        "                                          shuffle=False)"
      ],
      "metadata": {
        "id": "TzkF2bFNhcQ2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a6423660-523a-492d-9af6-d1aef23db1d1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9912422/9912422 [00:00<00:00, 118261314.62it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28881/28881 [00:00<00:00, 91492215.88it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1648877/1648877 [00:00<00:00, 33903090.33it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4542/4542 [00:00<00:00, 8403409.25it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=0)의 파라미터\n",
        "\n",
        "*   in_channels: 입력 채널 수. 흑백 이미지일 경우 1, RGB 값을 가진 이미지일 경우 3 을 가진 경우가 많음\n",
        "*   out_channels: 출력 채널 수\n",
        "*   kernel_size: 커널 사이즈(필터 사이즈)\n",
        "*   stride: kernel을 적용하는 간격. 간격이 커질수록 출력 데이터 배열의 크기는 작아짐. 기본 값은 1\n",
        "*   padding: 출력 데이터 배열의 크기를 조정하기 위해서 이미지의 주변을 채워줌. 패딩 사이즈 기본 값은 0"
      ],
      "metadata": {
        "id": "VpjKgWkx39j1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Net(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(Net, self).__init__()\n",
        "    self.conv1 = nn.Conv2d(1, 10, kernel_size = 5) #합성곱 레이어1 : 입력 크기는 28x28x1 -> 출력 크기는 24x24x10\n",
        "    self.conv2 = nn.Conv2d(10, 20, kernel_size = 5) #합성곱 레이어2 : 입력 크기는 12x12x10 -> 출력 크기는 8x8x20\n",
        "    self.mp = nn.MaxPool2d(2)\n",
        "    self.fc = nn.Linear(320, 10)\n",
        "\n",
        "  def forward(self, x):\n",
        "    in_size = x.size(0)\n",
        "    x = F.relu(self.mp(self.conv1(x)))#맥스풀링 레이어 : 12*12로 변환\n",
        "    x = F.relu(self.mp(self.conv2(x))) #맥스풀링 레이어 : 4*4로 변환, 최종적으로 [64, 20, 4, 4] 형태의 그림으로 존재함.\n",
        "    x = x.view(in_size, -1) # -> batch를 제외한 남은 차원을 하나로 합침. 즉, [64, 320] 형태로 변환\n",
        "    x = self.fc(x)\n",
        "    return F.log_softmax(x) # 로그 소프트맥스 함수 이용??"
      ],
      "metadata": {
        "id": "tLCSvgganBrH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Net()\n",
        "optimizer = optim.SGD(model.parameters(), lr = 0.01, momentum = 0.5)"
      ],
      "metadata": {
        "id": "lkYZ4pUdnUHc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(epoch):\n",
        "  model.train()\n",
        "  for batch_idx, (data, target) in enumerate(train_loader):\n",
        "    data, target = Variable(data), Variable(target)\n",
        "    optimizer.zero_grad()\n",
        "    output = model(data)\n",
        "    loss = F.nll_loss(output, target)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    if batch_idx % 10 == 0:\n",
        "      print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "          epoch, batch_idx * len(data), len(train_loader.dataset),\n",
        "          100. * batch_idx / len(train_loader), loss.item()))"
      ],
      "metadata": {
        "id": "IzUrEM3EnXJb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "model.eval()은 PyTorch에서 모델을 평가 모드로 전환하는 메서드입니다.\n",
        "\n",
        "model.eval()은 별도의 인수를 받지 않습니다. 단순히 model.eval()을 호출하면 모델이 평가 모드로 전환됩니다. 모델이 평가 모드로 전환되면, 드롭아웃이 비활성화되고 배치 정규화의 이동 평균과 이동 분산이 업데이트되지 않습니다.\n",
        "\n",
        "model.eval()은 주로 테스트 데이터나 검증 데이터를 사용하여 모델을 평가할 때 사용됩니다. 평가 모드에서는 모델이 추론 시에 동일한 동작을 수행하도록 설정되어 있어, 모델의 성능 평가에 불필요한 노이즈를 줄이고 일관된 결과를 얻을 수 있습니다.\n",
        "\n",
        "모델을 학습(training)하는 동안 사용한 모델 객체를 추론(inference)할 때 model.eval()을 호출하여 추론 모드로 전환하고, 추론이 끝난 후에는 다시 model.train()을 호출하여 학습 모드로 전환하는 것이 일반적입니다."
      ],
      "metadata": {
        "id": "vKTMoqR84xd7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def test():\n",
        "    model.eval() #model.eval() 의 기능은? 모델을 평가모드로 전환\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    for data, target in test_loader:\n",
        "        data, target = Variable(data, volatile=True), Variable(target)\n",
        "        output = model(data)\n",
        "        test_loss += F.nll_loss(output, target, size_average=False).data # nll_loss?? / cross entropy loss와의 관계 확인!\n",
        "        pred = output.data.max(1, keepdim=True)[1]\n",
        "        correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
        "\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "        test_loss, correct, len(test_loader.dataset),\n",
        "        100. * correct / len(test_loader.dataset)))"
      ],
      "metadata": {
        "id": "EFi0gYJGn2aa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(1, 10):\n",
        "    train(epoch)\n",
        "    test()"
      ],
      "metadata": {
        "id": "zSvSZb_Bn4Nx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5a9d593a-18ad-461c-b686-642b54e8205a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-26-ce771b9addde>:15: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return F.log_softmax(x)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.333787\n",
            "Train Epoch: 1 [640/60000 (1%)]\tLoss: 2.310540\n",
            "Train Epoch: 1 [1280/60000 (2%)]\tLoss: 2.303401\n",
            "Train Epoch: 1 [1920/60000 (3%)]\tLoss: 2.287821\n",
            "Train Epoch: 1 [2560/60000 (4%)]\tLoss: 2.272456\n",
            "Train Epoch: 1 [3200/60000 (5%)]\tLoss: 2.252066\n",
            "Train Epoch: 1 [3840/60000 (6%)]\tLoss: 2.247658\n",
            "Train Epoch: 1 [4480/60000 (7%)]\tLoss: 2.187569\n",
            "Train Epoch: 1 [5120/60000 (9%)]\tLoss: 2.163913\n",
            "Train Epoch: 1 [5760/60000 (10%)]\tLoss: 2.149293\n",
            "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 2.066093\n",
            "Train Epoch: 1 [7040/60000 (12%)]\tLoss: 1.952025\n",
            "Train Epoch: 1 [7680/60000 (13%)]\tLoss: 1.835536\n",
            "Train Epoch: 1 [8320/60000 (14%)]\tLoss: 1.660598\n",
            "Train Epoch: 1 [8960/60000 (15%)]\tLoss: 1.417570\n",
            "Train Epoch: 1 [9600/60000 (16%)]\tLoss: 1.141111\n",
            "Train Epoch: 1 [10240/60000 (17%)]\tLoss: 1.011117\n",
            "Train Epoch: 1 [10880/60000 (18%)]\tLoss: 0.854689\n",
            "Train Epoch: 1 [11520/60000 (19%)]\tLoss: 0.832860\n",
            "Train Epoch: 1 [12160/60000 (20%)]\tLoss: 0.572486\n",
            "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.703015\n",
            "Train Epoch: 1 [13440/60000 (22%)]\tLoss: 0.749190\n",
            "Train Epoch: 1 [14080/60000 (23%)]\tLoss: 0.558810\n",
            "Train Epoch: 1 [14720/60000 (25%)]\tLoss: 0.598403\n",
            "Train Epoch: 1 [15360/60000 (26%)]\tLoss: 0.609348\n",
            "Train Epoch: 1 [16000/60000 (27%)]\tLoss: 0.559624\n",
            "Train Epoch: 1 [16640/60000 (28%)]\tLoss: 0.439670\n",
            "Train Epoch: 1 [17280/60000 (29%)]\tLoss: 0.371133\n",
            "Train Epoch: 1 [17920/60000 (30%)]\tLoss: 0.570943\n",
            "Train Epoch: 1 [18560/60000 (31%)]\tLoss: 0.348253\n",
            "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.535237\n",
            "Train Epoch: 1 [19840/60000 (33%)]\tLoss: 0.459077\n",
            "Train Epoch: 1 [20480/60000 (34%)]\tLoss: 0.542556\n",
            "Train Epoch: 1 [21120/60000 (35%)]\tLoss: 0.367493\n",
            "Train Epoch: 1 [21760/60000 (36%)]\tLoss: 0.542089\n",
            "Train Epoch: 1 [22400/60000 (37%)]\tLoss: 0.341557\n",
            "Train Epoch: 1 [23040/60000 (38%)]\tLoss: 0.417260\n",
            "Train Epoch: 1 [23680/60000 (39%)]\tLoss: 0.518869\n",
            "Train Epoch: 1 [24320/60000 (41%)]\tLoss: 0.449530\n",
            "Train Epoch: 1 [24960/60000 (42%)]\tLoss: 0.609798\n",
            "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.413774\n",
            "Train Epoch: 1 [26240/60000 (44%)]\tLoss: 0.298516\n",
            "Train Epoch: 1 [26880/60000 (45%)]\tLoss: 0.300948\n",
            "Train Epoch: 1 [27520/60000 (46%)]\tLoss: 0.374837\n",
            "Train Epoch: 1 [28160/60000 (47%)]\tLoss: 0.414436\n",
            "Train Epoch: 1 [28800/60000 (48%)]\tLoss: 0.297800\n",
            "Train Epoch: 1 [29440/60000 (49%)]\tLoss: 0.232377\n",
            "Train Epoch: 1 [30080/60000 (50%)]\tLoss: 0.442840\n",
            "Train Epoch: 1 [30720/60000 (51%)]\tLoss: 0.445673\n",
            "Train Epoch: 1 [31360/60000 (52%)]\tLoss: 0.304161\n",
            "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.332907\n",
            "Train Epoch: 1 [32640/60000 (54%)]\tLoss: 0.241728\n",
            "Train Epoch: 1 [33280/60000 (55%)]\tLoss: 0.265309\n",
            "Train Epoch: 1 [33920/60000 (57%)]\tLoss: 0.213752\n",
            "Train Epoch: 1 [34560/60000 (58%)]\tLoss: 0.241322\n",
            "Train Epoch: 1 [35200/60000 (59%)]\tLoss: 0.289027\n",
            "Train Epoch: 1 [35840/60000 (60%)]\tLoss: 0.389594\n",
            "Train Epoch: 1 [36480/60000 (61%)]\tLoss: 0.238783\n",
            "Train Epoch: 1 [37120/60000 (62%)]\tLoss: 0.153158\n",
            "Train Epoch: 1 [37760/60000 (63%)]\tLoss: 0.488259\n",
            "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.219549\n",
            "Train Epoch: 1 [39040/60000 (65%)]\tLoss: 0.320606\n",
            "Train Epoch: 1 [39680/60000 (66%)]\tLoss: 0.162327\n",
            "Train Epoch: 1 [40320/60000 (67%)]\tLoss: 0.181148\n",
            "Train Epoch: 1 [40960/60000 (68%)]\tLoss: 0.381228\n",
            "Train Epoch: 1 [41600/60000 (69%)]\tLoss: 0.157185\n",
            "Train Epoch: 1 [42240/60000 (70%)]\tLoss: 0.219173\n",
            "Train Epoch: 1 [42880/60000 (71%)]\tLoss: 0.242442\n",
            "Train Epoch: 1 [43520/60000 (72%)]\tLoss: 0.204465\n",
            "Train Epoch: 1 [44160/60000 (74%)]\tLoss: 0.385940\n",
            "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.295545\n",
            "Train Epoch: 1 [45440/60000 (76%)]\tLoss: 0.238064\n",
            "Train Epoch: 1 [46080/60000 (77%)]\tLoss: 0.290930\n",
            "Train Epoch: 1 [46720/60000 (78%)]\tLoss: 0.265414\n",
            "Train Epoch: 1 [47360/60000 (79%)]\tLoss: 0.315341\n",
            "Train Epoch: 1 [48000/60000 (80%)]\tLoss: 0.460405\n",
            "Train Epoch: 1 [48640/60000 (81%)]\tLoss: 0.278927\n",
            "Train Epoch: 1 [49280/60000 (82%)]\tLoss: 0.210826\n",
            "Train Epoch: 1 [49920/60000 (83%)]\tLoss: 0.181036\n",
            "Train Epoch: 1 [50560/60000 (84%)]\tLoss: 0.326154\n",
            "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.191835\n",
            "Train Epoch: 1 [51840/60000 (86%)]\tLoss: 0.240852\n",
            "Train Epoch: 1 [52480/60000 (87%)]\tLoss: 0.205442\n",
            "Train Epoch: 1 [53120/60000 (88%)]\tLoss: 0.181959\n",
            "Train Epoch: 1 [53760/60000 (90%)]\tLoss: 0.247258\n",
            "Train Epoch: 1 [54400/60000 (91%)]\tLoss: 0.181522\n",
            "Train Epoch: 1 [55040/60000 (92%)]\tLoss: 0.226252\n",
            "Train Epoch: 1 [55680/60000 (93%)]\tLoss: 0.165659\n",
            "Train Epoch: 1 [56320/60000 (94%)]\tLoss: 0.274683\n",
            "Train Epoch: 1 [56960/60000 (95%)]\tLoss: 0.122391\n",
            "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.142147\n",
            "Train Epoch: 1 [58240/60000 (97%)]\tLoss: 0.178061\n",
            "Train Epoch: 1 [58880/60000 (98%)]\tLoss: 0.313970\n",
            "Train Epoch: 1 [59520/60000 (99%)]\tLoss: 0.192378\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-29-f52337105c2a>:6: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
            "  data, target = Variable(data, volatile=True), Variable(target)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
            "  warnings.warn(warning.format(ret))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.1904, Accuracy: 9445/10000 (94%)\n",
            "\n",
            "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.144356\n",
            "Train Epoch: 2 [640/60000 (1%)]\tLoss: 0.311371\n",
            "Train Epoch: 2 [1280/60000 (2%)]\tLoss: 0.255028\n",
            "Train Epoch: 2 [1920/60000 (3%)]\tLoss: 0.413055\n",
            "Train Epoch: 2 [2560/60000 (4%)]\tLoss: 0.196605\n",
            "Train Epoch: 2 [3200/60000 (5%)]\tLoss: 0.076946\n",
            "Train Epoch: 2 [3840/60000 (6%)]\tLoss: 0.178278\n",
            "Train Epoch: 2 [4480/60000 (7%)]\tLoss: 0.220730\n",
            "Train Epoch: 2 [5120/60000 (9%)]\tLoss: 0.303001\n",
            "Train Epoch: 2 [5760/60000 (10%)]\tLoss: 0.197868\n",
            "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 0.430021\n",
            "Train Epoch: 2 [7040/60000 (12%)]\tLoss: 0.187761\n",
            "Train Epoch: 2 [7680/60000 (13%)]\tLoss: 0.238727\n",
            "Train Epoch: 2 [8320/60000 (14%)]\tLoss: 0.208112\n",
            "Train Epoch: 2 [8960/60000 (15%)]\tLoss: 0.139427\n",
            "Train Epoch: 2 [9600/60000 (16%)]\tLoss: 0.139031\n",
            "Train Epoch: 2 [10240/60000 (17%)]\tLoss: 0.285748\n",
            "Train Epoch: 2 [10880/60000 (18%)]\tLoss: 0.137164\n",
            "Train Epoch: 2 [11520/60000 (19%)]\tLoss: 0.213065\n",
            "Train Epoch: 2 [12160/60000 (20%)]\tLoss: 0.206633\n",
            "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 0.302592\n",
            "Train Epoch: 2 [13440/60000 (22%)]\tLoss: 0.114460\n",
            "Train Epoch: 2 [14080/60000 (23%)]\tLoss: 0.106351\n",
            "Train Epoch: 2 [14720/60000 (25%)]\tLoss: 0.116124\n",
            "Train Epoch: 2 [15360/60000 (26%)]\tLoss: 0.057333\n",
            "Train Epoch: 2 [16000/60000 (27%)]\tLoss: 0.240288\n",
            "Train Epoch: 2 [16640/60000 (28%)]\tLoss: 0.163225\n",
            "Train Epoch: 2 [17280/60000 (29%)]\tLoss: 0.137646\n",
            "Train Epoch: 2 [17920/60000 (30%)]\tLoss: 0.237491\n",
            "Train Epoch: 2 [18560/60000 (31%)]\tLoss: 0.143913\n",
            "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 0.116964\n",
            "Train Epoch: 2 [19840/60000 (33%)]\tLoss: 0.152819\n",
            "Train Epoch: 2 [20480/60000 (34%)]\tLoss: 0.137491\n",
            "Train Epoch: 2 [21120/60000 (35%)]\tLoss: 0.112696\n",
            "Train Epoch: 2 [21760/60000 (36%)]\tLoss: 0.066887\n",
            "Train Epoch: 2 [22400/60000 (37%)]\tLoss: 0.058237\n",
            "Train Epoch: 2 [23040/60000 (38%)]\tLoss: 0.138319\n",
            "Train Epoch: 2 [23680/60000 (39%)]\tLoss: 0.163975\n",
            "Train Epoch: 2 [24320/60000 (41%)]\tLoss: 0.142663\n",
            "Train Epoch: 2 [24960/60000 (42%)]\tLoss: 0.394549\n",
            "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.357047\n",
            "Train Epoch: 2 [26240/60000 (44%)]\tLoss: 0.097383\n",
            "Train Epoch: 2 [26880/60000 (45%)]\tLoss: 0.157672\n",
            "Train Epoch: 2 [27520/60000 (46%)]\tLoss: 0.144575\n",
            "Train Epoch: 2 [28160/60000 (47%)]\tLoss: 0.212024\n",
            "Train Epoch: 2 [28800/60000 (48%)]\tLoss: 0.030672\n",
            "Train Epoch: 2 [29440/60000 (49%)]\tLoss: 0.226930\n",
            "Train Epoch: 2 [30080/60000 (50%)]\tLoss: 0.141244\n",
            "Train Epoch: 2 [30720/60000 (51%)]\tLoss: 0.029569\n",
            "Train Epoch: 2 [31360/60000 (52%)]\tLoss: 0.241121\n",
            "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 0.103163\n",
            "Train Epoch: 2 [32640/60000 (54%)]\tLoss: 0.164587\n",
            "Train Epoch: 2 [33280/60000 (55%)]\tLoss: 0.155826\n",
            "Train Epoch: 2 [33920/60000 (57%)]\tLoss: 0.114702\n",
            "Train Epoch: 2 [34560/60000 (58%)]\tLoss: 0.133255\n",
            "Train Epoch: 2 [35200/60000 (59%)]\tLoss: 0.065432\n",
            "Train Epoch: 2 [35840/60000 (60%)]\tLoss: 0.050516\n",
            "Train Epoch: 2 [36480/60000 (61%)]\tLoss: 0.070055\n",
            "Train Epoch: 2 [37120/60000 (62%)]\tLoss: 0.080821\n",
            "Train Epoch: 2 [37760/60000 (63%)]\tLoss: 0.059031\n",
            "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 0.084125\n",
            "Train Epoch: 2 [39040/60000 (65%)]\tLoss: 0.099697\n",
            "Train Epoch: 2 [39680/60000 (66%)]\tLoss: 0.159398\n",
            "Train Epoch: 2 [40320/60000 (67%)]\tLoss: 0.312969\n",
            "Train Epoch: 2 [40960/60000 (68%)]\tLoss: 0.188966\n",
            "Train Epoch: 2 [41600/60000 (69%)]\tLoss: 0.176765\n",
            "Train Epoch: 2 [42240/60000 (70%)]\tLoss: 0.070908\n",
            "Train Epoch: 2 [42880/60000 (71%)]\tLoss: 0.072420\n",
            "Train Epoch: 2 [43520/60000 (72%)]\tLoss: 0.081735\n",
            "Train Epoch: 2 [44160/60000 (74%)]\tLoss: 0.119557\n",
            "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 0.168438\n",
            "Train Epoch: 2 [45440/60000 (76%)]\tLoss: 0.178975\n",
            "Train Epoch: 2 [46080/60000 (77%)]\tLoss: 0.096817\n",
            "Train Epoch: 2 [46720/60000 (78%)]\tLoss: 0.222678\n",
            "Train Epoch: 2 [47360/60000 (79%)]\tLoss: 0.137492\n",
            "Train Epoch: 2 [48000/60000 (80%)]\tLoss: 0.066321\n",
            "Train Epoch: 2 [48640/60000 (81%)]\tLoss: 0.179125\n",
            "Train Epoch: 2 [49280/60000 (82%)]\tLoss: 0.221779\n",
            "Train Epoch: 2 [49920/60000 (83%)]\tLoss: 0.076782\n",
            "Train Epoch: 2 [50560/60000 (84%)]\tLoss: 0.242762\n",
            "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.116180\n",
            "Train Epoch: 2 [51840/60000 (86%)]\tLoss: 0.156056\n",
            "Train Epoch: 2 [52480/60000 (87%)]\tLoss: 0.156455\n",
            "Train Epoch: 2 [53120/60000 (88%)]\tLoss: 0.154493\n",
            "Train Epoch: 2 [53760/60000 (90%)]\tLoss: 0.167383\n",
            "Train Epoch: 2 [54400/60000 (91%)]\tLoss: 0.091337\n",
            "Train Epoch: 2 [55040/60000 (92%)]\tLoss: 0.071718\n",
            "Train Epoch: 2 [55680/60000 (93%)]\tLoss: 0.153093\n",
            "Train Epoch: 2 [56320/60000 (94%)]\tLoss: 0.059545\n",
            "Train Epoch: 2 [56960/60000 (95%)]\tLoss: 0.067212\n",
            "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 0.064805\n",
            "Train Epoch: 2 [58240/60000 (97%)]\tLoss: 0.075404\n",
            "Train Epoch: 2 [58880/60000 (98%)]\tLoss: 0.276411\n",
            "Train Epoch: 2 [59520/60000 (99%)]\tLoss: 0.143042\n",
            "\n",
            "Test set: Average loss: 0.1097, Accuracy: 9675/10000 (97%)\n",
            "\n",
            "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.279287\n",
            "Train Epoch: 3 [640/60000 (1%)]\tLoss: 0.151386\n",
            "Train Epoch: 3 [1280/60000 (2%)]\tLoss: 0.240140\n",
            "Train Epoch: 3 [1920/60000 (3%)]\tLoss: 0.126971\n",
            "Train Epoch: 3 [2560/60000 (4%)]\tLoss: 0.163389\n",
            "Train Epoch: 3 [3200/60000 (5%)]\tLoss: 0.216354\n",
            "Train Epoch: 3 [3840/60000 (6%)]\tLoss: 0.083683\n",
            "Train Epoch: 3 [4480/60000 (7%)]\tLoss: 0.104455\n",
            "Train Epoch: 3 [5120/60000 (9%)]\tLoss: 0.097301\n",
            "Train Epoch: 3 [5760/60000 (10%)]\tLoss: 0.114270\n",
            "Train Epoch: 3 [6400/60000 (11%)]\tLoss: 0.058782\n",
            "Train Epoch: 3 [7040/60000 (12%)]\tLoss: 0.078543\n",
            "Train Epoch: 3 [7680/60000 (13%)]\tLoss: 0.112123\n",
            "Train Epoch: 3 [8320/60000 (14%)]\tLoss: 0.190449\n",
            "Train Epoch: 3 [8960/60000 (15%)]\tLoss: 0.195774\n",
            "Train Epoch: 3 [9600/60000 (16%)]\tLoss: 0.080848\n",
            "Train Epoch: 3 [10240/60000 (17%)]\tLoss: 0.095517\n",
            "Train Epoch: 3 [10880/60000 (18%)]\tLoss: 0.115512\n",
            "Train Epoch: 3 [11520/60000 (19%)]\tLoss: 0.094411\n",
            "Train Epoch: 3 [12160/60000 (20%)]\tLoss: 0.086004\n",
            "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 0.114754\n",
            "Train Epoch: 3 [13440/60000 (22%)]\tLoss: 0.091391\n",
            "Train Epoch: 3 [14080/60000 (23%)]\tLoss: 0.043570\n",
            "Train Epoch: 3 [14720/60000 (25%)]\tLoss: 0.280677\n",
            "Train Epoch: 3 [15360/60000 (26%)]\tLoss: 0.074316\n",
            "Train Epoch: 3 [16000/60000 (27%)]\tLoss: 0.285275\n",
            "Train Epoch: 3 [16640/60000 (28%)]\tLoss: 0.062609\n",
            "Train Epoch: 3 [17280/60000 (29%)]\tLoss: 0.184553\n",
            "Train Epoch: 3 [17920/60000 (30%)]\tLoss: 0.196213\n",
            "Train Epoch: 3 [18560/60000 (31%)]\tLoss: 0.063665\n",
            "Train Epoch: 3 [19200/60000 (32%)]\tLoss: 0.065336\n",
            "Train Epoch: 3 [19840/60000 (33%)]\tLoss: 0.027220\n",
            "Train Epoch: 3 [20480/60000 (34%)]\tLoss: 0.094020\n",
            "Train Epoch: 3 [21120/60000 (35%)]\tLoss: 0.030203\n",
            "Train Epoch: 3 [21760/60000 (36%)]\tLoss: 0.147956\n",
            "Train Epoch: 3 [22400/60000 (37%)]\tLoss: 0.030611\n",
            "Train Epoch: 3 [23040/60000 (38%)]\tLoss: 0.180677\n",
            "Train Epoch: 3 [23680/60000 (39%)]\tLoss: 0.108756\n",
            "Train Epoch: 3 [24320/60000 (41%)]\tLoss: 0.097346\n",
            "Train Epoch: 3 [24960/60000 (42%)]\tLoss: 0.094644\n",
            "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.070948\n",
            "Train Epoch: 3 [26240/60000 (44%)]\tLoss: 0.066944\n",
            "Train Epoch: 3 [26880/60000 (45%)]\tLoss: 0.101363\n",
            "Train Epoch: 3 [27520/60000 (46%)]\tLoss: 0.111808\n",
            "Train Epoch: 3 [28160/60000 (47%)]\tLoss: 0.037803\n",
            "Train Epoch: 3 [28800/60000 (48%)]\tLoss: 0.133368\n",
            "Train Epoch: 3 [29440/60000 (49%)]\tLoss: 0.111165\n",
            "Train Epoch: 3 [30080/60000 (50%)]\tLoss: 0.022798\n",
            "Train Epoch: 3 [30720/60000 (51%)]\tLoss: 0.038211\n",
            "Train Epoch: 3 [31360/60000 (52%)]\tLoss: 0.126180\n",
            "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 0.131277\n",
            "Train Epoch: 3 [32640/60000 (54%)]\tLoss: 0.021910\n",
            "Train Epoch: 3 [33280/60000 (55%)]\tLoss: 0.061551\n",
            "Train Epoch: 3 [33920/60000 (57%)]\tLoss: 0.106254\n",
            "Train Epoch: 3 [34560/60000 (58%)]\tLoss: 0.226288\n",
            "Train Epoch: 3 [35200/60000 (59%)]\tLoss: 0.098813\n",
            "Train Epoch: 3 [35840/60000 (60%)]\tLoss: 0.368432\n",
            "Train Epoch: 3 [36480/60000 (61%)]\tLoss: 0.028984\n",
            "Train Epoch: 3 [37120/60000 (62%)]\tLoss: 0.068840\n",
            "Train Epoch: 3 [37760/60000 (63%)]\tLoss: 0.030652\n",
            "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 0.149351\n",
            "Train Epoch: 3 [39040/60000 (65%)]\tLoss: 0.088004\n",
            "Train Epoch: 3 [39680/60000 (66%)]\tLoss: 0.114255\n",
            "Train Epoch: 3 [40320/60000 (67%)]\tLoss: 0.069936\n",
            "Train Epoch: 3 [40960/60000 (68%)]\tLoss: 0.204824\n",
            "Train Epoch: 3 [41600/60000 (69%)]\tLoss: 0.204106\n",
            "Train Epoch: 3 [42240/60000 (70%)]\tLoss: 0.218536\n",
            "Train Epoch: 3 [42880/60000 (71%)]\tLoss: 0.161797\n",
            "Train Epoch: 3 [43520/60000 (72%)]\tLoss: 0.252149\n",
            "Train Epoch: 3 [44160/60000 (74%)]\tLoss: 0.141282\n",
            "Train Epoch: 3 [44800/60000 (75%)]\tLoss: 0.185160\n",
            "Train Epoch: 3 [45440/60000 (76%)]\tLoss: 0.107604\n",
            "Train Epoch: 3 [46080/60000 (77%)]\tLoss: 0.065679\n",
            "Train Epoch: 3 [46720/60000 (78%)]\tLoss: 0.085489\n",
            "Train Epoch: 3 [47360/60000 (79%)]\tLoss: 0.076792\n",
            "Train Epoch: 3 [48000/60000 (80%)]\tLoss: 0.113037\n",
            "Train Epoch: 3 [48640/60000 (81%)]\tLoss: 0.201147\n",
            "Train Epoch: 3 [49280/60000 (82%)]\tLoss: 0.132667\n",
            "Train Epoch: 3 [49920/60000 (83%)]\tLoss: 0.188540\n",
            "Train Epoch: 3 [50560/60000 (84%)]\tLoss: 0.043435\n",
            "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.129248\n",
            "Train Epoch: 3 [51840/60000 (86%)]\tLoss: 0.111016\n",
            "Train Epoch: 3 [52480/60000 (87%)]\tLoss: 0.076773\n",
            "Train Epoch: 3 [53120/60000 (88%)]\tLoss: 0.158153\n",
            "Train Epoch: 3 [53760/60000 (90%)]\tLoss: 0.104576\n",
            "Train Epoch: 3 [54400/60000 (91%)]\tLoss: 0.066525\n",
            "Train Epoch: 3 [55040/60000 (92%)]\tLoss: 0.071951\n",
            "Train Epoch: 3 [55680/60000 (93%)]\tLoss: 0.133271\n",
            "Train Epoch: 3 [56320/60000 (94%)]\tLoss: 0.219276\n",
            "Train Epoch: 3 [56960/60000 (95%)]\tLoss: 0.123717\n",
            "Train Epoch: 3 [57600/60000 (96%)]\tLoss: 0.067002\n",
            "Train Epoch: 3 [58240/60000 (97%)]\tLoss: 0.128004\n",
            "Train Epoch: 3 [58880/60000 (98%)]\tLoss: 0.063086\n",
            "Train Epoch: 3 [59520/60000 (99%)]\tLoss: 0.046166\n",
            "\n",
            "Test set: Average loss: 0.0897, Accuracy: 9733/10000 (97%)\n",
            "\n",
            "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.093918\n",
            "Train Epoch: 4 [640/60000 (1%)]\tLoss: 0.058811\n",
            "Train Epoch: 4 [1280/60000 (2%)]\tLoss: 0.068787\n",
            "Train Epoch: 4 [1920/60000 (3%)]\tLoss: 0.068830\n",
            "Train Epoch: 4 [2560/60000 (4%)]\tLoss: 0.039370\n",
            "Train Epoch: 4 [3200/60000 (5%)]\tLoss: 0.160360\n",
            "Train Epoch: 4 [3840/60000 (6%)]\tLoss: 0.090309\n",
            "Train Epoch: 4 [4480/60000 (7%)]\tLoss: 0.095974\n",
            "Train Epoch: 4 [5120/60000 (9%)]\tLoss: 0.060971\n",
            "Train Epoch: 4 [5760/60000 (10%)]\tLoss: 0.072961\n",
            "Train Epoch: 4 [6400/60000 (11%)]\tLoss: 0.080801\n",
            "Train Epoch: 4 [7040/60000 (12%)]\tLoss: 0.044849\n",
            "Train Epoch: 4 [7680/60000 (13%)]\tLoss: 0.140490\n",
            "Train Epoch: 4 [8320/60000 (14%)]\tLoss: 0.064624\n",
            "Train Epoch: 4 [8960/60000 (15%)]\tLoss: 0.123358\n",
            "Train Epoch: 4 [9600/60000 (16%)]\tLoss: 0.088494\n",
            "Train Epoch: 4 [10240/60000 (17%)]\tLoss: 0.067440\n",
            "Train Epoch: 4 [10880/60000 (18%)]\tLoss: 0.129630\n",
            "Train Epoch: 4 [11520/60000 (19%)]\tLoss: 0.290064\n",
            "Train Epoch: 4 [12160/60000 (20%)]\tLoss: 0.065546\n",
            "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 0.130422\n",
            "Train Epoch: 4 [13440/60000 (22%)]\tLoss: 0.097893\n",
            "Train Epoch: 4 [14080/60000 (23%)]\tLoss: 0.141866\n",
            "Train Epoch: 4 [14720/60000 (25%)]\tLoss: 0.055362\n",
            "Train Epoch: 4 [15360/60000 (26%)]\tLoss: 0.220460\n",
            "Train Epoch: 4 [16000/60000 (27%)]\tLoss: 0.152207\n",
            "Train Epoch: 4 [16640/60000 (28%)]\tLoss: 0.066396\n",
            "Train Epoch: 4 [17280/60000 (29%)]\tLoss: 0.055575\n",
            "Train Epoch: 4 [17920/60000 (30%)]\tLoss: 0.048589\n",
            "Train Epoch: 4 [18560/60000 (31%)]\tLoss: 0.082854\n",
            "Train Epoch: 4 [19200/60000 (32%)]\tLoss: 0.039744\n",
            "Train Epoch: 4 [19840/60000 (33%)]\tLoss: 0.128191\n",
            "Train Epoch: 4 [20480/60000 (34%)]\tLoss: 0.048941\n",
            "Train Epoch: 4 [21120/60000 (35%)]\tLoss: 0.223699\n",
            "Train Epoch: 4 [21760/60000 (36%)]\tLoss: 0.096532\n",
            "Train Epoch: 4 [22400/60000 (37%)]\tLoss: 0.099808\n",
            "Train Epoch: 4 [23040/60000 (38%)]\tLoss: 0.034608\n",
            "Train Epoch: 4 [23680/60000 (39%)]\tLoss: 0.116083\n",
            "Train Epoch: 4 [24320/60000 (41%)]\tLoss: 0.074307\n",
            "Train Epoch: 4 [24960/60000 (42%)]\tLoss: 0.145321\n",
            "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 0.069350\n",
            "Train Epoch: 4 [26240/60000 (44%)]\tLoss: 0.196896\n",
            "Train Epoch: 4 [26880/60000 (45%)]\tLoss: 0.024684\n",
            "Train Epoch: 4 [27520/60000 (46%)]\tLoss: 0.078877\n",
            "Train Epoch: 4 [28160/60000 (47%)]\tLoss: 0.152150\n",
            "Train Epoch: 4 [28800/60000 (48%)]\tLoss: 0.180867\n",
            "Train Epoch: 4 [29440/60000 (49%)]\tLoss: 0.048009\n",
            "Train Epoch: 4 [30080/60000 (50%)]\tLoss: 0.053340\n",
            "Train Epoch: 4 [30720/60000 (51%)]\tLoss: 0.175575\n",
            "Train Epoch: 4 [31360/60000 (52%)]\tLoss: 0.180591\n",
            "Train Epoch: 4 [32000/60000 (53%)]\tLoss: 0.098672\n",
            "Train Epoch: 4 [32640/60000 (54%)]\tLoss: 0.069336\n",
            "Train Epoch: 4 [33280/60000 (55%)]\tLoss: 0.135268\n",
            "Train Epoch: 4 [33920/60000 (57%)]\tLoss: 0.183470\n",
            "Train Epoch: 4 [34560/60000 (58%)]\tLoss: 0.114046\n",
            "Train Epoch: 4 [35200/60000 (59%)]\tLoss: 0.030014\n",
            "Train Epoch: 4 [35840/60000 (60%)]\tLoss: 0.050093\n",
            "Train Epoch: 4 [36480/60000 (61%)]\tLoss: 0.025536\n",
            "Train Epoch: 4 [37120/60000 (62%)]\tLoss: 0.151813\n",
            "Train Epoch: 4 [37760/60000 (63%)]\tLoss: 0.092760\n",
            "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 0.077963\n",
            "Train Epoch: 4 [39040/60000 (65%)]\tLoss: 0.103524\n",
            "Train Epoch: 4 [39680/60000 (66%)]\tLoss: 0.169213\n",
            "Train Epoch: 4 [40320/60000 (67%)]\tLoss: 0.047390\n",
            "Train Epoch: 4 [40960/60000 (68%)]\tLoss: 0.068263\n",
            "Train Epoch: 4 [41600/60000 (69%)]\tLoss: 0.114268\n",
            "Train Epoch: 4 [42240/60000 (70%)]\tLoss: 0.068146\n",
            "Train Epoch: 4 [42880/60000 (71%)]\tLoss: 0.108505\n",
            "Train Epoch: 4 [43520/60000 (72%)]\tLoss: 0.069549\n",
            "Train Epoch: 4 [44160/60000 (74%)]\tLoss: 0.128094\n",
            "Train Epoch: 4 [44800/60000 (75%)]\tLoss: 0.107813\n",
            "Train Epoch: 4 [45440/60000 (76%)]\tLoss: 0.076056\n",
            "Train Epoch: 4 [46080/60000 (77%)]\tLoss: 0.034039\n",
            "Train Epoch: 4 [46720/60000 (78%)]\tLoss: 0.053364\n",
            "Train Epoch: 4 [47360/60000 (79%)]\tLoss: 0.037560\n",
            "Train Epoch: 4 [48000/60000 (80%)]\tLoss: 0.039038\n",
            "Train Epoch: 4 [48640/60000 (81%)]\tLoss: 0.085382\n",
            "Train Epoch: 4 [49280/60000 (82%)]\tLoss: 0.073095\n",
            "Train Epoch: 4 [49920/60000 (83%)]\tLoss: 0.169346\n",
            "Train Epoch: 4 [50560/60000 (84%)]\tLoss: 0.108508\n",
            "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 0.279198\n",
            "Train Epoch: 4 [51840/60000 (86%)]\tLoss: 0.060418\n",
            "Train Epoch: 4 [52480/60000 (87%)]\tLoss: 0.025818\n",
            "Train Epoch: 4 [53120/60000 (88%)]\tLoss: 0.029112\n",
            "Train Epoch: 4 [53760/60000 (90%)]\tLoss: 0.140272\n",
            "Train Epoch: 4 [54400/60000 (91%)]\tLoss: 0.082550\n",
            "Train Epoch: 4 [55040/60000 (92%)]\tLoss: 0.176839\n",
            "Train Epoch: 4 [55680/60000 (93%)]\tLoss: 0.052421\n",
            "Train Epoch: 4 [56320/60000 (94%)]\tLoss: 0.099306\n",
            "Train Epoch: 4 [56960/60000 (95%)]\tLoss: 0.019411\n",
            "Train Epoch: 4 [57600/60000 (96%)]\tLoss: 0.057232\n",
            "Train Epoch: 4 [58240/60000 (97%)]\tLoss: 0.063996\n",
            "Train Epoch: 4 [58880/60000 (98%)]\tLoss: 0.046023\n",
            "Train Epoch: 4 [59520/60000 (99%)]\tLoss: 0.090290\n",
            "\n",
            "Test set: Average loss: 0.0782, Accuracy: 9758/10000 (98%)\n",
            "\n",
            "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.123057\n",
            "Train Epoch: 5 [640/60000 (1%)]\tLoss: 0.091485\n",
            "Train Epoch: 5 [1280/60000 (2%)]\tLoss: 0.038750\n",
            "Train Epoch: 5 [1920/60000 (3%)]\tLoss: 0.095279\n",
            "Train Epoch: 5 [2560/60000 (4%)]\tLoss: 0.055159\n",
            "Train Epoch: 5 [3200/60000 (5%)]\tLoss: 0.061817\n",
            "Train Epoch: 5 [3840/60000 (6%)]\tLoss: 0.077172\n",
            "Train Epoch: 5 [4480/60000 (7%)]\tLoss: 0.071562\n",
            "Train Epoch: 5 [5120/60000 (9%)]\tLoss: 0.123775\n",
            "Train Epoch: 5 [5760/60000 (10%)]\tLoss: 0.028661\n",
            "Train Epoch: 5 [6400/60000 (11%)]\tLoss: 0.022170\n",
            "Train Epoch: 5 [7040/60000 (12%)]\tLoss: 0.092635\n",
            "Train Epoch: 5 [7680/60000 (13%)]\tLoss: 0.157056\n",
            "Train Epoch: 5 [8320/60000 (14%)]\tLoss: 0.109885\n",
            "Train Epoch: 5 [8960/60000 (15%)]\tLoss: 0.036720\n",
            "Train Epoch: 5 [9600/60000 (16%)]\tLoss: 0.067317\n",
            "Train Epoch: 5 [10240/60000 (17%)]\tLoss: 0.195340\n",
            "Train Epoch: 5 [10880/60000 (18%)]\tLoss: 0.085029\n",
            "Train Epoch: 5 [11520/60000 (19%)]\tLoss: 0.171206\n",
            "Train Epoch: 5 [12160/60000 (20%)]\tLoss: 0.109351\n",
            "Train Epoch: 5 [12800/60000 (21%)]\tLoss: 0.119086\n",
            "Train Epoch: 5 [13440/60000 (22%)]\tLoss: 0.049313\n",
            "Train Epoch: 5 [14080/60000 (23%)]\tLoss: 0.100514\n",
            "Train Epoch: 5 [14720/60000 (25%)]\tLoss: 0.045849\n",
            "Train Epoch: 5 [15360/60000 (26%)]\tLoss: 0.083178\n",
            "Train Epoch: 5 [16000/60000 (27%)]\tLoss: 0.114393\n",
            "Train Epoch: 5 [16640/60000 (28%)]\tLoss: 0.133807\n",
            "Train Epoch: 5 [17280/60000 (29%)]\tLoss: 0.018379\n",
            "Train Epoch: 5 [17920/60000 (30%)]\tLoss: 0.052906\n",
            "Train Epoch: 5 [18560/60000 (31%)]\tLoss: 0.063193\n",
            "Train Epoch: 5 [19200/60000 (32%)]\tLoss: 0.041343\n",
            "Train Epoch: 5 [19840/60000 (33%)]\tLoss: 0.089684\n",
            "Train Epoch: 5 [20480/60000 (34%)]\tLoss: 0.182253\n",
            "Train Epoch: 5 [21120/60000 (35%)]\tLoss: 0.057988\n",
            "Train Epoch: 5 [21760/60000 (36%)]\tLoss: 0.096035\n",
            "Train Epoch: 5 [22400/60000 (37%)]\tLoss: 0.079918\n",
            "Train Epoch: 5 [23040/60000 (38%)]\tLoss: 0.067602\n",
            "Train Epoch: 5 [23680/60000 (39%)]\tLoss: 0.052104\n",
            "Train Epoch: 5 [24320/60000 (41%)]\tLoss: 0.113752\n",
            "Train Epoch: 5 [24960/60000 (42%)]\tLoss: 0.062337\n",
            "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 0.050777\n",
            "Train Epoch: 5 [26240/60000 (44%)]\tLoss: 0.173294\n",
            "Train Epoch: 5 [26880/60000 (45%)]\tLoss: 0.125836\n",
            "Train Epoch: 5 [27520/60000 (46%)]\tLoss: 0.055263\n",
            "Train Epoch: 5 [28160/60000 (47%)]\tLoss: 0.070503\n",
            "Train Epoch: 5 [28800/60000 (48%)]\tLoss: 0.039797\n",
            "Train Epoch: 5 [29440/60000 (49%)]\tLoss: 0.029848\n",
            "Train Epoch: 5 [30080/60000 (50%)]\tLoss: 0.033558\n",
            "Train Epoch: 5 [30720/60000 (51%)]\tLoss: 0.119694\n",
            "Train Epoch: 5 [31360/60000 (52%)]\tLoss: 0.083414\n",
            "Train Epoch: 5 [32000/60000 (53%)]\tLoss: 0.025683\n",
            "Train Epoch: 5 [32640/60000 (54%)]\tLoss: 0.152665\n",
            "Train Epoch: 5 [33280/60000 (55%)]\tLoss: 0.084605\n",
            "Train Epoch: 5 [33920/60000 (57%)]\tLoss: 0.162494\n",
            "Train Epoch: 5 [34560/60000 (58%)]\tLoss: 0.033064\n",
            "Train Epoch: 5 [35200/60000 (59%)]\tLoss: 0.263035\n",
            "Train Epoch: 5 [35840/60000 (60%)]\tLoss: 0.117018\n",
            "Train Epoch: 5 [36480/60000 (61%)]\tLoss: 0.076671\n",
            "Train Epoch: 5 [37120/60000 (62%)]\tLoss: 0.188886\n",
            "Train Epoch: 5 [37760/60000 (63%)]\tLoss: 0.232041\n",
            "Train Epoch: 5 [38400/60000 (64%)]\tLoss: 0.172845\n",
            "Train Epoch: 5 [39040/60000 (65%)]\tLoss: 0.083785\n",
            "Train Epoch: 5 [39680/60000 (66%)]\tLoss: 0.053971\n",
            "Train Epoch: 5 [40320/60000 (67%)]\tLoss: 0.011600\n",
            "Train Epoch: 5 [40960/60000 (68%)]\tLoss: 0.079664\n",
            "Train Epoch: 5 [41600/60000 (69%)]\tLoss: 0.075925\n",
            "Train Epoch: 5 [42240/60000 (70%)]\tLoss: 0.147860\n",
            "Train Epoch: 5 [42880/60000 (71%)]\tLoss: 0.352741\n",
            "Train Epoch: 5 [43520/60000 (72%)]\tLoss: 0.086860\n",
            "Train Epoch: 5 [44160/60000 (74%)]\tLoss: 0.102908\n",
            "Train Epoch: 5 [44800/60000 (75%)]\tLoss: 0.161174\n",
            "Train Epoch: 5 [45440/60000 (76%)]\tLoss: 0.089496\n",
            "Train Epoch: 5 [46080/60000 (77%)]\tLoss: 0.016057\n",
            "Train Epoch: 5 [46720/60000 (78%)]\tLoss: 0.017853\n",
            "Train Epoch: 5 [47360/60000 (79%)]\tLoss: 0.116890\n",
            "Train Epoch: 5 [48000/60000 (80%)]\tLoss: 0.029127\n",
            "Train Epoch: 5 [48640/60000 (81%)]\tLoss: 0.076864\n",
            "Train Epoch: 5 [49280/60000 (82%)]\tLoss: 0.041620\n",
            "Train Epoch: 5 [49920/60000 (83%)]\tLoss: 0.072837\n",
            "Train Epoch: 5 [50560/60000 (84%)]\tLoss: 0.038783\n",
            "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 0.055480\n",
            "Train Epoch: 5 [51840/60000 (86%)]\tLoss: 0.039860\n",
            "Train Epoch: 5 [52480/60000 (87%)]\tLoss: 0.022253\n",
            "Train Epoch: 5 [53120/60000 (88%)]\tLoss: 0.067187\n",
            "Train Epoch: 5 [53760/60000 (90%)]\tLoss: 0.012386\n",
            "Train Epoch: 5 [54400/60000 (91%)]\tLoss: 0.018178\n",
            "Train Epoch: 5 [55040/60000 (92%)]\tLoss: 0.105049\n",
            "Train Epoch: 5 [55680/60000 (93%)]\tLoss: 0.100480\n",
            "Train Epoch: 5 [56320/60000 (94%)]\tLoss: 0.046648\n",
            "Train Epoch: 5 [56960/60000 (95%)]\tLoss: 0.092567\n",
            "Train Epoch: 5 [57600/60000 (96%)]\tLoss: 0.066248\n",
            "Train Epoch: 5 [58240/60000 (97%)]\tLoss: 0.146060\n",
            "Train Epoch: 5 [58880/60000 (98%)]\tLoss: 0.082471\n",
            "Train Epoch: 5 [59520/60000 (99%)]\tLoss: 0.175378\n",
            "\n",
            "Test set: Average loss: 0.0713, Accuracy: 9779/10000 (98%)\n",
            "\n",
            "Train Epoch: 6 [0/60000 (0%)]\tLoss: 0.143624\n",
            "Train Epoch: 6 [640/60000 (1%)]\tLoss: 0.092140\n",
            "Train Epoch: 6 [1280/60000 (2%)]\tLoss: 0.052217\n",
            "Train Epoch: 6 [1920/60000 (3%)]\tLoss: 0.085495\n",
            "Train Epoch: 6 [2560/60000 (4%)]\tLoss: 0.121218\n",
            "Train Epoch: 6 [3200/60000 (5%)]\tLoss: 0.026589\n",
            "Train Epoch: 6 [3840/60000 (6%)]\tLoss: 0.094014\n",
            "Train Epoch: 6 [4480/60000 (7%)]\tLoss: 0.050173\n",
            "Train Epoch: 6 [5120/60000 (9%)]\tLoss: 0.078352\n",
            "Train Epoch: 6 [5760/60000 (10%)]\tLoss: 0.112726\n",
            "Train Epoch: 6 [6400/60000 (11%)]\tLoss: 0.029107\n",
            "Train Epoch: 6 [7040/60000 (12%)]\tLoss: 0.159337\n",
            "Train Epoch: 6 [7680/60000 (13%)]\tLoss: 0.040421\n",
            "Train Epoch: 6 [8320/60000 (14%)]\tLoss: 0.010656\n",
            "Train Epoch: 6 [8960/60000 (15%)]\tLoss: 0.131272\n",
            "Train Epoch: 6 [9600/60000 (16%)]\tLoss: 0.116097\n",
            "Train Epoch: 6 [10240/60000 (17%)]\tLoss: 0.101909\n",
            "Train Epoch: 6 [10880/60000 (18%)]\tLoss: 0.074132\n",
            "Train Epoch: 6 [11520/60000 (19%)]\tLoss: 0.141720\n",
            "Train Epoch: 6 [12160/60000 (20%)]\tLoss: 0.070449\n",
            "Train Epoch: 6 [12800/60000 (21%)]\tLoss: 0.017120\n",
            "Train Epoch: 6 [13440/60000 (22%)]\tLoss: 0.096290\n",
            "Train Epoch: 6 [14080/60000 (23%)]\tLoss: 0.046068\n",
            "Train Epoch: 6 [14720/60000 (25%)]\tLoss: 0.076214\n",
            "Train Epoch: 6 [15360/60000 (26%)]\tLoss: 0.100794\n",
            "Train Epoch: 6 [16000/60000 (27%)]\tLoss: 0.196372\n",
            "Train Epoch: 6 [16640/60000 (28%)]\tLoss: 0.059232\n",
            "Train Epoch: 6 [17280/60000 (29%)]\tLoss: 0.051229\n",
            "Train Epoch: 6 [17920/60000 (30%)]\tLoss: 0.104371\n",
            "Train Epoch: 6 [18560/60000 (31%)]\tLoss: 0.051347\n",
            "Train Epoch: 6 [19200/60000 (32%)]\tLoss: 0.022142\n",
            "Train Epoch: 6 [19840/60000 (33%)]\tLoss: 0.161872\n",
            "Train Epoch: 6 [20480/60000 (34%)]\tLoss: 0.040291\n",
            "Train Epoch: 6 [21120/60000 (35%)]\tLoss: 0.029400\n",
            "Train Epoch: 6 [21760/60000 (36%)]\tLoss: 0.054956\n",
            "Train Epoch: 6 [22400/60000 (37%)]\tLoss: 0.066509\n",
            "Train Epoch: 6 [23040/60000 (38%)]\tLoss: 0.147359\n",
            "Train Epoch: 6 [23680/60000 (39%)]\tLoss: 0.123891\n",
            "Train Epoch: 6 [24320/60000 (41%)]\tLoss: 0.072847\n",
            "Train Epoch: 6 [24960/60000 (42%)]\tLoss: 0.045332\n",
            "Train Epoch: 6 [25600/60000 (43%)]\tLoss: 0.049625\n",
            "Train Epoch: 6 [26240/60000 (44%)]\tLoss: 0.008553\n",
            "Train Epoch: 6 [26880/60000 (45%)]\tLoss: 0.115551\n",
            "Train Epoch: 6 [27520/60000 (46%)]\tLoss: 0.073535\n",
            "Train Epoch: 6 [28160/60000 (47%)]\tLoss: 0.051345\n",
            "Train Epoch: 6 [28800/60000 (48%)]\tLoss: 0.022181\n",
            "Train Epoch: 6 [29440/60000 (49%)]\tLoss: 0.073401\n",
            "Train Epoch: 6 [30080/60000 (50%)]\tLoss: 0.198606\n",
            "Train Epoch: 6 [30720/60000 (51%)]\tLoss: 0.113041\n",
            "Train Epoch: 6 [31360/60000 (52%)]\tLoss: 0.026662\n",
            "Train Epoch: 6 [32000/60000 (53%)]\tLoss: 0.029697\n",
            "Train Epoch: 6 [32640/60000 (54%)]\tLoss: 0.043156\n",
            "Train Epoch: 6 [33280/60000 (55%)]\tLoss: 0.058873\n",
            "Train Epoch: 6 [33920/60000 (57%)]\tLoss: 0.021511\n",
            "Train Epoch: 6 [34560/60000 (58%)]\tLoss: 0.083771\n",
            "Train Epoch: 6 [35200/60000 (59%)]\tLoss: 0.079888\n",
            "Train Epoch: 6 [35840/60000 (60%)]\tLoss: 0.071933\n",
            "Train Epoch: 6 [36480/60000 (61%)]\tLoss: 0.021050\n",
            "Train Epoch: 6 [37120/60000 (62%)]\tLoss: 0.090606\n",
            "Train Epoch: 6 [37760/60000 (63%)]\tLoss: 0.018534\n",
            "Train Epoch: 6 [38400/60000 (64%)]\tLoss: 0.059235\n",
            "Train Epoch: 6 [39040/60000 (65%)]\tLoss: 0.061693\n",
            "Train Epoch: 6 [39680/60000 (66%)]\tLoss: 0.042649\n",
            "Train Epoch: 6 [40320/60000 (67%)]\tLoss: 0.023223\n",
            "Train Epoch: 6 [40960/60000 (68%)]\tLoss: 0.191758\n",
            "Train Epoch: 6 [41600/60000 (69%)]\tLoss: 0.152806\n",
            "Train Epoch: 6 [42240/60000 (70%)]\tLoss: 0.090488\n",
            "Train Epoch: 6 [42880/60000 (71%)]\tLoss: 0.052228\n",
            "Train Epoch: 6 [43520/60000 (72%)]\tLoss: 0.013245\n",
            "Train Epoch: 6 [44160/60000 (74%)]\tLoss: 0.081894\n",
            "Train Epoch: 6 [44800/60000 (75%)]\tLoss: 0.034659\n",
            "Train Epoch: 6 [45440/60000 (76%)]\tLoss: 0.139200\n",
            "Train Epoch: 6 [46080/60000 (77%)]\tLoss: 0.076329\n",
            "Train Epoch: 6 [46720/60000 (78%)]\tLoss: 0.179394\n",
            "Train Epoch: 6 [47360/60000 (79%)]\tLoss: 0.058641\n",
            "Train Epoch: 6 [48000/60000 (80%)]\tLoss: 0.027237\n",
            "Train Epoch: 6 [48640/60000 (81%)]\tLoss: 0.129135\n",
            "Train Epoch: 6 [49280/60000 (82%)]\tLoss: 0.054038\n",
            "Train Epoch: 6 [49920/60000 (83%)]\tLoss: 0.182551\n",
            "Train Epoch: 6 [50560/60000 (84%)]\tLoss: 0.081722\n",
            "Train Epoch: 6 [51200/60000 (85%)]\tLoss: 0.111568\n",
            "Train Epoch: 6 [51840/60000 (86%)]\tLoss: 0.113677\n",
            "Train Epoch: 6 [52480/60000 (87%)]\tLoss: 0.054940\n",
            "Train Epoch: 6 [53120/60000 (88%)]\tLoss: 0.054269\n",
            "Train Epoch: 6 [53760/60000 (90%)]\tLoss: 0.076103\n",
            "Train Epoch: 6 [54400/60000 (91%)]\tLoss: 0.035910\n",
            "Train Epoch: 6 [55040/60000 (92%)]\tLoss: 0.049744\n",
            "Train Epoch: 6 [55680/60000 (93%)]\tLoss: 0.030261\n",
            "Train Epoch: 6 [56320/60000 (94%)]\tLoss: 0.029426\n",
            "Train Epoch: 6 [56960/60000 (95%)]\tLoss: 0.029113\n",
            "Train Epoch: 6 [57600/60000 (96%)]\tLoss: 0.039213\n",
            "Train Epoch: 6 [58240/60000 (97%)]\tLoss: 0.068291\n",
            "Train Epoch: 6 [58880/60000 (98%)]\tLoss: 0.008392\n",
            "Train Epoch: 6 [59520/60000 (99%)]\tLoss: 0.025863\n",
            "\n",
            "Test set: Average loss: 0.0683, Accuracy: 9787/10000 (98%)\n",
            "\n",
            "Train Epoch: 7 [0/60000 (0%)]\tLoss: 0.040439\n",
            "Train Epoch: 7 [640/60000 (1%)]\tLoss: 0.041476\n",
            "Train Epoch: 7 [1280/60000 (2%)]\tLoss: 0.080624\n",
            "Train Epoch: 7 [1920/60000 (3%)]\tLoss: 0.086962\n",
            "Train Epoch: 7 [2560/60000 (4%)]\tLoss: 0.038438\n",
            "Train Epoch: 7 [3200/60000 (5%)]\tLoss: 0.051324\n",
            "Train Epoch: 7 [3840/60000 (6%)]\tLoss: 0.089680\n",
            "Train Epoch: 7 [4480/60000 (7%)]\tLoss: 0.106039\n",
            "Train Epoch: 7 [5120/60000 (9%)]\tLoss: 0.033214\n",
            "Train Epoch: 7 [5760/60000 (10%)]\tLoss: 0.172875\n",
            "Train Epoch: 7 [6400/60000 (11%)]\tLoss: 0.051788\n",
            "Train Epoch: 7 [7040/60000 (12%)]\tLoss: 0.007203\n",
            "Train Epoch: 7 [7680/60000 (13%)]\tLoss: 0.078233\n",
            "Train Epoch: 7 [8320/60000 (14%)]\tLoss: 0.141770\n",
            "Train Epoch: 7 [8960/60000 (15%)]\tLoss: 0.065308\n",
            "Train Epoch: 7 [9600/60000 (16%)]\tLoss: 0.015910\n",
            "Train Epoch: 7 [10240/60000 (17%)]\tLoss: 0.022752\n",
            "Train Epoch: 7 [10880/60000 (18%)]\tLoss: 0.019028\n",
            "Train Epoch: 7 [11520/60000 (19%)]\tLoss: 0.070968\n",
            "Train Epoch: 7 [12160/60000 (20%)]\tLoss: 0.050681\n",
            "Train Epoch: 7 [12800/60000 (21%)]\tLoss: 0.057492\n",
            "Train Epoch: 7 [13440/60000 (22%)]\tLoss: 0.138174\n",
            "Train Epoch: 7 [14080/60000 (23%)]\tLoss: 0.118550\n",
            "Train Epoch: 7 [14720/60000 (25%)]\tLoss: 0.064005\n",
            "Train Epoch: 7 [15360/60000 (26%)]\tLoss: 0.119730\n",
            "Train Epoch: 7 [16000/60000 (27%)]\tLoss: 0.058989\n",
            "Train Epoch: 7 [16640/60000 (28%)]\tLoss: 0.047326\n",
            "Train Epoch: 7 [17280/60000 (29%)]\tLoss: 0.033688\n",
            "Train Epoch: 7 [17920/60000 (30%)]\tLoss: 0.127874\n",
            "Train Epoch: 7 [18560/60000 (31%)]\tLoss: 0.131454\n",
            "Train Epoch: 7 [19200/60000 (32%)]\tLoss: 0.056359\n",
            "Train Epoch: 7 [19840/60000 (33%)]\tLoss: 0.080083\n",
            "Train Epoch: 7 [20480/60000 (34%)]\tLoss: 0.216344\n",
            "Train Epoch: 7 [21120/60000 (35%)]\tLoss: 0.015438\n",
            "Train Epoch: 7 [21760/60000 (36%)]\tLoss: 0.135263\n",
            "Train Epoch: 7 [22400/60000 (37%)]\tLoss: 0.136430\n",
            "Train Epoch: 7 [23040/60000 (38%)]\tLoss: 0.070806\n",
            "Train Epoch: 7 [23680/60000 (39%)]\tLoss: 0.140177\n",
            "Train Epoch: 7 [24320/60000 (41%)]\tLoss: 0.043520\n",
            "Train Epoch: 7 [24960/60000 (42%)]\tLoss: 0.054830\n",
            "Train Epoch: 7 [25600/60000 (43%)]\tLoss: 0.044611\n",
            "Train Epoch: 7 [26240/60000 (44%)]\tLoss: 0.049225\n",
            "Train Epoch: 7 [26880/60000 (45%)]\tLoss: 0.102979\n",
            "Train Epoch: 7 [27520/60000 (46%)]\tLoss: 0.030515\n",
            "Train Epoch: 7 [28160/60000 (47%)]\tLoss: 0.052029\n",
            "Train Epoch: 7 [28800/60000 (48%)]\tLoss: 0.061742\n",
            "Train Epoch: 7 [29440/60000 (49%)]\tLoss: 0.024803\n",
            "Train Epoch: 7 [30080/60000 (50%)]\tLoss: 0.044605\n",
            "Train Epoch: 7 [30720/60000 (51%)]\tLoss: 0.137537\n",
            "Train Epoch: 7 [31360/60000 (52%)]\tLoss: 0.060652\n",
            "Train Epoch: 7 [32000/60000 (53%)]\tLoss: 0.028470\n",
            "Train Epoch: 7 [32640/60000 (54%)]\tLoss: 0.072434\n",
            "Train Epoch: 7 [33280/60000 (55%)]\tLoss: 0.095346\n",
            "Train Epoch: 7 [33920/60000 (57%)]\tLoss: 0.020566\n",
            "Train Epoch: 7 [34560/60000 (58%)]\tLoss: 0.135850\n",
            "Train Epoch: 7 [35200/60000 (59%)]\tLoss: 0.048929\n",
            "Train Epoch: 7 [35840/60000 (60%)]\tLoss: 0.180502\n",
            "Train Epoch: 7 [36480/60000 (61%)]\tLoss: 0.018929\n",
            "Train Epoch: 7 [37120/60000 (62%)]\tLoss: 0.050082\n",
            "Train Epoch: 7 [37760/60000 (63%)]\tLoss: 0.035723\n",
            "Train Epoch: 7 [38400/60000 (64%)]\tLoss: 0.028180\n",
            "Train Epoch: 7 [39040/60000 (65%)]\tLoss: 0.032134\n",
            "Train Epoch: 7 [39680/60000 (66%)]\tLoss: 0.041375\n",
            "Train Epoch: 7 [40320/60000 (67%)]\tLoss: 0.084175\n",
            "Train Epoch: 7 [40960/60000 (68%)]\tLoss: 0.063929\n",
            "Train Epoch: 7 [41600/60000 (69%)]\tLoss: 0.032549\n",
            "Train Epoch: 7 [42240/60000 (70%)]\tLoss: 0.069349\n",
            "Train Epoch: 7 [42880/60000 (71%)]\tLoss: 0.166430\n",
            "Train Epoch: 7 [43520/60000 (72%)]\tLoss: 0.074929\n",
            "Train Epoch: 7 [44160/60000 (74%)]\tLoss: 0.033747\n",
            "Train Epoch: 7 [44800/60000 (75%)]\tLoss: 0.022842\n",
            "Train Epoch: 7 [45440/60000 (76%)]\tLoss: 0.034559\n",
            "Train Epoch: 7 [46080/60000 (77%)]\tLoss: 0.043220\n",
            "Train Epoch: 7 [46720/60000 (78%)]\tLoss: 0.067521\n",
            "Train Epoch: 7 [47360/60000 (79%)]\tLoss: 0.065631\n",
            "Train Epoch: 7 [48000/60000 (80%)]\tLoss: 0.074457\n",
            "Train Epoch: 7 [48640/60000 (81%)]\tLoss: 0.139710\n",
            "Train Epoch: 7 [49280/60000 (82%)]\tLoss: 0.088806\n",
            "Train Epoch: 7 [49920/60000 (83%)]\tLoss: 0.077597\n",
            "Train Epoch: 7 [50560/60000 (84%)]\tLoss: 0.065537\n",
            "Train Epoch: 7 [51200/60000 (85%)]\tLoss: 0.056749\n",
            "Train Epoch: 7 [51840/60000 (86%)]\tLoss: 0.031376\n",
            "Train Epoch: 7 [52480/60000 (87%)]\tLoss: 0.087861\n",
            "Train Epoch: 7 [53120/60000 (88%)]\tLoss: 0.142046\n",
            "Train Epoch: 7 [53760/60000 (90%)]\tLoss: 0.033809\n",
            "Train Epoch: 7 [54400/60000 (91%)]\tLoss: 0.107050\n",
            "Train Epoch: 7 [55040/60000 (92%)]\tLoss: 0.102748\n",
            "Train Epoch: 7 [55680/60000 (93%)]\tLoss: 0.016113\n",
            "Train Epoch: 7 [56320/60000 (94%)]\tLoss: 0.073633\n",
            "Train Epoch: 7 [56960/60000 (95%)]\tLoss: 0.082848\n",
            "Train Epoch: 7 [57600/60000 (96%)]\tLoss: 0.215375\n",
            "Train Epoch: 7 [58240/60000 (97%)]\tLoss: 0.188476\n",
            "Train Epoch: 7 [58880/60000 (98%)]\tLoss: 0.096700\n",
            "Train Epoch: 7 [59520/60000 (99%)]\tLoss: 0.086860\n",
            "\n",
            "Test set: Average loss: 0.0619, Accuracy: 9798/10000 (98%)\n",
            "\n",
            "Train Epoch: 8 [0/60000 (0%)]\tLoss: 0.090989\n",
            "Train Epoch: 8 [640/60000 (1%)]\tLoss: 0.008470\n",
            "Train Epoch: 8 [1280/60000 (2%)]\tLoss: 0.019627\n",
            "Train Epoch: 8 [1920/60000 (3%)]\tLoss: 0.024061\n",
            "Train Epoch: 8 [2560/60000 (4%)]\tLoss: 0.028409\n",
            "Train Epoch: 8 [3200/60000 (5%)]\tLoss: 0.014908\n",
            "Train Epoch: 8 [3840/60000 (6%)]\tLoss: 0.072696\n",
            "Train Epoch: 8 [4480/60000 (7%)]\tLoss: 0.103281\n",
            "Train Epoch: 8 [5120/60000 (9%)]\tLoss: 0.022239\n",
            "Train Epoch: 8 [5760/60000 (10%)]\tLoss: 0.052838\n",
            "Train Epoch: 8 [6400/60000 (11%)]\tLoss: 0.034033\n",
            "Train Epoch: 8 [7040/60000 (12%)]\tLoss: 0.043952\n",
            "Train Epoch: 8 [7680/60000 (13%)]\tLoss: 0.042832\n",
            "Train Epoch: 8 [8320/60000 (14%)]\tLoss: 0.019438\n",
            "Train Epoch: 8 [8960/60000 (15%)]\tLoss: 0.283164\n",
            "Train Epoch: 8 [9600/60000 (16%)]\tLoss: 0.029202\n",
            "Train Epoch: 8 [10240/60000 (17%)]\tLoss: 0.132719\n",
            "Train Epoch: 8 [10880/60000 (18%)]\tLoss: 0.188950\n",
            "Train Epoch: 8 [11520/60000 (19%)]\tLoss: 0.077518\n",
            "Train Epoch: 8 [12160/60000 (20%)]\tLoss: 0.035193\n",
            "Train Epoch: 8 [12800/60000 (21%)]\tLoss: 0.118636\n",
            "Train Epoch: 8 [13440/60000 (22%)]\tLoss: 0.069076\n",
            "Train Epoch: 8 [14080/60000 (23%)]\tLoss: 0.115621\n",
            "Train Epoch: 8 [14720/60000 (25%)]\tLoss: 0.081844\n",
            "Train Epoch: 8 [15360/60000 (26%)]\tLoss: 0.042333\n",
            "Train Epoch: 8 [16000/60000 (27%)]\tLoss: 0.066640\n",
            "Train Epoch: 8 [16640/60000 (28%)]\tLoss: 0.096716\n",
            "Train Epoch: 8 [17280/60000 (29%)]\tLoss: 0.137289\n",
            "Train Epoch: 8 [17920/60000 (30%)]\tLoss: 0.028153\n",
            "Train Epoch: 8 [18560/60000 (31%)]\tLoss: 0.051837\n",
            "Train Epoch: 8 [19200/60000 (32%)]\tLoss: 0.116348\n",
            "Train Epoch: 8 [19840/60000 (33%)]\tLoss: 0.107834\n",
            "Train Epoch: 8 [20480/60000 (34%)]\tLoss: 0.070137\n",
            "Train Epoch: 8 [21120/60000 (35%)]\tLoss: 0.058599\n",
            "Train Epoch: 8 [21760/60000 (36%)]\tLoss: 0.089028\n",
            "Train Epoch: 8 [22400/60000 (37%)]\tLoss: 0.007532\n",
            "Train Epoch: 8 [23040/60000 (38%)]\tLoss: 0.056466\n",
            "Train Epoch: 8 [23680/60000 (39%)]\tLoss: 0.058082\n",
            "Train Epoch: 8 [24320/60000 (41%)]\tLoss: 0.051021\n",
            "Train Epoch: 8 [24960/60000 (42%)]\tLoss: 0.079891\n",
            "Train Epoch: 8 [25600/60000 (43%)]\tLoss: 0.029178\n",
            "Train Epoch: 8 [26240/60000 (44%)]\tLoss: 0.093749\n",
            "Train Epoch: 8 [26880/60000 (45%)]\tLoss: 0.104057\n",
            "Train Epoch: 8 [27520/60000 (46%)]\tLoss: 0.032692\n",
            "Train Epoch: 8 [28160/60000 (47%)]\tLoss: 0.081681\n",
            "Train Epoch: 8 [28800/60000 (48%)]\tLoss: 0.022375\n",
            "Train Epoch: 8 [29440/60000 (49%)]\tLoss: 0.039535\n",
            "Train Epoch: 8 [30080/60000 (50%)]\tLoss: 0.061739\n",
            "Train Epoch: 8 [30720/60000 (51%)]\tLoss: 0.144855\n",
            "Train Epoch: 8 [31360/60000 (52%)]\tLoss: 0.154467\n",
            "Train Epoch: 8 [32000/60000 (53%)]\tLoss: 0.022960\n",
            "Train Epoch: 8 [32640/60000 (54%)]\tLoss: 0.030062\n",
            "Train Epoch: 8 [33280/60000 (55%)]\tLoss: 0.021711\n",
            "Train Epoch: 8 [33920/60000 (57%)]\tLoss: 0.020199\n",
            "Train Epoch: 8 [34560/60000 (58%)]\tLoss: 0.161566\n",
            "Train Epoch: 8 [35200/60000 (59%)]\tLoss: 0.091358\n",
            "Train Epoch: 8 [35840/60000 (60%)]\tLoss: 0.033618\n",
            "Train Epoch: 8 [36480/60000 (61%)]\tLoss: 0.017889\n",
            "Train Epoch: 8 [37120/60000 (62%)]\tLoss: 0.035265\n",
            "Train Epoch: 8 [37760/60000 (63%)]\tLoss: 0.078381\n",
            "Train Epoch: 8 [38400/60000 (64%)]\tLoss: 0.079663\n",
            "Train Epoch: 8 [39040/60000 (65%)]\tLoss: 0.077321\n",
            "Train Epoch: 8 [39680/60000 (66%)]\tLoss: 0.013657\n",
            "Train Epoch: 8 [40320/60000 (67%)]\tLoss: 0.066169\n",
            "Train Epoch: 8 [40960/60000 (68%)]\tLoss: 0.051776\n",
            "Train Epoch: 8 [41600/60000 (69%)]\tLoss: 0.065484\n",
            "Train Epoch: 8 [42240/60000 (70%)]\tLoss: 0.018657\n",
            "Train Epoch: 8 [42880/60000 (71%)]\tLoss: 0.016982\n",
            "Train Epoch: 8 [43520/60000 (72%)]\tLoss: 0.066006\n",
            "Train Epoch: 8 [44160/60000 (74%)]\tLoss: 0.183632\n",
            "Train Epoch: 8 [44800/60000 (75%)]\tLoss: 0.017892\n",
            "Train Epoch: 8 [45440/60000 (76%)]\tLoss: 0.114879\n",
            "Train Epoch: 8 [46080/60000 (77%)]\tLoss: 0.133889\n",
            "Train Epoch: 8 [46720/60000 (78%)]\tLoss: 0.083526\n",
            "Train Epoch: 8 [47360/60000 (79%)]\tLoss: 0.014153\n",
            "Train Epoch: 8 [48000/60000 (80%)]\tLoss: 0.005534\n",
            "Train Epoch: 8 [48640/60000 (81%)]\tLoss: 0.139073\n",
            "Train Epoch: 8 [49280/60000 (82%)]\tLoss: 0.072176\n",
            "Train Epoch: 8 [49920/60000 (83%)]\tLoss: 0.045919\n",
            "Train Epoch: 8 [50560/60000 (84%)]\tLoss: 0.010424\n",
            "Train Epoch: 8 [51200/60000 (85%)]\tLoss: 0.061715\n",
            "Train Epoch: 8 [51840/60000 (86%)]\tLoss: 0.042998\n",
            "Train Epoch: 8 [52480/60000 (87%)]\tLoss: 0.004538\n",
            "Train Epoch: 8 [53120/60000 (88%)]\tLoss: 0.016535\n",
            "Train Epoch: 8 [53760/60000 (90%)]\tLoss: 0.036886\n",
            "Train Epoch: 8 [54400/60000 (91%)]\tLoss: 0.196557\n",
            "Train Epoch: 8 [55040/60000 (92%)]\tLoss: 0.171675\n",
            "Train Epoch: 8 [55680/60000 (93%)]\tLoss: 0.054367\n",
            "Train Epoch: 8 [56320/60000 (94%)]\tLoss: 0.021303\n",
            "Train Epoch: 8 [56960/60000 (95%)]\tLoss: 0.068968\n",
            "Train Epoch: 8 [57600/60000 (96%)]\tLoss: 0.130293\n",
            "Train Epoch: 8 [58240/60000 (97%)]\tLoss: 0.039244\n",
            "Train Epoch: 8 [58880/60000 (98%)]\tLoss: 0.024936\n",
            "Train Epoch: 8 [59520/60000 (99%)]\tLoss: 0.013477\n",
            "\n",
            "Test set: Average loss: 0.0517, Accuracy: 9829/10000 (98%)\n",
            "\n",
            "Train Epoch: 9 [0/60000 (0%)]\tLoss: 0.013349\n",
            "Train Epoch: 9 [640/60000 (1%)]\tLoss: 0.045616\n",
            "Train Epoch: 9 [1280/60000 (2%)]\tLoss: 0.010423\n",
            "Train Epoch: 9 [1920/60000 (3%)]\tLoss: 0.023725\n",
            "Train Epoch: 9 [2560/60000 (4%)]\tLoss: 0.118275\n",
            "Train Epoch: 9 [3200/60000 (5%)]\tLoss: 0.035484\n",
            "Train Epoch: 9 [3840/60000 (6%)]\tLoss: 0.011323\n",
            "Train Epoch: 9 [4480/60000 (7%)]\tLoss: 0.072761\n",
            "Train Epoch: 9 [5120/60000 (9%)]\tLoss: 0.217697\n",
            "Train Epoch: 9 [5760/60000 (10%)]\tLoss: 0.053505\n",
            "Train Epoch: 9 [6400/60000 (11%)]\tLoss: 0.097454\n",
            "Train Epoch: 9 [7040/60000 (12%)]\tLoss: 0.021630\n",
            "Train Epoch: 9 [7680/60000 (13%)]\tLoss: 0.088723\n",
            "Train Epoch: 9 [8320/60000 (14%)]\tLoss: 0.048940\n",
            "Train Epoch: 9 [8960/60000 (15%)]\tLoss: 0.131828\n",
            "Train Epoch: 9 [9600/60000 (16%)]\tLoss: 0.028067\n",
            "Train Epoch: 9 [10240/60000 (17%)]\tLoss: 0.045998\n",
            "Train Epoch: 9 [10880/60000 (18%)]\tLoss: 0.060839\n",
            "Train Epoch: 9 [11520/60000 (19%)]\tLoss: 0.158306\n",
            "Train Epoch: 9 [12160/60000 (20%)]\tLoss: 0.077579\n",
            "Train Epoch: 9 [12800/60000 (21%)]\tLoss: 0.044701\n",
            "Train Epoch: 9 [13440/60000 (22%)]\tLoss: 0.057212\n",
            "Train Epoch: 9 [14080/60000 (23%)]\tLoss: 0.064404\n",
            "Train Epoch: 9 [14720/60000 (25%)]\tLoss: 0.027552\n",
            "Train Epoch: 9 [15360/60000 (26%)]\tLoss: 0.056525\n",
            "Train Epoch: 9 [16000/60000 (27%)]\tLoss: 0.090116\n",
            "Train Epoch: 9 [16640/60000 (28%)]\tLoss: 0.040886\n",
            "Train Epoch: 9 [17280/60000 (29%)]\tLoss: 0.062012\n",
            "Train Epoch: 9 [17920/60000 (30%)]\tLoss: 0.135984\n",
            "Train Epoch: 9 [18560/60000 (31%)]\tLoss: 0.057879\n",
            "Train Epoch: 9 [19200/60000 (32%)]\tLoss: 0.033305\n",
            "Train Epoch: 9 [19840/60000 (33%)]\tLoss: 0.006071\n",
            "Train Epoch: 9 [20480/60000 (34%)]\tLoss: 0.074139\n",
            "Train Epoch: 9 [21120/60000 (35%)]\tLoss: 0.028711\n",
            "Train Epoch: 9 [21760/60000 (36%)]\tLoss: 0.130342\n",
            "Train Epoch: 9 [22400/60000 (37%)]\tLoss: 0.012664\n",
            "Train Epoch: 9 [23040/60000 (38%)]\tLoss: 0.068998\n",
            "Train Epoch: 9 [23680/60000 (39%)]\tLoss: 0.059335\n",
            "Train Epoch: 9 [24320/60000 (41%)]\tLoss: 0.182616\n",
            "Train Epoch: 9 [24960/60000 (42%)]\tLoss: 0.041096\n",
            "Train Epoch: 9 [25600/60000 (43%)]\tLoss: 0.050557\n",
            "Train Epoch: 9 [26240/60000 (44%)]\tLoss: 0.041334\n",
            "Train Epoch: 9 [26880/60000 (45%)]\tLoss: 0.057146\n",
            "Train Epoch: 9 [27520/60000 (46%)]\tLoss: 0.091474\n",
            "Train Epoch: 9 [28160/60000 (47%)]\tLoss: 0.012643\n",
            "Train Epoch: 9 [28800/60000 (48%)]\tLoss: 0.029657\n",
            "Train Epoch: 9 [29440/60000 (49%)]\tLoss: 0.025615\n",
            "Train Epoch: 9 [30080/60000 (50%)]\tLoss: 0.048118\n",
            "Train Epoch: 9 [30720/60000 (51%)]\tLoss: 0.058921\n",
            "Train Epoch: 9 [31360/60000 (52%)]\tLoss: 0.093164\n",
            "Train Epoch: 9 [32000/60000 (53%)]\tLoss: 0.047369\n",
            "Train Epoch: 9 [32640/60000 (54%)]\tLoss: 0.092881\n",
            "Train Epoch: 9 [33280/60000 (55%)]\tLoss: 0.071850\n",
            "Train Epoch: 9 [33920/60000 (57%)]\tLoss: 0.217938\n",
            "Train Epoch: 9 [34560/60000 (58%)]\tLoss: 0.059143\n",
            "Train Epoch: 9 [35200/60000 (59%)]\tLoss: 0.030295\n",
            "Train Epoch: 9 [35840/60000 (60%)]\tLoss: 0.060688\n",
            "Train Epoch: 9 [36480/60000 (61%)]\tLoss: 0.051281\n",
            "Train Epoch: 9 [37120/60000 (62%)]\tLoss: 0.022938\n",
            "Train Epoch: 9 [37760/60000 (63%)]\tLoss: 0.056893\n",
            "Train Epoch: 9 [38400/60000 (64%)]\tLoss: 0.071075\n",
            "Train Epoch: 9 [39040/60000 (65%)]\tLoss: 0.027225\n",
            "Train Epoch: 9 [39680/60000 (66%)]\tLoss: 0.019818\n",
            "Train Epoch: 9 [40320/60000 (67%)]\tLoss: 0.114242\n",
            "Train Epoch: 9 [40960/60000 (68%)]\tLoss: 0.127202\n",
            "Train Epoch: 9 [41600/60000 (69%)]\tLoss: 0.033226\n",
            "Train Epoch: 9 [42240/60000 (70%)]\tLoss: 0.028151\n",
            "Train Epoch: 9 [42880/60000 (71%)]\tLoss: 0.064612\n",
            "Train Epoch: 9 [43520/60000 (72%)]\tLoss: 0.008050\n",
            "Train Epoch: 9 [44160/60000 (74%)]\tLoss: 0.075138\n",
            "Train Epoch: 9 [44800/60000 (75%)]\tLoss: 0.074545\n",
            "Train Epoch: 9 [45440/60000 (76%)]\tLoss: 0.093080\n",
            "Train Epoch: 9 [46080/60000 (77%)]\tLoss: 0.076389\n",
            "Train Epoch: 9 [46720/60000 (78%)]\tLoss: 0.021357\n",
            "Train Epoch: 9 [47360/60000 (79%)]\tLoss: 0.008993\n",
            "Train Epoch: 9 [48000/60000 (80%)]\tLoss: 0.096017\n",
            "Train Epoch: 9 [48640/60000 (81%)]\tLoss: 0.058316\n",
            "Train Epoch: 9 [49280/60000 (82%)]\tLoss: 0.039997\n",
            "Train Epoch: 9 [49920/60000 (83%)]\tLoss: 0.034946\n",
            "Train Epoch: 9 [50560/60000 (84%)]\tLoss: 0.010148\n",
            "Train Epoch: 9 [51200/60000 (85%)]\tLoss: 0.015658\n",
            "Train Epoch: 9 [51840/60000 (86%)]\tLoss: 0.055397\n",
            "Train Epoch: 9 [52480/60000 (87%)]\tLoss: 0.029527\n",
            "Train Epoch: 9 [53120/60000 (88%)]\tLoss: 0.054938\n",
            "Train Epoch: 9 [53760/60000 (90%)]\tLoss: 0.045598\n",
            "Train Epoch: 9 [54400/60000 (91%)]\tLoss: 0.075018\n",
            "Train Epoch: 9 [55040/60000 (92%)]\tLoss: 0.020592\n",
            "Train Epoch: 9 [55680/60000 (93%)]\tLoss: 0.030133\n",
            "Train Epoch: 9 [56320/60000 (94%)]\tLoss: 0.023864\n",
            "Train Epoch: 9 [56960/60000 (95%)]\tLoss: 0.012125\n",
            "Train Epoch: 9 [57600/60000 (96%)]\tLoss: 0.039742\n",
            "Train Epoch: 9 [58240/60000 (97%)]\tLoss: 0.023395\n",
            "Train Epoch: 9 [58880/60000 (98%)]\tLoss: 0.086071\n",
            "Train Epoch: 9 [59520/60000 (99%)]\tLoss: 0.019391\n",
            "\n",
            "Test set: Average loss: 0.0537, Accuracy: 9818/10000 (98%)\n",
            "\n"
          ]
        }
      ]
    }
  ]
}