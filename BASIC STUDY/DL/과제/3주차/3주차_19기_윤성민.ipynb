{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# < 3주차 과제 1 : MLP 마음대로 다뤄보기>\n",
        "- dataset을 임의로 선정해서 직접 분석 해보기(제공한 코드를 활용해서 해보기)\n",
        "- activation functions 중 relu사용시 함수 직접 정의\n",
        "- lr, optimizer 등 바꿔보기\n",
        "- hidden layer/neuron 수를 바꾸기\n",
        "- 전처리도 추가\n",
        "- 모든 시도를 올려주세요!\n",
        "- 제일 높은 acc를 보인 시도를 명시해주세요!\n"
      ],
      "metadata": {
        "id": "sgAYo4nrw2F4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "fX437IL6qbI-"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn, optim\n",
        "from sklearn.datasets import load_breast_cancer, load_wine\n",
        "from torch.utils.data import  TensorDataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 아래 데이터셋 중 원하는 데이터셋 하나를 선택하여, 코드 과제 진행하기!\n",
        "- 1) load_digits() <br>\n",
        "- 2) load_wine()"
      ],
      "metadata": {
        "id": "oxkFzBDNmWNk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# 데이터셋 종류 :\n",
        "# data = load_breast_cancer()\n",
        "data = load_wine()"
      ],
      "metadata": {
        "id": "FywYbfsKtjcR"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input = data.data\n",
        "output = data.target"
      ],
      "metadata": {
        "id": "C2P0hqZ9yBGm"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "torch.manual_seed(777)\n",
        "if device == \"cuda\":\n",
        "  torch.cuda.manual_seed_all(777)"
      ],
      "metadata": {
        "id": "SggpQfSPt85C"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train, x_test, y_train, y_test = train_test_split(input, output, test_size = 0.3, random_state = 42, stratify= data.target, shuffle = True)\n",
        "\n",
        "x_train = torch.FloatTensor(x_train).to(device)\n",
        "y_train = torch.LongTensor(y_train).to(device)\n",
        "x_test = torch.FloatTensor(x_test)\n",
        "y_test = torch.LongTensor(y_test)\n",
        "\n",
        "# 데이터를 tensor로 바꿔주고 gpu 연산이 가능해지도록 gpu에 옮김\n",
        "# label 값을 왜 long 에 옮겨놓는가? loss function이 다르기 때문"
      ],
      "metadata": {
        "id": "bLMzf-2ntYeX"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(x_train[0])\n",
        "print(y_train[0])\n",
        "\n",
        "#input 30개 (속성이 30개)\n",
        "#y의 class는 2개 (양성과 음성)\n",
        "\n",
        "print('\\n',x_train[0].size(), torch.unique(y_train), len(x_train))\n",
        "# wine 데이터의 input 13개, y의 class 는 0, 1, 2로 3개"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "umEdiTZkrVqS",
        "outputId": "9d510084-166e-41d5-e398-942da43d4988"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([1.3750e+01, 1.7300e+00, 2.4100e+00, 1.6000e+01, 8.9000e+01, 2.6000e+00,\n",
            "        2.7600e+00, 2.9000e-01, 1.8100e+00, 5.6000e+00, 1.1500e+00, 2.9000e+00,\n",
            "        1.3200e+03], device='cuda:0')\n",
            "tensor(0, device='cuda:0')\n",
            "\n",
            " torch.Size([13]) tensor([0, 1, 2], device='cuda:0') 124\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "파이토치에서는 데이터셋을 좀 더 쉽게 다룰 수 있도록 유용한 도구로서 torch.utils.data.Dataset과 torch.utils.data.DataLoader를 제공합니다. 이를 사용하면 미니 배치 학습, 데이터 셔플(shuffle), 병렬 처리까지 간단히 수행할 수 있습니다. 기본적인 사용 방법은 Dataset을 정의하고, 이를 DataLoader에 전달하는 것\n",
        "- init : class 에서 객체가 생성되면 바로 실행되는 함수\n",
        "- len : observation 수를 정의하는 함수\n",
        "- getitem : iteration 마다 해당하는 데이터를 돌려주는 함수"
      ],
      "metadata": {
        "id": "combmxzmYFyn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset\n",
        "class CustomDataset(Dataset):\n",
        "  def __init__(self):\n",
        "    self.x_data = x_train\n",
        "    self.y_data = [[y] for y in y_train]\n",
        "#  데이터셋의 전처리를 해주는 부분\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.x_data)\n",
        "#  데이터셋의 길이. 즉, 총 샘플의 수를 적어주는 부분\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    x = torch.FloatTensor(self.x_data[idx]).to(device)\n",
        "    y = torch.LongTensor(self.y_data[idx]).to(device)\n",
        "#  데이터셋에서 특정 1개의 샘플을 가져오는 함수\n",
        "\n",
        "    return x,y"
      ],
      "metadata": {
        "id": "y38TlgXoqV5Z"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 8\n",
        "\n",
        "dataset = CustomDataset()\n",
        "dataloader = DataLoader(dataset, batch_size=batch_size)"
      ],
      "metadata": {
        "id": "x8VHwnuFqino"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# input_layer와 hidden_layer의 노드 개수를 어떻게 바꿔줘야할까?\n",
        "# hidden layer/neuron 수를 바꾸기\n",
        "'''\n",
        "model = nn.Sequential(\n",
        "          nn.Linear(30,398, bias=True),\n",
        "          nn.Sigmoid(),\n",
        "          nn.Linear(398,15, bias=True),\n",
        "          nn.Sigmoid(),\n",
        "          nn.Linear(15,5, bias=True),\n",
        "          nn.Softmax()\n",
        "          ).to(device)\n",
        "'''\n",
        "model = nn.Sequential(\n",
        "          nn.Linear(x_train[0].size()[0],398, bias=True),\n",
        "          nn.Sigmoid(),\n",
        "          nn.Linear(398,15, bias=True),\n",
        "          nn.Sigmoid(),\n",
        "          nn.Linear(15,5, bias=True),\n",
        "          nn.Softmax(dim=1)\n",
        "          ).to(device)"
      ],
      "metadata": {
        "id": "C6V7a4tyq6Jc"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "class로 구현 가능\n",
        "- init : 초기 생성 함수\n",
        "- foward : 순전파(입력값 => 예측값 의 과정)"
      ],
      "metadata": {
        "id": "07uV8RY7Yr_5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "class Model(torch.nn.Module):\n",
        "\n",
        "  def __init__(self):\n",
        "    super(Model, self).__init__()\n",
        "    self.layer1 = nn.Sequential(\n",
        "          nn.Linear(30,398, bias=True), # input_layer = 30, hidden_layer1 = 398\n",
        "          nn.Sigmoid(),\n",
        "        nn.BatchNorm1d(398)\n",
        "    )\n",
        "  # activation function 이용\n",
        "  #   nn.ReLU()\n",
        "  #   nn.tanH()\n",
        "  #   https://pytorch.org/docs/stable/nn.html 그 외에도 여기서 확인 가능함\n",
        "  #   파라미터가 필요하지 않다는 것이 특징\n",
        "\n",
        "  # batch normazliation 1d, 파라미터 값으로 vector의 길이를 전해줌\n",
        "  # 추후에 이미지를 다루게 된다면, 그때는 batch normalization 2d를 이용하게 됨\n",
        "  # 그때는 파라미터 값으로 채널, 가로, 세로 길이를 전달해주게 됨\n",
        "\n",
        "    self.layer2 = nn.Sequential(\n",
        "          nn.Linear(398,15, bias=True), # hidden_layer1 = 398, hidden_layer2 = 15\n",
        "        nn.Sigmoid()\n",
        "    )\n",
        "    self.layer3 = nn.Sequential(\n",
        "          nn.Linear(15,10, bias=True), # hidden_layer1 = 15, hidden_layer2 = 10\n",
        "        nn.Sigmoid()\n",
        "    )\n",
        "    self.layer4 = nn.Sequential(\n",
        "        nn.Linear(10, 5, bias=True), # hidden_layer3 = 10, output_layer = 5\n",
        "        nn.Softmax()\n",
        "    )\n",
        "\n",
        "  def forward(self,x):\n",
        "    output = self.layer1(x)\n",
        "    output = self.layer2(output)\n",
        "    output = self.layer3(output)\n",
        "    output = self.layer4(output)\n",
        "    return output\n",
        "'''\n",
        "print()"
      ],
      "metadata": {
        "id": "a0zLstbMqxEZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "531e3c3a-02b6-44fb-ccda-c5a6dbeefc36"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Model(torch.nn.Module):\n",
        "\n",
        "  def __init__(self):\n",
        "    super(Model, self).__init__()\n",
        "\n",
        "    self.layer1 = nn.Sequential(\n",
        "          nn.Linear(x_train[0].size()[0], 398, bias=True),\n",
        "          nn.Sigmoid(),\n",
        "        nn.BatchNorm1d(398)\n",
        "    )\n",
        "    self.layer2 = nn.Sequential(\n",
        "          nn.Linear(398,15, bias=True),\n",
        "        nn.Sigmoid()\n",
        "    )\n",
        "    self.layer3 = nn.Sequential(\n",
        "          nn.Linear(15,10, bias=True),\n",
        "        nn.Sigmoid()\n",
        "    )\n",
        "    self.layer4 = nn.Sequential(\n",
        "        nn.Linear(10, 5, bias=True),\n",
        "        nn.Softmax(dim=1)\n",
        "    )\n",
        "\n",
        "  def forward(self,x):\n",
        "    output = self.layer1(x)\n",
        "    output = self.layer2(output)\n",
        "    output = self.layer3(output)\n",
        "    output = self.layer4(output)\n",
        "    return output"
      ],
      "metadata": {
        "id": "hF5vBlN5i1K_"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Model(torch.nn.Module):\n",
        "\n",
        "  def __init__(self, nin, n1, n2, n3, n4):\n",
        "    super(Model, self).__init__()\n",
        "\n",
        "    self.layer1 = nn.Sequential(\n",
        "          nn.Linear(nin, n1, bias=True),\n",
        "          nn.Sigmoid(),\n",
        "        nn.BatchNorm1d(n1)\n",
        "    )\n",
        "    self.layer2 = nn.Sequential(\n",
        "          nn.Linear(n1,n2, bias=True),\n",
        "        nn.Sigmoid()\n",
        "    )\n",
        "    self.layer3 = nn.Sequential(\n",
        "          nn.Linear(n2,n3, bias=True),\n",
        "        nn.Sigmoid()\n",
        "    )\n",
        "    self.layer4 = nn.Sequential(\n",
        "        nn.Linear(n3, n4, bias=True),\n",
        "        nn.Softmax(dim=1)\n",
        "    )\n",
        "\n",
        "  def forward(self,x):\n",
        "    output = self.layer1(x)\n",
        "    output = self.layer2(output)\n",
        "    output = self.layer3(output)\n",
        "    output = self.layer4(output)\n",
        "    return output"
      ],
      "metadata": {
        "id": "Tm-zSIfJ1NNT"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def init_weights(layer):\n",
        "    if isinstance(layer, nn.Linear):\n",
        "        torch.nn.init.xavier_uniform_(layer.weight)\n",
        "        layer.bias.data.fill_(0.01)\n",
        "\n",
        "        #xavier사용\n",
        "        # Layer의 weight를 어떤 분포를 가지도록 초기화시켜줌+global minimum찾기 위해서"
      ],
      "metadata": {
        "id": "kqcqqkECrSGK"
      },
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nin = x_train[0].size()[0]\n",
        "nout = 3\n",
        "n1 = 400\n",
        "n2 = 100\n",
        "n3 = 50\n",
        "\n",
        "model = Model(nin, n1, n2, n3, nout).to(device)\n",
        "model.apply(init_weights)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oMDUBFg6rUpw",
        "outputId": "cc1b3583-c22f-4ee5-e787-cd2b804fa5ca"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-71-6196a47a4462>:3: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "  torch.nn.init.xavier_uniform(layer.weight)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Model(\n",
              "  (layer1): Sequential(\n",
              "    (0): Linear(in_features=13, out_features=400, bias=True)\n",
              "    (1): Sigmoid()\n",
              "    (2): BatchNorm1d(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  )\n",
              "  (layer2): Sequential(\n",
              "    (0): Linear(in_features=400, out_features=100, bias=True)\n",
              "    (1): Sigmoid()\n",
              "  )\n",
              "  (layer3): Sequential(\n",
              "    (0): Linear(in_features=100, out_features=50, bias=True)\n",
              "    (1): Sigmoid()\n",
              "  )\n",
              "  (layer4): Sequential(\n",
              "    (0): Linear(in_features=50, out_features=3, bias=True)\n",
              "    (1): Softmax(dim=1)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OwZt5CetrYFb",
        "outputId": "aa8117b9-3be0-4bf4-a5d5-653c1399d543"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model(\n",
            "  (layer1): Sequential(\n",
            "    (0): Linear(in_features=13, out_features=400, bias=True)\n",
            "    (1): Sigmoid()\n",
            "    (2): BatchNorm1d(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  )\n",
            "  (layer2): Sequential(\n",
            "    (0): Linear(in_features=400, out_features=100, bias=True)\n",
            "    (1): Sigmoid()\n",
            "  )\n",
            "  (layer3): Sequential(\n",
            "    (0): Linear(in_features=100, out_features=50, bias=True)\n",
            "    (1): Sigmoid()\n",
            "  )\n",
            "  (layer4): Sequential(\n",
            "    (0): Linear(in_features=50, out_features=3, bias=True)\n",
            "    (1): Softmax(dim=1)\n",
            "  )\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss_fn  = nn.CrossEntropyLoss().to(device)\n",
        "\n",
        "# 여러가지 optimizer 시도해보기\n",
        "# lr 바꿔보기\n",
        "\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
        "\n",
        "\n",
        "# https://pytorch.org/docs/stable/optim.html#module-torch.optim 페이지 참조\n",
        "# optimizer = optim.SGD(model.parameters(), lr=0.1, momentum=0.9)\n",
        "# sgd 등등"
      ],
      "metadata": {
        "id": "AYFp-eTErh7b"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer_classes = [name for name in dir(optim) if name[0].isupper()]\n",
        "print(optimizer_classes)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1B0zw10djukv",
        "outputId": "7c2f54d8-83bf-40b4-b041-90c13aa8fc19"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['ASGD', 'Adadelta', 'Adagrad', 'Adam', 'AdamW', 'Adamax', 'LBFGS', 'NAdam', 'Optimizer', 'RAdam', 'RMSprop', 'Rprop', 'SGD', 'SparseAdam']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "losses = []\n",
        "for epoch in range(100):\n",
        "\n",
        "  optimizer.zero_grad()\n",
        "  hypothesis = model(x_train)\n",
        "\n",
        "  # 비용 함수\n",
        "  cost = loss_fn(hypothesis, y_train)\n",
        "  cost.backward()\n",
        "  optimizer.step()\n",
        "  losses.append(cost.item())\n",
        "\n",
        "  if epoch % 10 == 0:\n",
        "    print(epoch, cost.item())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "90QxHvlIrjS7",
        "outputId": "ad62a713-ebc0-4f54-9f68-5877593a8677"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 1.1359058618545532\n",
            "10 0.8627452850341797\n",
            "20 0.8525475263595581\n",
            "30 0.8464255332946777\n",
            "40 0.7973694205284119\n",
            "50 0.6658904552459717\n",
            "60 0.6376820802688599\n",
            "70 0.6004887819290161\n",
            "80 0.6206172108650208\n",
            "90 0.5982028841972351\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'model의 output은 :  {y_pred[0]}')\n",
        "print(f'argmax를 한 후의 output은 {predicted[0]}')\n",
        "print(f'accuracy는 {accuracy}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vyIKhs3Nr6Ay",
        "outputId": "6fa11321-a2d9-496e-ed9a-2b76724e3773"
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model의 output은 :  [9.9921358e-01 1.0115174e-06 7.8545825e-04]\n",
            "argmax를 한 후의 output은 0\n",
            "accuracy는 0.8888888888888888\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Overfitting 인 건 맞는데... train-valid 나누지 않고 진행하는 상황이고, accuracy 1.0 나온 만큼 2layer 진행할 필요는 없을 것으로 판단."
      ],
      "metadata": {
        "id": "C7rPqSDKdL-x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nin = x_train[0].size()[0]\n",
        "nout = 3\n",
        "n1_ = np.random.randint(200, 501, size=100)\n",
        "n2_ = np.random.randint(80, 151, size=100)\n",
        "n3_ = np.random.randint(20, 71, size=100)\n",
        "\n",
        "acc_list = []\n",
        "\n",
        "for i in range(100):\n",
        "\n",
        "  n1, n2, n3 = n1_[i], n2_[i], n3_[i]\n",
        "  model = Model(nin, n1, n2, n3, nout).to(device)\n",
        "  model.apply(init_weights)\n",
        "\n",
        "  # 'Adagrad', 'Adam', 'AdamW', 'Adamax', 'RAdam', 'RMSprop', 'SGD'\n",
        "\n",
        "  loss_fn  = nn.CrossEntropyLoss().to(device)\n",
        "  optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
        "\n",
        "  losses = []\n",
        "  for epoch in range(100):\n",
        "    optimizer.zero_grad()\n",
        "    hypothesis = model(x_train)\n",
        "    cost = loss_fn(hypothesis, y_train)\n",
        "    cost.backward()\n",
        "    optimizer.step()\n",
        "    losses.append(cost.item())\n",
        "\n",
        "  with torch.no_grad():\n",
        "    model = model.to('cpu')\n",
        "    y_pred = model(x_test)\n",
        "    y_pred = y_pred.detach().numpy()\n",
        "    predicted = np.argmax(y_pred, axis =1)\n",
        "    accuracy = (accuracy_score(predicted, y_test))\n",
        "    acc_list.append(accuracy)\n",
        "  print(f'accuracy는 {accuracy:.4f}')\n",
        "\n",
        "print()\n",
        "idx = np.argmax(acc_list)\n",
        "print(f'best : {np.max(acc_list)},  {n1_[idx]},{n2_[idx]},{n3_[idx]}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H3r5sMyIIGOV",
        "outputId": "71e737d4-e4d3-4de0-8f3c-85d681e1b544"
      },
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy는 0.9815\n",
            "accuracy는 0.9259\n",
            "accuracy는 0.9259\n",
            "accuracy는 0.9630\n",
            "accuracy는 0.7963\n",
            "accuracy는 0.8889\n",
            "accuracy는 0.9259\n",
            "accuracy는 0.9444\n",
            "accuracy는 0.8889\n",
            "accuracy는 0.9259\n",
            "accuracy는 0.9074\n",
            "accuracy는 0.9630\n",
            "accuracy는 0.8148\n",
            "accuracy는 0.9259\n",
            "accuracy는 0.8889\n",
            "accuracy는 0.9259\n",
            "accuracy는 0.8333\n",
            "accuracy는 0.8519\n",
            "accuracy는 0.9259\n",
            "accuracy는 0.9259\n",
            "accuracy는 0.9259\n",
            "accuracy는 0.6481\n",
            "accuracy는 0.7963\n",
            "accuracy는 0.9444\n",
            "accuracy는 0.8704\n",
            "accuracy는 0.9259\n",
            "accuracy는 0.9630\n",
            "accuracy는 0.6111\n",
            "accuracy는 0.8889\n",
            "accuracy는 0.8704\n",
            "accuracy는 0.9259\n",
            "accuracy는 0.9074\n",
            "accuracy는 0.9630\n",
            "accuracy는 0.8704\n",
            "accuracy는 0.8704\n",
            "accuracy는 0.9630\n",
            "accuracy는 0.9630\n",
            "accuracy는 0.9630\n",
            "accuracy는 0.9074\n",
            "accuracy는 0.8519\n",
            "accuracy는 0.8519\n",
            "accuracy는 0.8889\n",
            "accuracy는 0.8333\n",
            "accuracy는 0.9815\n",
            "accuracy는 0.9259\n",
            "accuracy는 0.9630\n",
            "accuracy는 0.9630\n",
            "accuracy는 0.9444\n",
            "accuracy는 0.7778\n",
            "accuracy는 0.9630\n",
            "accuracy는 0.9259\n",
            "accuracy는 0.9630\n",
            "accuracy는 0.9815\n",
            "accuracy는 0.9630\n",
            "accuracy는 0.8519\n",
            "accuracy는 0.9815\n",
            "accuracy는 0.9815\n",
            "accuracy는 0.9630\n",
            "accuracy는 0.7963\n",
            "accuracy는 0.9815\n",
            "accuracy는 0.9630\n",
            "accuracy는 0.9074\n",
            "accuracy는 0.6852\n",
            "accuracy는 0.8704\n",
            "accuracy는 0.9074\n",
            "accuracy는 0.9815\n",
            "accuracy는 0.9259\n",
            "accuracy는 0.9444\n",
            "accuracy는 0.9074\n",
            "accuracy는 0.8704\n",
            "accuracy는 0.9444\n",
            "accuracy는 0.8889\n",
            "accuracy는 1.0000\n",
            "accuracy는 0.8704\n",
            "accuracy는 0.7963\n",
            "accuracy는 0.8704\n",
            "accuracy는 0.9630\n",
            "accuracy는 0.8333\n",
            "accuracy는 0.9259\n",
            "accuracy는 0.9630\n",
            "accuracy는 0.8889\n",
            "accuracy는 0.8519\n",
            "accuracy는 0.6296\n",
            "accuracy는 0.8889\n",
            "accuracy는 0.9259\n",
            "accuracy는 0.9074\n",
            "accuracy는 0.9444\n",
            "accuracy는 0.8519\n",
            "accuracy는 0.8704\n",
            "accuracy는 0.9630\n",
            "accuracy는 0.8333\n",
            "accuracy는 0.9444\n",
            "accuracy는 0.9259\n",
            "accuracy는 0.9815\n",
            "accuracy는 0.9630\n",
            "accuracy는 0.9074\n",
            "accuracy는 0.6111\n",
            "accuracy는 0.9074\n",
            "accuracy는 0.9630\n",
            "accuracy는 0.8704\n",
            "\n",
            "best : 1.0,  303,120,35\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "optim_list = ['Adagrad', 'Adam', 'AdamW', 'Adamax', 'RAdam', 'RMSprop', 'SGD']\n",
        "\n",
        "nin = x_train[0].size()[0]\n",
        "nout = 3\n",
        "n1_ = np.random.randint(280, 321, size=40)\n",
        "n2_ = np.random.randint(100, 141, size=40)\n",
        "n3_ = np.random.randint(25, 46, size=40)\n",
        "\n",
        "acc_dict = {'acc': 0}\n",
        "\n",
        "for idx in range(7):\n",
        "  for i in range(40):\n",
        "    for lr in np.logspace(-3, -2, 10):\n",
        "      n1, n2, n3 = n1_[i], n2_[i], n3_[i]\n",
        "      model = Model(nin, n1, n2, n3, nout).to(device)\n",
        "      model.apply(init_weights)\n",
        "\n",
        "      # 'Adagrad', 'Adam', 'AdamW', 'Adamax', 'RAdam', 'RMSprop', 'SGD'\n",
        "\n",
        "      loss_fn  = nn.CrossEntropyLoss().to(device)\n",
        "      if idx == 0:  optimizer = optim.Adagrad(model.parameters(), lr=lr)\n",
        "      if idx == 1:  optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "      if idx == 2:  optimizer = optim.AdamW(model.parameters(), lr=lr)\n",
        "      if idx == 3:  optimizer = optim.Adamax(model.parameters(), lr=lr)\n",
        "      if idx == 4:  optimizer = optim.RAdam(model.parameters(), lr=lr)\n",
        "      if idx == 5:  optimizer = optim.RMSprop(model.parameters(), lr=lr)\n",
        "      if idx == 6:  optimizer = optim.SGD(model.parameters(), lr=lr)\n",
        "      losses = []\n",
        "      for epoch in range(100):\n",
        "        optimizer.zero_grad()\n",
        "        hypothesis = model(x_train)\n",
        "        cost = loss_fn(hypothesis, y_train)\n",
        "        cost.backward()\n",
        "        optimizer.step()\n",
        "        losses.append(cost.item())\n",
        "\n",
        "      with torch.no_grad():\n",
        "        model = model.to('cpu')\n",
        "        y_pred = model(x_test)\n",
        "        y_pred = y_pred.detach().numpy()\n",
        "        predicted = np.argmax(y_pred, axis =1)\n",
        "        accuracy = (accuracy_score(predicted, y_test))\n",
        "        if acc_dict['acc'] < accuracy:\n",
        "          acc_dict['acc'] = accuracy\n",
        "          acc_dict['optim'] = optim_list[idx]\n",
        "          acc_dict['lr'] = lr\n",
        "          acc_dict['n1'] = n1\n",
        "          acc_dict['n2'] = n2\n",
        "          acc_dict['n3'] = n3\n",
        "          acc_dict['loss'] = losses\n",
        "      #print(f'accuracy는 {accuracy:.4f}')\n",
        "\n",
        "\n",
        "print(f'''best_accuracy : {acc_dict['acc']},\n",
        "      optimizer : {acc_dict['optim']},\n",
        "      learning_rate : {acc_dict['lr']},\n",
        "      hidden_nodes : {acc_dict['n1']}, {acc_dict['n2']}, {acc_dict['n3']},\n",
        "      ''')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p5MNPGJNNe9H",
        "outputId": "6d9431d0-7ae7-43a0-8fd6-c65fa4247797"
      },
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "best_accuracy : 1.0,\n",
            "      optimizer : Adagrad,\n",
            "      learning_rate : 0.003593813663804626,\n",
            "      hidden_nodes : 307, 104, 33,\n",
            "      \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(losses)\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 430
        },
        "id": "81ASYrW7roFM",
        "outputId": "54b24c17-ddd9-4fb5-af52-ebeaf34c8718"
      },
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjUAAAGdCAYAAADqsoKGAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAABDAklEQVR4nO3deXRU9f3/8ddkD0sStkw2lkCAsJOAYHBBChIwIpu2WBTUokWxClpRavH784sUtVXrgtpvtWBdUFRABQRDEBCJ7EGWsAcD2ViTyb7N/f0RGY2MSmCSm0yej3PmnObez9x559Nj5sW9n8ViGIYhAACABs7D7AIAAABcgVADAADcAqEGAAC4BUINAABwC4QaAADgFgg1AADALRBqAACAWyDUAAAAt+BldgF1xW63KzMzU82bN5fFYjG7HAAAcBEMw1B+fr7CwsLk4fHL92IaTajJzMxU27ZtzS4DAABcguPHjysiIuIX2zSaUNO8eXNJVZ0SEBBgcjUAAOBi2Gw2tW3b1vE9/ksaTag5/8gpICCAUAMAQANzMUNHGCgMAADcAqEGAAC4BUINAABwC4QaAADgFgg1AADALRBqAACAWyDUAAAAt0CoAQAAbqHGoWbDhg0aNWqUwsLCZLFYtGzZsl99z7p16xQbGytfX19FRUVp4cKF1c7n5+dr+vTpat++vfz9/TVo0CBt3bq1WhvDMPTEE08oNDRU/v7+GjZsmA4dOlTT8gEAgJuqcagpLCxUnz59NH/+/Itqn5aWpoSEBA0ZMkQpKSmaPn26pkyZotWrVzvaTJkyRYmJiXr77be1e/duDR8+XMOGDVNGRoajzbPPPquXXnpJr7/+ujZv3qymTZsqPj5eJSUlNf0VAACAG7IYhmFc8pstFi1dulRjxoz52TaPPvqoVqxYoT179jiOTZgwQbm5uVq1apWKi4vVvHlzffLJJ0pISHC06devn0aOHKmnnnpKhmEoLCxMDz/8sP785z9LkvLy8mS1WrVw4UJNmDDhV2u12WwKDAxUXl4e2yQAANBA1OT7u9bH1CQnJ2vYsGHVjsXHxys5OVmSVFFRocrKSvn5+VVr4+/vr40bN0qqutuTnZ1d7TqBgYEaOHCg4zoAAKBxq/UNLbOzs2W1Wqsds1qtstlsjrs0cXFxmjNnjrp16yar1apFixYpOTlZUVFRjmucf99Pr3P+3E+VlpaqtLTU8bPNZnPlr+WQlVeshZuOSZJmjexWK58BAAB+Xb2Y/fT222/LMAyFh4fL19dXL730km699VZ5eFx6efPmzVNgYKDj1bZtWxdW/IPC0kr9a/1RvfdNeq1cHwAAXJxaDzUhISHKycmpdiwnJ0cBAQHy9/eXJHXq1Enr169XQUGBjh8/ri1btqi8vFwdO3Z0XOP8+356nfPnfmrWrFnKy8tzvI4fP+7qX02SFBzgK0nKL61QcVllrXwGAAD4dbUeauLi4pSUlFTtWGJiouLi4i5o27RpU4WGhurcuXNavXq1Ro8eLUmKjIxUSEhItevYbDZt3rzZ6XUkydfXVwEBAdVetaG5r5f8vKu68WQ+M7EAADBLjUNNQUGBUlJSlJKSIqlqEG9KSorS06sev8yaNUuTJk1ytJ86daqOHj2qmTNnav/+/Xr11Ve1ePFizZgxw9Fm9erVWrVqldLS0pSYmKghQ4YoOjpad955p6SqWVbTp0/XU089pU8//VS7d+/WpEmTFBYW9oszr+qCxWJRcPOqQc4n80t/pTUAAKgtNR4ovG3bNg0ZMsTx80MPPSRJmjx5shYuXKisrCxHwJGq7rKsWLFCM2bM0IsvvqiIiAi98cYbio+Pd7TJy8vTrFmzdOLECbVs2VLjx4/X3Llz5e3t7Wgzc+ZMFRYW6p577lFubq6uvvpqrVq16oJZU2YIbu6r9LNFOmkj1AAAYJbLWqemIanNdWrue3e7Vu7O1v+M6q47r4p06bUBAGjM6tU6NY0Bj58AADAfocYF2jSvmgHF4ycAAMxDqHGB4POhhtlPAACYhlDjAsEBVY+fTvH4CQAA0xBqXOCHOzWEGgAAzEKocYHzoeZsYZnKKuwmVwMAQONEqHGBFk185OVhkSSdLuBuDQAAZiDUuICHh8UxAyrHxmBhAADMQKhxEcbVAABgLkKNi7RhAT4AAExFqHGR4ICqOzWnePwEAIApCDUuwuMnAADMRahxEfZ/AgDAXIQaF7EGsFUCAABmItS4iONODZtaAgBgCkKNi5wfKHy6oFSVdsPkagAAaHwINS7SqqmPLBbJbkhnCrlbAwBAXSPUuIiXp4daNf1+XA2PoAAAqHOEGhc6P637FDOgAACoc4QaFwpmBhQAAKYh1LiQYwE+Hj8BAFDnCDUuxAJ8AACYh1DjQjx+AgDAPIQaF2L/JwAAzEOocaE2rCoMAIBpCDUu9OMp3YbBqsIAANQlQo0Ltfk+1JRV2pVXXG5yNQAANC6EGhfy8/ZUoL+3JMbVAABQ1wg1LsZaNQAAmINQ42JM6wYAwByEGhdjAT4AAMxBqHExHj8BAGAOQo2LtWnO4ycAAMxAqHGx4AAePwEAYAZCjYv9eAE+AABQdwg1LnY+1OTYePwEAEBdItS42PnHT0VllSoorTC5GgAAGg9CjYs18/VSEx9PSdJJ7tYAAFBnCDW1wDGtm3E1AADUGUJNLWABPgAA6h6hpha0Ob9VAo+fAACoM4SaWtCuZRNJ0pFThSZXAgBA40GoqQW9wgMlSbszcs0tBACARoRQUwvOh5oD2fkqrag0uRoAABoHQk0tiGjhr6Am3iqvNHQgO9/scgAAaBQINbXAYrH86BFUnsnVAADQOBBqaokj1Jwg1AAAUBcINbWEOzUAANQtQk0t6RXxw2DhknIGCwMAUNsINbUkPMhfLZp4q8LOYGEAAOoCoaaWWCwW9YoIkiR9yyMoAABqHaGmFvUKD5Ak7WGwMAAAtY5QU4t6hQdJ4k4NAAB1gVBTi84PFj6Uw2BhAABqG6GmFoUF+qlVUx9V2A2lZtnMLgcAALdGqKlFFotFPb9fr2YPj6AAAKhVhJpadn4Rvm8ZLAwAQK0i1NSy8+NqWFkYAIDaRaipZefv1Bw6WcBgYQAAahGhppaFBvqpdTMfVdoN7WOwMAAAtYZQU8sYLAwAQN0g1NSB3gwWBgCg1hFq6gB3agAAqH2EmjrQ+/uNLQ/m5Cu3qMzcYgAAcFOEmjoQEuin6JDmshvSp7syzS4HAAC3VONQs2HDBo0aNUphYWGyWCxatmzZr75n3bp1io2Nla+vr6KiorRw4cJq5ysrKzV79mxFRkbK399fnTp10pw5c2QYhqPNHXfcIYvFUu01YsSImpZvmt/2bytJWrztuMmVAADgnmocagoLC9WnTx/Nnz//otqnpaUpISFBQ4YMUUpKiqZPn64pU6Zo9erVjjbPPPOMXnvtNb3yyitKTU3VM888o2effVYvv/xytWuNGDFCWVlZjteiRYtqWr5pxsSEy9vToj0ZNu3NZGwNAACu5lXTN4wcOVIjR4686Pavv/66IiMj9dxzz0mSunXrpo0bN+qFF15QfHy8JGnTpk0aPXq0EhISJEkdOnTQokWLtGXLlmrX8vX1VUhISE1LrhdaNvXR9d2tWrk7Wx9uO6EeNwWaXRIAAG6l1sfUJCcna9iwYdWOxcfHKzk52fHzoEGDlJSUpIMHD0qSdu3apY0bN14QntatW6fg4GB17dpV9957r86cOfOzn1taWiqbzVbtZbZbvn8EtSwlQ6UVrC4MAIAr1fhOTU1lZ2fLarVWO2a1WmWz2VRcXCx/f3899thjstlsio6OlqenpyorKzV37lxNnDjR8Z4RI0Zo3LhxioyM1JEjR/SXv/xFI0eOVHJysjw9PS/43Hnz5unJJ5+s7V+vRq7t3EYhAX7KtpUoKfWkbugVanZJAAC4jXox+2nx4sV699139d5772nHjh1666239I9//ENvvfWWo82ECRN00003qVevXhozZoyWL1+urVu3at26dU6vOWvWLOXl5Tlex4+bP0DX08Oi8f3CJTFgGAAAV6v1UBMSEqKcnJxqx3JychQQECB/f39J0iOPPKLHHntMEyZMUK9evXT77bdrxowZmjdv3s9et2PHjmrdurUOHz7s9Lyvr68CAgKqveqDm/tVPYLacPCUsvKKTa4GAAD3UeuhJi4uTklJSdWOJSYmKi4uzvFzUVGRPDyql+Lp6Sm73f6z1z1x4oTOnDmj0NCG9QgnsnVTDejQUnZDWrIjw+xyAABwGzUONQUFBUpJSVFKSoqkqinbKSkpSk9Pl1T12GfSpEmO9lOnTtXRo0c1c+ZM7d+/X6+++qoWL16sGTNmONqMGjVKc+fO1YoVK3Ts2DEtXbpUzz//vMaOHev4zEceeUTffPONjh07pqSkJI0ePVpRUVGOGVQNyS39IyRVPYL68Vo8AADgMhg19OWXXxqSLnhNnjzZMAzDmDx5sjF48OAL3tO3b1/Dx8fH6Nixo7FgwYJq5202m/Hggw8a7dq1M/z8/IyOHTsajz/+uFFaWmoYhmEUFRUZw4cPN9q0aWN4e3sb7du3N+6++24jOzv7ouvOy8szJBl5eXk1/ZVdrqCk3Og++3Oj/aPLjW+OnDa7HAAA6q2afH9bDKNx3Cqw2WwKDAxUXl5evRhf8+hH3+qDbcc1um+YXpwQY3Y5AADUSzX5/q4Xs58ao9uubC9JWvFtFgOGAQBwAUKNSXpFBGpgZEtV2A0t/PqY2eUAANDgEWpMdPc1HSVJ721JV0FphcnVAADQsBFqTPSb6GB1bNNU+SUV+mAri/EBAHA5CDUm8vCwaMrVVXdr/rMxTRWVP78uDwAA+GWEGpONiw1Xq6Y+ysgt1qq92WaXAwBAg0WoMZmft6djJtS/v0pjMT4AAC4RoaYeuD2uvXy8PLTreK62fXfO7HIAAGiQCDX1QOtmvhofW7V79783HDW5GgAAGiZCTT3xh+8HDCem5ujwyQKTqwEAoOEh1NQTUcHNdH13qwxDeinpkNnlAADQ4BBq6pHpwzpLkj77NlMHc/JNrgYAgIaFUFOP9AgL1MieITIM6cU13K0BAKAmCDX1zIPf361ZsTtL+7NtJlcDAEDDQaipZ6JDApTQO1SS9M9E7tYAAHCxCDX10PShnWWxSKv2ZmtvZp7Z5QAA0CAQauqhztbmGtU7TJL0T8bWAABwUQg19dQDQzvLwyIl7svR7hPcrQEA4NcQauqpqOBmGtO3apXhZ1fvN7kaAADqP0JNPfbgsM7y9rToq0On9eWBk2aXAwBAvUaoqcfat2qqOwZ1kCTNXZGq8kq7uQUBAFCPEWrquft/01ktm/ro8MkCLdqSbnY5AADUW4Saei7Q31szru8iSXo+8aDyispNrggAgPqJUNMA3HpFW3WxNlNuUbleWssUbwAAnCHUNABenh56PKG7JOm/yceUdrrQ5IoAAKh/CDUNxOAubTSkaxuVVxr628pUs8sBAKDeIdQ0II8ndJOnh0WJ+3K0jineAABUQ6hpQKKCmzumeD++dI8KSyvMLQgAgHqEUNPAPHR9F4UH+Ssjt1jPfXHQ7HIAAKg3CDUNTFNfL80d21OStGBTmnamnzO5IgAA6gdCTQN0XddgjY0Jl2FIs5bsVlkFKw0DAECoaaBm39hdLZv6aH92vv61/ojZ5QAAYDpCTQPVsqmP/mdU1do1L689rMMnC0yuCAAAcxFqGrCb+oTpuq5tVFZp1yMf7VIFG14CABoxQk0DZrFY9NSYnmru66Wd6bl6MYktFAAAjRehpoGLaNFEfxvXS5L0ypeHlXzkjMkVAQBgDkKNGxjVJ0y/7R8hw5Cmf7BTZwvLzC4JAIA6R6hxE//vph7q2KapcmylmvnRLhmGYXZJAADUKUKNm2ji46WXb42Rj6eH1qSe1H+TvzO7JAAA6hShxo30CAvUX26IliTNXZGqPRl5JlcEAEDdIdS4mcmDOmhYN6vKKu2657/bdCq/1OySAACoE4QaN2OxWPTcb/uoY5umyswr0dR3tqu0otLssgAAqHWEGjcU6O+tNyb1V3M/L23/7pxmL9vDwGEAgNsj1Lipjm2a6ZXfx8rDIi3edkILNx0zuyQAAGoVocaNDe7SRn+5oZskac7yffrq0CmTKwIAoPYQatzcH66O1M39ImQ3pPve3aH92TazSwIAoFYQatycxWLR3LE9NaBDS+WXVGjyf7boxLkis8sCAMDlCDWNgK+Xp/49qb+6Wpsrx1aqSf/ZwlYKAAC3Q6hpJAKbeGvhXVcoLNBPR08V6s6FW1VUVmF2WQAAuAyhphEJDfTXf/8wQEFNvLXreK7ue3eHyivtZpcFAIBLEGoamajg5npz8hXy8/bQugOnNOODFFUQbAAAboBQ0wj1a99Cr03sJ29Pi5Z/m6WZH32rSjuL8wEAGjZCTSM1JDpYL98aK08Pi5bszNCsJd/KTrABADRghJpGbETPEL04oa9j1eG/fsJ2CgCAhotQ08jd2DtMz/+2rywW6b3N6fp/n+4l2AAAGiRCDTQmJlzPju8tSXor+Ts9vmwPj6IAAA0OoQaSpFv6t9WzN/d23LGZ+TGDhwEADQuhBg6/7d9W//xdX3l6WPTR9hOa/kEK69gAABoMQg2qGd03XK/cGiNvT4s+25Wp+9/bobIKgg0AoP4j1OACI3uF6vXb+snH00Or9+bo7v9uU3FZpdllAQDwiwg1cGpoN6vevKO//L09tf7gKd3+5mblFZebXRYAAD+LUIOfdU3nNnpnygAF+Hlp23fnNOH/vtGp/FKzywIAwClCDX5Rv/Yt9cEf49S6ma9Ss2y65fVNOnGuyOyyAAC4AKEGv6pbaIA+nBqn8CB/HTtTpJtfS9bBnHyzywIAoJoah5oNGzZo1KhRCgsLk8Vi0bJly371PevWrVNsbKx8fX0VFRWlhQsXVjtfWVmp2bNnKzIyUv7+/urUqZPmzJlTbWVbwzD0xBNPKDQ0VP7+/ho2bJgOHTpU0/JxiSJbN9XH9w5S5+BmyraV6ObXNmnbsbNmlwUAgEONQ01hYaH69Omj+fPnX1T7tLQ0JSQkaMiQIUpJSdH06dM1ZcoUrV692tHmmWee0WuvvaZXXnlFqampeuaZZ/Tss8/q5ZdfdrR59tln9dJLL+n111/X5s2b1bRpU8XHx6ukpKSmvwIuUUignz6cGqfYdkGylVRo4hublbgvx+yyAACQJFmMy9jox2KxaOnSpRozZszPtnn00Ue1YsUK7dmzx3FswoQJys3N1apVqyRJN954o6xWq958801Hm/Hjx8vf31/vvPOODMNQWFiYHn74Yf35z3+WJOXl5clqtWrhwoWaMGHCr9Zqs9kUGBiovLw8BQQEXOJvDEkqLqvU/e/tUNL+k/KwSPPG9dLvrmhndlkAADdUk+/vWh9Tk5ycrGHDhlU7Fh8fr+TkZMfPgwYNUlJSkg4ePChJ2rVrlzZu3KiRI0dKqrrbk52dXe06gYGBGjhwYLXr/FhpaalsNlu1F1zD38dT/7q9n27pFyG7IT368W69nHSIjTABAKbyqu0PyM7OltVqrXbMarXKZrOpuLhY/v7+euyxx2Sz2RQdHS1PT09VVlZq7ty5mjhxouMa59/30+ucP/dT8+bN05NPPlkLvxEkycvTQ8/e3Fttmvvq1XVH9FziQWXkFmvOmJ7y9mT8OQCg7tWLb5/Fixfr3Xff1XvvvacdO3borbfe0j/+8Q+99dZbl3zNWbNmKS8vz/E6fvy4CyuGVPX4ceaIaM0Z3UMeFun9rcc15a1tKiitMLs0AEAjVOuhJiQkRDk51QeT5uTkKCAgQP7+/pKkRx55RI899pgmTJigXr166fbbb9eMGTM0b948xzXOv++n1zl/7qd8fX0VEBBQ7YXacXtcB/3r9v7y8/bQ+oOn9Lt/JeukjQHcAIC6VeuhJi4uTklJSdWOJSYmKi4uzvFzUVGRPDyql+Lp6Sm7vWojxcjISIWEhFS7js1m0+bNm6tdB+a5vrtVi+6+Uq2a+mhvpk1jX92kA9msZQMAqDs1DjUFBQVKSUlRSkqKpKpBvCkpKUpPT5dU9dhn0qRJjvZTp07V0aNHNXPmTO3fv1+vvvqqFi9erBkzZjjajBo1SnPnztWKFSt07NgxLV26VM8//7zGjh0rqeoxx/Tp0/XUU0/p008/1e7duzVp0iSFhYX94swr1K2Ydi205L5BimzdVBm5xRr/2iatO3DS7LIAAI2FUUNffvmlIemC1+TJkw3DMIzJkycbgwcPvuA9ffv2NXx8fIyOHTsaCxYsqHbeZrMZDz74oNGuXTvDz8/P6Nixo/H4448bpaWljjZ2u92YPXu2YbVaDV9fX2Po0KHGgQMHLrruvLw8Q5KRl5dX018ZNXS2oNS45fVNRvtHlxuRjy033tqUZnZJAIAGqibf35e1Tk1Dwjo1dauswq6/LN2tj7afkCTdMaiDZt/YXZ4eFpMrAwA0JPVqnRo0Tj5eHvr7zb01c0RXSdLCTcd018KtyisuN7kyAIC7ItSg1lgsFt13XZRenRjrmBk1Zv7XOnySAcQAANcj1KDW3dArVB9NHaTwIH+lnS7UmPmbtIY9owAALkaoQZ3oGR6oT+6/SgMiW6qgtEJ3v71NLycdkt3eKIZ0AQDqAKEGdaZ1M1+9O2WgJsW1l2FIzyUe1D1vb2ecDQDAJQg1qFPenh7639E99cz4XvLx8tCa1Bzd9MpGpWax4SgA4PIQamCK313RTh9NjVN4kL++O1Oksa9+rY+/n/4NAMClINTANL0jgrT8T1drcJc2Kim36+EPd+kvS3erpLzS7NIAAA0QoQamatHURwvuuELTh3WWxSK9tzld417dpLTThWaXBgBoYAg1MJ2Hh0XTh3XRW3cOUKumPtqXZdONL32lz3Zlml0aAKABIdSg3ri2SxutfPAaDYhsqcKySv1p0U49zuMoAMBFItSgXrEG+Om9KQN1/5AoWSzSu5vTWYUYAHBRCDWod7w8PfTn+K6Ox1H7s/M16uWvtXjrcTWS/VcBAJeAUIN669oubfT5g9foqqhWKi6v1MyPv9WD76cov4TF+gAAFyLUoF4LDvDT23cN1CPxXeXpYdGnuzKV8NJG7Uw/Z3ZpAIB6hlCDes/Dw6JpQ6K0+I9XKjzIX+lni3Tz68ma/+VhVbJ3FADge4QaNBj92rfUygevUULvUFXaDf199QFNfOMbZeUVm10aAKAeINSgQQn099Yrt8bo7zf3VhMfT31z9KxG/PMrfb47y+zSAAAmI9SgwbFYLLqlf1uteOAa9Y4IVF5xue59d4ce+XCXCkorzC4PAGASQg0arMjWTfXR1EG677pOslikD7ef0A0vfqXt3501uzQAgAkINWjQfLw8NHNEtD64J84xiPiW15P13BcHVFZhN7s8AEAdItTALQyIbKnPp1+jcTHhshvSy2sPa/T8r7Uv02Z2aQCAOkKogdsI8PPW87/rq1d+H6MWTbyVmmXTTa9s1ItrDqm8krs2AODuCDVwOzf2DtMXMwYrvodVFXZDL6w5qDHctQEAt0eogVtq09xXr9/WTy9O6KugJt7am2nTqFc2at7nqSouY9dvAHBHhBq4LYvFotF9w/XFjGs1smeIKu2G/rX+qIb/c73WHzxldnkAABcj1MDtBTf302u39dMbk/orLNBPx88Wa/J/tuiBRTuVYysxuzwAgIsQatBoDOtuVeJDg3XXVZHysEif7srUb/6xTv/ecJSBxADgBiyGYTSKHQFtNpsCAwOVl5engIAAs8uByXafyNPsT/Yo5XiuJCkquJmevKmHropqbW5hAIBqavL9TahBo2W3G/poxwk98/l+nSkskySN7BmiWSO7qV2rJiZXBwCQCDVOEWrwc/KKyvXCmoP6b/Ix2Q3Jx9NDf7gmUtOGRKmZr5fZ5QFAo0aocYJQg1+zP9umOcv36evDZyRJrZv5amZ8V43vFyFPD4vJ1QFA40SocYJQg4thGIaSUk9q7spUpZ0ulCR1Dw3QXxO6aRDjbQCgzhFqnCDUoCbKKuz6b/IxvZh0SPklFZKkYd2smnVDtDq1aWZydQDQeBBqnCDU4FKcLSzTi2sO6p3N6aq0G/LysOi2K9vrgaGd1bKpj9nlAYDbI9Q4QajB5Th8skDzVqYqaf9JSVJzPy/96TdRmjyog3y9PE2uDgDcF6HGCUINXOHrw6f11IpUpWZVbY4Z0cJfj46I1o29Q2WxMJgYAFyNUOMEoQauUmk39PGOE3ruiwPKsZVKkmLaBemvCd3Ur31Lk6sDAPdCqHGCUANXKyqr0L83pOlfG46o6PudvxN6herREdEs3gcALkKocYJQg9py0lai5744qMXbj8v4fvG+yYPa6/4hnRXYxNvs8gCgQSPUOEGoQW1LzbLpbytT9dWh05KkoCbeenBoZ912ZXt5e7J3LABcCkKNE4Qa1AXDMLTu4Cn9bUWqDp0skCRFtm6qx0ZGa3h3K4OJAaCGCDVOEGpQlyoq7fpg23G9kHhQpwuqNsscGNlSs2/srp7hgSZXBwANB6HGCUINzJBfUq7X1x/RG1+lqbTCLotFGhcToUfiuyok0M/s8gCg3iPUOEGogZkycov191X7tSwlU5Lk5+2he67tpKmDO6qJDzuBA8DPIdQ4QahBfZByPFdPLd+nbd+dkyRZA3z1SHy0xsWEy4OdwAHgAoQaJwg1qC8Mw9DK3dl6elWqjp8tliT1Cg/U7Bu7a0Aki/cBwI8Rapwg1KC+KSmv1MJNx/TK2sMqKK3aCfyGXiGaNbKb2rZk8T4AkAg1ThFqUF+dLijV84kH9f6WdNkNycfLQ3dfE6n7rotSU1/G2wBo3Ag1ThBqUN+lZtk0Z/k+bTpyRpIU3NxXj46I1ljG2wBoxAg1ThBq0BAYhqEv9uVo7opUpZ8tkiTFtgvS/47uyfo2ABolQo0ThBo0JKUVlfrPxmN6ee0hFZVVymKRJg5spz8P76qgJj5mlwcAdaYm399sSAPUQ75enrr3uk5a+/B1uqlPmAxDeuebdA35xzp9sDVdjeTfIgBQI4QaoB4LCfTTS7fGaNHdV6qrtbnOFZXr0Y9369Z/f6O004VmlwcA9QqhBmgA4jq10vIHrtbjN3STn7eHvjl6VvH/3KBX1x1WeaXd7PIAoF4g1AANhLenh+6+tqO+mD5Y13RurbIKu55ddUA3vfK1UrNsZpcHAKYj1AANTLtWTfTfuwbouVv6KKiJt1KzbBr9ytd6c2Oa7HbG2gBovAg1QANksVg0vl+E1jw0WEOjg1VWadec5fs0ecEW5dhKzC4PAExBqAEasNbNfPXG5P6aM6an/Lw99NWh0xrxzw1K3JdjdmkAUOcINUADZ7FYdPuV7bX8T1erR1iAzhWV6+7/btNzXxzgcRSARoVQA7iJqODmWnrfVbrzqg6SpJfXHtYf3tqqvOJycwsDgDpCqAHciI+Xh/5nVA+98Ls+8vXy0JcHTmn0Kxt1MCff7NIAoNYRagA3NDYmQh/fO0jhQf46dqZIY+Z/zTgbAG6PUAO4qZ7hgfrsT1frqqhWKiqr1D1vb9OCr9PMLgsAak2NQ82GDRs0atQohYWFyWKxaNmyZb/6nnXr1ik2Nla+vr6KiorSwoULq53v0KGDLBbLBa9p06Y52lx33XUXnJ86dWpNywcalZZNffTWnQP0+4HtZBjSk5/t05Of7VUlA4gBuKEah5rCwkL16dNH8+fPv6j2aWlpSkhI0JAhQ5SSkqLp06drypQpWr16taPN1q1blZWV5XglJiZKkm655ZZq17r77rurtXv22WdrWj7Q6Hh5emjumJ6aNTJakrTg62P649vbVVRWYXJlAOBaXjV9w8iRIzVy5MiLbv/6668rMjJSzz33nCSpW7du2rhxo1544QXFx8dLktq0aVPtPU8//bQ6deqkwYMHVzvepEkThYSE1LRkoNGzWCz64+BOimjRRDMWp2hNao5+969v9Mbk/rIG+JldHgC4RK2PqUlOTtawYcOqHYuPj1dycrLT9mVlZXrnnXd01113yWKxVDv37rvvqnXr1urZs6dmzZqloqKiWqsbcEcJvUO16O6BatnUR7sz8jRm/tfal8m+UQDcQ62HmuzsbFmt1mrHrFarbDabiouLL2i/bNky5ebm6o477qh2/Pe//73eeecdffnll5o1a5befvtt3XbbbT/7uaWlpbLZbNVeAKR+7Vtq6X2D1KlNU2Xllejm1zdpDTOjALiBGj9+qm1vvvmmRo4cqbCwsGrH77nnHsf/7tWrl0JDQzV06FAdOXJEnTp1uuA68+bN05NPPlnr9QINUftWTbXkvqt037vb9fXhM7r77W36a0J33XVVhwvukAJAQ1Hrd2pCQkKUk1P9X4E5OTkKCAiQv79/tePfffed1qxZoylTpvzqdQcOHChJOnz4sNPzs2bNUl5enuN1/PjxS/wNAPcU6O+thXcO0K0DqmZGzVm+T3/+8FsGEANosGo91MTFxSkpKanascTERMXFxV3QdsGCBQoODlZCQsKvXjclJUWSFBoa6vS8r6+vAgICqr0AVOft6aG/je2pvyZ0k4dF+njHCd30ytc6kM0KxAAanhqHmoKCAqWkpDhCRVpamlJSUpSeni6p6g7JpEmTHO2nTp2qo0ePaubMmdq/f79effVVLV68WDNmzKh2XbvdrgULFmjy5Mny8qr+VOzIkSOaM2eOtm/frmPHjunTTz/VpEmTdO2116p37941/RUA/IjFYtGUazrqvbuvVHBzXx0+WaDR8zfqg63pMgzWswHQcNQ41Gzbtk0xMTGKiYmRJD300EOKiYnRE088IUnKyspyBBxJioyM1IoVK5SYmKg+ffroueee0xtvvOGYzn3emjVrlJ6errvuuuuCz/Tx8dGaNWs0fPhwRUdH6+GHH9b48eP12Wef1bR8AD/jyo6ttPLBa3RN59YqKbfr0Y93a/oHKTpXWGZ2aQBwUSxGI/mnmM1mU2BgoPLy8ngUBfwCu93Qa+uP6LkvDshuVK1K/PgN3TQuNpxBxADqXE2+v9n7CUA1Hh4WTRsSpQ+nDlJXa3OdLSzTwx/u0sQ3NuvoqQKzywOAn0WoAeBUv/YttPyBq/XoiGj5eXto05EzGvHPr/T31fuVX1JudnkAcAEePwH4VelnivTXT/Zow8FTkqoeST04tLNuHdBOPl782whA7anJ9zehBsBFMQxDX+zL0TOr9uvoqUJJUodWTfRIfLRG9gyRhwfjbQC4HqHGCUIN4BoVlXZ9sO24Xkg8pNMFpZKkHmEB+vPwrrquaxsGEwNwKUKNE4QawLUKSiv07w1H9cZXR1VYVilJim0XpD8P76pBUa1Nrg6AuyDUOEGoAWrH2cIy/Wv9Eb2VfEwl5XZJ0pUdW+rPw7uqf4eWJlcHoKEj1DhBqAFq10lbiV5dd0TvbU5XWWVVuBncpY0eHt5FvSOCzC0OQINFqHGCUAPUjYzcYr2y9pAWbzuhSnvVn5fru1v18PAuig7hvz0ANUOocYJQA9St784U6sWkQ1q2M0N2Q7JYpNF9wjTj+i5q36qp2eUBaCAINU4QagBzHD5ZoBcSD2rF7ixJkpeHRb+7oq0eGNpZ1gA/k6sDUN8Rapwg1ADm2pORp7+vPqD13y/g5+ftoT9cHak/Du6kAD9vk6sDUF8Rapwg1AD1w+ajZ/Ts6gPa/t05SVKLJt76028667Yr27M6MYALEGqcINQA9Yez1YnbtvTXoyOildArlAX8ADgQapwg1AD1T0WlXYu3ndALaw7qVH7V6sT92rfQ7Bu7q2/bIHOLA1AvEGqcINQA9VdRWYX+b8NR/Wv9URWXV61OPLpvmGaOiFZ4kL/J1QEwE6HGCUINUP9l55XoH18c0Mc7TsgwJF8vD/3x2o764+BOaurrZXZ5AExAqHGCUAM0HHsy8jRn+T5tTjsrSbIG+GpmfLTGxoSzGzjQyBBqnCDUAA2LYRhavTdbc1em6vjZYklS74hAPXFjd/aUAhoRQo0ThBqgYSqtqNSCr4/plbWHVVBaIUlK6B2qx0ZEq23LJiZXB6C2EWqcINQADdup/FI9n3hAH2w9Lrsh+XhVLd5333Wd1JzF+wC3RahxglADuIfULJueWrFPXx8+I0lq3cxHDw7roglXtJW3J4v3Ae6GUOMEoQZwH4ZhaE3qSf1tZarSTlct3texdVM9Et9VI3qGsHgf4EYINU4QagD3U15p16It6XpxzSGdKSyTJMW0C9KjI6J1ZcdWJlcHwBUINU4QagD3lV9Srn9vOKp/f5XmWLxvUKdWenh4F/Vrz0wpoCEj1DhBqAHc30lbiV5ae0gfbD2u8sqqP22Du7TRjOu7sO0C0EARapwg1ACNx4lzRXpl7WF9uP2EKu1Vf+Kujmqte6/rpEGdWjHmBmhACDVOEGqAxue7M4V6KemwlqVkOMJN74hA3Tu4k4b3CJEnqxMD9R6hxglCDdB4nThXpDe+StP7W9NVUm6XJLVv1UST4jrolv4RCmCdG6DeItQ4QagBcKagVG9tOqa3kr9TXnG5JKmJj6du7hehSXEdFBXczOQKAfwUocYJQg2A84rKKrRsZ6YWbkrTwZwCx/ErO7bUrQPaKb5HiPy8PU2sEMB5hBonCDUAfsowDG06ckYLvj6mtftz9P2wGwU18da4mAhNGNBWXazNzS0SaOQINU4QagD8kszcYi3edlwfbD2urLwSx/HeEYEaHxuhm/qEqUVTHxMrBBonQo0ThBoAF6PSbmj9wZNatOW4vtx/UhXf377x9rRoaLRVY2PDdV3XNvL14vEUUBcINU4QagDU1JmCUn2SkqmPd5zQ3kyb43iAn5cSeodqdN9wDejQUh5MDQdqDaHGCUINgMuRmmXT0p0Z+jQlU9m2Hx5PhQb66cbeoRrVJ0y9wgNZ2A9wMUKNE4QaAK5QaTe0Oe2Mlu3M0Oe7s5VfWuE4175VE93YO1Q39g5TdEhzAg7gAoQaJwg1AFytpLxS6w+e0me7MrUmNcexsJ8kdWzTVDf2ClVC7zB1sTYj4ACXiFDjBKEGQG0qLK1Q0v6T+mxXptYfPKWyih8CTlRwM93QM0Q39A5VVyt3cICaINQ4QagBUFfyS8qVlHpSy7/N0oaDp1RWWf0Ozg09QzWyV4i6hwYQcIBfQahxglADwAy2knKtTT2pFbuzLriD065lE43sFaKRPUPVJ4JBxoAzhBonCDUAzFZQWqGk1Bx9vjtb6w6erDYGJyzQT8N7hGhkzxD179CSHcSB7xFqnCDUAKhPisoqtO7AKX2+J1trU3NUWFbpONe6mY+u7x6iET1DFNexlXy8PEysFDAXocYJQg2A+qqkvFIbD53W53uytSY1x7GDuFS10N+wblbF9wzR4C5t2GgTjQ6hxglCDYCGoLzSrm+OntHne7L1xd4cnS4odZzz9/bUkOg2iu8Rot9EB6u5n7eJlQJ1g1DjBKEGQENTaTe0I/2cVu3J1qo92crILXac8/H00FVRrTSiZ4iGdbOqVTNfEysFag+hxglCDYCGzDAM7cmwadXeLH2+J1tHTxU6znlYpAGRLTWiR4iG9whRWJC/iZUCrkWocYJQA8CdHD6Zr893Z2v1vmztybBVO9enbZDie1g1okeIOrZpZlKFgGsQapwg1ABwV8fPFmn13myt3putbd+d04//qnexNtOIHiGK78lif2iYCDVOEGoANAYn80uUuC9Hq/ZkK/nIGVXYf/gT365lE43oGaL4HiGKaRskD9bCQQNAqHGCUAOgsckrKlfS/qqAs/7gKZX+aDXjkAA/xfeomio+oENLeXmyFg7qJ0KNE4QaAI1ZUVmF1p9f7G//SRWUVjjOtWzqo+HdrRrRM0SDOrVmsT/UK4QaJwg1AFCltKJSXx8+rVV7spW4L0fnin5Y7K+5n5euZ7E/1COEGicINQBwoYpKuzanndXne7K0em+OTuX/sNhfEx9P/SY6WDf0CtV1XduoiY+XiZWisSLUOEGoAYBfdn6xv893Z2vVnixl5pU4zvl5e+i6LsG6oXeofhMdrGa+BBzUDUKNE4QaALh4hmFo14k8fb47Syv3ZOn42R9WM/b18tDgLm10Q69QDe3Gdg2oXYQaJwg1AHBpDMPQ3kybVu7O0srdWTp2pshxzuf7gJNAwEEtIdQ4QagBgMtnGIb2Z+dr5e4srdidVW27hvMB58beoRrazcojKrgEocYJQg0AuJZhGDqQk6+V32Zp+U8Cjq+Xh34THayE78fgMMgYl4pQ4wShBgBqz4/v4Cz/Nktpp38IOP7enhraLVij+oQxTRw1RqhxglADAHXDMAzty7Jp+bdZWv5tZrVBxs19vXR9d6tG9QnT1Z1by5uVjPErCDVOEGoAoO4ZhqFvT+Tps12ZWrE7S1k/mibeoom3RvYK1ajeYRoY2ZK9qOAUocYJQg0AmMtuN7Q9/Zw+25WplbuzdLqgzHHOGuCrUb3DNLpvuHqGs5s4fkCocYJQAwD1R0WlXclHz+izXZn6fE+28kt+2IuqY+umGtUnTKP7hqljm2YmVon6gFDjBKEGAOqn0opKrT9wSp/sylRSao5Kyn/YTbx3RKBu6hOmm/qEKTjAz8QqYZaafH/XeITWhg0bNGrUKIWFhclisWjZsmW/+p5169YpNjZWvr6+ioqK0sKFC6ud79ChgywWywWvadOmOdqUlJRo2rRpatWqlZo1a6bx48crJyenpuUDAOoZXy9PDe8Rovm/j9W2v16vF37XR9d1bSNPD4u+PZGnp1ak6sp5SZr4xjdavO24bCXlv35RNEo1DjWFhYXq06eP5s+ff1Ht09LSlJCQoCFDhiglJUXTp0/XlClTtHr1akebrVu3Kisry/FKTEyUJN1yyy2ONjNmzNBnn32mDz/8UOvXr1dmZqbGjRtX0/IBAPVYM18vjY2J0MI7B2jzX4bqf0f3UGy7INkN6evDZzTzo2/V/6k1uu/d7Vq9N1ulFZVml4x65LIeP1ksFi1dulRjxoz52TaPPvqoVqxYoT179jiOTZgwQbm5uVq1apXT90yfPl3Lly/XoUOHZLFYlJeXpzZt2ui9997TzTffLEnav3+/unXrpuTkZF155ZW/WiuPnwCg4Uo/U6RPd2VoWUqmDp8scBwP9PfWDb1CNTYmXP3bt2AGlRuqyfd3rS/xmJycrGHDhlU7Fh8fr+nTpzttX1ZWpnfeeUcPPfSQY/T79u3bVV5eXu060dHRateu3c+GmtLSUpWWljp+ttlsLvhtAABmaNeqie7/TWdNGxKlvZk2fZKSoU93ZSrHVqpFW9K1aEu6woP8NbpvmMbGhKuztbnZJcMEtR5qsrOzZbVaqx2zWq2y2WwqLi6Wv79/tXPLli1Tbm6u7rjjjmrX8PHxUVBQ0AXXyc7Odvq58+bN05NPPumS3wEAUD9YLBb1DA9Uz/BAPTaymzYfPaOlOzO0ak+2MnKL9eq6I3p13RH1CAvQ2JhwBhg3MvVuM44333xTI0eOVFhY2GVdZ9asWXrooYccP9tsNrVt2/ZyywMA1BOeHhYNimqtQVGtNWdMTyWlntSylAytO3BSezNt2ptp099WpuqqqNYa0zdc8T1D2GTTzdX6/7shISEXzFLKyclRQEDABXdpvvvuO61Zs0ZLliy54BplZWXKzc2tdrcmJydHISEhTj/X19dXvr6+rvklAAD1mp+3pxJ6hyqhd6jOFZZpxe4sLduZoW3fndNXh07rq0On9fiy3RrePURjY8N1TVRrebFFg9up9VATFxenlStXVjuWmJiouLi4C9ouWLBAwcHBSkhIqHa8X79+8vb2VlJSksaPHy9JOnDggNLT051eBwDQeLVo6qPbrmyv265sr/QzRVqWkqFlOzN09HShPt2VqU93Zap1Mx+N6lM1/qZXeCArGLuJGs9+Kigo0OHDhyVJMTExev755zVkyBC1bNlS7dq106xZs5SRkaH//ve/kqqmdPfs2VPTpk3TXXfdpbVr1+qBBx7QihUrFB8f77iu3W5XZGSkbr31Vj399NMXfO69996rlStXauHChQoICNCf/vQnSdKmTZsuqm5mPwFA43V+D6qlOzP02a5MnSn8YYuGTm2aalxshEb3DVNEiyYmVglnanVF4XXr1mnIkCEXHJ88ebIWLlyoO+64Q8eOHdO6deuqvWfGjBnat2+fIiIiNHv27GoDgSXpiy++UHx8vA4cOKAuXbpccP2SkhI9/PDDWrRokUpLSxUfH69XX331Zx8//RShBgAgSeWVdn116JSW7szUF3uzVVrxwwrGAyJbalxMuEb2ClWgv7eJVeI8tklwglADAPip/JJyfb4nW8t2Zij56Bmd/0b08fLQ9d2tGhcTrmu7tJE3429MQ6hxglADAPglmbnF+iQlU0t3ntDBnB8W+GvV1Ec39Q3T+NgI9QhjB/G6RqhxglADALgYhmFob6ZNS3Zk6NNdGTpd8MP4my7WZhoXG6ExfcMVEsj6N3WBUOMEoQYAUFPllXZtPHRaH+04ocR9OSr7fvyNxSJd1am1xsWGa0TPEDXxYf2b2kKocYJQAwC4HHnF5fp8d5aW7MjQlmNnHceb+HhqZM9Qje8XrisjW7H/lIsRapwg1AAAXOX42SIt2ZGhJTtP6LszRY7j4UH+GhMTpnGxEerUppmJFboPQo0ThBoAgKsZhqEd6ef00fYMLf82U/klFY5zfdsGaXy/CI3qHaqgJj4mVtmwEWqcINQAAGpTSXml1qTm6OPtJ7Th0GlV2qu+Xn08PTS0W7DGx0ZocFemh9cUocYJQg0AoK6czC/RpymZ+mj7Ce3Pznccb93MRzf1Cdf4fuHqERZoYoUNB6HGCUINAMAM+zJt+njHCX2SUn16eHRIc43/fnuG4ACmh/8cQo0ThBoAgJnKK+3acPCUPt5xQmv2nVRZZdX0cA+LdG2XNhoXG6Hh3a3y8/Y0udL6hVDjBKEGAFBf5BWV67NvM/XxjhPamZ7rON7c10sJvUM1LjZCV3RowerFItQ4RagBANRHR08VaOnODC3ZkaGM3GLH8bYt/TU2JkLjY8PVvlVTEys0F6HGCUINAKA+s9sNbU47qyU7Tmjl7iwVllU6zvVv30LjYiOU0CtUgU0a1+7hhBonCDUAgIaiuKxSX+zL1sc7MrTx0CnZz+8e7umhYd2DNS6m8UwPJ9Q4QagBADREObYSLfv+8dSBnB+mh7ds6qOb+oRpXGy4eoUHuu34G0KNE4QaAEBDZhiG9mVV7R7+SUqmTheUOs5FBTfTuNhwjekbrrAgfxOrdD1CjROEGgCAu6iotOurQ6e1ZGeGvtibrdIf7R4e17GVxsVGaETPEDXzbfi7hxNqnCDUAADcka2kXKt2Z+ujHSe0Je2H3cP9vT0V38OqcbERuiqqtTwb6O7hhBonCDUAAHd3/GyRPkmpGn9z9HSh47g1wFdj+oZrXGyEuoY0N7HCmiPUOEGoAQA0FoZhKOV4rpbsyNBn32Yqt6jcca5HWIDGxUbopj5hatPc18QqLw6hxglCDQCgMSqrsOvLAye1ZMcJrd1/UuWVVV/7nh4WXdu5tcbFRuj6erw9A6HGCUINAKCxO1dYpuW7s7TkF7Zn6N++hTzq0fgbQo0ThBoAAH7wa9szjIsJV4fW5m/PQKhxglADAMCF7HZDW46d354hWwWlFY5z/dq30HiTt2cg1DhBqAEA4Jf97PYMXh66vptV42LDdW2Xut2egVDjBKEGAICLl2Mr0ScpGfp4e/XtGVo19dGoOtyegVDjBKEGAICaMwxDezNtWrozQ5+kZOh0QZnjXFRwM42NCdeYmHCF19L2DIQaJwg1AABcnvPbM3y844QS9+VU255hYGRLjYuN0NiYcJc+nqrJ93fD3xQCAADUCS9PDw2JDtaQ6GDH9gxLdp7QN0fP6pujZ5V2ulDjYyPMq8+0TwYAAA1WgJ+3fntFW/32irY6ca5In6Rkyt/b09Q9pgg1AADgskS0aKJpQ6LMLkN1NycLAACgFhFqAACAWyDUAAAAt0CoAQAAboFQAwAA3AKhBgAAuAVCDQAAcAuEGgAA4BYINQAAwC0QagAAgFsg1AAAALdAqAEAAG6BUAMAANxCo9ml2zAMSZLNZjO5EgAAcLHOf2+f/x7/JY0m1OTn50uS2rZta3IlAACgpvLz8xUYGPiLbSzGxUQfN2C325WZmanmzZvLYrG49No2m01t27bV8ePHFRAQ4NJrozr6uu7Q13WHvq479HXdcVVfG4ah/Px8hYWFycPjl0fNNJo7NR4eHoqIiKjVzwgICOA/kjpCX9cd+rru0Nd1h76uO67o61+7Q3MeA4UBAIBbINQAAAC3QKhxAV9fX/3P//yPfH19zS7F7dHXdYe+rjv0dd2hr+uOGX3daAYKAwAA98adGgAA4BYINQAAwC0QagAAgFsg1AAAALdAqLlM8+fPV4cOHeTn56eBAwdqy5YtZpfU4M2bN09XXHGFmjdvruDgYI0ZM0YHDhyo1qakpETTpk1Tq1at1KxZM40fP145OTkmVew+nn76aVksFk2fPt1xjL52nYyMDN12221q1aqV/P391atXL23bts1x3jAMPfHEEwoNDZW/v7+GDRumQ4cOmVhxw1RZWanZs2crMjJS/v7+6tSpk+bMmVNt7yD6+tJt2LBBo0aNUlhYmCwWi5YtW1bt/MX07dmzZzVx4kQFBAQoKChIf/jDH1RQUHD5xRm4ZO+//77h4+Nj/Oc//zH27t1r3H333UZQUJCRk5NjdmkNWnx8vLFgwQJjz549RkpKinHDDTcY7dq1MwoKChxtpk6darRt29ZISkoytm3bZlx55ZXGoEGDTKy64duyZYvRoUMHo3fv3saDDz7oOE5fu8bZs2eN9u3bG3fccYexefNm4+jRo8bq1auNw4cPO9o8/fTTRmBgoLFs2TJj165dxk033WRERkYaxcXFJlbe8MydO9do1aqVsXz5ciMtLc348MMPjWbNmhkvvviiow19felWrlxpPP7448aSJUsMScbSpUurnb+Yvh0xYoTRp08f45tvvjG++uorIyoqyrj11lsvuzZCzWUYMGCAMW3aNMfPlZWVRlhYmDFv3jwTq3I/J0+eNCQZ69evNwzDMHJzcw1vb2/jww8/dLRJTU01JBnJyclmldmg5efnG507dzYSExONwYMHO0INfe06jz76qHH11Vf/7Hm73W6EhIQYf//73x3HcnNzDV9fX2PRokV1UaLbSEhIMO66665qx8aNG2dMnDjRMAz62pV+Gmoupm/37dtnSDK2bt3qaPP5558bFovFyMjIuKx6ePx0icrKyrR9+3YNGzbMcczDw0PDhg1TcnKyiZW5n7y8PElSy5YtJUnbt29XeXl5tb6Pjo5Wu3bt6PtLNG3aNCUkJFTrU4m+dqVPP/1U/fv31y233KLg4GDFxMTo3//+t+N8WlqasrOzq/V1YGCgBg4cSF/X0KBBg5SUlKSDBw9Kknbt2qWNGzdq5MiRkujr2nQxfZucnKygoCD179/f0WbYsGHy8PDQ5s2bL+vzG82Glq52+vRpVVZWymq1VjtutVq1f/9+k6pyP3a7XdOnT9dVV12lnj17SpKys7Pl4+OjoKCgam2tVquys7NNqLJhe//997Vjxw5t3br1gnP0tescPXpUr732mh566CH95S9/0datW/XAAw/Ix8dHkydPdvSns78p9HXNPPbYY7LZbIqOjpanp6cqKys1d+5cTZw4UZLo61p0MX2bnZ2t4ODgaue9vLzUsmXLy+5/Qg3qtWnTpmnPnj3auHGj2aW4pePHj+vBBx9UYmKi/Pz8zC7HrdntdvXv319/+9vfJEkxMTHas2ePXn/9dU2ePNnk6tzL4sWL9e677+q9995Tjx49lJKSounTpyssLIy+dnM8frpErVu3lqen5wWzQHJychQSEmJSVe7l/vvv1/Lly/Xll18qIiLCcTwkJERlZWXKzc2t1p6+r7nt27fr5MmTio2NlZeXl7y8vLR+/Xq99NJL8vLyktVqpa9dJDQ0VN27d692rFu3bkpPT5ckR3/yN+XyPfLII3rsscc0YcIE9erVS7fffrtmzJihefPmSaKva9PF9G1ISIhOnjxZ7XxFRYXOnj172f1PqLlEPj4+6tevn5KSkhzH7Ha7kpKSFBcXZ2JlDZ9hGLr//vu1dOlSrV27VpGRkdXO9+vXT97e3tX6/sCBA0pPT6fva2jo0KHavXu3UlJSHK/+/ftr4sSJjv9NX7vGVVdddcHSBAcPHlT79u0lSZGRkQoJCanW1zabTZs3b6ava6ioqEgeHtW/3jw9PWW32yXR17XpYvo2Li5Oubm52r59u6PN2rVrZbfbNXDgwMsr4LKGGTdy77//vuHr62ssXLjQ2Ldvn3HPPfcYQUFBRnZ2ttmlNWj33nuvERgYaKxbt87IyspyvIqKihxtpk6darRr185Yu3atsW3bNiMuLs6Ii4szsWr38ePZT4ZBX7vKli1bDC8vL2Pu3LnGoUOHjHfffddo0qSJ8c477zjaPP3000ZQUJDxySefGN9++60xevRophlfgsmTJxvh4eGOKd1LliwxWrdubcycOdPRhr6+dPn5+cbOnTuNnTt3GpKM559/3ti5c6fx3XffGYZxcX07YsQIIyYmxti8ebOxceNGo3Pnzkzprg9efvllo127doaPj48xYMAA45tvvjG7pAZPktPXggULHG2Ki4uN++67z2jRooXRpEkTY+zYsUZWVpZ5RbuRn4Ya+tp1PvvsM6Nnz56Gr6+vER0dbfzf//1ftfN2u92YPXu2YbVaDV9fX2Po0KHGgQMHTKq24bLZbMaDDz5otGvXzvDz8zM6duxoPP7440ZpaamjDX196b788kunf6MnT55sGMbF9e2ZM2eMW2+91WjWrJkREBBg3HnnnUZ+fv5l12YxjB8tsQgAANBAMaYGAAC4BUINAABwC4QaAADgFgg1AADALRBqAACAWyDUAAAAt0CoAQAAboFQAwAA3AKhBgAAuAVCDQAAcAuEGgAA4BYINQAAwC38f6APa1MUy+Q2AAAAAElFTkSuQmCC\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# < 3주차 과제 2 : CNN 맛보기>"
      ],
      "metadata": {
        "id": "3RzRM7xThZV_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from __future__ import print_function\n",
        "import argparse\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.autograd import Variable"
      ],
      "metadata": {
        "id": "56xqgtLxhZw6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training settings\n",
        "\n",
        "batch_size = 64\n",
        "\n",
        "# MNIST Dataset\n",
        "train_dataset = datasets.MNIST(root='./data/',\n",
        "                               train=True,\n",
        "                               transform=transforms.ToTensor(),\n",
        "                               download=True)\n",
        "\n",
        "test_dataset = datasets.MNIST(root='./data/',\n",
        "                              train=False,\n",
        "                              transform=transforms.ToTensor())\n",
        "\n",
        "# Data Loader (Input Pipeline)\n",
        "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
        "                                           batch_size=batch_size,\n",
        "                                           shuffle=True)\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
        "                                          batch_size=batch_size,\n",
        "                                          shuffle=False)"
      ],
      "metadata": {
        "id": "TzkF2bFNhcQ2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f3ee3cac-354d-4d88-9290-baf396c01a62"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9912422/9912422 [00:00<00:00, 151176158.55it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28881/28881 [00:00<00:00, 96753749.06it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1648877/1648877 [00:00<00:00, 47193275.72it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4542/4542 [00:00<00:00, 18370808.84it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(train_dataset), train_dataset[0][0].size()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fi-JmFPwYZkI",
        "outputId": "a39974ce-86fd-4667-c51c-a484eea5e380"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(60000, torch.Size([1, 28, 28]))"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Net(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(Net, self).__init__()\n",
        "    self.conv1 = nn.Conv2d(1, 10, kernel_size = 5)\n",
        "    self.conv2 = nn.Conv2d(10, 20, kernel_size = 5)\n",
        "    self.mp = nn.MaxPool2d(2)\n",
        "    self.fc = nn.Linear(4 * 4 * 20 , 10) ### : 알맞는 input은?\n",
        "\n",
        "  def forward(self, x):\n",
        "    in_size = x.size(0)\n",
        "    x = F.relu(self.mp(self.conv1(x)))\n",
        "    x = F.relu(self.mp(self.conv2(x)))\n",
        "    x = x.view(in_size, -1)\n",
        "    x = self.fc(x)\n",
        "    #return F.log_softmax(x)\n",
        "    '''\n",
        "    <ipython-input-24-5c2d5110dbb5>:15: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
        "      return F.log_softmax(x)\n",
        "\n",
        "    UserWarning 에 따라서 차원을 명시화.\n",
        "    '''\n",
        "    return F.log_softmax(x, dim=1)\n"
      ],
      "metadata": {
        "id": "tLCSvgganBrH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Net()\n",
        "optimizer = optim.SGD(model.parameters(), lr = 0.01, momentum = 0.5)"
      ],
      "metadata": {
        "id": "lkYZ4pUdnUHc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(epoch):\n",
        "  model.train()\n",
        "  for batch_idx, (data, target) in enumerate(train_loader):\n",
        "    data, target = Variable(data), Variable(target)\n",
        "    optimizer.zero_grad()\n",
        "    output = model(data)\n",
        "    loss = F.nll_loss(output, target)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    if batch_idx % 10 == 0:\n",
        "      print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "          epoch, batch_idx * len(data), len(train_loader.dataset),\n",
        "          100. * batch_idx / len(train_loader), loss.item()))"
      ],
      "metadata": {
        "id": "IzUrEM3EnXJb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test():\n",
        "    model.eval() #model.eval() 의 기능은?\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    for data, target in test_loader:\n",
        "        #data, target = Variable(data, volatile=True), Variable(target)\n",
        "        '''\n",
        "         <ipython-input-21-f52337105c2a>:6: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
        "        data, target = Variable(data, volatile=True), Variable(target)\n",
        "\n",
        "        volatile=True 더 이상 사용되지 않음.\n",
        "        with torch.no_grad() 내에서 data, target 변수 불러오는게 타당한 듯 보임.\n",
        "        '''\n",
        "        with torch.no_grad():\n",
        "          data, target = Variable(data), Variable(target)\n",
        "        output = model(data)\n",
        "        #test_loss += F.nll_loss(output, target, size_average=False).data # nll_loss?? / cross entropy loss와의 관계 확인!\n",
        "        '''\n",
        "        /usr/local/lib/python3.10/dist-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
        "        warnings.warn(warning.format(ret))\n",
        "\n",
        "        size_average=False 기능 곧 사진다는 UserWarning\n",
        "        '''\n",
        "        test_loss += F.nll_loss(output, target, reduction='sum').data\n",
        "        pred = output.data.max(1, keepdim=True)[1]\n",
        "        correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
        "\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "        test_loss, correct, len(test_loader.dataset),\n",
        "        100. * correct / len(test_loader.dataset)))"
      ],
      "metadata": {
        "id": "EFi0gYJGn2aa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(1, 10):\n",
        "    train(epoch)\n",
        "    test()"
      ],
      "metadata": {
        "id": "zSvSZb_Bn4Nx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8c4a9ef2-f364-4b3a-cb19-1f6a76a37f12"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.313982\n",
            "Train Epoch: 1 [640/60000 (1%)]\tLoss: 2.311526\n",
            "Train Epoch: 1 [1280/60000 (2%)]\tLoss: 2.286257\n",
            "Train Epoch: 1 [1920/60000 (3%)]\tLoss: 2.280571\n",
            "Train Epoch: 1 [2560/60000 (4%)]\tLoss: 2.264812\n",
            "Train Epoch: 1 [3200/60000 (5%)]\tLoss: 2.244339\n",
            "Train Epoch: 1 [3840/60000 (6%)]\tLoss: 2.228913\n",
            "Train Epoch: 1 [4480/60000 (7%)]\tLoss: 2.153345\n",
            "Train Epoch: 1 [5120/60000 (9%)]\tLoss: 2.126422\n",
            "Train Epoch: 1 [5760/60000 (10%)]\tLoss: 2.076624\n",
            "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 1.884866\n",
            "Train Epoch: 1 [7040/60000 (12%)]\tLoss: 1.676399\n",
            "Train Epoch: 1 [7680/60000 (13%)]\tLoss: 1.519793\n",
            "Train Epoch: 1 [8320/60000 (14%)]\tLoss: 1.371693\n",
            "Train Epoch: 1 [8960/60000 (15%)]\tLoss: 1.059892\n",
            "Train Epoch: 1 [9600/60000 (16%)]\tLoss: 0.897721\n",
            "Train Epoch: 1 [10240/60000 (17%)]\tLoss: 0.931941\n",
            "Train Epoch: 1 [10880/60000 (18%)]\tLoss: 0.697954\n",
            "Train Epoch: 1 [11520/60000 (19%)]\tLoss: 0.595195\n",
            "Train Epoch: 1 [12160/60000 (20%)]\tLoss: 0.586475\n",
            "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.547571\n",
            "Train Epoch: 1 [13440/60000 (22%)]\tLoss: 0.595538\n",
            "Train Epoch: 1 [14080/60000 (23%)]\tLoss: 0.326887\n",
            "Train Epoch: 1 [14720/60000 (25%)]\tLoss: 0.510425\n",
            "Train Epoch: 1 [15360/60000 (26%)]\tLoss: 0.456533\n",
            "Train Epoch: 1 [16000/60000 (27%)]\tLoss: 0.470128\n",
            "Train Epoch: 1 [16640/60000 (28%)]\tLoss: 0.640292\n",
            "Train Epoch: 1 [17280/60000 (29%)]\tLoss: 0.422639\n",
            "Train Epoch: 1 [17920/60000 (30%)]\tLoss: 0.357702\n",
            "Train Epoch: 1 [18560/60000 (31%)]\tLoss: 0.252489\n",
            "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.421499\n",
            "Train Epoch: 1 [19840/60000 (33%)]\tLoss: 0.466167\n",
            "Train Epoch: 1 [20480/60000 (34%)]\tLoss: 0.628551\n",
            "Train Epoch: 1 [21120/60000 (35%)]\tLoss: 0.372429\n",
            "Train Epoch: 1 [21760/60000 (36%)]\tLoss: 0.379611\n",
            "Train Epoch: 1 [22400/60000 (37%)]\tLoss: 0.235826\n",
            "Train Epoch: 1 [23040/60000 (38%)]\tLoss: 0.398342\n",
            "Train Epoch: 1 [23680/60000 (39%)]\tLoss: 0.282820\n",
            "Train Epoch: 1 [24320/60000 (41%)]\tLoss: 0.312084\n",
            "Train Epoch: 1 [24960/60000 (42%)]\tLoss: 0.296135\n",
            "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.328635\n",
            "Train Epoch: 1 [26240/60000 (44%)]\tLoss: 0.378341\n",
            "Train Epoch: 1 [26880/60000 (45%)]\tLoss: 0.323039\n",
            "Train Epoch: 1 [27520/60000 (46%)]\tLoss: 0.259983\n",
            "Train Epoch: 1 [28160/60000 (47%)]\tLoss: 0.323611\n",
            "Train Epoch: 1 [28800/60000 (48%)]\tLoss: 0.233900\n",
            "Train Epoch: 1 [29440/60000 (49%)]\tLoss: 0.322512\n",
            "Train Epoch: 1 [30080/60000 (50%)]\tLoss: 0.405195\n",
            "Train Epoch: 1 [30720/60000 (51%)]\tLoss: 0.333995\n",
            "Train Epoch: 1 [31360/60000 (52%)]\tLoss: 0.317659\n",
            "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.273794\n",
            "Train Epoch: 1 [32640/60000 (54%)]\tLoss: 0.265376\n",
            "Train Epoch: 1 [33280/60000 (55%)]\tLoss: 0.324594\n",
            "Train Epoch: 1 [33920/60000 (57%)]\tLoss: 0.362406\n",
            "Train Epoch: 1 [34560/60000 (58%)]\tLoss: 0.300865\n",
            "Train Epoch: 1 [35200/60000 (59%)]\tLoss: 0.278310\n",
            "Train Epoch: 1 [35840/60000 (60%)]\tLoss: 0.329593\n",
            "Train Epoch: 1 [36480/60000 (61%)]\tLoss: 0.211300\n",
            "Train Epoch: 1 [37120/60000 (62%)]\tLoss: 0.253994\n",
            "Train Epoch: 1 [37760/60000 (63%)]\tLoss: 0.246570\n",
            "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.308352\n",
            "Train Epoch: 1 [39040/60000 (65%)]\tLoss: 0.237471\n",
            "Train Epoch: 1 [39680/60000 (66%)]\tLoss: 0.296365\n",
            "Train Epoch: 1 [40320/60000 (67%)]\tLoss: 0.295912\n",
            "Train Epoch: 1 [40960/60000 (68%)]\tLoss: 0.167078\n",
            "Train Epoch: 1 [41600/60000 (69%)]\tLoss: 0.347237\n",
            "Train Epoch: 1 [42240/60000 (70%)]\tLoss: 0.223941\n",
            "Train Epoch: 1 [42880/60000 (71%)]\tLoss: 0.242104\n",
            "Train Epoch: 1 [43520/60000 (72%)]\tLoss: 0.180169\n",
            "Train Epoch: 1 [44160/60000 (74%)]\tLoss: 0.182501\n",
            "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.144004\n",
            "Train Epoch: 1 [45440/60000 (76%)]\tLoss: 0.226749\n",
            "Train Epoch: 1 [46080/60000 (77%)]\tLoss: 0.273228\n",
            "Train Epoch: 1 [46720/60000 (78%)]\tLoss: 0.211646\n",
            "Train Epoch: 1 [47360/60000 (79%)]\tLoss: 0.351178\n",
            "Train Epoch: 1 [48000/60000 (80%)]\tLoss: 0.174076\n",
            "Train Epoch: 1 [48640/60000 (81%)]\tLoss: 0.102027\n",
            "Train Epoch: 1 [49280/60000 (82%)]\tLoss: 0.240277\n",
            "Train Epoch: 1 [49920/60000 (83%)]\tLoss: 0.146993\n",
            "Train Epoch: 1 [50560/60000 (84%)]\tLoss: 0.152798\n",
            "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.440409\n",
            "Train Epoch: 1 [51840/60000 (86%)]\tLoss: 0.124672\n",
            "Train Epoch: 1 [52480/60000 (87%)]\tLoss: 0.309195\n",
            "Train Epoch: 1 [53120/60000 (88%)]\tLoss: 0.178393\n",
            "Train Epoch: 1 [53760/60000 (90%)]\tLoss: 0.266019\n",
            "Train Epoch: 1 [54400/60000 (91%)]\tLoss: 0.295275\n",
            "Train Epoch: 1 [55040/60000 (92%)]\tLoss: 0.068238\n",
            "Train Epoch: 1 [55680/60000 (93%)]\tLoss: 0.182299\n",
            "Train Epoch: 1 [56320/60000 (94%)]\tLoss: 0.338612\n",
            "Train Epoch: 1 [56960/60000 (95%)]\tLoss: 0.129474\n",
            "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.141493\n",
            "Train Epoch: 1 [58240/60000 (97%)]\tLoss: 0.108032\n",
            "Train Epoch: 1 [58880/60000 (98%)]\tLoss: 0.194955\n",
            "Train Epoch: 1 [59520/60000 (99%)]\tLoss: 0.213474\n",
            "\n",
            "Test set: Average loss: 0.1916, Accuracy: 9430/10000 (94%)\n",
            "\n",
            "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.139326\n",
            "Train Epoch: 2 [640/60000 (1%)]\tLoss: 0.242090\n",
            "Train Epoch: 2 [1280/60000 (2%)]\tLoss: 0.216298\n",
            "Train Epoch: 2 [1920/60000 (3%)]\tLoss: 0.338426\n",
            "Train Epoch: 2 [2560/60000 (4%)]\tLoss: 0.148618\n",
            "Train Epoch: 2 [3200/60000 (5%)]\tLoss: 0.478885\n",
            "Train Epoch: 2 [3840/60000 (6%)]\tLoss: 0.125038\n",
            "Train Epoch: 2 [4480/60000 (7%)]\tLoss: 0.193199\n",
            "Train Epoch: 2 [5120/60000 (9%)]\tLoss: 0.177776\n",
            "Train Epoch: 2 [5760/60000 (10%)]\tLoss: 0.260411\n",
            "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 0.105989\n",
            "Train Epoch: 2 [7040/60000 (12%)]\tLoss: 0.210313\n",
            "Train Epoch: 2 [7680/60000 (13%)]\tLoss: 0.210724\n",
            "Train Epoch: 2 [8320/60000 (14%)]\tLoss: 0.223992\n",
            "Train Epoch: 2 [8960/60000 (15%)]\tLoss: 0.280964\n",
            "Train Epoch: 2 [9600/60000 (16%)]\tLoss: 0.225211\n",
            "Train Epoch: 2 [10240/60000 (17%)]\tLoss: 0.206197\n",
            "Train Epoch: 2 [10880/60000 (18%)]\tLoss: 0.188936\n",
            "Train Epoch: 2 [11520/60000 (19%)]\tLoss: 0.098500\n",
            "Train Epoch: 2 [12160/60000 (20%)]\tLoss: 0.088045\n",
            "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 0.264067\n",
            "Train Epoch: 2 [13440/60000 (22%)]\tLoss: 0.169413\n",
            "Train Epoch: 2 [14080/60000 (23%)]\tLoss: 0.210816\n",
            "Train Epoch: 2 [14720/60000 (25%)]\tLoss: 0.216209\n",
            "Train Epoch: 2 [15360/60000 (26%)]\tLoss: 0.102143\n",
            "Train Epoch: 2 [16000/60000 (27%)]\tLoss: 0.093592\n",
            "Train Epoch: 2 [16640/60000 (28%)]\tLoss: 0.297918\n",
            "Train Epoch: 2 [17280/60000 (29%)]\tLoss: 0.267428\n",
            "Train Epoch: 2 [17920/60000 (30%)]\tLoss: 0.291654\n",
            "Train Epoch: 2 [18560/60000 (31%)]\tLoss: 0.056537\n",
            "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 0.207104\n",
            "Train Epoch: 2 [19840/60000 (33%)]\tLoss: 0.125005\n",
            "Train Epoch: 2 [20480/60000 (34%)]\tLoss: 0.154062\n",
            "Train Epoch: 2 [21120/60000 (35%)]\tLoss: 0.235604\n",
            "Train Epoch: 2 [21760/60000 (36%)]\tLoss: 0.167454\n",
            "Train Epoch: 2 [22400/60000 (37%)]\tLoss: 0.088216\n",
            "Train Epoch: 2 [23040/60000 (38%)]\tLoss: 0.226475\n",
            "Train Epoch: 2 [23680/60000 (39%)]\tLoss: 0.135928\n",
            "Train Epoch: 2 [24320/60000 (41%)]\tLoss: 0.198448\n",
            "Train Epoch: 2 [24960/60000 (42%)]\tLoss: 0.159579\n",
            "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.230981\n",
            "Train Epoch: 2 [26240/60000 (44%)]\tLoss: 0.138343\n",
            "Train Epoch: 2 [26880/60000 (45%)]\tLoss: 0.216212\n",
            "Train Epoch: 2 [27520/60000 (46%)]\tLoss: 0.117686\n",
            "Train Epoch: 2 [28160/60000 (47%)]\tLoss: 0.237253\n",
            "Train Epoch: 2 [28800/60000 (48%)]\tLoss: 0.152152\n",
            "Train Epoch: 2 [29440/60000 (49%)]\tLoss: 0.093059\n",
            "Train Epoch: 2 [30080/60000 (50%)]\tLoss: 0.127493\n",
            "Train Epoch: 2 [30720/60000 (51%)]\tLoss: 0.109021\n",
            "Train Epoch: 2 [31360/60000 (52%)]\tLoss: 0.224117\n",
            "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 0.275377\n",
            "Train Epoch: 2 [32640/60000 (54%)]\tLoss: 0.322492\n",
            "Train Epoch: 2 [33280/60000 (55%)]\tLoss: 0.137079\n",
            "Train Epoch: 2 [33920/60000 (57%)]\tLoss: 0.313554\n",
            "Train Epoch: 2 [34560/60000 (58%)]\tLoss: 0.093088\n",
            "Train Epoch: 2 [35200/60000 (59%)]\tLoss: 0.113279\n",
            "Train Epoch: 2 [35840/60000 (60%)]\tLoss: 0.101933\n",
            "Train Epoch: 2 [36480/60000 (61%)]\tLoss: 0.065997\n",
            "Train Epoch: 2 [37120/60000 (62%)]\tLoss: 0.089643\n",
            "Train Epoch: 2 [37760/60000 (63%)]\tLoss: 0.030694\n",
            "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 0.056886\n",
            "Train Epoch: 2 [39040/60000 (65%)]\tLoss: 0.110827\n",
            "Train Epoch: 2 [39680/60000 (66%)]\tLoss: 0.039385\n",
            "Train Epoch: 2 [40320/60000 (67%)]\tLoss: 0.186145\n",
            "Train Epoch: 2 [40960/60000 (68%)]\tLoss: 0.204293\n",
            "Train Epoch: 2 [41600/60000 (69%)]\tLoss: 0.121882\n",
            "Train Epoch: 2 [42240/60000 (70%)]\tLoss: 0.115983\n",
            "Train Epoch: 2 [42880/60000 (71%)]\tLoss: 0.169195\n",
            "Train Epoch: 2 [43520/60000 (72%)]\tLoss: 0.088355\n",
            "Train Epoch: 2 [44160/60000 (74%)]\tLoss: 0.085045\n",
            "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 0.136773\n",
            "Train Epoch: 2 [45440/60000 (76%)]\tLoss: 0.147413\n",
            "Train Epoch: 2 [46080/60000 (77%)]\tLoss: 0.053087\n",
            "Train Epoch: 2 [46720/60000 (78%)]\tLoss: 0.152863\n",
            "Train Epoch: 2 [47360/60000 (79%)]\tLoss: 0.139815\n",
            "Train Epoch: 2 [48000/60000 (80%)]\tLoss: 0.079554\n",
            "Train Epoch: 2 [48640/60000 (81%)]\tLoss: 0.146854\n",
            "Train Epoch: 2 [49280/60000 (82%)]\tLoss: 0.120611\n",
            "Train Epoch: 2 [49920/60000 (83%)]\tLoss: 0.200528\n",
            "Train Epoch: 2 [50560/60000 (84%)]\tLoss: 0.190755\n",
            "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.062695\n",
            "Train Epoch: 2 [51840/60000 (86%)]\tLoss: 0.195160\n",
            "Train Epoch: 2 [52480/60000 (87%)]\tLoss: 0.113171\n",
            "Train Epoch: 2 [53120/60000 (88%)]\tLoss: 0.103362\n",
            "Train Epoch: 2 [53760/60000 (90%)]\tLoss: 0.056743\n",
            "Train Epoch: 2 [54400/60000 (91%)]\tLoss: 0.067774\n",
            "Train Epoch: 2 [55040/60000 (92%)]\tLoss: 0.069115\n",
            "Train Epoch: 2 [55680/60000 (93%)]\tLoss: 0.093056\n",
            "Train Epoch: 2 [56320/60000 (94%)]\tLoss: 0.198982\n",
            "Train Epoch: 2 [56960/60000 (95%)]\tLoss: 0.043259\n",
            "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 0.076819\n",
            "Train Epoch: 2 [58240/60000 (97%)]\tLoss: 0.118422\n",
            "Train Epoch: 2 [58880/60000 (98%)]\tLoss: 0.189250\n",
            "Train Epoch: 2 [59520/60000 (99%)]\tLoss: 0.165926\n",
            "\n",
            "Test set: Average loss: 0.1180, Accuracy: 9655/10000 (97%)\n",
            "\n",
            "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.219344\n",
            "Train Epoch: 3 [640/60000 (1%)]\tLoss: 0.137879\n",
            "Train Epoch: 3 [1280/60000 (2%)]\tLoss: 0.138341\n",
            "Train Epoch: 3 [1920/60000 (3%)]\tLoss: 0.053980\n",
            "Train Epoch: 3 [2560/60000 (4%)]\tLoss: 0.131098\n",
            "Train Epoch: 3 [3200/60000 (5%)]\tLoss: 0.173205\n",
            "Train Epoch: 3 [3840/60000 (6%)]\tLoss: 0.028295\n",
            "Train Epoch: 3 [4480/60000 (7%)]\tLoss: 0.079482\n",
            "Train Epoch: 3 [5120/60000 (9%)]\tLoss: 0.096884\n",
            "Train Epoch: 3 [5760/60000 (10%)]\tLoss: 0.131378\n",
            "Train Epoch: 3 [6400/60000 (11%)]\tLoss: 0.056246\n",
            "Train Epoch: 3 [7040/60000 (12%)]\tLoss: 0.246160\n",
            "Train Epoch: 3 [7680/60000 (13%)]\tLoss: 0.086220\n",
            "Train Epoch: 3 [8320/60000 (14%)]\tLoss: 0.196060\n",
            "Train Epoch: 3 [8960/60000 (15%)]\tLoss: 0.065640\n",
            "Train Epoch: 3 [9600/60000 (16%)]\tLoss: 0.209659\n",
            "Train Epoch: 3 [10240/60000 (17%)]\tLoss: 0.176972\n",
            "Train Epoch: 3 [10880/60000 (18%)]\tLoss: 0.091060\n",
            "Train Epoch: 3 [11520/60000 (19%)]\tLoss: 0.143150\n",
            "Train Epoch: 3 [12160/60000 (20%)]\tLoss: 0.104467\n",
            "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 0.280527\n",
            "Train Epoch: 3 [13440/60000 (22%)]\tLoss: 0.143920\n",
            "Train Epoch: 3 [14080/60000 (23%)]\tLoss: 0.152059\n",
            "Train Epoch: 3 [14720/60000 (25%)]\tLoss: 0.074658\n",
            "Train Epoch: 3 [15360/60000 (26%)]\tLoss: 0.102975\n",
            "Train Epoch: 3 [16000/60000 (27%)]\tLoss: 0.193258\n",
            "Train Epoch: 3 [16640/60000 (28%)]\tLoss: 0.053958\n",
            "Train Epoch: 3 [17280/60000 (29%)]\tLoss: 0.104672\n",
            "Train Epoch: 3 [17920/60000 (30%)]\tLoss: 0.068511\n",
            "Train Epoch: 3 [18560/60000 (31%)]\tLoss: 0.032821\n",
            "Train Epoch: 3 [19200/60000 (32%)]\tLoss: 0.073747\n",
            "Train Epoch: 3 [19840/60000 (33%)]\tLoss: 0.150985\n",
            "Train Epoch: 3 [20480/60000 (34%)]\tLoss: 0.056718\n",
            "Train Epoch: 3 [21120/60000 (35%)]\tLoss: 0.050616\n",
            "Train Epoch: 3 [21760/60000 (36%)]\tLoss: 0.048075\n",
            "Train Epoch: 3 [22400/60000 (37%)]\tLoss: 0.123645\n",
            "Train Epoch: 3 [23040/60000 (38%)]\tLoss: 0.073055\n",
            "Train Epoch: 3 [23680/60000 (39%)]\tLoss: 0.052983\n",
            "Train Epoch: 3 [24320/60000 (41%)]\tLoss: 0.085183\n",
            "Train Epoch: 3 [24960/60000 (42%)]\tLoss: 0.231476\n",
            "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.095777\n",
            "Train Epoch: 3 [26240/60000 (44%)]\tLoss: 0.222378\n",
            "Train Epoch: 3 [26880/60000 (45%)]\tLoss: 0.137019\n",
            "Train Epoch: 3 [27520/60000 (46%)]\tLoss: 0.020090\n",
            "Train Epoch: 3 [28160/60000 (47%)]\tLoss: 0.241083\n",
            "Train Epoch: 3 [28800/60000 (48%)]\tLoss: 0.048227\n",
            "Train Epoch: 3 [29440/60000 (49%)]\tLoss: 0.206227\n",
            "Train Epoch: 3 [30080/60000 (50%)]\tLoss: 0.097459\n",
            "Train Epoch: 3 [30720/60000 (51%)]\tLoss: 0.042989\n",
            "Train Epoch: 3 [31360/60000 (52%)]\tLoss: 0.267344\n",
            "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 0.091129\n",
            "Train Epoch: 3 [32640/60000 (54%)]\tLoss: 0.131081\n",
            "Train Epoch: 3 [33280/60000 (55%)]\tLoss: 0.080095\n",
            "Train Epoch: 3 [33920/60000 (57%)]\tLoss: 0.035912\n",
            "Train Epoch: 3 [34560/60000 (58%)]\tLoss: 0.074863\n",
            "Train Epoch: 3 [35200/60000 (59%)]\tLoss: 0.075362\n",
            "Train Epoch: 3 [35840/60000 (60%)]\tLoss: 0.141812\n",
            "Train Epoch: 3 [36480/60000 (61%)]\tLoss: 0.068181\n",
            "Train Epoch: 3 [37120/60000 (62%)]\tLoss: 0.056467\n",
            "Train Epoch: 3 [37760/60000 (63%)]\tLoss: 0.056481\n",
            "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 0.148730\n",
            "Train Epoch: 3 [39040/60000 (65%)]\tLoss: 0.057418\n",
            "Train Epoch: 3 [39680/60000 (66%)]\tLoss: 0.118848\n",
            "Train Epoch: 3 [40320/60000 (67%)]\tLoss: 0.175960\n",
            "Train Epoch: 3 [40960/60000 (68%)]\tLoss: 0.240353\n",
            "Train Epoch: 3 [41600/60000 (69%)]\tLoss: 0.030630\n",
            "Train Epoch: 3 [42240/60000 (70%)]\tLoss: 0.071424\n",
            "Train Epoch: 3 [42880/60000 (71%)]\tLoss: 0.093005\n",
            "Train Epoch: 3 [43520/60000 (72%)]\tLoss: 0.072367\n",
            "Train Epoch: 3 [44160/60000 (74%)]\tLoss: 0.277763\n",
            "Train Epoch: 3 [44800/60000 (75%)]\tLoss: 0.185264\n",
            "Train Epoch: 3 [45440/60000 (76%)]\tLoss: 0.184873\n",
            "Train Epoch: 3 [46080/60000 (77%)]\tLoss: 0.043580\n",
            "Train Epoch: 3 [46720/60000 (78%)]\tLoss: 0.038419\n",
            "Train Epoch: 3 [47360/60000 (79%)]\tLoss: 0.062664\n",
            "Train Epoch: 3 [48000/60000 (80%)]\tLoss: 0.100123\n",
            "Train Epoch: 3 [48640/60000 (81%)]\tLoss: 0.101837\n",
            "Train Epoch: 3 [49280/60000 (82%)]\tLoss: 0.245788\n",
            "Train Epoch: 3 [49920/60000 (83%)]\tLoss: 0.092700\n",
            "Train Epoch: 3 [50560/60000 (84%)]\tLoss: 0.203308\n",
            "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.078688\n",
            "Train Epoch: 3 [51840/60000 (86%)]\tLoss: 0.088477\n",
            "Train Epoch: 3 [52480/60000 (87%)]\tLoss: 0.227236\n",
            "Train Epoch: 3 [53120/60000 (88%)]\tLoss: 0.142454\n",
            "Train Epoch: 3 [53760/60000 (90%)]\tLoss: 0.071032\n",
            "Train Epoch: 3 [54400/60000 (91%)]\tLoss: 0.141165\n",
            "Train Epoch: 3 [55040/60000 (92%)]\tLoss: 0.214177\n",
            "Train Epoch: 3 [55680/60000 (93%)]\tLoss: 0.076917\n",
            "Train Epoch: 3 [56320/60000 (94%)]\tLoss: 0.225706\n",
            "Train Epoch: 3 [56960/60000 (95%)]\tLoss: 0.163742\n",
            "Train Epoch: 3 [57600/60000 (96%)]\tLoss: 0.071611\n",
            "Train Epoch: 3 [58240/60000 (97%)]\tLoss: 0.035486\n",
            "Train Epoch: 3 [58880/60000 (98%)]\tLoss: 0.210511\n",
            "Train Epoch: 3 [59520/60000 (99%)]\tLoss: 0.134811\n",
            "\n",
            "Test set: Average loss: 0.0873, Accuracy: 9727/10000 (97%)\n",
            "\n",
            "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.122879\n",
            "Train Epoch: 4 [640/60000 (1%)]\tLoss: 0.288728\n",
            "Train Epoch: 4 [1280/60000 (2%)]\tLoss: 0.345128\n",
            "Train Epoch: 4 [1920/60000 (3%)]\tLoss: 0.091310\n",
            "Train Epoch: 4 [2560/60000 (4%)]\tLoss: 0.039501\n",
            "Train Epoch: 4 [3200/60000 (5%)]\tLoss: 0.142069\n",
            "Train Epoch: 4 [3840/60000 (6%)]\tLoss: 0.065325\n",
            "Train Epoch: 4 [4480/60000 (7%)]\tLoss: 0.060569\n",
            "Train Epoch: 4 [5120/60000 (9%)]\tLoss: 0.027655\n",
            "Train Epoch: 4 [5760/60000 (10%)]\tLoss: 0.080014\n",
            "Train Epoch: 4 [6400/60000 (11%)]\tLoss: 0.040925\n",
            "Train Epoch: 4 [7040/60000 (12%)]\tLoss: 0.186555\n",
            "Train Epoch: 4 [7680/60000 (13%)]\tLoss: 0.082894\n",
            "Train Epoch: 4 [8320/60000 (14%)]\tLoss: 0.078799\n",
            "Train Epoch: 4 [8960/60000 (15%)]\tLoss: 0.112207\n",
            "Train Epoch: 4 [9600/60000 (16%)]\tLoss: 0.211818\n",
            "Train Epoch: 4 [10240/60000 (17%)]\tLoss: 0.124826\n",
            "Train Epoch: 4 [10880/60000 (18%)]\tLoss: 0.069658\n",
            "Train Epoch: 4 [11520/60000 (19%)]\tLoss: 0.087690\n",
            "Train Epoch: 4 [12160/60000 (20%)]\tLoss: 0.080511\n",
            "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 0.079202\n",
            "Train Epoch: 4 [13440/60000 (22%)]\tLoss: 0.086832\n",
            "Train Epoch: 4 [14080/60000 (23%)]\tLoss: 0.033360\n",
            "Train Epoch: 4 [14720/60000 (25%)]\tLoss: 0.019963\n",
            "Train Epoch: 4 [15360/60000 (26%)]\tLoss: 0.022682\n",
            "Train Epoch: 4 [16000/60000 (27%)]\tLoss: 0.053345\n",
            "Train Epoch: 4 [16640/60000 (28%)]\tLoss: 0.101952\n",
            "Train Epoch: 4 [17280/60000 (29%)]\tLoss: 0.153687\n",
            "Train Epoch: 4 [17920/60000 (30%)]\tLoss: 0.160683\n",
            "Train Epoch: 4 [18560/60000 (31%)]\tLoss: 0.116102\n",
            "Train Epoch: 4 [19200/60000 (32%)]\tLoss: 0.059711\n",
            "Train Epoch: 4 [19840/60000 (33%)]\tLoss: 0.223802\n",
            "Train Epoch: 4 [20480/60000 (34%)]\tLoss: 0.072854\n",
            "Train Epoch: 4 [21120/60000 (35%)]\tLoss: 0.120787\n",
            "Train Epoch: 4 [21760/60000 (36%)]\tLoss: 0.037814\n",
            "Train Epoch: 4 [22400/60000 (37%)]\tLoss: 0.049864\n",
            "Train Epoch: 4 [23040/60000 (38%)]\tLoss: 0.140605\n",
            "Train Epoch: 4 [23680/60000 (39%)]\tLoss: 0.097558\n",
            "Train Epoch: 4 [24320/60000 (41%)]\tLoss: 0.060448\n",
            "Train Epoch: 4 [24960/60000 (42%)]\tLoss: 0.090013\n",
            "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 0.114972\n",
            "Train Epoch: 4 [26240/60000 (44%)]\tLoss: 0.020248\n",
            "Train Epoch: 4 [26880/60000 (45%)]\tLoss: 0.103196\n",
            "Train Epoch: 4 [27520/60000 (46%)]\tLoss: 0.168593\n",
            "Train Epoch: 4 [28160/60000 (47%)]\tLoss: 0.189950\n",
            "Train Epoch: 4 [28800/60000 (48%)]\tLoss: 0.190709\n",
            "Train Epoch: 4 [29440/60000 (49%)]\tLoss: 0.049366\n",
            "Train Epoch: 4 [30080/60000 (50%)]\tLoss: 0.161680\n",
            "Train Epoch: 4 [30720/60000 (51%)]\tLoss: 0.073711\n",
            "Train Epoch: 4 [31360/60000 (52%)]\tLoss: 0.018654\n",
            "Train Epoch: 4 [32000/60000 (53%)]\tLoss: 0.136784\n",
            "Train Epoch: 4 [32640/60000 (54%)]\tLoss: 0.195195\n",
            "Train Epoch: 4 [33280/60000 (55%)]\tLoss: 0.100984\n",
            "Train Epoch: 4 [33920/60000 (57%)]\tLoss: 0.038396\n",
            "Train Epoch: 4 [34560/60000 (58%)]\tLoss: 0.066821\n",
            "Train Epoch: 4 [35200/60000 (59%)]\tLoss: 0.068646\n",
            "Train Epoch: 4 [35840/60000 (60%)]\tLoss: 0.265395\n",
            "Train Epoch: 4 [36480/60000 (61%)]\tLoss: 0.109026\n",
            "Train Epoch: 4 [37120/60000 (62%)]\tLoss: 0.032977\n",
            "Train Epoch: 4 [37760/60000 (63%)]\tLoss: 0.124564\n",
            "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 0.098812\n",
            "Train Epoch: 4 [39040/60000 (65%)]\tLoss: 0.081379\n",
            "Train Epoch: 4 [39680/60000 (66%)]\tLoss: 0.099150\n",
            "Train Epoch: 4 [40320/60000 (67%)]\tLoss: 0.023783\n",
            "Train Epoch: 4 [40960/60000 (68%)]\tLoss: 0.081954\n",
            "Train Epoch: 4 [41600/60000 (69%)]\tLoss: 0.050283\n",
            "Train Epoch: 4 [42240/60000 (70%)]\tLoss: 0.026109\n",
            "Train Epoch: 4 [42880/60000 (71%)]\tLoss: 0.026440\n",
            "Train Epoch: 4 [43520/60000 (72%)]\tLoss: 0.057447\n",
            "Train Epoch: 4 [44160/60000 (74%)]\tLoss: 0.139573\n",
            "Train Epoch: 4 [44800/60000 (75%)]\tLoss: 0.097529\n",
            "Train Epoch: 4 [45440/60000 (76%)]\tLoss: 0.184989\n",
            "Train Epoch: 4 [46080/60000 (77%)]\tLoss: 0.121915\n",
            "Train Epoch: 4 [46720/60000 (78%)]\tLoss: 0.107196\n",
            "Train Epoch: 4 [47360/60000 (79%)]\tLoss: 0.017903\n",
            "Train Epoch: 4 [48000/60000 (80%)]\tLoss: 0.055980\n",
            "Train Epoch: 4 [48640/60000 (81%)]\tLoss: 0.102930\n",
            "Train Epoch: 4 [49280/60000 (82%)]\tLoss: 0.138110\n",
            "Train Epoch: 4 [49920/60000 (83%)]\tLoss: 0.035157\n",
            "Train Epoch: 4 [50560/60000 (84%)]\tLoss: 0.067497\n",
            "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 0.061956\n",
            "Train Epoch: 4 [51840/60000 (86%)]\tLoss: 0.055626\n",
            "Train Epoch: 4 [52480/60000 (87%)]\tLoss: 0.121522\n",
            "Train Epoch: 4 [53120/60000 (88%)]\tLoss: 0.168189\n",
            "Train Epoch: 4 [53760/60000 (90%)]\tLoss: 0.054108\n",
            "Train Epoch: 4 [54400/60000 (91%)]\tLoss: 0.129389\n",
            "Train Epoch: 4 [55040/60000 (92%)]\tLoss: 0.041679\n",
            "Train Epoch: 4 [55680/60000 (93%)]\tLoss: 0.053120\n",
            "Train Epoch: 4 [56320/60000 (94%)]\tLoss: 0.094643\n",
            "Train Epoch: 4 [56960/60000 (95%)]\tLoss: 0.058210\n",
            "Train Epoch: 4 [57600/60000 (96%)]\tLoss: 0.015960\n",
            "Train Epoch: 4 [58240/60000 (97%)]\tLoss: 0.053757\n",
            "Train Epoch: 4 [58880/60000 (98%)]\tLoss: 0.147154\n",
            "Train Epoch: 4 [59520/60000 (99%)]\tLoss: 0.088942\n",
            "\n",
            "Test set: Average loss: 0.0885, Accuracy: 9740/10000 (97%)\n",
            "\n",
            "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.064891\n",
            "Train Epoch: 5 [640/60000 (1%)]\tLoss: 0.042225\n",
            "Train Epoch: 5 [1280/60000 (2%)]\tLoss: 0.078221\n",
            "Train Epoch: 5 [1920/60000 (3%)]\tLoss: 0.036886\n",
            "Train Epoch: 5 [2560/60000 (4%)]\tLoss: 0.021217\n",
            "Train Epoch: 5 [3200/60000 (5%)]\tLoss: 0.045910\n",
            "Train Epoch: 5 [3840/60000 (6%)]\tLoss: 0.034984\n",
            "Train Epoch: 5 [4480/60000 (7%)]\tLoss: 0.061691\n",
            "Train Epoch: 5 [5120/60000 (9%)]\tLoss: 0.129444\n",
            "Train Epoch: 5 [5760/60000 (10%)]\tLoss: 0.062799\n",
            "Train Epoch: 5 [6400/60000 (11%)]\tLoss: 0.077629\n",
            "Train Epoch: 5 [7040/60000 (12%)]\tLoss: 0.065649\n",
            "Train Epoch: 5 [7680/60000 (13%)]\tLoss: 0.157224\n",
            "Train Epoch: 5 [8320/60000 (14%)]\tLoss: 0.135770\n",
            "Train Epoch: 5 [8960/60000 (15%)]\tLoss: 0.029113\n",
            "Train Epoch: 5 [9600/60000 (16%)]\tLoss: 0.086480\n",
            "Train Epoch: 5 [10240/60000 (17%)]\tLoss: 0.029360\n",
            "Train Epoch: 5 [10880/60000 (18%)]\tLoss: 0.189163\n",
            "Train Epoch: 5 [11520/60000 (19%)]\tLoss: 0.124906\n",
            "Train Epoch: 5 [12160/60000 (20%)]\tLoss: 0.093955\n",
            "Train Epoch: 5 [12800/60000 (21%)]\tLoss: 0.111232\n",
            "Train Epoch: 5 [13440/60000 (22%)]\tLoss: 0.065808\n",
            "Train Epoch: 5 [14080/60000 (23%)]\tLoss: 0.168601\n",
            "Train Epoch: 5 [14720/60000 (25%)]\tLoss: 0.134650\n",
            "Train Epoch: 5 [15360/60000 (26%)]\tLoss: 0.015246\n",
            "Train Epoch: 5 [16000/60000 (27%)]\tLoss: 0.100950\n",
            "Train Epoch: 5 [16640/60000 (28%)]\tLoss: 0.076631\n",
            "Train Epoch: 5 [17280/60000 (29%)]\tLoss: 0.047951\n",
            "Train Epoch: 5 [17920/60000 (30%)]\tLoss: 0.150691\n",
            "Train Epoch: 5 [18560/60000 (31%)]\tLoss: 0.033617\n",
            "Train Epoch: 5 [19200/60000 (32%)]\tLoss: 0.063360\n",
            "Train Epoch: 5 [19840/60000 (33%)]\tLoss: 0.214478\n",
            "Train Epoch: 5 [20480/60000 (34%)]\tLoss: 0.048096\n",
            "Train Epoch: 5 [21120/60000 (35%)]\tLoss: 0.139591\n",
            "Train Epoch: 5 [21760/60000 (36%)]\tLoss: 0.118565\n",
            "Train Epoch: 5 [22400/60000 (37%)]\tLoss: 0.030053\n",
            "Train Epoch: 5 [23040/60000 (38%)]\tLoss: 0.089270\n",
            "Train Epoch: 5 [23680/60000 (39%)]\tLoss: 0.088645\n",
            "Train Epoch: 5 [24320/60000 (41%)]\tLoss: 0.080570\n",
            "Train Epoch: 5 [24960/60000 (42%)]\tLoss: 0.016062\n",
            "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 0.108477\n",
            "Train Epoch: 5 [26240/60000 (44%)]\tLoss: 0.147607\n",
            "Train Epoch: 5 [26880/60000 (45%)]\tLoss: 0.009564\n",
            "Train Epoch: 5 [27520/60000 (46%)]\tLoss: 0.041442\n",
            "Train Epoch: 5 [28160/60000 (47%)]\tLoss: 0.088591\n",
            "Train Epoch: 5 [28800/60000 (48%)]\tLoss: 0.048655\n",
            "Train Epoch: 5 [29440/60000 (49%)]\tLoss: 0.157759\n",
            "Train Epoch: 5 [30080/60000 (50%)]\tLoss: 0.018836\n",
            "Train Epoch: 5 [30720/60000 (51%)]\tLoss: 0.089817\n",
            "Train Epoch: 5 [31360/60000 (52%)]\tLoss: 0.074846\n",
            "Train Epoch: 5 [32000/60000 (53%)]\tLoss: 0.043135\n",
            "Train Epoch: 5 [32640/60000 (54%)]\tLoss: 0.120812\n",
            "Train Epoch: 5 [33280/60000 (55%)]\tLoss: 0.067278\n",
            "Train Epoch: 5 [33920/60000 (57%)]\tLoss: 0.034292\n",
            "Train Epoch: 5 [34560/60000 (58%)]\tLoss: 0.054398\n",
            "Train Epoch: 5 [35200/60000 (59%)]\tLoss: 0.058702\n",
            "Train Epoch: 5 [35840/60000 (60%)]\tLoss: 0.163743\n",
            "Train Epoch: 5 [36480/60000 (61%)]\tLoss: 0.088162\n",
            "Train Epoch: 5 [37120/60000 (62%)]\tLoss: 0.021812\n",
            "Train Epoch: 5 [37760/60000 (63%)]\tLoss: 0.174584\n",
            "Train Epoch: 5 [38400/60000 (64%)]\tLoss: 0.174077\n",
            "Train Epoch: 5 [39040/60000 (65%)]\tLoss: 0.149493\n",
            "Train Epoch: 5 [39680/60000 (66%)]\tLoss: 0.069635\n",
            "Train Epoch: 5 [40320/60000 (67%)]\tLoss: 0.335645\n",
            "Train Epoch: 5 [40960/60000 (68%)]\tLoss: 0.024168\n",
            "Train Epoch: 5 [41600/60000 (69%)]\tLoss: 0.142934\n",
            "Train Epoch: 5 [42240/60000 (70%)]\tLoss: 0.019399\n",
            "Train Epoch: 5 [42880/60000 (71%)]\tLoss: 0.018331\n",
            "Train Epoch: 5 [43520/60000 (72%)]\tLoss: 0.080945\n",
            "Train Epoch: 5 [44160/60000 (74%)]\tLoss: 0.129123\n",
            "Train Epoch: 5 [44800/60000 (75%)]\tLoss: 0.070774\n",
            "Train Epoch: 5 [45440/60000 (76%)]\tLoss: 0.055742\n",
            "Train Epoch: 5 [46080/60000 (77%)]\tLoss: 0.067804\n",
            "Train Epoch: 5 [46720/60000 (78%)]\tLoss: 0.020145\n",
            "Train Epoch: 5 [47360/60000 (79%)]\tLoss: 0.034570\n",
            "Train Epoch: 5 [48000/60000 (80%)]\tLoss: 0.030283\n",
            "Train Epoch: 5 [48640/60000 (81%)]\tLoss: 0.014685\n",
            "Train Epoch: 5 [49280/60000 (82%)]\tLoss: 0.028777\n",
            "Train Epoch: 5 [49920/60000 (83%)]\tLoss: 0.016848\n",
            "Train Epoch: 5 [50560/60000 (84%)]\tLoss: 0.069179\n",
            "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 0.030386\n",
            "Train Epoch: 5 [51840/60000 (86%)]\tLoss: 0.074363\n",
            "Train Epoch: 5 [52480/60000 (87%)]\tLoss: 0.115093\n",
            "Train Epoch: 5 [53120/60000 (88%)]\tLoss: 0.131431\n",
            "Train Epoch: 5 [53760/60000 (90%)]\tLoss: 0.182281\n",
            "Train Epoch: 5 [54400/60000 (91%)]\tLoss: 0.104292\n",
            "Train Epoch: 5 [55040/60000 (92%)]\tLoss: 0.135401\n",
            "Train Epoch: 5 [55680/60000 (93%)]\tLoss: 0.201579\n",
            "Train Epoch: 5 [56320/60000 (94%)]\tLoss: 0.246662\n",
            "Train Epoch: 5 [56960/60000 (95%)]\tLoss: 0.032050\n",
            "Train Epoch: 5 [57600/60000 (96%)]\tLoss: 0.018663\n",
            "Train Epoch: 5 [58240/60000 (97%)]\tLoss: 0.069953\n",
            "Train Epoch: 5 [58880/60000 (98%)]\tLoss: 0.095697\n",
            "Train Epoch: 5 [59520/60000 (99%)]\tLoss: 0.006416\n",
            "\n",
            "Test set: Average loss: 0.0677, Accuracy: 9792/10000 (98%)\n",
            "\n",
            "Train Epoch: 6 [0/60000 (0%)]\tLoss: 0.168054\n",
            "Train Epoch: 6 [640/60000 (1%)]\tLoss: 0.021044\n",
            "Train Epoch: 6 [1280/60000 (2%)]\tLoss: 0.026772\n",
            "Train Epoch: 6 [1920/60000 (3%)]\tLoss: 0.052324\n",
            "Train Epoch: 6 [2560/60000 (4%)]\tLoss: 0.097353\n",
            "Train Epoch: 6 [3200/60000 (5%)]\tLoss: 0.120327\n",
            "Train Epoch: 6 [3840/60000 (6%)]\tLoss: 0.039487\n",
            "Train Epoch: 6 [4480/60000 (7%)]\tLoss: 0.050619\n",
            "Train Epoch: 6 [5120/60000 (9%)]\tLoss: 0.032500\n",
            "Train Epoch: 6 [5760/60000 (10%)]\tLoss: 0.097626\n",
            "Train Epoch: 6 [6400/60000 (11%)]\tLoss: 0.048907\n",
            "Train Epoch: 6 [7040/60000 (12%)]\tLoss: 0.074719\n",
            "Train Epoch: 6 [7680/60000 (13%)]\tLoss: 0.102253\n",
            "Train Epoch: 6 [8320/60000 (14%)]\tLoss: 0.099599\n",
            "Train Epoch: 6 [8960/60000 (15%)]\tLoss: 0.064599\n",
            "Train Epoch: 6 [9600/60000 (16%)]\tLoss: 0.125885\n",
            "Train Epoch: 6 [10240/60000 (17%)]\tLoss: 0.158479\n",
            "Train Epoch: 6 [10880/60000 (18%)]\tLoss: 0.051861\n",
            "Train Epoch: 6 [11520/60000 (19%)]\tLoss: 0.028974\n",
            "Train Epoch: 6 [12160/60000 (20%)]\tLoss: 0.035669\n",
            "Train Epoch: 6 [12800/60000 (21%)]\tLoss: 0.066228\n",
            "Train Epoch: 6 [13440/60000 (22%)]\tLoss: 0.097528\n",
            "Train Epoch: 6 [14080/60000 (23%)]\tLoss: 0.111833\n",
            "Train Epoch: 6 [14720/60000 (25%)]\tLoss: 0.036461\n",
            "Train Epoch: 6 [15360/60000 (26%)]\tLoss: 0.102507\n",
            "Train Epoch: 6 [16000/60000 (27%)]\tLoss: 0.013560\n",
            "Train Epoch: 6 [16640/60000 (28%)]\tLoss: 0.008930\n",
            "Train Epoch: 6 [17280/60000 (29%)]\tLoss: 0.074646\n",
            "Train Epoch: 6 [17920/60000 (30%)]\tLoss: 0.120544\n",
            "Train Epoch: 6 [18560/60000 (31%)]\tLoss: 0.005513\n",
            "Train Epoch: 6 [19200/60000 (32%)]\tLoss: 0.020604\n",
            "Train Epoch: 6 [19840/60000 (33%)]\tLoss: 0.055869\n",
            "Train Epoch: 6 [20480/60000 (34%)]\tLoss: 0.100108\n",
            "Train Epoch: 6 [21120/60000 (35%)]\tLoss: 0.209487\n",
            "Train Epoch: 6 [21760/60000 (36%)]\tLoss: 0.054948\n",
            "Train Epoch: 6 [22400/60000 (37%)]\tLoss: 0.108754\n",
            "Train Epoch: 6 [23040/60000 (38%)]\tLoss: 0.061564\n",
            "Train Epoch: 6 [23680/60000 (39%)]\tLoss: 0.094614\n",
            "Train Epoch: 6 [24320/60000 (41%)]\tLoss: 0.045047\n",
            "Train Epoch: 6 [24960/60000 (42%)]\tLoss: 0.143568\n",
            "Train Epoch: 6 [25600/60000 (43%)]\tLoss: 0.178726\n",
            "Train Epoch: 6 [26240/60000 (44%)]\tLoss: 0.032304\n",
            "Train Epoch: 6 [26880/60000 (45%)]\tLoss: 0.044359\n",
            "Train Epoch: 6 [27520/60000 (46%)]\tLoss: 0.182319\n",
            "Train Epoch: 6 [28160/60000 (47%)]\tLoss: 0.417526\n",
            "Train Epoch: 6 [28800/60000 (48%)]\tLoss: 0.009696\n",
            "Train Epoch: 6 [29440/60000 (49%)]\tLoss: 0.096188\n",
            "Train Epoch: 6 [30080/60000 (50%)]\tLoss: 0.092988\n",
            "Train Epoch: 6 [30720/60000 (51%)]\tLoss: 0.103693\n",
            "Train Epoch: 6 [31360/60000 (52%)]\tLoss: 0.039901\n",
            "Train Epoch: 6 [32000/60000 (53%)]\tLoss: 0.297549\n",
            "Train Epoch: 6 [32640/60000 (54%)]\tLoss: 0.047111\n",
            "Train Epoch: 6 [33280/60000 (55%)]\tLoss: 0.213645\n",
            "Train Epoch: 6 [33920/60000 (57%)]\tLoss: 0.172202\n",
            "Train Epoch: 6 [34560/60000 (58%)]\tLoss: 0.102395\n",
            "Train Epoch: 6 [35200/60000 (59%)]\tLoss: 0.035137\n",
            "Train Epoch: 6 [35840/60000 (60%)]\tLoss: 0.075100\n",
            "Train Epoch: 6 [36480/60000 (61%)]\tLoss: 0.027377\n",
            "Train Epoch: 6 [37120/60000 (62%)]\tLoss: 0.015641\n",
            "Train Epoch: 6 [37760/60000 (63%)]\tLoss: 0.068385\n",
            "Train Epoch: 6 [38400/60000 (64%)]\tLoss: 0.031470\n",
            "Train Epoch: 6 [39040/60000 (65%)]\tLoss: 0.054893\n",
            "Train Epoch: 6 [39680/60000 (66%)]\tLoss: 0.046999\n",
            "Train Epoch: 6 [40320/60000 (67%)]\tLoss: 0.018158\n",
            "Train Epoch: 6 [40960/60000 (68%)]\tLoss: 0.139384\n",
            "Train Epoch: 6 [41600/60000 (69%)]\tLoss: 0.035757\n",
            "Train Epoch: 6 [42240/60000 (70%)]\tLoss: 0.075087\n",
            "Train Epoch: 6 [42880/60000 (71%)]\tLoss: 0.042579\n",
            "Train Epoch: 6 [43520/60000 (72%)]\tLoss: 0.056705\n",
            "Train Epoch: 6 [44160/60000 (74%)]\tLoss: 0.043643\n",
            "Train Epoch: 6 [44800/60000 (75%)]\tLoss: 0.013815\n",
            "Train Epoch: 6 [45440/60000 (76%)]\tLoss: 0.044645\n",
            "Train Epoch: 6 [46080/60000 (77%)]\tLoss: 0.112918\n",
            "Train Epoch: 6 [46720/60000 (78%)]\tLoss: 0.028523\n",
            "Train Epoch: 6 [47360/60000 (79%)]\tLoss: 0.032132\n",
            "Train Epoch: 6 [48000/60000 (80%)]\tLoss: 0.026885\n",
            "Train Epoch: 6 [48640/60000 (81%)]\tLoss: 0.011574\n",
            "Train Epoch: 6 [49280/60000 (82%)]\tLoss: 0.017050\n",
            "Train Epoch: 6 [49920/60000 (83%)]\tLoss: 0.013894\n",
            "Train Epoch: 6 [50560/60000 (84%)]\tLoss: 0.019614\n",
            "Train Epoch: 6 [51200/60000 (85%)]\tLoss: 0.014649\n",
            "Train Epoch: 6 [51840/60000 (86%)]\tLoss: 0.179831\n",
            "Train Epoch: 6 [52480/60000 (87%)]\tLoss: 0.026269\n",
            "Train Epoch: 6 [53120/60000 (88%)]\tLoss: 0.059885\n",
            "Train Epoch: 6 [53760/60000 (90%)]\tLoss: 0.032773\n",
            "Train Epoch: 6 [54400/60000 (91%)]\tLoss: 0.045967\n",
            "Train Epoch: 6 [55040/60000 (92%)]\tLoss: 0.040362\n",
            "Train Epoch: 6 [55680/60000 (93%)]\tLoss: 0.018482\n",
            "Train Epoch: 6 [56320/60000 (94%)]\tLoss: 0.069104\n",
            "Train Epoch: 6 [56960/60000 (95%)]\tLoss: 0.138851\n",
            "Train Epoch: 6 [57600/60000 (96%)]\tLoss: 0.021392\n",
            "Train Epoch: 6 [58240/60000 (97%)]\tLoss: 0.167223\n",
            "Train Epoch: 6 [58880/60000 (98%)]\tLoss: 0.040583\n",
            "Train Epoch: 6 [59520/60000 (99%)]\tLoss: 0.089868\n",
            "\n",
            "Test set: Average loss: 0.0658, Accuracy: 9784/10000 (98%)\n",
            "\n",
            "Train Epoch: 7 [0/60000 (0%)]\tLoss: 0.023645\n",
            "Train Epoch: 7 [640/60000 (1%)]\tLoss: 0.016674\n",
            "Train Epoch: 7 [1280/60000 (2%)]\tLoss: 0.122523\n",
            "Train Epoch: 7 [1920/60000 (3%)]\tLoss: 0.045869\n",
            "Train Epoch: 7 [2560/60000 (4%)]\tLoss: 0.028503\n",
            "Train Epoch: 7 [3200/60000 (5%)]\tLoss: 0.047719\n",
            "Train Epoch: 7 [3840/60000 (6%)]\tLoss: 0.057216\n",
            "Train Epoch: 7 [4480/60000 (7%)]\tLoss: 0.038486\n",
            "Train Epoch: 7 [5120/60000 (9%)]\tLoss: 0.003774\n",
            "Train Epoch: 7 [5760/60000 (10%)]\tLoss: 0.142762\n",
            "Train Epoch: 7 [6400/60000 (11%)]\tLoss: 0.014560\n",
            "Train Epoch: 7 [7040/60000 (12%)]\tLoss: 0.073266\n",
            "Train Epoch: 7 [7680/60000 (13%)]\tLoss: 0.103096\n",
            "Train Epoch: 7 [8320/60000 (14%)]\tLoss: 0.012308\n",
            "Train Epoch: 7 [8960/60000 (15%)]\tLoss: 0.095374\n",
            "Train Epoch: 7 [9600/60000 (16%)]\tLoss: 0.025061\n",
            "Train Epoch: 7 [10240/60000 (17%)]\tLoss: 0.130341\n",
            "Train Epoch: 7 [10880/60000 (18%)]\tLoss: 0.030625\n",
            "Train Epoch: 7 [11520/60000 (19%)]\tLoss: 0.035175\n",
            "Train Epoch: 7 [12160/60000 (20%)]\tLoss: 0.076559\n",
            "Train Epoch: 7 [12800/60000 (21%)]\tLoss: 0.057618\n",
            "Train Epoch: 7 [13440/60000 (22%)]\tLoss: 0.082780\n",
            "Train Epoch: 7 [14080/60000 (23%)]\tLoss: 0.344371\n",
            "Train Epoch: 7 [14720/60000 (25%)]\tLoss: 0.011167\n",
            "Train Epoch: 7 [15360/60000 (26%)]\tLoss: 0.241767\n",
            "Train Epoch: 7 [16000/60000 (27%)]\tLoss: 0.041204\n",
            "Train Epoch: 7 [16640/60000 (28%)]\tLoss: 0.050848\n",
            "Train Epoch: 7 [17280/60000 (29%)]\tLoss: 0.024554\n",
            "Train Epoch: 7 [17920/60000 (30%)]\tLoss: 0.026388\n",
            "Train Epoch: 7 [18560/60000 (31%)]\tLoss: 0.047351\n",
            "Train Epoch: 7 [19200/60000 (32%)]\tLoss: 0.082105\n",
            "Train Epoch: 7 [19840/60000 (33%)]\tLoss: 0.122452\n",
            "Train Epoch: 7 [20480/60000 (34%)]\tLoss: 0.149692\n",
            "Train Epoch: 7 [21120/60000 (35%)]\tLoss: 0.034332\n",
            "Train Epoch: 7 [21760/60000 (36%)]\tLoss: 0.037670\n",
            "Train Epoch: 7 [22400/60000 (37%)]\tLoss: 0.040459\n",
            "Train Epoch: 7 [23040/60000 (38%)]\tLoss: 0.032497\n",
            "Train Epoch: 7 [23680/60000 (39%)]\tLoss: 0.016124\n",
            "Train Epoch: 7 [24320/60000 (41%)]\tLoss: 0.507448\n",
            "Train Epoch: 7 [24960/60000 (42%)]\tLoss: 0.029686\n",
            "Train Epoch: 7 [25600/60000 (43%)]\tLoss: 0.013309\n",
            "Train Epoch: 7 [26240/60000 (44%)]\tLoss: 0.109648\n",
            "Train Epoch: 7 [26880/60000 (45%)]\tLoss: 0.023216\n",
            "Train Epoch: 7 [27520/60000 (46%)]\tLoss: 0.127622\n",
            "Train Epoch: 7 [28160/60000 (47%)]\tLoss: 0.043083\n",
            "Train Epoch: 7 [28800/60000 (48%)]\tLoss: 0.106326\n",
            "Train Epoch: 7 [29440/60000 (49%)]\tLoss: 0.039494\n",
            "Train Epoch: 7 [30080/60000 (50%)]\tLoss: 0.136651\n",
            "Train Epoch: 7 [30720/60000 (51%)]\tLoss: 0.031375\n",
            "Train Epoch: 7 [31360/60000 (52%)]\tLoss: 0.087277\n",
            "Train Epoch: 7 [32000/60000 (53%)]\tLoss: 0.027107\n",
            "Train Epoch: 7 [32640/60000 (54%)]\tLoss: 0.016659\n",
            "Train Epoch: 7 [33280/60000 (55%)]\tLoss: 0.022284\n",
            "Train Epoch: 7 [33920/60000 (57%)]\tLoss: 0.132529\n",
            "Train Epoch: 7 [34560/60000 (58%)]\tLoss: 0.028939\n",
            "Train Epoch: 7 [35200/60000 (59%)]\tLoss: 0.082599\n",
            "Train Epoch: 7 [35840/60000 (60%)]\tLoss: 0.047161\n",
            "Train Epoch: 7 [36480/60000 (61%)]\tLoss: 0.054484\n",
            "Train Epoch: 7 [37120/60000 (62%)]\tLoss: 0.034426\n",
            "Train Epoch: 7 [37760/60000 (63%)]\tLoss: 0.085385\n",
            "Train Epoch: 7 [38400/60000 (64%)]\tLoss: 0.161082\n",
            "Train Epoch: 7 [39040/60000 (65%)]\tLoss: 0.066452\n",
            "Train Epoch: 7 [39680/60000 (66%)]\tLoss: 0.123006\n",
            "Train Epoch: 7 [40320/60000 (67%)]\tLoss: 0.062498\n",
            "Train Epoch: 7 [40960/60000 (68%)]\tLoss: 0.094176\n",
            "Train Epoch: 7 [41600/60000 (69%)]\tLoss: 0.035417\n",
            "Train Epoch: 7 [42240/60000 (70%)]\tLoss: 0.029178\n",
            "Train Epoch: 7 [42880/60000 (71%)]\tLoss: 0.020363\n",
            "Train Epoch: 7 [43520/60000 (72%)]\tLoss: 0.041089\n",
            "Train Epoch: 7 [44160/60000 (74%)]\tLoss: 0.030509\n",
            "Train Epoch: 7 [44800/60000 (75%)]\tLoss: 0.096177\n",
            "Train Epoch: 7 [45440/60000 (76%)]\tLoss: 0.111335\n",
            "Train Epoch: 7 [46080/60000 (77%)]\tLoss: 0.207913\n",
            "Train Epoch: 7 [46720/60000 (78%)]\tLoss: 0.015228\n",
            "Train Epoch: 7 [47360/60000 (79%)]\tLoss: 0.067126\n",
            "Train Epoch: 7 [48000/60000 (80%)]\tLoss: 0.099535\n",
            "Train Epoch: 7 [48640/60000 (81%)]\tLoss: 0.077343\n",
            "Train Epoch: 7 [49280/60000 (82%)]\tLoss: 0.035455\n",
            "Train Epoch: 7 [49920/60000 (83%)]\tLoss: 0.045400\n",
            "Train Epoch: 7 [50560/60000 (84%)]\tLoss: 0.096960\n",
            "Train Epoch: 7 [51200/60000 (85%)]\tLoss: 0.014791\n",
            "Train Epoch: 7 [51840/60000 (86%)]\tLoss: 0.188358\n",
            "Train Epoch: 7 [52480/60000 (87%)]\tLoss: 0.063143\n",
            "Train Epoch: 7 [53120/60000 (88%)]\tLoss: 0.031017\n",
            "Train Epoch: 7 [53760/60000 (90%)]\tLoss: 0.062489\n",
            "Train Epoch: 7 [54400/60000 (91%)]\tLoss: 0.081064\n",
            "Train Epoch: 7 [55040/60000 (92%)]\tLoss: 0.088809\n",
            "Train Epoch: 7 [55680/60000 (93%)]\tLoss: 0.029272\n",
            "Train Epoch: 7 [56320/60000 (94%)]\tLoss: 0.128884\n",
            "Train Epoch: 7 [56960/60000 (95%)]\tLoss: 0.050550\n",
            "Train Epoch: 7 [57600/60000 (96%)]\tLoss: 0.016551\n",
            "Train Epoch: 7 [58240/60000 (97%)]\tLoss: 0.084110\n",
            "Train Epoch: 7 [58880/60000 (98%)]\tLoss: 0.059172\n",
            "Train Epoch: 7 [59520/60000 (99%)]\tLoss: 0.028199\n",
            "\n",
            "Test set: Average loss: 0.0569, Accuracy: 9829/10000 (98%)\n",
            "\n",
            "Train Epoch: 8 [0/60000 (0%)]\tLoss: 0.003899\n",
            "Train Epoch: 8 [640/60000 (1%)]\tLoss: 0.013532\n",
            "Train Epoch: 8 [1280/60000 (2%)]\tLoss: 0.298732\n",
            "Train Epoch: 8 [1920/60000 (3%)]\tLoss: 0.042053\n",
            "Train Epoch: 8 [2560/60000 (4%)]\tLoss: 0.064463\n",
            "Train Epoch: 8 [3200/60000 (5%)]\tLoss: 0.008032\n",
            "Train Epoch: 8 [3840/60000 (6%)]\tLoss: 0.033361\n",
            "Train Epoch: 8 [4480/60000 (7%)]\tLoss: 0.058066\n",
            "Train Epoch: 8 [5120/60000 (9%)]\tLoss: 0.132404\n",
            "Train Epoch: 8 [5760/60000 (10%)]\tLoss: 0.146221\n",
            "Train Epoch: 8 [6400/60000 (11%)]\tLoss: 0.094202\n",
            "Train Epoch: 8 [7040/60000 (12%)]\tLoss: 0.076572\n",
            "Train Epoch: 8 [7680/60000 (13%)]\tLoss: 0.066271\n",
            "Train Epoch: 8 [8320/60000 (14%)]\tLoss: 0.013229\n",
            "Train Epoch: 8 [8960/60000 (15%)]\tLoss: 0.067122\n",
            "Train Epoch: 8 [9600/60000 (16%)]\tLoss: 0.025621\n",
            "Train Epoch: 8 [10240/60000 (17%)]\tLoss: 0.016467\n",
            "Train Epoch: 8 [10880/60000 (18%)]\tLoss: 0.048400\n",
            "Train Epoch: 8 [11520/60000 (19%)]\tLoss: 0.103451\n",
            "Train Epoch: 8 [12160/60000 (20%)]\tLoss: 0.018738\n",
            "Train Epoch: 8 [12800/60000 (21%)]\tLoss: 0.050653\n",
            "Train Epoch: 8 [13440/60000 (22%)]\tLoss: 0.049227\n",
            "Train Epoch: 8 [14080/60000 (23%)]\tLoss: 0.022402\n",
            "Train Epoch: 8 [14720/60000 (25%)]\tLoss: 0.053102\n",
            "Train Epoch: 8 [15360/60000 (26%)]\tLoss: 0.089457\n",
            "Train Epoch: 8 [16000/60000 (27%)]\tLoss: 0.054516\n",
            "Train Epoch: 8 [16640/60000 (28%)]\tLoss: 0.082493\n",
            "Train Epoch: 8 [17280/60000 (29%)]\tLoss: 0.050568\n",
            "Train Epoch: 8 [17920/60000 (30%)]\tLoss: 0.030003\n",
            "Train Epoch: 8 [18560/60000 (31%)]\tLoss: 0.130442\n",
            "Train Epoch: 8 [19200/60000 (32%)]\tLoss: 0.148022\n",
            "Train Epoch: 8 [19840/60000 (33%)]\tLoss: 0.004610\n",
            "Train Epoch: 8 [20480/60000 (34%)]\tLoss: 0.056404\n",
            "Train Epoch: 8 [21120/60000 (35%)]\tLoss: 0.037325\n",
            "Train Epoch: 8 [21760/60000 (36%)]\tLoss: 0.106467\n",
            "Train Epoch: 8 [22400/60000 (37%)]\tLoss: 0.119982\n",
            "Train Epoch: 8 [23040/60000 (38%)]\tLoss: 0.052939\n",
            "Train Epoch: 8 [23680/60000 (39%)]\tLoss: 0.051193\n",
            "Train Epoch: 8 [24320/60000 (41%)]\tLoss: 0.072867\n",
            "Train Epoch: 8 [24960/60000 (42%)]\tLoss: 0.026427\n",
            "Train Epoch: 8 [25600/60000 (43%)]\tLoss: 0.115853\n",
            "Train Epoch: 8 [26240/60000 (44%)]\tLoss: 0.009986\n",
            "Train Epoch: 8 [26880/60000 (45%)]\tLoss: 0.044305\n",
            "Train Epoch: 8 [27520/60000 (46%)]\tLoss: 0.099062\n",
            "Train Epoch: 8 [28160/60000 (47%)]\tLoss: 0.009835\n",
            "Train Epoch: 8 [28800/60000 (48%)]\tLoss: 0.031613\n",
            "Train Epoch: 8 [29440/60000 (49%)]\tLoss: 0.018438\n",
            "Train Epoch: 8 [30080/60000 (50%)]\tLoss: 0.015953\n",
            "Train Epoch: 8 [30720/60000 (51%)]\tLoss: 0.056476\n",
            "Train Epoch: 8 [31360/60000 (52%)]\tLoss: 0.025684\n",
            "Train Epoch: 8 [32000/60000 (53%)]\tLoss: 0.048251\n",
            "Train Epoch: 8 [32640/60000 (54%)]\tLoss: 0.048812\n",
            "Train Epoch: 8 [33280/60000 (55%)]\tLoss: 0.077076\n",
            "Train Epoch: 8 [33920/60000 (57%)]\tLoss: 0.105444\n",
            "Train Epoch: 8 [34560/60000 (58%)]\tLoss: 0.158844\n",
            "Train Epoch: 8 [35200/60000 (59%)]\tLoss: 0.079784\n",
            "Train Epoch: 8 [35840/60000 (60%)]\tLoss: 0.006399\n",
            "Train Epoch: 8 [36480/60000 (61%)]\tLoss: 0.115245\n",
            "Train Epoch: 8 [37120/60000 (62%)]\tLoss: 0.063500\n",
            "Train Epoch: 8 [37760/60000 (63%)]\tLoss: 0.148137\n",
            "Train Epoch: 8 [38400/60000 (64%)]\tLoss: 0.028322\n",
            "Train Epoch: 8 [39040/60000 (65%)]\tLoss: 0.102279\n",
            "Train Epoch: 8 [39680/60000 (66%)]\tLoss: 0.014527\n",
            "Train Epoch: 8 [40320/60000 (67%)]\tLoss: 0.014286\n",
            "Train Epoch: 8 [40960/60000 (68%)]\tLoss: 0.112443\n",
            "Train Epoch: 8 [41600/60000 (69%)]\tLoss: 0.033298\n",
            "Train Epoch: 8 [42240/60000 (70%)]\tLoss: 0.073240\n",
            "Train Epoch: 8 [42880/60000 (71%)]\tLoss: 0.154578\n",
            "Train Epoch: 8 [43520/60000 (72%)]\tLoss: 0.032379\n",
            "Train Epoch: 8 [44160/60000 (74%)]\tLoss: 0.009668\n",
            "Train Epoch: 8 [44800/60000 (75%)]\tLoss: 0.093961\n",
            "Train Epoch: 8 [45440/60000 (76%)]\tLoss: 0.090674\n",
            "Train Epoch: 8 [46080/60000 (77%)]\tLoss: 0.033113\n",
            "Train Epoch: 8 [46720/60000 (78%)]\tLoss: 0.091049\n",
            "Train Epoch: 8 [47360/60000 (79%)]\tLoss: 0.038821\n",
            "Train Epoch: 8 [48000/60000 (80%)]\tLoss: 0.004633\n",
            "Train Epoch: 8 [48640/60000 (81%)]\tLoss: 0.026047\n",
            "Train Epoch: 8 [49280/60000 (82%)]\tLoss: 0.045615\n",
            "Train Epoch: 8 [49920/60000 (83%)]\tLoss: 0.056447\n",
            "Train Epoch: 8 [50560/60000 (84%)]\tLoss: 0.033912\n",
            "Train Epoch: 8 [51200/60000 (85%)]\tLoss: 0.130529\n",
            "Train Epoch: 8 [51840/60000 (86%)]\tLoss: 0.116932\n",
            "Train Epoch: 8 [52480/60000 (87%)]\tLoss: 0.093243\n",
            "Train Epoch: 8 [53120/60000 (88%)]\tLoss: 0.077592\n",
            "Train Epoch: 8 [53760/60000 (90%)]\tLoss: 0.124867\n",
            "Train Epoch: 8 [54400/60000 (91%)]\tLoss: 0.067414\n",
            "Train Epoch: 8 [55040/60000 (92%)]\tLoss: 0.036374\n",
            "Train Epoch: 8 [55680/60000 (93%)]\tLoss: 0.062400\n",
            "Train Epoch: 8 [56320/60000 (94%)]\tLoss: 0.060648\n",
            "Train Epoch: 8 [56960/60000 (95%)]\tLoss: 0.086331\n",
            "Train Epoch: 8 [57600/60000 (96%)]\tLoss: 0.037983\n",
            "Train Epoch: 8 [58240/60000 (97%)]\tLoss: 0.042414\n",
            "Train Epoch: 8 [58880/60000 (98%)]\tLoss: 0.020766\n",
            "Train Epoch: 8 [59520/60000 (99%)]\tLoss: 0.107122\n",
            "\n",
            "Test set: Average loss: 0.0523, Accuracy: 9836/10000 (98%)\n",
            "\n",
            "Train Epoch: 9 [0/60000 (0%)]\tLoss: 0.024257\n",
            "Train Epoch: 9 [640/60000 (1%)]\tLoss: 0.131342\n",
            "Train Epoch: 9 [1280/60000 (2%)]\tLoss: 0.117655\n",
            "Train Epoch: 9 [1920/60000 (3%)]\tLoss: 0.034641\n",
            "Train Epoch: 9 [2560/60000 (4%)]\tLoss: 0.089017\n",
            "Train Epoch: 9 [3200/60000 (5%)]\tLoss: 0.008196\n",
            "Train Epoch: 9 [3840/60000 (6%)]\tLoss: 0.103406\n",
            "Train Epoch: 9 [4480/60000 (7%)]\tLoss: 0.075406\n",
            "Train Epoch: 9 [5120/60000 (9%)]\tLoss: 0.010731\n",
            "Train Epoch: 9 [5760/60000 (10%)]\tLoss: 0.018285\n",
            "Train Epoch: 9 [6400/60000 (11%)]\tLoss: 0.071232\n",
            "Train Epoch: 9 [7040/60000 (12%)]\tLoss: 0.024992\n",
            "Train Epoch: 9 [7680/60000 (13%)]\tLoss: 0.030790\n",
            "Train Epoch: 9 [8320/60000 (14%)]\tLoss: 0.021081\n",
            "Train Epoch: 9 [8960/60000 (15%)]\tLoss: 0.066983\n",
            "Train Epoch: 9 [9600/60000 (16%)]\tLoss: 0.023477\n",
            "Train Epoch: 9 [10240/60000 (17%)]\tLoss: 0.046751\n",
            "Train Epoch: 9 [10880/60000 (18%)]\tLoss: 0.028631\n",
            "Train Epoch: 9 [11520/60000 (19%)]\tLoss: 0.020940\n",
            "Train Epoch: 9 [12160/60000 (20%)]\tLoss: 0.058257\n",
            "Train Epoch: 9 [12800/60000 (21%)]\tLoss: 0.019608\n",
            "Train Epoch: 9 [13440/60000 (22%)]\tLoss: 0.130650\n",
            "Train Epoch: 9 [14080/60000 (23%)]\tLoss: 0.023365\n",
            "Train Epoch: 9 [14720/60000 (25%)]\tLoss: 0.019147\n",
            "Train Epoch: 9 [15360/60000 (26%)]\tLoss: 0.041081\n",
            "Train Epoch: 9 [16000/60000 (27%)]\tLoss: 0.068670\n",
            "Train Epoch: 9 [16640/60000 (28%)]\tLoss: 0.096699\n",
            "Train Epoch: 9 [17280/60000 (29%)]\tLoss: 0.030342\n",
            "Train Epoch: 9 [17920/60000 (30%)]\tLoss: 0.058124\n",
            "Train Epoch: 9 [18560/60000 (31%)]\tLoss: 0.018728\n",
            "Train Epoch: 9 [19200/60000 (32%)]\tLoss: 0.069166\n",
            "Train Epoch: 9 [19840/60000 (33%)]\tLoss: 0.111536\n",
            "Train Epoch: 9 [20480/60000 (34%)]\tLoss: 0.019903\n",
            "Train Epoch: 9 [21120/60000 (35%)]\tLoss: 0.103090\n",
            "Train Epoch: 9 [21760/60000 (36%)]\tLoss: 0.020926\n",
            "Train Epoch: 9 [22400/60000 (37%)]\tLoss: 0.105663\n",
            "Train Epoch: 9 [23040/60000 (38%)]\tLoss: 0.037970\n",
            "Train Epoch: 9 [23680/60000 (39%)]\tLoss: 0.017519\n",
            "Train Epoch: 9 [24320/60000 (41%)]\tLoss: 0.138717\n",
            "Train Epoch: 9 [24960/60000 (42%)]\tLoss: 0.064240\n",
            "Train Epoch: 9 [25600/60000 (43%)]\tLoss: 0.019623\n",
            "Train Epoch: 9 [26240/60000 (44%)]\tLoss: 0.082970\n",
            "Train Epoch: 9 [26880/60000 (45%)]\tLoss: 0.007250\n",
            "Train Epoch: 9 [27520/60000 (46%)]\tLoss: 0.011654\n",
            "Train Epoch: 9 [28160/60000 (47%)]\tLoss: 0.080292\n",
            "Train Epoch: 9 [28800/60000 (48%)]\tLoss: 0.102698\n",
            "Train Epoch: 9 [29440/60000 (49%)]\tLoss: 0.026849\n",
            "Train Epoch: 9 [30080/60000 (50%)]\tLoss: 0.011162\n",
            "Train Epoch: 9 [30720/60000 (51%)]\tLoss: 0.022775\n",
            "Train Epoch: 9 [31360/60000 (52%)]\tLoss: 0.033825\n",
            "Train Epoch: 9 [32000/60000 (53%)]\tLoss: 0.017432\n",
            "Train Epoch: 9 [32640/60000 (54%)]\tLoss: 0.037895\n",
            "Train Epoch: 9 [33280/60000 (55%)]\tLoss: 0.013825\n",
            "Train Epoch: 9 [33920/60000 (57%)]\tLoss: 0.099360\n",
            "Train Epoch: 9 [34560/60000 (58%)]\tLoss: 0.063582\n",
            "Train Epoch: 9 [35200/60000 (59%)]\tLoss: 0.029154\n",
            "Train Epoch: 9 [35840/60000 (60%)]\tLoss: 0.060768\n",
            "Train Epoch: 9 [36480/60000 (61%)]\tLoss: 0.043246\n",
            "Train Epoch: 9 [37120/60000 (62%)]\tLoss: 0.008849\n",
            "Train Epoch: 9 [37760/60000 (63%)]\tLoss: 0.052255\n",
            "Train Epoch: 9 [38400/60000 (64%)]\tLoss: 0.257009\n",
            "Train Epoch: 9 [39040/60000 (65%)]\tLoss: 0.015763\n",
            "Train Epoch: 9 [39680/60000 (66%)]\tLoss: 0.016770\n",
            "Train Epoch: 9 [40320/60000 (67%)]\tLoss: 0.060657\n",
            "Train Epoch: 9 [40960/60000 (68%)]\tLoss: 0.042023\n",
            "Train Epoch: 9 [41600/60000 (69%)]\tLoss: 0.041860\n",
            "Train Epoch: 9 [42240/60000 (70%)]\tLoss: 0.026621\n",
            "Train Epoch: 9 [42880/60000 (71%)]\tLoss: 0.054946\n",
            "Train Epoch: 9 [43520/60000 (72%)]\tLoss: 0.198131\n",
            "Train Epoch: 9 [44160/60000 (74%)]\tLoss: 0.034572\n",
            "Train Epoch: 9 [44800/60000 (75%)]\tLoss: 0.018900\n",
            "Train Epoch: 9 [45440/60000 (76%)]\tLoss: 0.027650\n",
            "Train Epoch: 9 [46080/60000 (77%)]\tLoss: 0.066540\n",
            "Train Epoch: 9 [46720/60000 (78%)]\tLoss: 0.011119\n",
            "Train Epoch: 9 [47360/60000 (79%)]\tLoss: 0.016062\n",
            "Train Epoch: 9 [48000/60000 (80%)]\tLoss: 0.010069\n",
            "Train Epoch: 9 [48640/60000 (81%)]\tLoss: 0.064153\n",
            "Train Epoch: 9 [49280/60000 (82%)]\tLoss: 0.034661\n",
            "Train Epoch: 9 [49920/60000 (83%)]\tLoss: 0.163300\n",
            "Train Epoch: 9 [50560/60000 (84%)]\tLoss: 0.001407\n",
            "Train Epoch: 9 [51200/60000 (85%)]\tLoss: 0.019835\n",
            "Train Epoch: 9 [51840/60000 (86%)]\tLoss: 0.023066\n",
            "Train Epoch: 9 [52480/60000 (87%)]\tLoss: 0.065834\n",
            "Train Epoch: 9 [53120/60000 (88%)]\tLoss: 0.011914\n",
            "Train Epoch: 9 [53760/60000 (90%)]\tLoss: 0.101501\n",
            "Train Epoch: 9 [54400/60000 (91%)]\tLoss: 0.025862\n",
            "Train Epoch: 9 [55040/60000 (92%)]\tLoss: 0.087371\n",
            "Train Epoch: 9 [55680/60000 (93%)]\tLoss: 0.071533\n",
            "Train Epoch: 9 [56320/60000 (94%)]\tLoss: 0.095072\n",
            "Train Epoch: 9 [56960/60000 (95%)]\tLoss: 0.010244\n",
            "Train Epoch: 9 [57600/60000 (96%)]\tLoss: 0.011267\n",
            "Train Epoch: 9 [58240/60000 (97%)]\tLoss: 0.038865\n",
            "Train Epoch: 9 [58880/60000 (98%)]\tLoss: 0.074786\n",
            "Train Epoch: 9 [59520/60000 (99%)]\tLoss: 0.057503\n",
            "\n",
            "Test set: Average loss: 0.0507, Accuracy: 9847/10000 (98%)\n",
            "\n"
          ]
        }
      ]
    }
  ]
}