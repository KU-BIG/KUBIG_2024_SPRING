{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# < 3주차 과제 1 : MLP 마음대로 다뤄보기>\n",
        "- dataset을 임의로 선정해서 직접 분석 해보기(제공한 코드를 활용해서 해보기)\n",
        "- activation functions 중 relu사용시 함수 직접 정의\n",
        "- lr, optimizer 등 바꿔보기\n",
        "- hidden layer/neuron 수를 바꾸기\n",
        "- 전처리도 추가\n",
        "- 모든 시도를 올려주세요!\n",
        "- 제일 높은 acc를 보인 시도를 명시해주세요!\n"
      ],
      "metadata": {
        "id": "sgAYo4nrw2F4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fX437IL6qbI-"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn, optim\n",
        "from sklearn.datasets import load_wine\n",
        "from torch.utils.data import  TensorDataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 아래 데이터셋 중 원하는 데이터셋 하나를 선택하여, 코드 과제 진행하기!\n",
        "- 1) load_digits() <br>\n",
        "- 2) load_wine()"
      ],
      "metadata": {
        "id": "oxkFzBDNmWNk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 데이터셋 종류 :\n",
        "wine = load_wine()"
      ],
      "metadata": {
        "id": "FywYbfsKtjcR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input = wine.data\n",
        "output = wine.target"
      ],
      "metadata": {
        "id": "C2P0hqZ9yBGm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "torch.manual_seed(777)\n",
        "if device == \"cuda\":\n",
        "  torch.cuda.manual_seed_all(777)"
      ],
      "metadata": {
        "id": "SggpQfSPt85C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-W5onxatM9S3",
        "outputId": "4be10d2f-baa3-4b15-c1b5-508f4ac91334"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1.423e+01, 1.710e+00, 2.430e+00, ..., 1.040e+00, 3.920e+00,\n",
              "        1.065e+03],\n",
              "       [1.320e+01, 1.780e+00, 2.140e+00, ..., 1.050e+00, 3.400e+00,\n",
              "        1.050e+03],\n",
              "       [1.316e+01, 2.360e+00, 2.670e+00, ..., 1.030e+00, 3.170e+00,\n",
              "        1.185e+03],\n",
              "       ...,\n",
              "       [1.327e+01, 4.280e+00, 2.260e+00, ..., 5.900e-01, 1.560e+00,\n",
              "        8.350e+02],\n",
              "       [1.317e+01, 2.590e+00, 2.370e+00, ..., 6.000e-01, 1.620e+00,\n",
              "        8.400e+02],\n",
              "       [1.413e+01, 4.100e+00, 2.740e+00, ..., 6.100e-01, 1.600e+00,\n",
              "        5.600e+02]])"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q-x9bkc2NBC7",
        "outputId": "8a50e23e-ff0e-4b56-822a-efa3600b016c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1,\n",
              "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2,\n",
              "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
              "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
              "       2, 2])"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#데이터 칼럼\n",
        "wine['feature_names']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wlhST0vZM68w",
        "outputId": "60d8989f-386b-4166-ce9d-7357444972f4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['alcohol',\n",
              " 'malic_acid',\n",
              " 'ash',\n",
              " 'alcalinity_of_ash',\n",
              " 'magnesium',\n",
              " 'total_phenols',\n",
              " 'flavanoids',\n",
              " 'nonflavanoid_phenols',\n",
              " 'proanthocyanins',\n",
              " 'color_intensity',\n",
              " 'hue',\n",
              " 'od280/od315_of_diluted_wines',\n",
              " 'proline']"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#샘플 수 178개, class는 0,1,2"
      ],
      "metadata": {
        "id": "rvWDIphhCyZP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# #y를 카테고리화\n",
        "# output = pd.get_dummies(output)\n",
        "# output"
      ],
      "metadata": {
        "id": "g5fwnrQxNXsr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import torch.nn.functional as F\n",
        "\n",
        "# #원핫 인코딩\n",
        "# output=F.one_hot(output,num_classes=3)\n",
        "# output\n",
        "###케라스에서는 라벨 원핫 인코딩을 하는데 파이토치도 하는 건지,,"
      ],
      "metadata": {
        "id": "bnkzrkB9OYpI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train, x_test, y_train, y_test = train_test_split(input, output, test_size = 0.3, random_state = 42, stratify= wine.target, shuffle = True)\n",
        "\n",
        "x_train = torch.FloatTensor(x_train).to(device)\n",
        "y_train = torch.LongTensor(y_train).to(device)\n",
        "x_test = torch.FloatTensor(x_test)\n",
        "y_test = torch.LongTensor(y_test)\n",
        "\n",
        "# 데이터를 tensor로 바꿔주고 gpu 연산이 가능해지도록 gpu에 옮김\n",
        "# label 값을 왜 long 에 옮겨놓는가? loss function이 다르기 때문"
      ],
      "metadata": {
        "id": "bLMzf-2ntYeX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(x_train[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9HyYM8kvCMkD",
        "outputId": "f82a8c00-c865-419f-d613-617fdde770e0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([1.3750e+01, 1.7300e+00, 2.4100e+00, 1.6000e+01, 8.9000e+01, 2.6000e+00,\n",
            "        2.7600e+00, 2.9000e-01, 1.8100e+00, 5.6000e+00, 1.1500e+00, 2.9000e+00,\n",
            "        1.3200e+03])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(y_train.unique())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "umEdiTZkrVqS",
        "outputId": "3b006cbd-9a5b-4bab-e468-4387c412ed51"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([0, 1, 2])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "파이토치에서는 데이터셋을 좀 더 쉽게 다룰 수 있도록 유용한 도구로서 torch.utils.data.Dataset과 torch.utils.data.DataLoader를 제공합니다. 이를 사용하면 미니 배치 학습, 데이터 셔플(shuffle), 병렬 처리까지 간단히 수행할 수 있습니다. 기본적인 사용 방법은 Dataset을 정의하고, 이를 DataLoader에 전달하는 것\n",
        "- init : class 에서 객체가 생성되면 바로 실행되는 함수\n",
        "- len : observation 수를 정의하는 함수\n",
        "- getitem : iteration 마다 해당하는 데이터를 돌려주는 함수"
      ],
      "metadata": {
        "id": "combmxzmYFyn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset\n",
        "class CustomDataset(Dataset):\n",
        "  def __init__(self):\n",
        "    self.x_data = x_train\n",
        "    self.y_data = [[y] for y in y_train]\n",
        "    self.device = device\n",
        "#  데이터셋의 전처리를 해주는 부분\n",
        "    self.x_data = self.standardize(self.x_data)\n",
        "\n",
        "  def standardize(self, data):\n",
        "          #각 feature의 평균과 표준편차를 이용하여 표준화하는 함수\n",
        "          mean = torch.mean(data, dim=0)\n",
        "          std = torch.std(data, dim=0)\n",
        "          standardized_data = (data - mean) / std\n",
        "          return standardized_data\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.x_data)\n",
        "#  데이터셋의 길이. 즉, 총 샘플의 수를 적어주는 부분\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    x = torch.FloatTensor(self.x_data[idx]).to(self.device)\n",
        "    y = torch.LongTensor(self.y_data[idx]).to(self.device)\n",
        "#  데이터셋에서 특정 1개의 샘플을 가져오는 함수\n",
        "\n",
        "    return x,y"
      ],
      "metadata": {
        "id": "y38TlgXoqV5Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 8\n",
        "\n",
        "dataset = CustomDataset()\n",
        "dataloader = DataLoader(dataset, batch_size=batch_size)"
      ],
      "metadata": {
        "id": "x8VHwnuFqino"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# input_layer와 hidden_layer의 노드 개수를 어떻게 바꿔줘야할까?\n",
        "# hidden layer/neuron 수를 바꾸기\n",
        "\n",
        "model = nn.Sequential(\n",
        "          nn.Linear(13,398, bias=True),\n",
        "          nn.ReLU(),\n",
        "          nn.Linear(398,15, bias=True),\n",
        "          nn.ReLU(),\n",
        "          nn.Linear(15,3, bias=True),\n",
        "          nn.Softmax(dim=1)\n",
        "          ).to(device)"
      ],
      "metadata": {
        "id": "C6V7a4tyq6Jc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "class로 구현 가능\n",
        "- init : 초기 생성 함수\n",
        "- foward : 순전파(입력값 => 예측값 의 과정)"
      ],
      "metadata": {
        "id": "07uV8RY7Yr_5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#relu 구현\n",
        "class Relu(torch.nn.Module):\n",
        "    def forward(self, x):\n",
        "        return torch.max(torch.tensor(0.0), x)\n",
        "\n",
        "class Model(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Model, self).__init__()\n",
        "        self.layer1 = nn.Sequential(\n",
        "            nn.Linear(13, 398, bias=True), # input_layer = 13, hidden_layer1 = 398\n",
        "            Relu(),\n",
        "            nn.BatchNorm1d(398)\n",
        "        )\n",
        "        self.layer2 = nn.Sequential(\n",
        "            nn.Linear(398, 15, bias=True),\n",
        "            Relu()\n",
        "        )\n",
        "        self.layer3 = nn.Sequential(\n",
        "            nn.Linear(15, 10, bias=True),\n",
        "            Relu()\n",
        "        )\n",
        "        self.layer4 = nn.Sequential(\n",
        "            nn.Linear(10, 3, bias=True),\n",
        "            nn.Softmax()\n",
        "        )\n",
        "\n",
        " # activation function 이용\n",
        "  #   nn.ReLU()\n",
        "  #   nn.tanH()\n",
        "  #   https://pytorch.org/docs/stable/nn.html 그 외에도 여기서 확인 가능함\n",
        "  #   파라미터가 필요하지 않다는 것이 특징\n",
        "\n",
        "  # batch normazliation 1d, 파라미터 값으로 vector의 길이를 전해줌\n",
        "  # 추후에 이미지를 다루게 된다면, 그때는 batch normalization 2d를 이용하게 됨\n",
        "  # 그때는 파라미터 값으로 채널, 가로, 세로 길이를 전달해주게 됨\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        output = self.layer1(x)\n",
        "        output = self.layer2(output)\n",
        "        output = self.layer3(output)\n",
        "        output = self.layer4(output)\n",
        "        return output"
      ],
      "metadata": {
        "id": "5xBCfMbVLQUb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def init_weights(layer):\n",
        "    if isinstance(layer, nn.Linear):\n",
        "        torch.nn.init.xavier_uniform(layer.weight)\n",
        "        layer.bias.data.fill_(0.01)\n",
        "\n",
        "        #xavier사용\n",
        "        # Layer의 weight를 어떤 분포를 가지도록 초기화시켜줌+global minimum찾기 위해서"
      ],
      "metadata": {
        "id": "kqcqqkECrSGK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Model().to(device)\n",
        "model.apply(init_weights)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oMDUBFg6rUpw",
        "outputId": "df939fc6-369a-4582-bc03-fdfe71298e72"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-22-6196a47a4462>:3: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "  torch.nn.init.xavier_uniform(layer.weight)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Model(\n",
              "  (layer1): Sequential(\n",
              "    (0): Linear(in_features=13, out_features=398, bias=True)\n",
              "    (1): Relu()\n",
              "    (2): BatchNorm1d(398, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  )\n",
              "  (layer2): Sequential(\n",
              "    (0): Linear(in_features=398, out_features=15, bias=True)\n",
              "    (1): Relu()\n",
              "  )\n",
              "  (layer3): Sequential(\n",
              "    (0): Linear(in_features=15, out_features=10, bias=True)\n",
              "    (1): Relu()\n",
              "  )\n",
              "  (layer4): Sequential(\n",
              "    (0): Linear(in_features=10, out_features=3, bias=True)\n",
              "    (1): Softmax(dim=None)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OwZt5CetrYFb",
        "outputId": "4c07a0fb-b571-4c37-dc5f-592bc8a67f62"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model(\n",
            "  (layer1): Sequential(\n",
            "    (0): Linear(in_features=13, out_features=398, bias=True)\n",
            "    (1): Relu()\n",
            "    (2): BatchNorm1d(398, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  )\n",
            "  (layer2): Sequential(\n",
            "    (0): Linear(in_features=398, out_features=15, bias=True)\n",
            "    (1): Relu()\n",
            "  )\n",
            "  (layer3): Sequential(\n",
            "    (0): Linear(in_features=15, out_features=10, bias=True)\n",
            "    (1): Relu()\n",
            "  )\n",
            "  (layer4): Sequential(\n",
            "    (0): Linear(in_features=10, out_features=3, bias=True)\n",
            "    (1): Softmax(dim=None)\n",
            "  )\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss_fn  = nn.CrossEntropyLoss().to(device)\n",
        "\n",
        "# 여러가지 optimizer 시도해보기\n",
        "# lr 바꿔보기\n",
        "\n",
        "optimizer = optim.Adagrad(model.parameters(), lr= 0.01)\n",
        "#optimizer = optim.Adam(model.parameters(), lr= 0.01)\n",
        "#optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
        "#optimizer = optim.Adadelta(model.parameters(), lr=0.03)\n",
        "\n",
        "# https://pytorch.org/docs/stable/optim.html#module-torch.optim 페이지 참조\n",
        "# optimizer = optim.SGD(model.parameters(), lr=0.1, momentum=0.9)\n",
        "# sgd 등등"
      ],
      "metadata": {
        "id": "AYFp-eTErh7b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "losses = []\n",
        "for epoch in range(100):\n",
        "\n",
        "  optimizer.zero_grad()\n",
        "  hypothesis = model(x_train)\n",
        "\n",
        "  # 비용 함수\n",
        "  cost = loss_fn(hypothesis, y_train)\n",
        "  cost.backward()\n",
        "  optimizer.step()\n",
        "  losses.append(cost.item())\n",
        "\n",
        "  if epoch % 10 == 0:\n",
        "    print(epoch, cost.item())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "90QxHvlIrjS7",
        "outputId": "bd13a4c1-38ac-4039-9079-4bd61b139037"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1518: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 0.9692224860191345\n",
            "10 0.79714035987854\n",
            "20 0.7350306510925293\n",
            "30 0.6505707502365112\n",
            "40 0.6190946698188782\n",
            "50 0.5871148705482483\n",
            "60 0.5762291550636292\n",
            "70 0.5930455327033997\n",
            "80 0.5880704522132874\n",
            "90 0.5721447467803955\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(losses)\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 430
        },
        "id": "81ASYrW7roFM",
        "outputId": "964d3f5d-fe42-4324-fcbc-2671105e28f3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "  model = model.to('cpu')\n",
        "  y_pred = model(x_test)\n",
        "  y_pred = y_pred.detach().cpu().numpy()\n",
        "  predicted = np.argmax(y_pred, axis =1)\n",
        "  accuracy = (accuracy_score(predicted, y_test))"
      ],
      "metadata": {
        "id": "h4kJzpLErqhZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'model의 output은 :  {y_pred[0]}')\n",
        "print(f'argmax를 한 후의 output은 {predicted[0]}')\n",
        "print(f'accuracy는 {accuracy}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vyIKhs3Nr6Ay",
        "outputId": "9682a753-d44c-4627-e8b3-0c241a870d39"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model의 output은 :  [0.99676466 0.00221027 0.00102507]\n",
            "argmax를 한 후의 output은 0\n",
            "accuracy는 0.9444444444444444\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# < 3주차 과제 2 : CNN 맛보기>"
      ],
      "metadata": {
        "id": "3RzRM7xThZV_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from __future__ import print_function\n",
        "import argparse\n",
        "import torch\n",
        "import torch.nn as nn # 신경망들이 포함됨\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim # 최적화 알고리즘들이 포함됨\n",
        "from torchvision import datasets, transforms # 이미지 데이터셋 집합, 이미지 변환 툴\n",
        "from torch.autograd import Variable"
      ],
      "metadata": {
        "id": "56xqgtLxhZw6"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training settings\n",
        "batch_size = 64\n",
        "\n",
        "# MNIST Dataset\n",
        "train_dataset = datasets.MNIST(root='./data/',\n",
        "                               train=True,\n",
        "                               transform=transforms.ToTensor(),\n",
        "                               download=True)\n",
        "\n",
        "test_dataset = datasets.MNIST(root='./data/',\n",
        "                              train=False,\n",
        "                              transform=transforms.ToTensor())\n",
        "\n",
        "# Data Loader (Input Pipeline)\n",
        "#앞서 정의한 데이터셋을 DataLoader에 넣으면 정의한 조건(배치 사이즈 등)에 따라 모델을 학습하고 추론할 때 데이터를 load 해줌\n",
        "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
        "                                           batch_size=batch_size,\n",
        "                                           shuffle=True)\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
        "                                          batch_size=batch_size,\n",
        "                                          shuffle=False)"
      ],
      "metadata": {
        "id": "TzkF2bFNhcQ2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "de2d1fdb-3fd4-498f-dd05-7c0deae97ae3"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9912422/9912422 [00:00<00:00, 98109373.84it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28881/28881 [00:00<00:00, 74545042.35it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "100%|██████████| 1648877/1648877 [00:00<00:00, 27945705.43it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4542/4542 [00:00<00:00, 10220240.76it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 첫번째 이미지만 가져오기\n",
        "img , label = next(iter(train_loader))"
      ],
      "metadata": {
        "id": "D1Mvg6FpQYji"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DyspnHTDD24C",
        "outputId": "eb2ef22b-2543-4c09-ccc3-5f92f7d1fca1"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([64, 1, 28, 28])"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#배치 크기 × 채널 × 높이(height) × 너비(widht)\n",
        "#총 64개의 샘플, 흑백이므로 채널 1, 가로 세로 28픽셀"
      ],
      "metadata": {
        "id": "52NoQZj0T_lo"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#한 개의 이미지만 출력해보기\n",
        "img_show = img[0, 0, :, :]    #배치 크기, 채널 0으로 만듦\n",
        "img_show.shape\n",
        "\n",
        "plt.imshow(img_show, cmap='Greys')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 430
        },
        "id": "5zSU9StsTpNO",
        "outputId": "101209bc-83d3-4dda-8967-878d442ea1a2"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=0)의 파라미터\n",
        "\n",
        "*   in_channels: 입력 채널 수. 흑백 이미지일 경우 1, RGB 값을 가진 이미지일 경우 3 을 가진 경우가 많음\n",
        "*   out_channels: 출력 채널 수\n",
        "*   kernel_size: 커널 사이즈(필터 사이즈)\n",
        "*   stride: kernel을 적용하는 간격. 간격이 커질수록 출력 데이터 배열의 크기는 작아짐. 기본 값은 1\n",
        "*   padding: 출력 데이터 배열의 크기를 조정하기 위해서 이미지의 주변을 채워줌. 패딩 사이즈 기본 값은 0\n",
        "\n"
      ],
      "metadata": {
        "id": "MsE2s_f6UX7z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#가로 28x세로 28 = 784 픽셀로 이루어진 이미지\n",
        "#각 픽셀은 밝기 정도에 따라 0부터 255까지 등급을 매김\n",
        "#흰 배경이 0, 글씨가 있는 곳은 1~255 숫자 중 하나로 채워져, 긴 행렬로 이루어진 하나의 집합으로 변환됨"
      ],
      "metadata": {
        "id": "er8ccA1Pwfuw"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**output size 계산**\n",
        "\n",
        "*   {input size - kernel size + (2*padding)} / stride +1\n",
        "*   우리가 가진 데이터에서 입력 이미지는 28x28 픽셀, conv1(1, 10, 5)일 때 output layer\n",
        "   *  ((28 - 5) + (2*0)) / 1 + 1 = 24\n",
        "   *  output size는 24x24\n",
        "\n"
      ],
      "metadata": {
        "id": "nmzoYHlaVbV-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#CNN class 정의\n",
        "class Net(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(Net, self).__init__()   # super함수는 CNN class의 부모 class인 nn.Module을 초기화\n",
        "    self.conv1 = nn.Conv2d(in_channels=1, out_channels=10, kernel_size = 5)\n",
        "        #합성곱 레이어1 : 입력 크기는 28x28x1 -> 출력 크기는 24x24x10 =>맥스 풀링(/2)을 거쳐 12x12x10로 변환\n",
        "    self.conv2 = nn.Conv2d(in_channels=10, out_channels=20, kernel_size = 5)\n",
        "        #합성곱 레이어2 : 입력 크기는 12x12x10 -> 출력 크기는 8x8x20 =>맥스 풀링을 거쳐 4x4x20\n",
        "    self.mp = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
        "        #맥스풀링 레이어 : 픽셀값을 절반으로 나눠줌\n",
        "    self.fc = nn.Linear(4*4*20, 10) ### : 알맞는 input\n",
        "        #입력 크기 320, 출력 클래스 수는 10(0~9까지 숫자 맞히기)\n",
        "        #입력층과 출력층 사이에 선형 변환 수행(입력 벡터를 받아서 가중치 행렬과의 행렬 곱 계산, bias 더해줌)\n",
        "\n",
        "  def forward(self, x):\n",
        "    in_size = x.size(0)\n",
        "    x = F.relu(self.mp(self.conv1(x)))\n",
        "      #첫 번째 합성곱 계층을 통과한 후 ReLU 활성화 함수를 적용하고, 그 결과를 최대 풀링 계층에 통과시킴\n",
        "    x = F.relu(self.mp(self.conv2(x)))\n",
        "      #두 번째 합성곱 계층을 통과...\n",
        "    x = x.view(in_size, -1)       #출력을 1차원으로 평탄화\n",
        "    x = self.fc(x)    #평탄화된 출력에 완전 연결 계층을 적용하여 클래스에 대한 로그 확률을 계산\n",
        "    return F.log_softmax(x)   #로그 소프트맥스 함수를 적용하여 각 클래스에 대한 예측값을 확률 값으로 반환\n",
        "\n",
        "    ##https://kh-kim.github.io/nlp_with_deep_learning_blog/docs/1-13-deep_neural_networks_ii/05-softmax_and_cross_entropy/\n",
        "    ##지수함수를 사용하는 소프트맥스의 특성상, 로그소프트맥스가 좀 더 빠른 연산 속도를 제공할 수 있기 때문에 후자가 좀 더 선호되는 편이라고,,\n",
        "    ##로그 소프트맥스 사용할 때는 NLL 손실 함수 사용해줘야 함"
      ],
      "metadata": {
        "id": "tLCSvgganBrH"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**완전연결 계층**\n",
        "\n",
        "*   Fully conneted layer, Dense layer\n",
        "*   '완전 연결되었다' : 한 layer의 모든 뉴런이 다음 layer의 모든 뉴런과 연결된 상태. 1차원 배열의 형태로 평탄화된 행렬을 통해 이미지를 분류하는 데 사용되는 계층\n",
        "*   2차월 벡터의 행렬을 1차원 배열로 평탄화, relu 함수로 활성화, softmax 함수로 이미지 분류하는 것까지가 fully connected layer라고 함\n",
        "\n"
      ],
      "metadata": {
        "id": "rHLc3psLjx_y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**pooling**\n",
        "\n",
        "*   컨볼루션 층을 통해 이미지 특징을 요약하여도, 그 결과가 여전히 크고 복잡하면 이를 다시 한번 축소함 (pooling/sub sampling)\n",
        "*   정해진 구역 안에서 최댓값을 뽑아내는 max pooling, 평균값을 뽑아내는 average pooling 등\n"
      ],
      "metadata": {
        "id": "Pq324AChdSvB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**drop out**\n",
        "\n",
        "*   은닉층에 배치된 노드 중 일부를 임의로 꺼주는 것. 특정 노드에 지나치게 치우쳐서 학습되는 과적합 방지\n",
        "*   매 학습이 일어날 때마다 일정한 수의 다른 노드가 랜덤하게 학습에서 배제됨"
      ],
      "metadata": {
        "id": "66h4TfnKd1m_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = Net()\n",
        "optimizer = optim.SGD(model.parameters(), lr = 0.01, momentum = 0.5)"
      ],
      "metadata": {
        "id": "lkYZ4pUdnUHc"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(epoch):\n",
        "  model.train()\n",
        "  for batch_idx, (data, target) in enumerate(train_loader):\n",
        "    data, target = Variable(data), Variable(target)   #데이터와 타겟을 Variable로 변환하여 GPU에서 연산할 수 있도록 함\n",
        "    optimizer.zero_grad()   #기울기를 초기화\n",
        "    output = model(data)    #모델에 데이터를 입력\n",
        "    loss = F.nll_loss(output, target)     #예측값과 실제값 사이의 손실 계산\n",
        "    loss.backward()         #역전파를 통해 파라미터 기울기 계산\n",
        "    optimizer.step()        #옵티마이저로 파라미터 업데이트\n",
        "    if batch_idx % 10 == 0:   #현재 미니배치의 인덱스가 10의 배수일 때마다 아래 코드 블록을 실행\n",
        "      print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "          epoch, batch_idx * len(data), len(train_loader.dataset),\n",
        "          100. * batch_idx / len(train_loader), loss.item()))\n",
        "          #에포크, 미니배치 인덱스, 전체 데이터셋 크기, 현재 미니배치의 손실을 출력"
      ],
      "metadata": {
        "id": "IzUrEM3EnXJb"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**model.eval()**\n",
        "\n",
        "*   모델의 모든 레이어가 evalutaion mode에 들어가도록 해줌 -> 학습할 때만 필요했던 drop out, batch norm 등의 기능을 추론할 때는 비활성화시킴\n",
        "    * 메모리와는 관련이 없다고\n",
        "*   torch.no_grad() : gradient 계산을 비활성화 함으로써 gradient를 더이상 트래킹하지 않음. 메모리가 줄어들고 연산 속도가 증가함"
      ],
      "metadata": {
        "id": "r_jvGggceda7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def test():\n",
        "    model.eval() #model.eval() 의 기능은?\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    for data, target in test_loader:\n",
        "        data, target = Variable(data, volatile=True), Variable(target)\n",
        "        #역전파 단계에서 그래디언트가 계산되지 않음, 메모리 절약하기 위해 사용 -> 최신 버전 파이토치에서는 with torch.no_grad()를 사용한다고..\n",
        "        output = model(data)\n",
        "        test_loss += F.nll_loss(output, target, size_average=False).data # nll_loss?? / cross entropy loss와의 관계 확인!\n",
        "        #size_average=False로 설정하여 각 배치의 손실 합을 구함\n",
        "        pred = output.data.max(1, keepdim=True)[1]\n",
        "        #각 입력 샘플에 대한 예측 저장\n",
        "        correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
        "        #예측값과 실제 타겟을 비교하여 올바른 예측수 계산\n",
        "\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "      #테스트 손실을 전체 테스트 데이터셋의 크기로 나누어 평균 손실을 계산\n",
        "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "        test_loss, correct, len(test_loader.dataset),\n",
        "        100. * correct / len(test_loader.dataset)))   #평균 손실과 정확도 출력함"
      ],
      "metadata": {
        "id": "EFi0gYJGn2aa"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**null_loss**\n",
        "\n",
        "\n",
        "*   음의 로그 우도(Negative Log Likelihood) 손실을 계산하는 손실 함수. 로그 소프트맥스(log softmax) 출력을 사용\n",
        "*   torch.nn.CrossEntropyLoss : nn.LogSoftmax와 nn.NLLLoss의 연산의 조합\n",
        "\n",
        "\n",
        "https://supermemi.tistory.com/entry/Loss-Cross-Entropy-Negative-Log-Likelihood-%EB%82%B4%EC%9A%A9-%EC%A0%95%EB%A6%AC-Pytorch-Code\n",
        "1.    NLLLoss 안에서는 softmax나 log함수가 이뤄지지 않습니다. 그래서 모델 output(raw data)을 input으로 그대로 사용하는 것이 아니라 LogSoftmax 함수를 적용한 후 input으로 사용해야합니다.\n",
        "2.   CrossEntropyLoss 안에서 LogSoftmax와 Negative Log-likelihood 가 진행되기 때문에 softmax나 log 함수가 적용되지 않은 모델 output(raw data)을 input으로 주어야 합니다.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "F9ayvAdIgUxa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(1, 10):\n",
        "    train(epoch)\n",
        "    test()"
      ],
      "metadata": {
        "id": "zSvSZb_Bn4Nx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e5f60908-674a-461c-90b7-579661de83c7"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-39-59477b7fa2b5>:23: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return F.log_softmax(x)   #로그 소프트맥스 함수를 적용하여 각 클래스에 대한 예측값을 확률 값으로 반환\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.300726\n",
            "Train Epoch: 1 [640/60000 (1%)]\tLoss: 2.273365\n",
            "Train Epoch: 1 [1280/60000 (2%)]\tLoss: 2.251930\n",
            "Train Epoch: 1 [1920/60000 (3%)]\tLoss: 2.232374\n",
            "Train Epoch: 1 [2560/60000 (4%)]\tLoss: 2.176073\n",
            "Train Epoch: 1 [3200/60000 (5%)]\tLoss: 2.193516\n",
            "Train Epoch: 1 [3840/60000 (6%)]\tLoss: 2.114929\n",
            "Train Epoch: 1 [4480/60000 (7%)]\tLoss: 1.945692\n",
            "Train Epoch: 1 [5120/60000 (9%)]\tLoss: 1.896105\n",
            "Train Epoch: 1 [5760/60000 (10%)]\tLoss: 1.576200\n",
            "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 1.396972\n",
            "Train Epoch: 1 [7040/60000 (12%)]\tLoss: 1.091363\n",
            "Train Epoch: 1 [7680/60000 (13%)]\tLoss: 0.931685\n",
            "Train Epoch: 1 [8320/60000 (14%)]\tLoss: 0.834722\n",
            "Train Epoch: 1 [8960/60000 (15%)]\tLoss: 0.689583\n",
            "Train Epoch: 1 [9600/60000 (16%)]\tLoss: 0.619575\n",
            "Train Epoch: 1 [10240/60000 (17%)]\tLoss: 0.486502\n",
            "Train Epoch: 1 [10880/60000 (18%)]\tLoss: 0.568944\n",
            "Train Epoch: 1 [11520/60000 (19%)]\tLoss: 0.434625\n",
            "Train Epoch: 1 [12160/60000 (20%)]\tLoss: 0.421943\n",
            "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.438076\n",
            "Train Epoch: 1 [13440/60000 (22%)]\tLoss: 0.320853\n",
            "Train Epoch: 1 [14080/60000 (23%)]\tLoss: 0.427527\n",
            "Train Epoch: 1 [14720/60000 (25%)]\tLoss: 0.832622\n",
            "Train Epoch: 1 [15360/60000 (26%)]\tLoss: 0.639795\n",
            "Train Epoch: 1 [16000/60000 (27%)]\tLoss: 0.575998\n",
            "Train Epoch: 1 [16640/60000 (28%)]\tLoss: 0.588662\n",
            "Train Epoch: 1 [17280/60000 (29%)]\tLoss: 0.367149\n",
            "Train Epoch: 1 [17920/60000 (30%)]\tLoss: 0.347469\n",
            "Train Epoch: 1 [18560/60000 (31%)]\tLoss: 0.492447\n",
            "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.559437\n",
            "Train Epoch: 1 [19840/60000 (33%)]\tLoss: 0.599654\n",
            "Train Epoch: 1 [20480/60000 (34%)]\tLoss: 0.429873\n",
            "Train Epoch: 1 [21120/60000 (35%)]\tLoss: 0.512625\n",
            "Train Epoch: 1 [21760/60000 (36%)]\tLoss: 0.350651\n",
            "Train Epoch: 1 [22400/60000 (37%)]\tLoss: 0.291412\n",
            "Train Epoch: 1 [23040/60000 (38%)]\tLoss: 0.224245\n",
            "Train Epoch: 1 [23680/60000 (39%)]\tLoss: 0.509261\n",
            "Train Epoch: 1 [24320/60000 (41%)]\tLoss: 0.679937\n",
            "Train Epoch: 1 [24960/60000 (42%)]\tLoss: 0.338788\n",
            "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.155449\n",
            "Train Epoch: 1 [26240/60000 (44%)]\tLoss: 0.307878\n",
            "Train Epoch: 1 [26880/60000 (45%)]\tLoss: 0.229660\n",
            "Train Epoch: 1 [27520/60000 (46%)]\tLoss: 0.254997\n",
            "Train Epoch: 1 [28160/60000 (47%)]\tLoss: 0.326377\n",
            "Train Epoch: 1 [28800/60000 (48%)]\tLoss: 0.236076\n",
            "Train Epoch: 1 [29440/60000 (49%)]\tLoss: 0.351522\n",
            "Train Epoch: 1 [30080/60000 (50%)]\tLoss: 0.389913\n",
            "Train Epoch: 1 [30720/60000 (51%)]\tLoss: 0.408065\n",
            "Train Epoch: 1 [31360/60000 (52%)]\tLoss: 0.291603\n",
            "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.312974\n",
            "Train Epoch: 1 [32640/60000 (54%)]\tLoss: 0.340250\n",
            "Train Epoch: 1 [33280/60000 (55%)]\tLoss: 0.281253\n",
            "Train Epoch: 1 [33920/60000 (57%)]\tLoss: 0.450093\n",
            "Train Epoch: 1 [34560/60000 (58%)]\tLoss: 0.330775\n",
            "Train Epoch: 1 [35200/60000 (59%)]\tLoss: 0.276068\n",
            "Train Epoch: 1 [35840/60000 (60%)]\tLoss: 0.406866\n",
            "Train Epoch: 1 [36480/60000 (61%)]\tLoss: 0.155488\n",
            "Train Epoch: 1 [37120/60000 (62%)]\tLoss: 0.235486\n",
            "Train Epoch: 1 [37760/60000 (63%)]\tLoss: 0.333458\n",
            "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.472020\n",
            "Train Epoch: 1 [39040/60000 (65%)]\tLoss: 0.778466\n",
            "Train Epoch: 1 [39680/60000 (66%)]\tLoss: 0.240473\n",
            "Train Epoch: 1 [40320/60000 (67%)]\tLoss: 0.390749\n",
            "Train Epoch: 1 [40960/60000 (68%)]\tLoss: 0.148269\n",
            "Train Epoch: 1 [41600/60000 (69%)]\tLoss: 0.425933\n",
            "Train Epoch: 1 [42240/60000 (70%)]\tLoss: 0.267877\n",
            "Train Epoch: 1 [42880/60000 (71%)]\tLoss: 0.235197\n",
            "Train Epoch: 1 [43520/60000 (72%)]\tLoss: 0.276291\n",
            "Train Epoch: 1 [44160/60000 (74%)]\tLoss: 0.132488\n",
            "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.242890\n",
            "Train Epoch: 1 [45440/60000 (76%)]\tLoss: 0.164905\n",
            "Train Epoch: 1 [46080/60000 (77%)]\tLoss: 0.082670\n",
            "Train Epoch: 1 [46720/60000 (78%)]\tLoss: 0.404764\n",
            "Train Epoch: 1 [47360/60000 (79%)]\tLoss: 0.277576\n",
            "Train Epoch: 1 [48000/60000 (80%)]\tLoss: 0.221470\n",
            "Train Epoch: 1 [48640/60000 (81%)]\tLoss: 0.217390\n",
            "Train Epoch: 1 [49280/60000 (82%)]\tLoss: 0.348762\n",
            "Train Epoch: 1 [49920/60000 (83%)]\tLoss: 0.256285\n",
            "Train Epoch: 1 [50560/60000 (84%)]\tLoss: 0.313704\n",
            "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.291751\n",
            "Train Epoch: 1 [51840/60000 (86%)]\tLoss: 0.223153\n",
            "Train Epoch: 1 [52480/60000 (87%)]\tLoss: 0.187136\n",
            "Train Epoch: 1 [53120/60000 (88%)]\tLoss: 0.238349\n",
            "Train Epoch: 1 [53760/60000 (90%)]\tLoss: 0.140114\n",
            "Train Epoch: 1 [54400/60000 (91%)]\tLoss: 0.324673\n",
            "Train Epoch: 1 [55040/60000 (92%)]\tLoss: 0.262730\n",
            "Train Epoch: 1 [55680/60000 (93%)]\tLoss: 0.102914\n",
            "Train Epoch: 1 [56320/60000 (94%)]\tLoss: 0.146324\n",
            "Train Epoch: 1 [56960/60000 (95%)]\tLoss: 0.145617\n",
            "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.235527\n",
            "Train Epoch: 1 [58240/60000 (97%)]\tLoss: 0.109058\n",
            "Train Epoch: 1 [58880/60000 (98%)]\tLoss: 0.239963\n",
            "Train Epoch: 1 [59520/60000 (99%)]\tLoss: 0.188166\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-42-daccfb86f72c>:6: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
            "  data, target = Variable(data, volatile=True), Variable(target)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
            "  warnings.warn(warning.format(ret))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.1807, Accuracy: 9458/10000 (95%)\n",
            "\n",
            "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.149874\n",
            "Train Epoch: 2 [640/60000 (1%)]\tLoss: 0.158970\n",
            "Train Epoch: 2 [1280/60000 (2%)]\tLoss: 0.120138\n",
            "Train Epoch: 2 [1920/60000 (3%)]\tLoss: 0.218476\n",
            "Train Epoch: 2 [2560/60000 (4%)]\tLoss: 0.229115\n",
            "Train Epoch: 2 [3200/60000 (5%)]\tLoss: 0.405875\n",
            "Train Epoch: 2 [3840/60000 (6%)]\tLoss: 0.221780\n",
            "Train Epoch: 2 [4480/60000 (7%)]\tLoss: 0.304464\n",
            "Train Epoch: 2 [5120/60000 (9%)]\tLoss: 0.265579\n",
            "Train Epoch: 2 [5760/60000 (10%)]\tLoss: 0.258661\n",
            "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 0.254684\n",
            "Train Epoch: 2 [7040/60000 (12%)]\tLoss: 0.198440\n",
            "Train Epoch: 2 [7680/60000 (13%)]\tLoss: 0.116211\n",
            "Train Epoch: 2 [8320/60000 (14%)]\tLoss: 0.292966\n",
            "Train Epoch: 2 [8960/60000 (15%)]\tLoss: 0.243772\n",
            "Train Epoch: 2 [9600/60000 (16%)]\tLoss: 0.154295\n",
            "Train Epoch: 2 [10240/60000 (17%)]\tLoss: 0.173079\n",
            "Train Epoch: 2 [10880/60000 (18%)]\tLoss: 0.122388\n",
            "Train Epoch: 2 [11520/60000 (19%)]\tLoss: 0.289004\n",
            "Train Epoch: 2 [12160/60000 (20%)]\tLoss: 0.113864\n",
            "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 0.263954\n",
            "Train Epoch: 2 [13440/60000 (22%)]\tLoss: 0.092774\n",
            "Train Epoch: 2 [14080/60000 (23%)]\tLoss: 0.128801\n",
            "Train Epoch: 2 [14720/60000 (25%)]\tLoss: 0.195315\n",
            "Train Epoch: 2 [15360/60000 (26%)]\tLoss: 0.248380\n",
            "Train Epoch: 2 [16000/60000 (27%)]\tLoss: 0.148746\n",
            "Train Epoch: 2 [16640/60000 (28%)]\tLoss: 0.106729\n",
            "Train Epoch: 2 [17280/60000 (29%)]\tLoss: 0.091693\n",
            "Train Epoch: 2 [17920/60000 (30%)]\tLoss: 0.110465\n",
            "Train Epoch: 2 [18560/60000 (31%)]\tLoss: 0.252077\n",
            "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 0.231072\n",
            "Train Epoch: 2 [19840/60000 (33%)]\tLoss: 0.213869\n",
            "Train Epoch: 2 [20480/60000 (34%)]\tLoss: 0.074441\n",
            "Train Epoch: 2 [21120/60000 (35%)]\tLoss: 0.354207\n",
            "Train Epoch: 2 [21760/60000 (36%)]\tLoss: 0.149927\n",
            "Train Epoch: 2 [22400/60000 (37%)]\tLoss: 0.152499\n",
            "Train Epoch: 2 [23040/60000 (38%)]\tLoss: 0.133417\n",
            "Train Epoch: 2 [23680/60000 (39%)]\tLoss: 0.129887\n",
            "Train Epoch: 2 [24320/60000 (41%)]\tLoss: 0.157020\n",
            "Train Epoch: 2 [24960/60000 (42%)]\tLoss: 0.116066\n",
            "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.102502\n",
            "Train Epoch: 2 [26240/60000 (44%)]\tLoss: 0.067189\n",
            "Train Epoch: 2 [26880/60000 (45%)]\tLoss: 0.154113\n",
            "Train Epoch: 2 [27520/60000 (46%)]\tLoss: 0.114417\n",
            "Train Epoch: 2 [28160/60000 (47%)]\tLoss: 0.190176\n",
            "Train Epoch: 2 [28800/60000 (48%)]\tLoss: 0.096115\n",
            "Train Epoch: 2 [29440/60000 (49%)]\tLoss: 0.162662\n",
            "Train Epoch: 2 [30080/60000 (50%)]\tLoss: 0.159428\n",
            "Train Epoch: 2 [30720/60000 (51%)]\tLoss: 0.373205\n",
            "Train Epoch: 2 [31360/60000 (52%)]\tLoss: 0.381781\n",
            "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 0.426097\n",
            "Train Epoch: 2 [32640/60000 (54%)]\tLoss: 0.107775\n",
            "Train Epoch: 2 [33280/60000 (55%)]\tLoss: 0.182523\n",
            "Train Epoch: 2 [33920/60000 (57%)]\tLoss: 0.203940\n",
            "Train Epoch: 2 [34560/60000 (58%)]\tLoss: 0.198450\n",
            "Train Epoch: 2 [35200/60000 (59%)]\tLoss: 0.077414\n",
            "Train Epoch: 2 [35840/60000 (60%)]\tLoss: 0.164213\n",
            "Train Epoch: 2 [36480/60000 (61%)]\tLoss: 0.156557\n",
            "Train Epoch: 2 [37120/60000 (62%)]\tLoss: 0.129338\n",
            "Train Epoch: 2 [37760/60000 (63%)]\tLoss: 0.236483\n",
            "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 0.094693\n",
            "Train Epoch: 2 [39040/60000 (65%)]\tLoss: 0.051886\n",
            "Train Epoch: 2 [39680/60000 (66%)]\tLoss: 0.257329\n",
            "Train Epoch: 2 [40320/60000 (67%)]\tLoss: 0.108642\n",
            "Train Epoch: 2 [40960/60000 (68%)]\tLoss: 0.158318\n",
            "Train Epoch: 2 [41600/60000 (69%)]\tLoss: 0.132915\n",
            "Train Epoch: 2 [42240/60000 (70%)]\tLoss: 0.257270\n",
            "Train Epoch: 2 [42880/60000 (71%)]\tLoss: 0.172143\n",
            "Train Epoch: 2 [43520/60000 (72%)]\tLoss: 0.059917\n",
            "Train Epoch: 2 [44160/60000 (74%)]\tLoss: 0.096935\n",
            "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 0.074897\n",
            "Train Epoch: 2 [45440/60000 (76%)]\tLoss: 0.129938\n",
            "Train Epoch: 2 [46080/60000 (77%)]\tLoss: 0.054574\n",
            "Train Epoch: 2 [46720/60000 (78%)]\tLoss: 0.328267\n",
            "Train Epoch: 2 [47360/60000 (79%)]\tLoss: 0.225704\n",
            "Train Epoch: 2 [48000/60000 (80%)]\tLoss: 0.098598\n",
            "Train Epoch: 2 [48640/60000 (81%)]\tLoss: 0.202536\n",
            "Train Epoch: 2 [49280/60000 (82%)]\tLoss: 0.087242\n",
            "Train Epoch: 2 [49920/60000 (83%)]\tLoss: 0.095711\n",
            "Train Epoch: 2 [50560/60000 (84%)]\tLoss: 0.055581\n",
            "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.144857\n",
            "Train Epoch: 2 [51840/60000 (86%)]\tLoss: 0.179419\n",
            "Train Epoch: 2 [52480/60000 (87%)]\tLoss: 0.304270\n",
            "Train Epoch: 2 [53120/60000 (88%)]\tLoss: 0.093400\n",
            "Train Epoch: 2 [53760/60000 (90%)]\tLoss: 0.246601\n",
            "Train Epoch: 2 [54400/60000 (91%)]\tLoss: 0.155573\n",
            "Train Epoch: 2 [55040/60000 (92%)]\tLoss: 0.080742\n",
            "Train Epoch: 2 [55680/60000 (93%)]\tLoss: 0.235053\n",
            "Train Epoch: 2 [56320/60000 (94%)]\tLoss: 0.093610\n",
            "Train Epoch: 2 [56960/60000 (95%)]\tLoss: 0.075802\n",
            "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 0.202854\n",
            "Train Epoch: 2 [58240/60000 (97%)]\tLoss: 0.165178\n",
            "Train Epoch: 2 [58880/60000 (98%)]\tLoss: 0.116279\n",
            "Train Epoch: 2 [59520/60000 (99%)]\tLoss: 0.055084\n",
            "\n",
            "Test set: Average loss: 0.1217, Accuracy: 9624/10000 (96%)\n",
            "\n",
            "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.120190\n",
            "Train Epoch: 3 [640/60000 (1%)]\tLoss: 0.132310\n",
            "Train Epoch: 3 [1280/60000 (2%)]\tLoss: 0.114838\n",
            "Train Epoch: 3 [1920/60000 (3%)]\tLoss: 0.150421\n",
            "Train Epoch: 3 [2560/60000 (4%)]\tLoss: 0.116574\n",
            "Train Epoch: 3 [3200/60000 (5%)]\tLoss: 0.097587\n",
            "Train Epoch: 3 [3840/60000 (6%)]\tLoss: 0.220477\n",
            "Train Epoch: 3 [4480/60000 (7%)]\tLoss: 0.038178\n",
            "Train Epoch: 3 [5120/60000 (9%)]\tLoss: 0.217885\n",
            "Train Epoch: 3 [5760/60000 (10%)]\tLoss: 0.090492\n",
            "Train Epoch: 3 [6400/60000 (11%)]\tLoss: 0.077313\n",
            "Train Epoch: 3 [7040/60000 (12%)]\tLoss: 0.135848\n",
            "Train Epoch: 3 [7680/60000 (13%)]\tLoss: 0.118677\n",
            "Train Epoch: 3 [8320/60000 (14%)]\tLoss: 0.079084\n",
            "Train Epoch: 3 [8960/60000 (15%)]\tLoss: 0.163817\n",
            "Train Epoch: 3 [9600/60000 (16%)]\tLoss: 0.167662\n",
            "Train Epoch: 3 [10240/60000 (17%)]\tLoss: 0.142983\n",
            "Train Epoch: 3 [10880/60000 (18%)]\tLoss: 0.104271\n",
            "Train Epoch: 3 [11520/60000 (19%)]\tLoss: 0.082800\n",
            "Train Epoch: 3 [12160/60000 (20%)]\tLoss: 0.094549\n",
            "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 0.082078\n",
            "Train Epoch: 3 [13440/60000 (22%)]\tLoss: 0.105443\n",
            "Train Epoch: 3 [14080/60000 (23%)]\tLoss: 0.075593\n",
            "Train Epoch: 3 [14720/60000 (25%)]\tLoss: 0.142881\n",
            "Train Epoch: 3 [15360/60000 (26%)]\tLoss: 0.123371\n",
            "Train Epoch: 3 [16000/60000 (27%)]\tLoss: 0.136324\n",
            "Train Epoch: 3 [16640/60000 (28%)]\tLoss: 0.244984\n",
            "Train Epoch: 3 [17280/60000 (29%)]\tLoss: 0.083320\n",
            "Train Epoch: 3 [17920/60000 (30%)]\tLoss: 0.198517\n",
            "Train Epoch: 3 [18560/60000 (31%)]\tLoss: 0.126209\n",
            "Train Epoch: 3 [19200/60000 (32%)]\tLoss: 0.107062\n",
            "Train Epoch: 3 [19840/60000 (33%)]\tLoss: 0.089619\n",
            "Train Epoch: 3 [20480/60000 (34%)]\tLoss: 0.111606\n",
            "Train Epoch: 3 [21120/60000 (35%)]\tLoss: 0.046573\n",
            "Train Epoch: 3 [21760/60000 (36%)]\tLoss: 0.164044\n",
            "Train Epoch: 3 [22400/60000 (37%)]\tLoss: 0.084545\n",
            "Train Epoch: 3 [23040/60000 (38%)]\tLoss: 0.129546\n",
            "Train Epoch: 3 [23680/60000 (39%)]\tLoss: 0.124459\n",
            "Train Epoch: 3 [24320/60000 (41%)]\tLoss: 0.141500\n",
            "Train Epoch: 3 [24960/60000 (42%)]\tLoss: 0.032457\n",
            "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.133937\n",
            "Train Epoch: 3 [26240/60000 (44%)]\tLoss: 0.441490\n",
            "Train Epoch: 3 [26880/60000 (45%)]\tLoss: 0.440069\n",
            "Train Epoch: 3 [27520/60000 (46%)]\tLoss: 0.038778\n",
            "Train Epoch: 3 [28160/60000 (47%)]\tLoss: 0.039024\n",
            "Train Epoch: 3 [28800/60000 (48%)]\tLoss: 0.081971\n",
            "Train Epoch: 3 [29440/60000 (49%)]\tLoss: 0.144739\n",
            "Train Epoch: 3 [30080/60000 (50%)]\tLoss: 0.085295\n",
            "Train Epoch: 3 [30720/60000 (51%)]\tLoss: 0.213181\n",
            "Train Epoch: 3 [31360/60000 (52%)]\tLoss: 0.103697\n",
            "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 0.052871\n",
            "Train Epoch: 3 [32640/60000 (54%)]\tLoss: 0.161083\n",
            "Train Epoch: 3 [33280/60000 (55%)]\tLoss: 0.080056\n",
            "Train Epoch: 3 [33920/60000 (57%)]\tLoss: 0.124691\n",
            "Train Epoch: 3 [34560/60000 (58%)]\tLoss: 0.111552\n",
            "Train Epoch: 3 [35200/60000 (59%)]\tLoss: 0.055628\n",
            "Train Epoch: 3 [35840/60000 (60%)]\tLoss: 0.133994\n",
            "Train Epoch: 3 [36480/60000 (61%)]\tLoss: 0.212637\n",
            "Train Epoch: 3 [37120/60000 (62%)]\tLoss: 0.275012\n",
            "Train Epoch: 3 [37760/60000 (63%)]\tLoss: 0.102614\n",
            "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 0.021724\n",
            "Train Epoch: 3 [39040/60000 (65%)]\tLoss: 0.114158\n",
            "Train Epoch: 3 [39680/60000 (66%)]\tLoss: 0.023831\n",
            "Train Epoch: 3 [40320/60000 (67%)]\tLoss: 0.114643\n",
            "Train Epoch: 3 [40960/60000 (68%)]\tLoss: 0.161981\n",
            "Train Epoch: 3 [41600/60000 (69%)]\tLoss: 0.181158\n",
            "Train Epoch: 3 [42240/60000 (70%)]\tLoss: 0.044115\n",
            "Train Epoch: 3 [42880/60000 (71%)]\tLoss: 0.148147\n",
            "Train Epoch: 3 [43520/60000 (72%)]\tLoss: 0.027279\n",
            "Train Epoch: 3 [44160/60000 (74%)]\tLoss: 0.133533\n",
            "Train Epoch: 3 [44800/60000 (75%)]\tLoss: 0.108650\n",
            "Train Epoch: 3 [45440/60000 (76%)]\tLoss: 0.246227\n",
            "Train Epoch: 3 [46080/60000 (77%)]\tLoss: 0.100263\n",
            "Train Epoch: 3 [46720/60000 (78%)]\tLoss: 0.079839\n",
            "Train Epoch: 3 [47360/60000 (79%)]\tLoss: 0.158232\n",
            "Train Epoch: 3 [48000/60000 (80%)]\tLoss: 0.123271\n",
            "Train Epoch: 3 [48640/60000 (81%)]\tLoss: 0.234599\n",
            "Train Epoch: 3 [49280/60000 (82%)]\tLoss: 0.044660\n",
            "Train Epoch: 3 [49920/60000 (83%)]\tLoss: 0.037594\n",
            "Train Epoch: 3 [50560/60000 (84%)]\tLoss: 0.037255\n",
            "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.104903\n",
            "Train Epoch: 3 [51840/60000 (86%)]\tLoss: 0.024206\n",
            "Train Epoch: 3 [52480/60000 (87%)]\tLoss: 0.043266\n",
            "Train Epoch: 3 [53120/60000 (88%)]\tLoss: 0.170208\n",
            "Train Epoch: 3 [53760/60000 (90%)]\tLoss: 0.051444\n",
            "Train Epoch: 3 [54400/60000 (91%)]\tLoss: 0.056640\n",
            "Train Epoch: 3 [55040/60000 (92%)]\tLoss: 0.049967\n",
            "Train Epoch: 3 [55680/60000 (93%)]\tLoss: 0.107331\n",
            "Train Epoch: 3 [56320/60000 (94%)]\tLoss: 0.121086\n",
            "Train Epoch: 3 [56960/60000 (95%)]\tLoss: 0.079495\n",
            "Train Epoch: 3 [57600/60000 (96%)]\tLoss: 0.026889\n",
            "Train Epoch: 3 [58240/60000 (97%)]\tLoss: 0.175260\n",
            "Train Epoch: 3 [58880/60000 (98%)]\tLoss: 0.033061\n",
            "Train Epoch: 3 [59520/60000 (99%)]\tLoss: 0.156329\n",
            "\n",
            "Test set: Average loss: 0.0937, Accuracy: 9710/10000 (97%)\n",
            "\n",
            "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.037529\n",
            "Train Epoch: 4 [640/60000 (1%)]\tLoss: 0.039688\n",
            "Train Epoch: 4 [1280/60000 (2%)]\tLoss: 0.095977\n",
            "Train Epoch: 4 [1920/60000 (3%)]\tLoss: 0.124998\n",
            "Train Epoch: 4 [2560/60000 (4%)]\tLoss: 0.052884\n",
            "Train Epoch: 4 [3200/60000 (5%)]\tLoss: 0.076721\n",
            "Train Epoch: 4 [3840/60000 (6%)]\tLoss: 0.057710\n",
            "Train Epoch: 4 [4480/60000 (7%)]\tLoss: 0.094879\n",
            "Train Epoch: 4 [5120/60000 (9%)]\tLoss: 0.118066\n",
            "Train Epoch: 4 [5760/60000 (10%)]\tLoss: 0.315378\n",
            "Train Epoch: 4 [6400/60000 (11%)]\tLoss: 0.081013\n",
            "Train Epoch: 4 [7040/60000 (12%)]\tLoss: 0.064233\n",
            "Train Epoch: 4 [7680/60000 (13%)]\tLoss: 0.261245\n",
            "Train Epoch: 4 [8320/60000 (14%)]\tLoss: 0.167292\n",
            "Train Epoch: 4 [8960/60000 (15%)]\tLoss: 0.155960\n",
            "Train Epoch: 4 [9600/60000 (16%)]\tLoss: 0.095859\n",
            "Train Epoch: 4 [10240/60000 (17%)]\tLoss: 0.075860\n",
            "Train Epoch: 4 [10880/60000 (18%)]\tLoss: 0.117770\n",
            "Train Epoch: 4 [11520/60000 (19%)]\tLoss: 0.098164\n",
            "Train Epoch: 4 [12160/60000 (20%)]\tLoss: 0.195782\n",
            "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 0.108160\n",
            "Train Epoch: 4 [13440/60000 (22%)]\tLoss: 0.080111\n",
            "Train Epoch: 4 [14080/60000 (23%)]\tLoss: 0.057782\n",
            "Train Epoch: 4 [14720/60000 (25%)]\tLoss: 0.090325\n",
            "Train Epoch: 4 [15360/60000 (26%)]\tLoss: 0.157072\n",
            "Train Epoch: 4 [16000/60000 (27%)]\tLoss: 0.220844\n",
            "Train Epoch: 4 [16640/60000 (28%)]\tLoss: 0.088337\n",
            "Train Epoch: 4 [17280/60000 (29%)]\tLoss: 0.044254\n",
            "Train Epoch: 4 [17920/60000 (30%)]\tLoss: 0.072379\n",
            "Train Epoch: 4 [18560/60000 (31%)]\tLoss: 0.070499\n",
            "Train Epoch: 4 [19200/60000 (32%)]\tLoss: 0.046186\n",
            "Train Epoch: 4 [19840/60000 (33%)]\tLoss: 0.119564\n",
            "Train Epoch: 4 [20480/60000 (34%)]\tLoss: 0.082062\n",
            "Train Epoch: 4 [21120/60000 (35%)]\tLoss: 0.114535\n",
            "Train Epoch: 4 [21760/60000 (36%)]\tLoss: 0.045164\n",
            "Train Epoch: 4 [22400/60000 (37%)]\tLoss: 0.068349\n",
            "Train Epoch: 4 [23040/60000 (38%)]\tLoss: 0.195178\n",
            "Train Epoch: 4 [23680/60000 (39%)]\tLoss: 0.078817\n",
            "Train Epoch: 4 [24320/60000 (41%)]\tLoss: 0.073790\n",
            "Train Epoch: 4 [24960/60000 (42%)]\tLoss: 0.044709\n",
            "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 0.084969\n",
            "Train Epoch: 4 [26240/60000 (44%)]\tLoss: 0.068384\n",
            "Train Epoch: 4 [26880/60000 (45%)]\tLoss: 0.032342\n",
            "Train Epoch: 4 [27520/60000 (46%)]\tLoss: 0.087894\n",
            "Train Epoch: 4 [28160/60000 (47%)]\tLoss: 0.103449\n",
            "Train Epoch: 4 [28800/60000 (48%)]\tLoss: 0.128744\n",
            "Train Epoch: 4 [29440/60000 (49%)]\tLoss: 0.100656\n",
            "Train Epoch: 4 [30080/60000 (50%)]\tLoss: 0.018045\n",
            "Train Epoch: 4 [30720/60000 (51%)]\tLoss: 0.091937\n",
            "Train Epoch: 4 [31360/60000 (52%)]\tLoss: 0.113897\n",
            "Train Epoch: 4 [32000/60000 (53%)]\tLoss: 0.035718\n",
            "Train Epoch: 4 [32640/60000 (54%)]\tLoss: 0.108710\n",
            "Train Epoch: 4 [33280/60000 (55%)]\tLoss: 0.062445\n",
            "Train Epoch: 4 [33920/60000 (57%)]\tLoss: 0.046720\n",
            "Train Epoch: 4 [34560/60000 (58%)]\tLoss: 0.102303\n",
            "Train Epoch: 4 [35200/60000 (59%)]\tLoss: 0.046244\n",
            "Train Epoch: 4 [35840/60000 (60%)]\tLoss: 0.136095\n",
            "Train Epoch: 4 [36480/60000 (61%)]\tLoss: 0.040687\n",
            "Train Epoch: 4 [37120/60000 (62%)]\tLoss: 0.050171\n",
            "Train Epoch: 4 [37760/60000 (63%)]\tLoss: 0.140984\n",
            "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 0.087226\n",
            "Train Epoch: 4 [39040/60000 (65%)]\tLoss: 0.097325\n",
            "Train Epoch: 4 [39680/60000 (66%)]\tLoss: 0.139788\n",
            "Train Epoch: 4 [40320/60000 (67%)]\tLoss: 0.136493\n",
            "Train Epoch: 4 [40960/60000 (68%)]\tLoss: 0.016179\n",
            "Train Epoch: 4 [41600/60000 (69%)]\tLoss: 0.033884\n",
            "Train Epoch: 4 [42240/60000 (70%)]\tLoss: 0.376362\n",
            "Train Epoch: 4 [42880/60000 (71%)]\tLoss: 0.115889\n",
            "Train Epoch: 4 [43520/60000 (72%)]\tLoss: 0.088208\n",
            "Train Epoch: 4 [44160/60000 (74%)]\tLoss: 0.095337\n",
            "Train Epoch: 4 [44800/60000 (75%)]\tLoss: 0.102778\n",
            "Train Epoch: 4 [45440/60000 (76%)]\tLoss: 0.075815\n",
            "Train Epoch: 4 [46080/60000 (77%)]\tLoss: 0.110356\n",
            "Train Epoch: 4 [46720/60000 (78%)]\tLoss: 0.064212\n",
            "Train Epoch: 4 [47360/60000 (79%)]\tLoss: 0.026303\n",
            "Train Epoch: 4 [48000/60000 (80%)]\tLoss: 0.207425\n",
            "Train Epoch: 4 [48640/60000 (81%)]\tLoss: 0.175018\n",
            "Train Epoch: 4 [49280/60000 (82%)]\tLoss: 0.074429\n",
            "Train Epoch: 4 [49920/60000 (83%)]\tLoss: 0.070064\n",
            "Train Epoch: 4 [50560/60000 (84%)]\tLoss: 0.051600\n",
            "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 0.213642\n",
            "Train Epoch: 4 [51840/60000 (86%)]\tLoss: 0.089209\n",
            "Train Epoch: 4 [52480/60000 (87%)]\tLoss: 0.060028\n",
            "Train Epoch: 4 [53120/60000 (88%)]\tLoss: 0.209485\n",
            "Train Epoch: 4 [53760/60000 (90%)]\tLoss: 0.207455\n",
            "Train Epoch: 4 [54400/60000 (91%)]\tLoss: 0.136669\n",
            "Train Epoch: 4 [55040/60000 (92%)]\tLoss: 0.079682\n",
            "Train Epoch: 4 [55680/60000 (93%)]\tLoss: 0.074339\n",
            "Train Epoch: 4 [56320/60000 (94%)]\tLoss: 0.204184\n",
            "Train Epoch: 4 [56960/60000 (95%)]\tLoss: 0.022853\n",
            "Train Epoch: 4 [57600/60000 (96%)]\tLoss: 0.031260\n",
            "Train Epoch: 4 [58240/60000 (97%)]\tLoss: 0.092441\n",
            "Train Epoch: 4 [58880/60000 (98%)]\tLoss: 0.035896\n",
            "Train Epoch: 4 [59520/60000 (99%)]\tLoss: 0.046379\n",
            "\n",
            "Test set: Average loss: 0.0848, Accuracy: 9733/10000 (97%)\n",
            "\n",
            "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.142908\n",
            "Train Epoch: 5 [640/60000 (1%)]\tLoss: 0.042599\n",
            "Train Epoch: 5 [1280/60000 (2%)]\tLoss: 0.021846\n",
            "Train Epoch: 5 [1920/60000 (3%)]\tLoss: 0.156329\n",
            "Train Epoch: 5 [2560/60000 (4%)]\tLoss: 0.098627\n",
            "Train Epoch: 5 [3200/60000 (5%)]\tLoss: 0.124385\n",
            "Train Epoch: 5 [3840/60000 (6%)]\tLoss: 0.113111\n",
            "Train Epoch: 5 [4480/60000 (7%)]\tLoss: 0.106470\n",
            "Train Epoch: 5 [5120/60000 (9%)]\tLoss: 0.062189\n",
            "Train Epoch: 5 [5760/60000 (10%)]\tLoss: 0.081231\n",
            "Train Epoch: 5 [6400/60000 (11%)]\tLoss: 0.278867\n",
            "Train Epoch: 5 [7040/60000 (12%)]\tLoss: 0.101383\n",
            "Train Epoch: 5 [7680/60000 (13%)]\tLoss: 0.029324\n",
            "Train Epoch: 5 [8320/60000 (14%)]\tLoss: 0.206682\n",
            "Train Epoch: 5 [8960/60000 (15%)]\tLoss: 0.033444\n",
            "Train Epoch: 5 [9600/60000 (16%)]\tLoss: 0.090031\n",
            "Train Epoch: 5 [10240/60000 (17%)]\tLoss: 0.015831\n",
            "Train Epoch: 5 [10880/60000 (18%)]\tLoss: 0.050284\n",
            "Train Epoch: 5 [11520/60000 (19%)]\tLoss: 0.070783\n",
            "Train Epoch: 5 [12160/60000 (20%)]\tLoss: 0.106393\n",
            "Train Epoch: 5 [12800/60000 (21%)]\tLoss: 0.071437\n",
            "Train Epoch: 5 [13440/60000 (22%)]\tLoss: 0.149155\n",
            "Train Epoch: 5 [14080/60000 (23%)]\tLoss: 0.020889\n",
            "Train Epoch: 5 [14720/60000 (25%)]\tLoss: 0.023237\n",
            "Train Epoch: 5 [15360/60000 (26%)]\tLoss: 0.057012\n",
            "Train Epoch: 5 [16000/60000 (27%)]\tLoss: 0.053208\n",
            "Train Epoch: 5 [16640/60000 (28%)]\tLoss: 0.041010\n",
            "Train Epoch: 5 [17280/60000 (29%)]\tLoss: 0.040565\n",
            "Train Epoch: 5 [17920/60000 (30%)]\tLoss: 0.039568\n",
            "Train Epoch: 5 [18560/60000 (31%)]\tLoss: 0.065866\n",
            "Train Epoch: 5 [19200/60000 (32%)]\tLoss: 0.073325\n",
            "Train Epoch: 5 [19840/60000 (33%)]\tLoss: 0.053294\n",
            "Train Epoch: 5 [20480/60000 (34%)]\tLoss: 0.092849\n",
            "Train Epoch: 5 [21120/60000 (35%)]\tLoss: 0.209344\n",
            "Train Epoch: 5 [21760/60000 (36%)]\tLoss: 0.095956\n",
            "Train Epoch: 5 [22400/60000 (37%)]\tLoss: 0.140005\n",
            "Train Epoch: 5 [23040/60000 (38%)]\tLoss: 0.118912\n",
            "Train Epoch: 5 [23680/60000 (39%)]\tLoss: 0.017808\n",
            "Train Epoch: 5 [24320/60000 (41%)]\tLoss: 0.080394\n",
            "Train Epoch: 5 [24960/60000 (42%)]\tLoss: 0.078870\n",
            "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 0.091033\n",
            "Train Epoch: 5 [26240/60000 (44%)]\tLoss: 0.033629\n",
            "Train Epoch: 5 [26880/60000 (45%)]\tLoss: 0.026156\n",
            "Train Epoch: 5 [27520/60000 (46%)]\tLoss: 0.119663\n",
            "Train Epoch: 5 [28160/60000 (47%)]\tLoss: 0.064650\n",
            "Train Epoch: 5 [28800/60000 (48%)]\tLoss: 0.196662\n",
            "Train Epoch: 5 [29440/60000 (49%)]\tLoss: 0.175980\n",
            "Train Epoch: 5 [30080/60000 (50%)]\tLoss: 0.207752\n",
            "Train Epoch: 5 [30720/60000 (51%)]\tLoss: 0.132085\n",
            "Train Epoch: 5 [31360/60000 (52%)]\tLoss: 0.137000\n",
            "Train Epoch: 5 [32000/60000 (53%)]\tLoss: 0.016246\n",
            "Train Epoch: 5 [32640/60000 (54%)]\tLoss: 0.084918\n",
            "Train Epoch: 5 [33280/60000 (55%)]\tLoss: 0.053254\n",
            "Train Epoch: 5 [33920/60000 (57%)]\tLoss: 0.109442\n",
            "Train Epoch: 5 [34560/60000 (58%)]\tLoss: 0.094773\n",
            "Train Epoch: 5 [35200/60000 (59%)]\tLoss: 0.169293\n",
            "Train Epoch: 5 [35840/60000 (60%)]\tLoss: 0.081905\n",
            "Train Epoch: 5 [36480/60000 (61%)]\tLoss: 0.159844\n",
            "Train Epoch: 5 [37120/60000 (62%)]\tLoss: 0.068052\n",
            "Train Epoch: 5 [37760/60000 (63%)]\tLoss: 0.011265\n",
            "Train Epoch: 5 [38400/60000 (64%)]\tLoss: 0.062358\n",
            "Train Epoch: 5 [39040/60000 (65%)]\tLoss: 0.085107\n",
            "Train Epoch: 5 [39680/60000 (66%)]\tLoss: 0.144456\n",
            "Train Epoch: 5 [40320/60000 (67%)]\tLoss: 0.017849\n",
            "Train Epoch: 5 [40960/60000 (68%)]\tLoss: 0.084205\n",
            "Train Epoch: 5 [41600/60000 (69%)]\tLoss: 0.014043\n",
            "Train Epoch: 5 [42240/60000 (70%)]\tLoss: 0.049282\n",
            "Train Epoch: 5 [42880/60000 (71%)]\tLoss: 0.057640\n",
            "Train Epoch: 5 [43520/60000 (72%)]\tLoss: 0.122245\n",
            "Train Epoch: 5 [44160/60000 (74%)]\tLoss: 0.214348\n",
            "Train Epoch: 5 [44800/60000 (75%)]\tLoss: 0.056699\n",
            "Train Epoch: 5 [45440/60000 (76%)]\tLoss: 0.164150\n",
            "Train Epoch: 5 [46080/60000 (77%)]\tLoss: 0.059653\n",
            "Train Epoch: 5 [46720/60000 (78%)]\tLoss: 0.116666\n",
            "Train Epoch: 5 [47360/60000 (79%)]\tLoss: 0.114179\n",
            "Train Epoch: 5 [48000/60000 (80%)]\tLoss: 0.106592\n",
            "Train Epoch: 5 [48640/60000 (81%)]\tLoss: 0.071296\n",
            "Train Epoch: 5 [49280/60000 (82%)]\tLoss: 0.081750\n",
            "Train Epoch: 5 [49920/60000 (83%)]\tLoss: 0.042631\n",
            "Train Epoch: 5 [50560/60000 (84%)]\tLoss: 0.037262\n",
            "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 0.049774\n",
            "Train Epoch: 5 [51840/60000 (86%)]\tLoss: 0.030694\n",
            "Train Epoch: 5 [52480/60000 (87%)]\tLoss: 0.021194\n",
            "Train Epoch: 5 [53120/60000 (88%)]\tLoss: 0.152023\n",
            "Train Epoch: 5 [53760/60000 (90%)]\tLoss: 0.032446\n",
            "Train Epoch: 5 [54400/60000 (91%)]\tLoss: 0.054004\n",
            "Train Epoch: 5 [55040/60000 (92%)]\tLoss: 0.143291\n",
            "Train Epoch: 5 [55680/60000 (93%)]\tLoss: 0.033178\n",
            "Train Epoch: 5 [56320/60000 (94%)]\tLoss: 0.084006\n",
            "Train Epoch: 5 [56960/60000 (95%)]\tLoss: 0.197744\n",
            "Train Epoch: 5 [57600/60000 (96%)]\tLoss: 0.120134\n",
            "Train Epoch: 5 [58240/60000 (97%)]\tLoss: 0.165462\n",
            "Train Epoch: 5 [58880/60000 (98%)]\tLoss: 0.115116\n",
            "Train Epoch: 5 [59520/60000 (99%)]\tLoss: 0.062650\n",
            "\n",
            "Test set: Average loss: 0.0736, Accuracy: 9781/10000 (98%)\n",
            "\n",
            "Train Epoch: 6 [0/60000 (0%)]\tLoss: 0.008718\n",
            "Train Epoch: 6 [640/60000 (1%)]\tLoss: 0.057303\n",
            "Train Epoch: 6 [1280/60000 (2%)]\tLoss: 0.061969\n",
            "Train Epoch: 6 [1920/60000 (3%)]\tLoss: 0.025291\n",
            "Train Epoch: 6 [2560/60000 (4%)]\tLoss: 0.029646\n",
            "Train Epoch: 6 [3200/60000 (5%)]\tLoss: 0.122976\n",
            "Train Epoch: 6 [3840/60000 (6%)]\tLoss: 0.058141\n",
            "Train Epoch: 6 [4480/60000 (7%)]\tLoss: 0.070638\n",
            "Train Epoch: 6 [5120/60000 (9%)]\tLoss: 0.029778\n",
            "Train Epoch: 6 [5760/60000 (10%)]\tLoss: 0.037523\n",
            "Train Epoch: 6 [6400/60000 (11%)]\tLoss: 0.068734\n",
            "Train Epoch: 6 [7040/60000 (12%)]\tLoss: 0.044177\n",
            "Train Epoch: 6 [7680/60000 (13%)]\tLoss: 0.110053\n",
            "Train Epoch: 6 [8320/60000 (14%)]\tLoss: 0.105979\n",
            "Train Epoch: 6 [8960/60000 (15%)]\tLoss: 0.056749\n",
            "Train Epoch: 6 [9600/60000 (16%)]\tLoss: 0.042641\n",
            "Train Epoch: 6 [10240/60000 (17%)]\tLoss: 0.022762\n",
            "Train Epoch: 6 [10880/60000 (18%)]\tLoss: 0.023308\n",
            "Train Epoch: 6 [11520/60000 (19%)]\tLoss: 0.128611\n",
            "Train Epoch: 6 [12160/60000 (20%)]\tLoss: 0.028049\n",
            "Train Epoch: 6 [12800/60000 (21%)]\tLoss: 0.071296\n",
            "Train Epoch: 6 [13440/60000 (22%)]\tLoss: 0.055174\n",
            "Train Epoch: 6 [14080/60000 (23%)]\tLoss: 0.159949\n",
            "Train Epoch: 6 [14720/60000 (25%)]\tLoss: 0.045196\n",
            "Train Epoch: 6 [15360/60000 (26%)]\tLoss: 0.044528\n",
            "Train Epoch: 6 [16000/60000 (27%)]\tLoss: 0.032339\n",
            "Train Epoch: 6 [16640/60000 (28%)]\tLoss: 0.164148\n",
            "Train Epoch: 6 [17280/60000 (29%)]\tLoss: 0.109787\n",
            "Train Epoch: 6 [17920/60000 (30%)]\tLoss: 0.024148\n",
            "Train Epoch: 6 [18560/60000 (31%)]\tLoss: 0.131544\n",
            "Train Epoch: 6 [19200/60000 (32%)]\tLoss: 0.176994\n",
            "Train Epoch: 6 [19840/60000 (33%)]\tLoss: 0.029637\n",
            "Train Epoch: 6 [20480/60000 (34%)]\tLoss: 0.050131\n",
            "Train Epoch: 6 [21120/60000 (35%)]\tLoss: 0.056632\n",
            "Train Epoch: 6 [21760/60000 (36%)]\tLoss: 0.018863\n",
            "Train Epoch: 6 [22400/60000 (37%)]\tLoss: 0.083041\n",
            "Train Epoch: 6 [23040/60000 (38%)]\tLoss: 0.036832\n",
            "Train Epoch: 6 [23680/60000 (39%)]\tLoss: 0.055308\n",
            "Train Epoch: 6 [24320/60000 (41%)]\tLoss: 0.082922\n",
            "Train Epoch: 6 [24960/60000 (42%)]\tLoss: 0.050604\n",
            "Train Epoch: 6 [25600/60000 (43%)]\tLoss: 0.291061\n",
            "Train Epoch: 6 [26240/60000 (44%)]\tLoss: 0.031204\n",
            "Train Epoch: 6 [26880/60000 (45%)]\tLoss: 0.110191\n",
            "Train Epoch: 6 [27520/60000 (46%)]\tLoss: 0.030055\n",
            "Train Epoch: 6 [28160/60000 (47%)]\tLoss: 0.037800\n",
            "Train Epoch: 6 [28800/60000 (48%)]\tLoss: 0.033322\n",
            "Train Epoch: 6 [29440/60000 (49%)]\tLoss: 0.182885\n",
            "Train Epoch: 6 [30080/60000 (50%)]\tLoss: 0.050483\n",
            "Train Epoch: 6 [30720/60000 (51%)]\tLoss: 0.059053\n",
            "Train Epoch: 6 [31360/60000 (52%)]\tLoss: 0.049448\n",
            "Train Epoch: 6 [32000/60000 (53%)]\tLoss: 0.130973\n",
            "Train Epoch: 6 [32640/60000 (54%)]\tLoss: 0.047181\n",
            "Train Epoch: 6 [33280/60000 (55%)]\tLoss: 0.056502\n",
            "Train Epoch: 6 [33920/60000 (57%)]\tLoss: 0.044866\n",
            "Train Epoch: 6 [34560/60000 (58%)]\tLoss: 0.099093\n",
            "Train Epoch: 6 [35200/60000 (59%)]\tLoss: 0.098822\n",
            "Train Epoch: 6 [35840/60000 (60%)]\tLoss: 0.035418\n",
            "Train Epoch: 6 [36480/60000 (61%)]\tLoss: 0.028571\n",
            "Train Epoch: 6 [37120/60000 (62%)]\tLoss: 0.096092\n",
            "Train Epoch: 6 [37760/60000 (63%)]\tLoss: 0.212995\n",
            "Train Epoch: 6 [38400/60000 (64%)]\tLoss: 0.069873\n",
            "Train Epoch: 6 [39040/60000 (65%)]\tLoss: 0.059435\n",
            "Train Epoch: 6 [39680/60000 (66%)]\tLoss: 0.093961\n",
            "Train Epoch: 6 [40320/60000 (67%)]\tLoss: 0.025470\n",
            "Train Epoch: 6 [40960/60000 (68%)]\tLoss: 0.105814\n",
            "Train Epoch: 6 [41600/60000 (69%)]\tLoss: 0.045585\n",
            "Train Epoch: 6 [42240/60000 (70%)]\tLoss: 0.070530\n",
            "Train Epoch: 6 [42880/60000 (71%)]\tLoss: 0.049272\n",
            "Train Epoch: 6 [43520/60000 (72%)]\tLoss: 0.018651\n",
            "Train Epoch: 6 [44160/60000 (74%)]\tLoss: 0.115405\n",
            "Train Epoch: 6 [44800/60000 (75%)]\tLoss: 0.120217\n",
            "Train Epoch: 6 [45440/60000 (76%)]\tLoss: 0.042395\n",
            "Train Epoch: 6 [46080/60000 (77%)]\tLoss: 0.050082\n",
            "Train Epoch: 6 [46720/60000 (78%)]\tLoss: 0.029750\n",
            "Train Epoch: 6 [47360/60000 (79%)]\tLoss: 0.080436\n",
            "Train Epoch: 6 [48000/60000 (80%)]\tLoss: 0.070132\n",
            "Train Epoch: 6 [48640/60000 (81%)]\tLoss: 0.014790\n",
            "Train Epoch: 6 [49280/60000 (82%)]\tLoss: 0.054561\n",
            "Train Epoch: 6 [49920/60000 (83%)]\tLoss: 0.100629\n",
            "Train Epoch: 6 [50560/60000 (84%)]\tLoss: 0.065215\n",
            "Train Epoch: 6 [51200/60000 (85%)]\tLoss: 0.010388\n",
            "Train Epoch: 6 [51840/60000 (86%)]\tLoss: 0.055727\n",
            "Train Epoch: 6 [52480/60000 (87%)]\tLoss: 0.021320\n",
            "Train Epoch: 6 [53120/60000 (88%)]\tLoss: 0.085586\n",
            "Train Epoch: 6 [53760/60000 (90%)]\tLoss: 0.050005\n",
            "Train Epoch: 6 [54400/60000 (91%)]\tLoss: 0.116438\n",
            "Train Epoch: 6 [55040/60000 (92%)]\tLoss: 0.081446\n",
            "Train Epoch: 6 [55680/60000 (93%)]\tLoss: 0.102302\n",
            "Train Epoch: 6 [56320/60000 (94%)]\tLoss: 0.041469\n",
            "Train Epoch: 6 [56960/60000 (95%)]\tLoss: 0.055823\n",
            "Train Epoch: 6 [57600/60000 (96%)]\tLoss: 0.034828\n",
            "Train Epoch: 6 [58240/60000 (97%)]\tLoss: 0.069517\n",
            "Train Epoch: 6 [58880/60000 (98%)]\tLoss: 0.121551\n",
            "Train Epoch: 6 [59520/60000 (99%)]\tLoss: 0.165930\n",
            "\n",
            "Test set: Average loss: 0.0628, Accuracy: 9801/10000 (98%)\n",
            "\n",
            "Train Epoch: 7 [0/60000 (0%)]\tLoss: 0.087295\n",
            "Train Epoch: 7 [640/60000 (1%)]\tLoss: 0.094826\n",
            "Train Epoch: 7 [1280/60000 (2%)]\tLoss: 0.255690\n",
            "Train Epoch: 7 [1920/60000 (3%)]\tLoss: 0.026215\n",
            "Train Epoch: 7 [2560/60000 (4%)]\tLoss: 0.012925\n",
            "Train Epoch: 7 [3200/60000 (5%)]\tLoss: 0.171594\n",
            "Train Epoch: 7 [3840/60000 (6%)]\tLoss: 0.166477\n",
            "Train Epoch: 7 [4480/60000 (7%)]\tLoss: 0.021704\n",
            "Train Epoch: 7 [5120/60000 (9%)]\tLoss: 0.034220\n",
            "Train Epoch: 7 [5760/60000 (10%)]\tLoss: 0.104643\n",
            "Train Epoch: 7 [6400/60000 (11%)]\tLoss: 0.057638\n",
            "Train Epoch: 7 [7040/60000 (12%)]\tLoss: 0.016654\n",
            "Train Epoch: 7 [7680/60000 (13%)]\tLoss: 0.050024\n",
            "Train Epoch: 7 [8320/60000 (14%)]\tLoss: 0.054301\n",
            "Train Epoch: 7 [8960/60000 (15%)]\tLoss: 0.069126\n",
            "Train Epoch: 7 [9600/60000 (16%)]\tLoss: 0.073252\n",
            "Train Epoch: 7 [10240/60000 (17%)]\tLoss: 0.033142\n",
            "Train Epoch: 7 [10880/60000 (18%)]\tLoss: 0.038502\n",
            "Train Epoch: 7 [11520/60000 (19%)]\tLoss: 0.026645\n",
            "Train Epoch: 7 [12160/60000 (20%)]\tLoss: 0.052171\n",
            "Train Epoch: 7 [12800/60000 (21%)]\tLoss: 0.032000\n",
            "Train Epoch: 7 [13440/60000 (22%)]\tLoss: 0.080877\n",
            "Train Epoch: 7 [14080/60000 (23%)]\tLoss: 0.017235\n",
            "Train Epoch: 7 [14720/60000 (25%)]\tLoss: 0.080741\n",
            "Train Epoch: 7 [15360/60000 (26%)]\tLoss: 0.049885\n",
            "Train Epoch: 7 [16000/60000 (27%)]\tLoss: 0.135253\n",
            "Train Epoch: 7 [16640/60000 (28%)]\tLoss: 0.266553\n",
            "Train Epoch: 7 [17280/60000 (29%)]\tLoss: 0.013211\n",
            "Train Epoch: 7 [17920/60000 (30%)]\tLoss: 0.018869\n",
            "Train Epoch: 7 [18560/60000 (31%)]\tLoss: 0.094202\n",
            "Train Epoch: 7 [19200/60000 (32%)]\tLoss: 0.082385\n",
            "Train Epoch: 7 [19840/60000 (33%)]\tLoss: 0.031780\n",
            "Train Epoch: 7 [20480/60000 (34%)]\tLoss: 0.058972\n",
            "Train Epoch: 7 [21120/60000 (35%)]\tLoss: 0.075422\n",
            "Train Epoch: 7 [21760/60000 (36%)]\tLoss: 0.062928\n",
            "Train Epoch: 7 [22400/60000 (37%)]\tLoss: 0.065876\n",
            "Train Epoch: 7 [23040/60000 (38%)]\tLoss: 0.036315\n",
            "Train Epoch: 7 [23680/60000 (39%)]\tLoss: 0.121280\n",
            "Train Epoch: 7 [24320/60000 (41%)]\tLoss: 0.023430\n",
            "Train Epoch: 7 [24960/60000 (42%)]\tLoss: 0.017001\n",
            "Train Epoch: 7 [25600/60000 (43%)]\tLoss: 0.026787\n",
            "Train Epoch: 7 [26240/60000 (44%)]\tLoss: 0.036444\n",
            "Train Epoch: 7 [26880/60000 (45%)]\tLoss: 0.060989\n",
            "Train Epoch: 7 [27520/60000 (46%)]\tLoss: 0.064794\n",
            "Train Epoch: 7 [28160/60000 (47%)]\tLoss: 0.100286\n",
            "Train Epoch: 7 [28800/60000 (48%)]\tLoss: 0.066925\n",
            "Train Epoch: 7 [29440/60000 (49%)]\tLoss: 0.034773\n",
            "Train Epoch: 7 [30080/60000 (50%)]\tLoss: 0.194185\n",
            "Train Epoch: 7 [30720/60000 (51%)]\tLoss: 0.010417\n",
            "Train Epoch: 7 [31360/60000 (52%)]\tLoss: 0.043938\n",
            "Train Epoch: 7 [32000/60000 (53%)]\tLoss: 0.119767\n",
            "Train Epoch: 7 [32640/60000 (54%)]\tLoss: 0.096852\n",
            "Train Epoch: 7 [33280/60000 (55%)]\tLoss: 0.034274\n",
            "Train Epoch: 7 [33920/60000 (57%)]\tLoss: 0.027228\n",
            "Train Epoch: 7 [34560/60000 (58%)]\tLoss: 0.058516\n",
            "Train Epoch: 7 [35200/60000 (59%)]\tLoss: 0.036983\n",
            "Train Epoch: 7 [35840/60000 (60%)]\tLoss: 0.028086\n",
            "Train Epoch: 7 [36480/60000 (61%)]\tLoss: 0.050482\n",
            "Train Epoch: 7 [37120/60000 (62%)]\tLoss: 0.151522\n",
            "Train Epoch: 7 [37760/60000 (63%)]\tLoss: 0.035690\n",
            "Train Epoch: 7 [38400/60000 (64%)]\tLoss: 0.026388\n",
            "Train Epoch: 7 [39040/60000 (65%)]\tLoss: 0.034667\n",
            "Train Epoch: 7 [39680/60000 (66%)]\tLoss: 0.032654\n",
            "Train Epoch: 7 [40320/60000 (67%)]\tLoss: 0.036481\n",
            "Train Epoch: 7 [40960/60000 (68%)]\tLoss: 0.027869\n",
            "Train Epoch: 7 [41600/60000 (69%)]\tLoss: 0.118876\n",
            "Train Epoch: 7 [42240/60000 (70%)]\tLoss: 0.107858\n",
            "Train Epoch: 7 [42880/60000 (71%)]\tLoss: 0.028597\n",
            "Train Epoch: 7 [43520/60000 (72%)]\tLoss: 0.135187\n",
            "Train Epoch: 7 [44160/60000 (74%)]\tLoss: 0.025305\n",
            "Train Epoch: 7 [44800/60000 (75%)]\tLoss: 0.137149\n",
            "Train Epoch: 7 [45440/60000 (76%)]\tLoss: 0.042093\n",
            "Train Epoch: 7 [46080/60000 (77%)]\tLoss: 0.054694\n",
            "Train Epoch: 7 [46720/60000 (78%)]\tLoss: 0.053567\n",
            "Train Epoch: 7 [47360/60000 (79%)]\tLoss: 0.022690\n",
            "Train Epoch: 7 [48000/60000 (80%)]\tLoss: 0.078900\n",
            "Train Epoch: 7 [48640/60000 (81%)]\tLoss: 0.095038\n",
            "Train Epoch: 7 [49280/60000 (82%)]\tLoss: 0.016434\n",
            "Train Epoch: 7 [49920/60000 (83%)]\tLoss: 0.115640\n",
            "Train Epoch: 7 [50560/60000 (84%)]\tLoss: 0.123462\n",
            "Train Epoch: 7 [51200/60000 (85%)]\tLoss: 0.093181\n",
            "Train Epoch: 7 [51840/60000 (86%)]\tLoss: 0.040616\n",
            "Train Epoch: 7 [52480/60000 (87%)]\tLoss: 0.036655\n",
            "Train Epoch: 7 [53120/60000 (88%)]\tLoss: 0.100396\n",
            "Train Epoch: 7 [53760/60000 (90%)]\tLoss: 0.012508\n",
            "Train Epoch: 7 [54400/60000 (91%)]\tLoss: 0.061342\n",
            "Train Epoch: 7 [55040/60000 (92%)]\tLoss: 0.097533\n",
            "Train Epoch: 7 [55680/60000 (93%)]\tLoss: 0.090349\n",
            "Train Epoch: 7 [56320/60000 (94%)]\tLoss: 0.033177\n",
            "Train Epoch: 7 [56960/60000 (95%)]\tLoss: 0.046334\n",
            "Train Epoch: 7 [57600/60000 (96%)]\tLoss: 0.080872\n",
            "Train Epoch: 7 [58240/60000 (97%)]\tLoss: 0.208647\n",
            "Train Epoch: 7 [58880/60000 (98%)]\tLoss: 0.091071\n",
            "Train Epoch: 7 [59520/60000 (99%)]\tLoss: 0.075420\n",
            "\n",
            "Test set: Average loss: 0.0695, Accuracy: 9771/10000 (98%)\n",
            "\n",
            "Train Epoch: 8 [0/60000 (0%)]\tLoss: 0.018913\n",
            "Train Epoch: 8 [640/60000 (1%)]\tLoss: 0.068610\n",
            "Train Epoch: 8 [1280/60000 (2%)]\tLoss: 0.069969\n",
            "Train Epoch: 8 [1920/60000 (3%)]\tLoss: 0.120444\n",
            "Train Epoch: 8 [2560/60000 (4%)]\tLoss: 0.026333\n",
            "Train Epoch: 8 [3200/60000 (5%)]\tLoss: 0.130576\n",
            "Train Epoch: 8 [3840/60000 (6%)]\tLoss: 0.049688\n",
            "Train Epoch: 8 [4480/60000 (7%)]\tLoss: 0.060558\n",
            "Train Epoch: 8 [5120/60000 (9%)]\tLoss: 0.102404\n",
            "Train Epoch: 8 [5760/60000 (10%)]\tLoss: 0.098608\n",
            "Train Epoch: 8 [6400/60000 (11%)]\tLoss: 0.047928\n",
            "Train Epoch: 8 [7040/60000 (12%)]\tLoss: 0.121657\n",
            "Train Epoch: 8 [7680/60000 (13%)]\tLoss: 0.015373\n",
            "Train Epoch: 8 [8320/60000 (14%)]\tLoss: 0.163497\n",
            "Train Epoch: 8 [8960/60000 (15%)]\tLoss: 0.069652\n",
            "Train Epoch: 8 [9600/60000 (16%)]\tLoss: 0.047733\n",
            "Train Epoch: 8 [10240/60000 (17%)]\tLoss: 0.044543\n",
            "Train Epoch: 8 [10880/60000 (18%)]\tLoss: 0.022499\n",
            "Train Epoch: 8 [11520/60000 (19%)]\tLoss: 0.051926\n",
            "Train Epoch: 8 [12160/60000 (20%)]\tLoss: 0.300355\n",
            "Train Epoch: 8 [12800/60000 (21%)]\tLoss: 0.053058\n",
            "Train Epoch: 8 [13440/60000 (22%)]\tLoss: 0.107002\n",
            "Train Epoch: 8 [14080/60000 (23%)]\tLoss: 0.107672\n",
            "Train Epoch: 8 [14720/60000 (25%)]\tLoss: 0.133562\n",
            "Train Epoch: 8 [15360/60000 (26%)]\tLoss: 0.040042\n",
            "Train Epoch: 8 [16000/60000 (27%)]\tLoss: 0.081180\n",
            "Train Epoch: 8 [16640/60000 (28%)]\tLoss: 0.048616\n",
            "Train Epoch: 8 [17280/60000 (29%)]\tLoss: 0.019081\n",
            "Train Epoch: 8 [17920/60000 (30%)]\tLoss: 0.034390\n",
            "Train Epoch: 8 [18560/60000 (31%)]\tLoss: 0.033583\n",
            "Train Epoch: 8 [19200/60000 (32%)]\tLoss: 0.024755\n",
            "Train Epoch: 8 [19840/60000 (33%)]\tLoss: 0.029656\n",
            "Train Epoch: 8 [20480/60000 (34%)]\tLoss: 0.098061\n",
            "Train Epoch: 8 [21120/60000 (35%)]\tLoss: 0.064213\n",
            "Train Epoch: 8 [21760/60000 (36%)]\tLoss: 0.032348\n",
            "Train Epoch: 8 [22400/60000 (37%)]\tLoss: 0.022737\n",
            "Train Epoch: 8 [23040/60000 (38%)]\tLoss: 0.012954\n",
            "Train Epoch: 8 [23680/60000 (39%)]\tLoss: 0.022264\n",
            "Train Epoch: 8 [24320/60000 (41%)]\tLoss: 0.021430\n",
            "Train Epoch: 8 [24960/60000 (42%)]\tLoss: 0.050645\n",
            "Train Epoch: 8 [25600/60000 (43%)]\tLoss: 0.030557\n",
            "Train Epoch: 8 [26240/60000 (44%)]\tLoss: 0.006586\n",
            "Train Epoch: 8 [26880/60000 (45%)]\tLoss: 0.063185\n",
            "Train Epoch: 8 [27520/60000 (46%)]\tLoss: 0.036986\n",
            "Train Epoch: 8 [28160/60000 (47%)]\tLoss: 0.108580\n",
            "Train Epoch: 8 [28800/60000 (48%)]\tLoss: 0.028581\n",
            "Train Epoch: 8 [29440/60000 (49%)]\tLoss: 0.080168\n",
            "Train Epoch: 8 [30080/60000 (50%)]\tLoss: 0.098311\n",
            "Train Epoch: 8 [30720/60000 (51%)]\tLoss: 0.034590\n",
            "Train Epoch: 8 [31360/60000 (52%)]\tLoss: 0.034991\n",
            "Train Epoch: 8 [32000/60000 (53%)]\tLoss: 0.096389\n",
            "Train Epoch: 8 [32640/60000 (54%)]\tLoss: 0.050675\n",
            "Train Epoch: 8 [33280/60000 (55%)]\tLoss: 0.036206\n",
            "Train Epoch: 8 [33920/60000 (57%)]\tLoss: 0.003082\n",
            "Train Epoch: 8 [34560/60000 (58%)]\tLoss: 0.038033\n",
            "Train Epoch: 8 [35200/60000 (59%)]\tLoss: 0.138927\n",
            "Train Epoch: 8 [35840/60000 (60%)]\tLoss: 0.071722\n",
            "Train Epoch: 8 [36480/60000 (61%)]\tLoss: 0.029298\n",
            "Train Epoch: 8 [37120/60000 (62%)]\tLoss: 0.061052\n",
            "Train Epoch: 8 [37760/60000 (63%)]\tLoss: 0.030173\n",
            "Train Epoch: 8 [38400/60000 (64%)]\tLoss: 0.013680\n",
            "Train Epoch: 8 [39040/60000 (65%)]\tLoss: 0.041961\n",
            "Train Epoch: 8 [39680/60000 (66%)]\tLoss: 0.092608\n",
            "Train Epoch: 8 [40320/60000 (67%)]\tLoss: 0.035365\n",
            "Train Epoch: 8 [40960/60000 (68%)]\tLoss: 0.025891\n",
            "Train Epoch: 8 [41600/60000 (69%)]\tLoss: 0.016855\n",
            "Train Epoch: 8 [42240/60000 (70%)]\tLoss: 0.034119\n",
            "Train Epoch: 8 [42880/60000 (71%)]\tLoss: 0.122436\n",
            "Train Epoch: 8 [43520/60000 (72%)]\tLoss: 0.021077\n",
            "Train Epoch: 8 [44160/60000 (74%)]\tLoss: 0.025824\n",
            "Train Epoch: 8 [44800/60000 (75%)]\tLoss: 0.023188\n",
            "Train Epoch: 8 [45440/60000 (76%)]\tLoss: 0.032643\n",
            "Train Epoch: 8 [46080/60000 (77%)]\tLoss: 0.014730\n",
            "Train Epoch: 8 [46720/60000 (78%)]\tLoss: 0.053428\n",
            "Train Epoch: 8 [47360/60000 (79%)]\tLoss: 0.014871\n",
            "Train Epoch: 8 [48000/60000 (80%)]\tLoss: 0.035404\n",
            "Train Epoch: 8 [48640/60000 (81%)]\tLoss: 0.014874\n",
            "Train Epoch: 8 [49280/60000 (82%)]\tLoss: 0.034518\n",
            "Train Epoch: 8 [49920/60000 (83%)]\tLoss: 0.013815\n",
            "Train Epoch: 8 [50560/60000 (84%)]\tLoss: 0.077663\n",
            "Train Epoch: 8 [51200/60000 (85%)]\tLoss: 0.046242\n",
            "Train Epoch: 8 [51840/60000 (86%)]\tLoss: 0.048329\n",
            "Train Epoch: 8 [52480/60000 (87%)]\tLoss: 0.057460\n",
            "Train Epoch: 8 [53120/60000 (88%)]\tLoss: 0.067061\n",
            "Train Epoch: 8 [53760/60000 (90%)]\tLoss: 0.040669\n",
            "Train Epoch: 8 [54400/60000 (91%)]\tLoss: 0.099371\n",
            "Train Epoch: 8 [55040/60000 (92%)]\tLoss: 0.039547\n",
            "Train Epoch: 8 [55680/60000 (93%)]\tLoss: 0.106096\n",
            "Train Epoch: 8 [56320/60000 (94%)]\tLoss: 0.019881\n",
            "Train Epoch: 8 [56960/60000 (95%)]\tLoss: 0.070306\n",
            "Train Epoch: 8 [57600/60000 (96%)]\tLoss: 0.048428\n",
            "Train Epoch: 8 [58240/60000 (97%)]\tLoss: 0.084325\n",
            "Train Epoch: 8 [58880/60000 (98%)]\tLoss: 0.028345\n",
            "Train Epoch: 8 [59520/60000 (99%)]\tLoss: 0.032636\n",
            "\n",
            "Test set: Average loss: 0.0601, Accuracy: 9811/10000 (98%)\n",
            "\n",
            "Train Epoch: 9 [0/60000 (0%)]\tLoss: 0.104150\n",
            "Train Epoch: 9 [640/60000 (1%)]\tLoss: 0.018952\n",
            "Train Epoch: 9 [1280/60000 (2%)]\tLoss: 0.048283\n",
            "Train Epoch: 9 [1920/60000 (3%)]\tLoss: 0.032619\n",
            "Train Epoch: 9 [2560/60000 (4%)]\tLoss: 0.013782\n",
            "Train Epoch: 9 [3200/60000 (5%)]\tLoss: 0.042770\n",
            "Train Epoch: 9 [3840/60000 (6%)]\tLoss: 0.042477\n",
            "Train Epoch: 9 [4480/60000 (7%)]\tLoss: 0.026397\n",
            "Train Epoch: 9 [5120/60000 (9%)]\tLoss: 0.015210\n",
            "Train Epoch: 9 [5760/60000 (10%)]\tLoss: 0.123172\n",
            "Train Epoch: 9 [6400/60000 (11%)]\tLoss: 0.140739\n",
            "Train Epoch: 9 [7040/60000 (12%)]\tLoss: 0.028440\n",
            "Train Epoch: 9 [7680/60000 (13%)]\tLoss: 0.083448\n",
            "Train Epoch: 9 [8320/60000 (14%)]\tLoss: 0.060091\n",
            "Train Epoch: 9 [8960/60000 (15%)]\tLoss: 0.053045\n",
            "Train Epoch: 9 [9600/60000 (16%)]\tLoss: 0.086726\n",
            "Train Epoch: 9 [10240/60000 (17%)]\tLoss: 0.051040\n",
            "Train Epoch: 9 [10880/60000 (18%)]\tLoss: 0.043382\n",
            "Train Epoch: 9 [11520/60000 (19%)]\tLoss: 0.019457\n",
            "Train Epoch: 9 [12160/60000 (20%)]\tLoss: 0.191461\n",
            "Train Epoch: 9 [12800/60000 (21%)]\tLoss: 0.029178\n",
            "Train Epoch: 9 [13440/60000 (22%)]\tLoss: 0.069380\n",
            "Train Epoch: 9 [14080/60000 (23%)]\tLoss: 0.058398\n",
            "Train Epoch: 9 [14720/60000 (25%)]\tLoss: 0.021446\n",
            "Train Epoch: 9 [15360/60000 (26%)]\tLoss: 0.147355\n",
            "Train Epoch: 9 [16000/60000 (27%)]\tLoss: 0.026448\n",
            "Train Epoch: 9 [16640/60000 (28%)]\tLoss: 0.146443\n",
            "Train Epoch: 9 [17280/60000 (29%)]\tLoss: 0.025464\n",
            "Train Epoch: 9 [17920/60000 (30%)]\tLoss: 0.022403\n",
            "Train Epoch: 9 [18560/60000 (31%)]\tLoss: 0.079687\n",
            "Train Epoch: 9 [19200/60000 (32%)]\tLoss: 0.017150\n",
            "Train Epoch: 9 [19840/60000 (33%)]\tLoss: 0.018950\n",
            "Train Epoch: 9 [20480/60000 (34%)]\tLoss: 0.021772\n",
            "Train Epoch: 9 [21120/60000 (35%)]\tLoss: 0.129326\n",
            "Train Epoch: 9 [21760/60000 (36%)]\tLoss: 0.038898\n",
            "Train Epoch: 9 [22400/60000 (37%)]\tLoss: 0.013073\n",
            "Train Epoch: 9 [23040/60000 (38%)]\tLoss: 0.060807\n",
            "Train Epoch: 9 [23680/60000 (39%)]\tLoss: 0.046604\n",
            "Train Epoch: 9 [24320/60000 (41%)]\tLoss: 0.007410\n",
            "Train Epoch: 9 [24960/60000 (42%)]\tLoss: 0.128120\n",
            "Train Epoch: 9 [25600/60000 (43%)]\tLoss: 0.120062\n",
            "Train Epoch: 9 [26240/60000 (44%)]\tLoss: 0.029310\n",
            "Train Epoch: 9 [26880/60000 (45%)]\tLoss: 0.074293\n",
            "Train Epoch: 9 [27520/60000 (46%)]\tLoss: 0.012049\n",
            "Train Epoch: 9 [28160/60000 (47%)]\tLoss: 0.054573\n",
            "Train Epoch: 9 [28800/60000 (48%)]\tLoss: 0.038645\n",
            "Train Epoch: 9 [29440/60000 (49%)]\tLoss: 0.078981\n",
            "Train Epoch: 9 [30080/60000 (50%)]\tLoss: 0.059561\n",
            "Train Epoch: 9 [30720/60000 (51%)]\tLoss: 0.077012\n",
            "Train Epoch: 9 [31360/60000 (52%)]\tLoss: 0.123519\n",
            "Train Epoch: 9 [32000/60000 (53%)]\tLoss: 0.032844\n",
            "Train Epoch: 9 [32640/60000 (54%)]\tLoss: 0.176298\n",
            "Train Epoch: 9 [33280/60000 (55%)]\tLoss: 0.081549\n",
            "Train Epoch: 9 [33920/60000 (57%)]\tLoss: 0.073428\n",
            "Train Epoch: 9 [34560/60000 (58%)]\tLoss: 0.213657\n",
            "Train Epoch: 9 [35200/60000 (59%)]\tLoss: 0.113573\n",
            "Train Epoch: 9 [35840/60000 (60%)]\tLoss: 0.016625\n",
            "Train Epoch: 9 [36480/60000 (61%)]\tLoss: 0.131346\n",
            "Train Epoch: 9 [37120/60000 (62%)]\tLoss: 0.009454\n",
            "Train Epoch: 9 [37760/60000 (63%)]\tLoss: 0.034931\n",
            "Train Epoch: 9 [38400/60000 (64%)]\tLoss: 0.116806\n",
            "Train Epoch: 9 [39040/60000 (65%)]\tLoss: 0.024779\n",
            "Train Epoch: 9 [39680/60000 (66%)]\tLoss: 0.049414\n",
            "Train Epoch: 9 [40320/60000 (67%)]\tLoss: 0.034587\n",
            "Train Epoch: 9 [40960/60000 (68%)]\tLoss: 0.031494\n",
            "Train Epoch: 9 [41600/60000 (69%)]\tLoss: 0.057331\n",
            "Train Epoch: 9 [42240/60000 (70%)]\tLoss: 0.016894\n",
            "Train Epoch: 9 [42880/60000 (71%)]\tLoss: 0.102090\n",
            "Train Epoch: 9 [43520/60000 (72%)]\tLoss: 0.033418\n",
            "Train Epoch: 9 [44160/60000 (74%)]\tLoss: 0.025512\n",
            "Train Epoch: 9 [44800/60000 (75%)]\tLoss: 0.026991\n",
            "Train Epoch: 9 [45440/60000 (76%)]\tLoss: 0.102733\n",
            "Train Epoch: 9 [46080/60000 (77%)]\tLoss: 0.178183\n",
            "Train Epoch: 9 [46720/60000 (78%)]\tLoss: 0.091310\n",
            "Train Epoch: 9 [47360/60000 (79%)]\tLoss: 0.055511\n",
            "Train Epoch: 9 [48000/60000 (80%)]\tLoss: 0.066244\n",
            "Train Epoch: 9 [48640/60000 (81%)]\tLoss: 0.094302\n",
            "Train Epoch: 9 [49280/60000 (82%)]\tLoss: 0.038120\n",
            "Train Epoch: 9 [49920/60000 (83%)]\tLoss: 0.008766\n",
            "Train Epoch: 9 [50560/60000 (84%)]\tLoss: 0.039125\n",
            "Train Epoch: 9 [51200/60000 (85%)]\tLoss: 0.154917\n",
            "Train Epoch: 9 [51840/60000 (86%)]\tLoss: 0.031794\n",
            "Train Epoch: 9 [52480/60000 (87%)]\tLoss: 0.123965\n",
            "Train Epoch: 9 [53120/60000 (88%)]\tLoss: 0.045971\n",
            "Train Epoch: 9 [53760/60000 (90%)]\tLoss: 0.046477\n",
            "Train Epoch: 9 [54400/60000 (91%)]\tLoss: 0.111884\n",
            "Train Epoch: 9 [55040/60000 (92%)]\tLoss: 0.038221\n",
            "Train Epoch: 9 [55680/60000 (93%)]\tLoss: 0.014608\n",
            "Train Epoch: 9 [56320/60000 (94%)]\tLoss: 0.074806\n",
            "Train Epoch: 9 [56960/60000 (95%)]\tLoss: 0.036033\n",
            "Train Epoch: 9 [57600/60000 (96%)]\tLoss: 0.007158\n",
            "Train Epoch: 9 [58240/60000 (97%)]\tLoss: 0.011612\n",
            "Train Epoch: 9 [58880/60000 (98%)]\tLoss: 0.042291\n",
            "Train Epoch: 9 [59520/60000 (99%)]\tLoss: 0.096893\n",
            "\n",
            "Test set: Average loss: 0.0548, Accuracy: 9829/10000 (98%)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MstdPhXOYtIg"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}