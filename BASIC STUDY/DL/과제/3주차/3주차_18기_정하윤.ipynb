{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# < 3주차 과제 1 : MLP 마음대로 다뤄보기>\n",
        "- dataset을 임의로 선정해서 직접 분석 해보기(제공한 코드를 활용해서 해보기)\n",
        "- activation functions 중 relu사용시 함수 직접 정의\n",
        "- lr, optimizer 등 바꿔보기\n",
        "- hidden layer/neuron 수를 바꾸기\n",
        "- 전처리도 추가\n",
        "- 모든 시도를 올려주세요!\n",
        "- 제일 높은 acc를 보인 시도를 명시해주세요!\n"
      ],
      "metadata": {
        "id": "sgAYo4nrw2F4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "fX437IL6qbI-"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn, optim\n",
        "from sklearn.datasets import load_wine\n",
        "from torch.utils.data import  TensorDataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 아래 데이터셋 중 원하는 데이터셋 하나를 선택하여, 코드 과제 진행하기!\n",
        "- 1) load_digits() <br>\n",
        "- 2) load_wine()"
      ],
      "metadata": {
        "id": "oxkFzBDNmWNk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 데이터셋 종류 :\n",
        "wine = load_wine()"
      ],
      "metadata": {
        "id": "FywYbfsKtjcR"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input = wine.data\n",
        "output = wine.target"
      ],
      "metadata": {
        "id": "C2P0hqZ9yBGm"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "torch.manual_seed(777)\n",
        "if device == \"cuda\":\n",
        "  torch.cuda.manual_seed_all(777)"
      ],
      "metadata": {
        "id": "SggpQfSPt85C"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-W5onxatM9S3",
        "outputId": "4be10d2f-baa3-4b15-c1b5-508f4ac91334"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1.423e+01, 1.710e+00, 2.430e+00, ..., 1.040e+00, 3.920e+00,\n",
              "        1.065e+03],\n",
              "       [1.320e+01, 1.780e+00, 2.140e+00, ..., 1.050e+00, 3.400e+00,\n",
              "        1.050e+03],\n",
              "       [1.316e+01, 2.360e+00, 2.670e+00, ..., 1.030e+00, 3.170e+00,\n",
              "        1.185e+03],\n",
              "       ...,\n",
              "       [1.327e+01, 4.280e+00, 2.260e+00, ..., 5.900e-01, 1.560e+00,\n",
              "        8.350e+02],\n",
              "       [1.317e+01, 2.590e+00, 2.370e+00, ..., 6.000e-01, 1.620e+00,\n",
              "        8.400e+02],\n",
              "       [1.413e+01, 4.100e+00, 2.740e+00, ..., 6.100e-01, 1.600e+00,\n",
              "        5.600e+02]])"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q-x9bkc2NBC7",
        "outputId": "8a50e23e-ff0e-4b56-822a-efa3600b016c"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1,\n",
              "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2,\n",
              "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
              "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
              "       2, 2])"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#데이터 칼럼\n",
        "wine['feature_names']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wlhST0vZM68w",
        "outputId": "60d8989f-386b-4166-ce9d-7357444972f4"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['alcohol',\n",
              " 'malic_acid',\n",
              " 'ash',\n",
              " 'alcalinity_of_ash',\n",
              " 'magnesium',\n",
              " 'total_phenols',\n",
              " 'flavanoids',\n",
              " 'nonflavanoid_phenols',\n",
              " 'proanthocyanins',\n",
              " 'color_intensity',\n",
              " 'hue',\n",
              " 'od280/od315_of_diluted_wines',\n",
              " 'proline']"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#샘플 수 178개, class는 0,1,2"
      ],
      "metadata": {
        "id": "rvWDIphhCyZP"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# #y를 카테고리화\n",
        "# output = pd.get_dummies(output)\n",
        "# output"
      ],
      "metadata": {
        "id": "g5fwnrQxNXsr"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import torch.nn.functional as F\n",
        "\n",
        "# #원핫 인코딩\n",
        "# output=F.one_hot(output,num_classes=3)\n",
        "# output\n",
        "###케라스에서는 라벨 원핫 인코딩을 하는데 파이토치도 하는 건지,,"
      ],
      "metadata": {
        "id": "bnkzrkB9OYpI"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train, x_test, y_train, y_test = train_test_split(input, output, test_size = 0.3, random_state = 42, stratify= wine.target, shuffle = True)\n",
        "\n",
        "x_train = torch.FloatTensor(x_train).to(device)\n",
        "y_train = torch.LongTensor(y_train).to(device)\n",
        "x_test = torch.FloatTensor(x_test)\n",
        "y_test = torch.LongTensor(y_test)\n",
        "\n",
        "# 데이터를 tensor로 바꿔주고 gpu 연산이 가능해지도록 gpu에 옮김\n",
        "# label 값을 왜 long 에 옮겨놓는가? loss function이 다르기 때문"
      ],
      "metadata": {
        "id": "bLMzf-2ntYeX"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(x_train[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9HyYM8kvCMkD",
        "outputId": "f82a8c00-c865-419f-d613-617fdde770e0"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([1.3750e+01, 1.7300e+00, 2.4100e+00, 1.6000e+01, 8.9000e+01, 2.6000e+00,\n",
            "        2.7600e+00, 2.9000e-01, 1.8100e+00, 5.6000e+00, 1.1500e+00, 2.9000e+00,\n",
            "        1.3200e+03])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(y_train.unique())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "umEdiTZkrVqS",
        "outputId": "3b006cbd-9a5b-4bab-e468-4387c412ed51"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([0, 1, 2])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "파이토치에서는 데이터셋을 좀 더 쉽게 다룰 수 있도록 유용한 도구로서 torch.utils.data.Dataset과 torch.utils.data.DataLoader를 제공합니다. 이를 사용하면 미니 배치 학습, 데이터 셔플(shuffle), 병렬 처리까지 간단히 수행할 수 있습니다. 기본적인 사용 방법은 Dataset을 정의하고, 이를 DataLoader에 전달하는 것\n",
        "- init : class 에서 객체가 생성되면 바로 실행되는 함수\n",
        "- len : observation 수를 정의하는 함수\n",
        "- getitem : iteration 마다 해당하는 데이터를 돌려주는 함수"
      ],
      "metadata": {
        "id": "combmxzmYFyn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset\n",
        "class CustomDataset(Dataset):\n",
        "  def __init__(self):\n",
        "    self.x_data = x_train\n",
        "    self.y_data = [[y] for y in y_train]\n",
        "    self.device = device\n",
        "#  데이터셋의 전처리를 해주는 부분\n",
        "    self.x_data = self.standardize(self.x_data)\n",
        "\n",
        "  def standardize(self, data):\n",
        "          #각 feature의 평균과 표준편차를 이용하여 표준화하는 함수\n",
        "          mean = torch.mean(data, dim=0)\n",
        "          std = torch.std(data, dim=0)\n",
        "          standardized_data = (data - mean) / std\n",
        "          return standardized_data\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.x_data)\n",
        "#  데이터셋의 길이. 즉, 총 샘플의 수를 적어주는 부분\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    x = torch.FloatTensor(self.x_data[idx]).to(self.device)\n",
        "    y = torch.LongTensor(self.y_data[idx]).to(self.device)\n",
        "#  데이터셋에서 특정 1개의 샘플을 가져오는 함수\n",
        "\n",
        "    return x,y"
      ],
      "metadata": {
        "id": "y38TlgXoqV5Z"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 8\n",
        "\n",
        "dataset = CustomDataset()\n",
        "dataloader = DataLoader(dataset, batch_size=batch_size)"
      ],
      "metadata": {
        "id": "x8VHwnuFqino"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# input_layer와 hidden_layer의 노드 개수를 어떻게 바꿔줘야할까?\n",
        "# hidden layer/neuron 수를 바꾸기\n",
        "\n",
        "model = nn.Sequential(\n",
        "          nn.Linear(13,398, bias=True),\n",
        "          nn.ReLU(),\n",
        "          nn.Linear(398,15, bias=True),\n",
        "          nn.ReLU(),\n",
        "          nn.Linear(15,3, bias=True),\n",
        "          nn.Softmax(dim=1)\n",
        "          ).to(device)"
      ],
      "metadata": {
        "id": "C6V7a4tyq6Jc"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "class로 구현 가능\n",
        "- init : 초기 생성 함수\n",
        "- foward : 순전파(입력값 => 예측값 의 과정)"
      ],
      "metadata": {
        "id": "07uV8RY7Yr_5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#relu 구현\n",
        "class Relu(torch.nn.Module):\n",
        "    def forward(self, x):\n",
        "        return torch.max(torch.tensor(0.0), x)\n",
        "\n",
        "class Model(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Model, self).__init__()\n",
        "        self.layer1 = nn.Sequential(\n",
        "            nn.Linear(13, 398, bias=True), # input_layer = 13, hidden_layer1 = 398\n",
        "            Relu(),\n",
        "            nn.BatchNorm1d(398)\n",
        "        )\n",
        "        self.layer2 = nn.Sequential(\n",
        "            nn.Linear(398, 15, bias=True),\n",
        "            Relu()\n",
        "        )\n",
        "        self.layer3 = nn.Sequential(\n",
        "            nn.Linear(15, 10, bias=True),\n",
        "            Relu()\n",
        "        )\n",
        "        self.layer4 = nn.Sequential(\n",
        "            nn.Linear(10, 3, bias=True),\n",
        "            nn.Softmax()\n",
        "        )\n",
        "\n",
        " # activation function 이용\n",
        "  #   nn.ReLU()\n",
        "  #   nn.tanH()\n",
        "  #   https://pytorch.org/docs/stable/nn.html 그 외에도 여기서 확인 가능함\n",
        "  #   파라미터가 필요하지 않다는 것이 특징\n",
        "\n",
        "  # batch normazliation 1d, 파라미터 값으로 vector의 길이를 전해줌\n",
        "  # 추후에 이미지를 다루게 된다면, 그때는 batch normalization 2d를 이용하게 됨\n",
        "  # 그때는 파라미터 값으로 채널, 가로, 세로 길이를 전달해주게 됨\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        output = self.layer1(x)\n",
        "        output = self.layer2(output)\n",
        "        output = self.layer3(output)\n",
        "        output = self.layer4(output)\n",
        "        return output"
      ],
      "metadata": {
        "id": "5xBCfMbVLQUb"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def init_weights(layer):\n",
        "    if isinstance(layer, nn.Linear):\n",
        "        torch.nn.init.xavier_uniform(layer.weight)\n",
        "        layer.bias.data.fill_(0.01)\n",
        "\n",
        "        #xavier사용\n",
        "        # Layer의 weight를 어떤 분포를 가지도록 초기화시켜줌+global minimum찾기 위해서"
      ],
      "metadata": {
        "id": "kqcqqkECrSGK"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Model().to(device)\n",
        "model.apply(init_weights)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oMDUBFg6rUpw",
        "outputId": "df939fc6-369a-4582-bc03-fdfe71298e72"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-22-6196a47a4462>:3: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "  torch.nn.init.xavier_uniform(layer.weight)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Model(\n",
              "  (layer1): Sequential(\n",
              "    (0): Linear(in_features=13, out_features=398, bias=True)\n",
              "    (1): Relu()\n",
              "    (2): BatchNorm1d(398, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  )\n",
              "  (layer2): Sequential(\n",
              "    (0): Linear(in_features=398, out_features=15, bias=True)\n",
              "    (1): Relu()\n",
              "  )\n",
              "  (layer3): Sequential(\n",
              "    (0): Linear(in_features=15, out_features=10, bias=True)\n",
              "    (1): Relu()\n",
              "  )\n",
              "  (layer4): Sequential(\n",
              "    (0): Linear(in_features=10, out_features=3, bias=True)\n",
              "    (1): Softmax(dim=None)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OwZt5CetrYFb",
        "outputId": "4c07a0fb-b571-4c37-dc5f-592bc8a67f62"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model(\n",
            "  (layer1): Sequential(\n",
            "    (0): Linear(in_features=13, out_features=398, bias=True)\n",
            "    (1): Relu()\n",
            "    (2): BatchNorm1d(398, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  )\n",
            "  (layer2): Sequential(\n",
            "    (0): Linear(in_features=398, out_features=15, bias=True)\n",
            "    (1): Relu()\n",
            "  )\n",
            "  (layer3): Sequential(\n",
            "    (0): Linear(in_features=15, out_features=10, bias=True)\n",
            "    (1): Relu()\n",
            "  )\n",
            "  (layer4): Sequential(\n",
            "    (0): Linear(in_features=10, out_features=3, bias=True)\n",
            "    (1): Softmax(dim=None)\n",
            "  )\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss_fn  = nn.CrossEntropyLoss().to(device)\n",
        "\n",
        "# 여러가지 optimizer 시도해보기\n",
        "# lr 바꿔보기\n",
        "\n",
        "optimizer = optim.Adagrad(model.parameters(), lr= 0.01)\n",
        "#optimizer = optim.Adam(model.parameters(), lr= 0.01)\n",
        "#optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
        "#optimizer = optim.Adadelta(model.parameters(), lr=0.03)\n",
        "\n",
        "# https://pytorch.org/docs/stable/optim.html#module-torch.optim 페이지 참조\n",
        "# optimizer = optim.SGD(model.parameters(), lr=0.1, momentum=0.9)\n",
        "# sgd 등등"
      ],
      "metadata": {
        "id": "AYFp-eTErh7b"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "losses = []\n",
        "for epoch in range(100):\n",
        "\n",
        "  optimizer.zero_grad()\n",
        "  hypothesis = model(x_train)\n",
        "\n",
        "  # 비용 함수\n",
        "  cost = loss_fn(hypothesis, y_train)\n",
        "  cost.backward()\n",
        "  optimizer.step()\n",
        "  losses.append(cost.item())\n",
        "\n",
        "  if epoch % 10 == 0:\n",
        "    print(epoch, cost.item())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "90QxHvlIrjS7",
        "outputId": "bd13a4c1-38ac-4039-9079-4bd61b139037"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1518: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 0.9692224860191345\n",
            "10 0.79714035987854\n",
            "20 0.7350306510925293\n",
            "30 0.6505707502365112\n",
            "40 0.6190946698188782\n",
            "50 0.5871148705482483\n",
            "60 0.5762291550636292\n",
            "70 0.5930455327033997\n",
            "80 0.5880704522132874\n",
            "90 0.5721447467803955\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(losses)\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 430
        },
        "id": "81ASYrW7roFM",
        "outputId": "964d3f5d-fe42-4324-fcbc-2671105e28f3"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "  model = model.to('cpu')\n",
        "  y_pred = model(x_test)\n",
        "  y_pred = y_pred.detach().cpu().numpy()\n",
        "  predicted = np.argmax(y_pred, axis =1)\n",
        "  accuracy = (accuracy_score(predicted, y_test))"
      ],
      "metadata": {
        "id": "h4kJzpLErqhZ"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'model의 output은 :  {y_pred[0]}')\n",
        "print(f'argmax를 한 후의 output은 {predicted[0]}')\n",
        "print(f'accuracy는 {accuracy}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vyIKhs3Nr6Ay",
        "outputId": "9682a753-d44c-4627-e8b3-0c241a870d39"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model의 output은 :  [0.99676466 0.00221027 0.00102507]\n",
            "argmax를 한 후의 output은 0\n",
            "accuracy는 0.9444444444444444\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# < 3주차 과제 2 : CNN 맛보기>"
      ],
      "metadata": {
        "id": "3RzRM7xThZV_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from __future__ import print_function\n",
        "import argparse\n",
        "import torch\n",
        "import torch.nn as nn # 신경망들이 포함됨\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim # 최적화 알고리즘들이 포함됨\n",
        "from torchvision import datasets, transforms # 이미지 데이터셋 집합, 이미지 변환 툴\n",
        "from torch.autograd import Variable"
      ],
      "metadata": {
        "id": "56xqgtLxhZw6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training settings\n",
        "batch_size = 64\n",
        "\n",
        "# MNIST Dataset\n",
        "train_dataset = datasets.MNIST(root='./data/',\n",
        "                               train=True,\n",
        "                               transform=transforms.ToTensor(),\n",
        "                               download=True)\n",
        "\n",
        "test_dataset = datasets.MNIST(root='./data/',\n",
        "                              train=False,\n",
        "                              transform=transforms.ToTensor())\n",
        "\n",
        "# Data Loader (Input Pipeline)\n",
        "#앞서 정의한 데이터셋을 DataLoader에 넣으면 정의한 조건(배치 사이즈 등)에 따라 모델을 학습하고 추론할 때 데이터를 load 해줌\n",
        "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
        "                                           batch_size=batch_size,\n",
        "                                           shuffle=True)\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
        "                                          batch_size=batch_size,\n",
        "                                          shuffle=False)"
      ],
      "metadata": {
        "id": "TzkF2bFNhcQ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 첫번째 이미지만 가져오기\n",
        "img , label = next(iter(train_loader))"
      ],
      "metadata": {
        "id": "D1Mvg6FpQYji"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DyspnHTDD24C",
        "outputId": "fdc82db6-30ac-404c-e27b-f8fe123cfda8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([64, 1, 28, 28])"
            ]
          },
          "metadata": {},
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#배치 크기 × 채널 × 높이(height) × 너비(widht)\n",
        "#총 64개의 샘플, 흑백이므로 채널 1, 가로 세로 28픽셀"
      ],
      "metadata": {
        "id": "52NoQZj0T_lo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#한 개의 이미지만 출력해보기\n",
        "img_show = img[0, 0, :, :]    #배치 크기, 채널 0으로 만듦\n",
        "img_show.shape\n",
        "\n",
        "plt.imshow(img_show, cmap='Greys')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 430
        },
        "id": "5zSU9StsTpNO",
        "outputId": "5c0661ee-d95c-43cb-ac2b-a701b4955622"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=0)의 파라미터\n",
        "\n",
        "*   in_channels: 입력 채널 수. 흑백 이미지일 경우 1, RGB 값을 가진 이미지일 경우 3 을 가진 경우가 많음\n",
        "*   out_channels: 출력 채널 수\n",
        "*   kernel_size: 커널 사이즈(필터 사이즈)\n",
        "*   stride: kernel을 적용하는 간격. 간격이 커질수록 출력 데이터 배열의 크기는 작아짐. 기본 값은 1\n",
        "*   padding: 출력 데이터 배열의 크기를 조정하기 위해서 이미지의 주변을 채워줌. 패딩 사이즈 기본 값은 0\n",
        "\n"
      ],
      "metadata": {
        "id": "MsE2s_f6UX7z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#가로 28x세로 28 = 784 픽셀로 이루어진 이미지\n",
        "#각 픽셀은 밝기 정도에 따라 0부터 255까지 등급을 매김\n",
        "#흰 배경이 0, 글씨가 있는 곳은 1~255 숫자 중 하나로 채워져, 긴 행렬로 이루어진 하나의 집합으로 변환됨"
      ],
      "metadata": {
        "id": "er8ccA1Pwfuw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**output size 계산**\n",
        "\n",
        "*   {input size - kernel size + (2*padding)} / stride +1\n",
        "*   우리가 가진 데이터에서 입력 이미지는 28x28 픽셀, conv1(1, 10, 5)일 때 output layer\n",
        "   *  ((28 - 5) + (2*0)) / 1 + 1 = 24\n",
        "   *  output size는 24x24\n",
        "\n"
      ],
      "metadata": {
        "id": "nmzoYHlaVbV-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#CNN class 정의\n",
        "class Net(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(Net, self).__init__()   # super함수는 CNN class의 부모 class인 nn.Module을 초기화\n",
        "    self.conv1 = nn.Conv2d(in_channels=1, out_channels=10, kernel_size = 5)\n",
        "        #합성곱 레이어1 : 입력 크기는 28x28x1 -> 출력 크기는 24x24x10\n",
        "    self.conv2 = nn.Conv2d(in_channels=10, out_channels=20, kernel_size = 5)\n",
        "        #합성곱 레이어2 : 입력 크기는 24x24x10 -> 출력 크기는 20x20x20\n",
        "    self.mp = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
        "        #맥스풀링 레이어 : 20/2 = 10 -> 출력 크기는 10x10x20\n",
        "    self.fc = nn.Linear(320, 10) ### : 알맞는 input <- 왜 2000이 아닌지..\n",
        "        #입력 크기 320, 출력 클래스 수는 10(0~9까지 숫자 맞히기)\n",
        "        #입력층과 출력층 사이에 선형 변환 수행(입력 벡터를 받아서 가중치 행렬과의 행렬 곱 계산, bias 더해줌)\n",
        "\n",
        "  def forward(self, x):\n",
        "    in_size = x.size(0)\n",
        "    x = F.relu(self.mp(self.conv1(x)))\n",
        "      #첫 번째 합성곱 계층을 통과한 후 ReLU 활성화 함수를 적용하고, 그 결과를 최대 풀링 계층에 통과시킴\n",
        "    x = F.relu(self.mp(self.conv2(x)))\n",
        "      #두 번째 합성곱 계층을 통과...\n",
        "    x = x.view(in_size, -1)       #출력을 1차원으로 평탄화\n",
        "    x = self.fc(x)    #평탄화된 출력에 완전 연결 계층을 적용하여 클래스에 대한 로그 확률을 계산\n",
        "    return F.log_softmax(x)   #로그 소프트맥스 함수를 적용하여 각 클래스에 대한 예측값을 확률 값으로 반환\n",
        "\n",
        "    ##https://kh-kim.github.io/nlp_with_deep_learning_blog/docs/1-13-deep_neural_networks_ii/05-softmax_and_cross_entropy/\n",
        "    ##지수함수를 사용하는 소프트맥스의 특성상, 로그소프트맥스가 좀 더 빠른 연산 속도를 제공할 수 있기 때문에 후자가 좀 더 선호되는 편이라고,,\n",
        "    ##로그 소프트맥스 사용할 때는 NLL 손실 함수 사용해줘야 함"
      ],
      "metadata": {
        "id": "tLCSvgganBrH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**완전연결 계층**\n",
        "\n",
        "*   Fully conneted layer, Dense layer\n",
        "*   '완전 연결되었다' : 한 layer의 모든 뉴런이 다음 layer의 모든 뉴런과 연결된 상태. 1차원 배열의 형태로 평탄화된 행렬을 통해 이미지를 분류하는 데 사용되는 계층\n",
        "*   2차월 벡터의 행렬을 1차원 배열로 평탄화, relu 함수로 활성화, softmax 함수로 이미지 분류하는 것까지가 fully connected layer라고 함\n",
        "\n"
      ],
      "metadata": {
        "id": "rHLc3psLjx_y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**pooling**\n",
        "\n",
        "*   컨볼루션 층을 통해 이미지 특징을 요약하여도, 그 결과가 여전히 크고 복잡하면 이를 다시 한번 축소함 (pooling/sub sampling)\n",
        "*   정해진 구역 안에서 최댓값을 뽑아내는 max pooling, 평균값을 뽑아내는 average pooling 등\n"
      ],
      "metadata": {
        "id": "Pq324AChdSvB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**drop out**\n",
        "\n",
        "*   은닉층에 배치된 노드 중 일부를 임의로 꺼주는 것. 특정 노드에 지나치게 치우쳐서 학습되는 과적합 방지\n",
        "*   매 학습이 일어날 때마다 일정한 수의 다른 노드가 랜덤하게 학습에서 배제됨"
      ],
      "metadata": {
        "id": "66h4TfnKd1m_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = Net()\n",
        "optimizer = optim.SGD(model.parameters(), lr = 0.01, momentum = 0.5)"
      ],
      "metadata": {
        "id": "lkYZ4pUdnUHc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(epoch):\n",
        "  model.train()\n",
        "  for batch_idx, (data, target) in enumerate(train_loader):\n",
        "    data, target = Variable(data), Variable(target)   #데이터와 타겟을 Variable로 변환하여 GPU에서 연산할 수 있도록 함\n",
        "    optimizer.zero_grad()   #기울기를 초기화\n",
        "    output = model(data)    #모델에 데이터를 입력\n",
        "    loss = F.nll_loss(output, target)     #예측값과 실제값 사이의 손실 계산\n",
        "    loss.backward()         #역전파를 통해 파라미터 기울기 계산\n",
        "    optimizer.step()        #옵티마이저로 파라미터 업데이트\n",
        "    if batch_idx % 10 == 0:   #현재 미니배치의 인덱스가 10의 배수일 때마다 아래 코드 블록을 실행\n",
        "      print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "          epoch, batch_idx * len(data), len(train_loader.dataset),\n",
        "          100. * batch_idx / len(train_loader), loss.item()))\n",
        "          #에포크, 미니배치 인덱스, 전체 데이터셋 크기, 현재 미니배치의 손실을 출력"
      ],
      "metadata": {
        "id": "IzUrEM3EnXJb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**model.eval()**\n",
        "\n",
        "*   모델의 모든 레이어가 evalutaion mode에 들어가도록 해줌 -> 학습할 때만 필요했던 drop out, batch norm 등의 기능을 추론할 때는 비활성화시킴\n",
        "    * 메모리와는 관련이 없다고\n",
        "*   torch.no_grad() : gradient 계산을 비활성화 함으로써 gradient를 더이상 트래킹하지 않음. 메모리가 줄어들고 연산 속도가 증가함"
      ],
      "metadata": {
        "id": "r_jvGggceda7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def test():\n",
        "    model.eval() #model.eval() 의 기능은?\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    for data, target in test_loader:\n",
        "        data, target = Variable(data, volatile=True), Variable(target)\n",
        "        #역전파 단계에서 그래디언트가 계산되지 않음, 메모리 절약하기 위해 사용 -> 최신 버전 파이토치에서는 with torch.no_grad()를 사용한다고..\n",
        "        output = model(data)\n",
        "        test_loss += F.nll_loss(output, target, size_average=False).data # nll_loss?? / cross entropy loss와의 관계 확인!\n",
        "        #size_average=False로 설정하여 각 배치의 손실 합을 구함\n",
        "        pred = output.data.max(1, keepdim=True)[1]\n",
        "        #각 입력 샘플에 대한 예측 저장\n",
        "        correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
        "        #예측값과 실제 타겟을 비교하여 올바른 예측수 계산\n",
        "\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "      #테스트 손실을 전체 테스트 데이터셋의 크기로 나누어 평균 손실을 계산\n",
        "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "        test_loss, correct, len(test_loader.dataset),\n",
        "        100. * correct / len(test_loader.dataset)))   #평균 손실과 정확도 출력함"
      ],
      "metadata": {
        "id": "EFi0gYJGn2aa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**null_loss**\n",
        "\n",
        "\n",
        "*   음의 로그 우도(Negative Log Likelihood) 손실을 계산하는 손실 함수. 로그 소프트맥스(log softmax) 출력을 사용\n",
        "*   torch.nn.CrossEntropyLoss : nn.LogSoftmax와 nn.NLLLoss의 연산의 조합\n",
        "\n",
        "\n",
        "https://supermemi.tistory.com/entry/Loss-Cross-Entropy-Negative-Log-Likelihood-%EB%82%B4%EC%9A%A9-%EC%A0%95%EB%A6%AC-Pytorch-Code\n",
        "1.    NLLLoss 안에서는 softmax나 log함수가 이뤄지지 않습니다. 그래서 모델 output(raw data)을 input으로 그대로 사용하는 것이 아니라 LogSoftmax 함수를 적용한 후 input으로 사용해야합니다.\n",
        "2.   CrossEntropyLoss 안에서 LogSoftmax와 Negative Log-likelihood 가 진행되기 때문에 softmax나 log 함수가 적용되지 않은 모델 output(raw data)을 input으로 주어야 합니다.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "F9ayvAdIgUxa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(1, 10):\n",
        "    train(epoch)\n",
        "    test()"
      ],
      "metadata": {
        "id": "zSvSZb_Bn4Nx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "de2646f9-9ca6-4a71-e7a9-9c09d2dd51cf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-53-32ddd7666038>:19: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return F.log_softmax(x)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.314939\n",
            "Train Epoch: 1 [640/60000 (1%)]\tLoss: 2.308357\n",
            "Train Epoch: 1 [1280/60000 (2%)]\tLoss: 2.294630\n",
            "Train Epoch: 1 [1920/60000 (3%)]\tLoss: 2.284297\n",
            "Train Epoch: 1 [2560/60000 (4%)]\tLoss: 2.269377\n",
            "Train Epoch: 1 [3200/60000 (5%)]\tLoss: 2.260186\n",
            "Train Epoch: 1 [3840/60000 (6%)]\tLoss: 2.240463\n",
            "Train Epoch: 1 [4480/60000 (7%)]\tLoss: 2.235462\n",
            "Train Epoch: 1 [5120/60000 (9%)]\tLoss: 2.184722\n",
            "Train Epoch: 1 [5760/60000 (10%)]\tLoss: 2.187236\n",
            "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 2.130372\n",
            "Train Epoch: 1 [7040/60000 (12%)]\tLoss: 2.055032\n",
            "Train Epoch: 1 [7680/60000 (13%)]\tLoss: 1.918186\n",
            "Train Epoch: 1 [8320/60000 (14%)]\tLoss: 1.743867\n",
            "Train Epoch: 1 [8960/60000 (15%)]\tLoss: 1.606936\n",
            "Train Epoch: 1 [9600/60000 (16%)]\tLoss: 1.281215\n",
            "Train Epoch: 1 [10240/60000 (17%)]\tLoss: 0.943631\n",
            "Train Epoch: 1 [10880/60000 (18%)]\tLoss: 0.916305\n",
            "Train Epoch: 1 [11520/60000 (19%)]\tLoss: 0.836914\n",
            "Train Epoch: 1 [12160/60000 (20%)]\tLoss: 0.698134\n",
            "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.526870\n",
            "Train Epoch: 1 [13440/60000 (22%)]\tLoss: 0.410949\n",
            "Train Epoch: 1 [14080/60000 (23%)]\tLoss: 0.480098\n",
            "Train Epoch: 1 [14720/60000 (25%)]\tLoss: 0.568562\n",
            "Train Epoch: 1 [15360/60000 (26%)]\tLoss: 0.440513\n",
            "Train Epoch: 1 [16000/60000 (27%)]\tLoss: 0.495628\n",
            "Train Epoch: 1 [16640/60000 (28%)]\tLoss: 0.501208\n",
            "Train Epoch: 1 [17280/60000 (29%)]\tLoss: 0.649012\n",
            "Train Epoch: 1 [17920/60000 (30%)]\tLoss: 0.529313\n",
            "Train Epoch: 1 [18560/60000 (31%)]\tLoss: 0.505722\n",
            "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.377594\n",
            "Train Epoch: 1 [19840/60000 (33%)]\tLoss: 0.340932\n",
            "Train Epoch: 1 [20480/60000 (34%)]\tLoss: 0.496550\n",
            "Train Epoch: 1 [21120/60000 (35%)]\tLoss: 0.415831\n",
            "Train Epoch: 1 [21760/60000 (36%)]\tLoss: 0.311891\n",
            "Train Epoch: 1 [22400/60000 (37%)]\tLoss: 0.323030\n",
            "Train Epoch: 1 [23040/60000 (38%)]\tLoss: 0.429853\n",
            "Train Epoch: 1 [23680/60000 (39%)]\tLoss: 0.292380\n",
            "Train Epoch: 1 [24320/60000 (41%)]\tLoss: 0.280003\n",
            "Train Epoch: 1 [24960/60000 (42%)]\tLoss: 0.162464\n",
            "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.319672\n",
            "Train Epoch: 1 [26240/60000 (44%)]\tLoss: 0.302224\n",
            "Train Epoch: 1 [26880/60000 (45%)]\tLoss: 0.295872\n",
            "Train Epoch: 1 [27520/60000 (46%)]\tLoss: 0.428969\n",
            "Train Epoch: 1 [28160/60000 (47%)]\tLoss: 0.270879\n",
            "Train Epoch: 1 [28800/60000 (48%)]\tLoss: 0.339737\n",
            "Train Epoch: 1 [29440/60000 (49%)]\tLoss: 0.460038\n",
            "Train Epoch: 1 [30080/60000 (50%)]\tLoss: 0.255416\n",
            "Train Epoch: 1 [30720/60000 (51%)]\tLoss: 0.444240\n",
            "Train Epoch: 1 [31360/60000 (52%)]\tLoss: 0.178954\n",
            "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.280241\n",
            "Train Epoch: 1 [32640/60000 (54%)]\tLoss: 0.214268\n",
            "Train Epoch: 1 [33280/60000 (55%)]\tLoss: 0.358350\n",
            "Train Epoch: 1 [33920/60000 (57%)]\tLoss: 0.318665\n",
            "Train Epoch: 1 [34560/60000 (58%)]\tLoss: 0.267087\n",
            "Train Epoch: 1 [35200/60000 (59%)]\tLoss: 0.221802\n",
            "Train Epoch: 1 [35840/60000 (60%)]\tLoss: 0.231229\n",
            "Train Epoch: 1 [36480/60000 (61%)]\tLoss: 0.363856\n",
            "Train Epoch: 1 [37120/60000 (62%)]\tLoss: 0.417221\n",
            "Train Epoch: 1 [37760/60000 (63%)]\tLoss: 0.232280\n",
            "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.187379\n",
            "Train Epoch: 1 [39040/60000 (65%)]\tLoss: 0.487812\n",
            "Train Epoch: 1 [39680/60000 (66%)]\tLoss: 0.306292\n",
            "Train Epoch: 1 [40320/60000 (67%)]\tLoss: 0.217648\n",
            "Train Epoch: 1 [40960/60000 (68%)]\tLoss: 0.316468\n",
            "Train Epoch: 1 [41600/60000 (69%)]\tLoss: 0.257041\n",
            "Train Epoch: 1 [42240/60000 (70%)]\tLoss: 0.264451\n",
            "Train Epoch: 1 [42880/60000 (71%)]\tLoss: 0.154645\n",
            "Train Epoch: 1 [43520/60000 (72%)]\tLoss: 0.262038\n",
            "Train Epoch: 1 [44160/60000 (74%)]\tLoss: 0.327255\n",
            "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.398172\n",
            "Train Epoch: 1 [45440/60000 (76%)]\tLoss: 0.166379\n",
            "Train Epoch: 1 [46080/60000 (77%)]\tLoss: 0.291810\n",
            "Train Epoch: 1 [46720/60000 (78%)]\tLoss: 0.252554\n",
            "Train Epoch: 1 [47360/60000 (79%)]\tLoss: 0.163389\n",
            "Train Epoch: 1 [48000/60000 (80%)]\tLoss: 0.319800\n",
            "Train Epoch: 1 [48640/60000 (81%)]\tLoss: 0.418160\n",
            "Train Epoch: 1 [49280/60000 (82%)]\tLoss: 0.138487\n",
            "Train Epoch: 1 [49920/60000 (83%)]\tLoss: 0.364128\n",
            "Train Epoch: 1 [50560/60000 (84%)]\tLoss: 0.136895\n",
            "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.159574\n",
            "Train Epoch: 1 [51840/60000 (86%)]\tLoss: 0.374641\n",
            "Train Epoch: 1 [52480/60000 (87%)]\tLoss: 0.205457\n",
            "Train Epoch: 1 [53120/60000 (88%)]\tLoss: 0.182104\n",
            "Train Epoch: 1 [53760/60000 (90%)]\tLoss: 0.251896\n",
            "Train Epoch: 1 [54400/60000 (91%)]\tLoss: 0.153657\n",
            "Train Epoch: 1 [55040/60000 (92%)]\tLoss: 0.185743\n",
            "Train Epoch: 1 [55680/60000 (93%)]\tLoss: 0.153076\n",
            "Train Epoch: 1 [56320/60000 (94%)]\tLoss: 0.110022\n",
            "Train Epoch: 1 [56960/60000 (95%)]\tLoss: 0.231037\n",
            "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.051838\n",
            "Train Epoch: 1 [58240/60000 (97%)]\tLoss: 0.066364\n",
            "Train Epoch: 1 [58880/60000 (98%)]\tLoss: 0.307823\n",
            "Train Epoch: 1 [59520/60000 (99%)]\tLoss: 0.363777\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-56-f52337105c2a>:6: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
            "  data, target = Variable(data, volatile=True), Variable(target)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
            "  warnings.warn(warning.format(ret))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.1646, Accuracy: 9504/10000 (95%)\n",
            "\n",
            "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.201864\n",
            "Train Epoch: 2 [640/60000 (1%)]\tLoss: 0.075613\n",
            "Train Epoch: 2 [1280/60000 (2%)]\tLoss: 0.154274\n",
            "Train Epoch: 2 [1920/60000 (3%)]\tLoss: 0.200834\n",
            "Train Epoch: 2 [2560/60000 (4%)]\tLoss: 0.259817\n",
            "Train Epoch: 2 [3200/60000 (5%)]\tLoss: 0.266904\n",
            "Train Epoch: 2 [3840/60000 (6%)]\tLoss: 0.070891\n",
            "Train Epoch: 2 [4480/60000 (7%)]\tLoss: 0.153136\n",
            "Train Epoch: 2 [5120/60000 (9%)]\tLoss: 0.222462\n",
            "Train Epoch: 2 [5760/60000 (10%)]\tLoss: 0.054175\n",
            "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 0.323388\n",
            "Train Epoch: 2 [7040/60000 (12%)]\tLoss: 0.103200\n",
            "Train Epoch: 2 [7680/60000 (13%)]\tLoss: 0.098906\n",
            "Train Epoch: 2 [8320/60000 (14%)]\tLoss: 0.266934\n",
            "Train Epoch: 2 [8960/60000 (15%)]\tLoss: 0.156470\n",
            "Train Epoch: 2 [9600/60000 (16%)]\tLoss: 0.246581\n",
            "Train Epoch: 2 [10240/60000 (17%)]\tLoss: 0.164587\n",
            "Train Epoch: 2 [10880/60000 (18%)]\tLoss: 0.244762\n",
            "Train Epoch: 2 [11520/60000 (19%)]\tLoss: 0.181817\n",
            "Train Epoch: 2 [12160/60000 (20%)]\tLoss: 0.089310\n",
            "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 0.085543\n",
            "Train Epoch: 2 [13440/60000 (22%)]\tLoss: 0.131681\n",
            "Train Epoch: 2 [14080/60000 (23%)]\tLoss: 0.225507\n",
            "Train Epoch: 2 [14720/60000 (25%)]\tLoss: 0.077062\n",
            "Train Epoch: 2 [15360/60000 (26%)]\tLoss: 0.162379\n",
            "Train Epoch: 2 [16000/60000 (27%)]\tLoss: 0.239600\n",
            "Train Epoch: 2 [16640/60000 (28%)]\tLoss: 0.153677\n",
            "Train Epoch: 2 [17280/60000 (29%)]\tLoss: 0.112976\n",
            "Train Epoch: 2 [17920/60000 (30%)]\tLoss: 0.407342\n",
            "Train Epoch: 2 [18560/60000 (31%)]\tLoss: 0.097452\n",
            "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 0.168803\n",
            "Train Epoch: 2 [19840/60000 (33%)]\tLoss: 0.238437\n",
            "Train Epoch: 2 [20480/60000 (34%)]\tLoss: 0.089953\n",
            "Train Epoch: 2 [21120/60000 (35%)]\tLoss: 0.076942\n",
            "Train Epoch: 2 [21760/60000 (36%)]\tLoss: 0.218127\n",
            "Train Epoch: 2 [22400/60000 (37%)]\tLoss: 0.106049\n",
            "Train Epoch: 2 [23040/60000 (38%)]\tLoss: 0.166475\n",
            "Train Epoch: 2 [23680/60000 (39%)]\tLoss: 0.105140\n",
            "Train Epoch: 2 [24320/60000 (41%)]\tLoss: 0.218152\n",
            "Train Epoch: 2 [24960/60000 (42%)]\tLoss: 0.230383\n",
            "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.078685\n",
            "Train Epoch: 2 [26240/60000 (44%)]\tLoss: 0.131399\n",
            "Train Epoch: 2 [26880/60000 (45%)]\tLoss: 0.210346\n",
            "Train Epoch: 2 [27520/60000 (46%)]\tLoss: 0.135414\n",
            "Train Epoch: 2 [28160/60000 (47%)]\tLoss: 0.128563\n",
            "Train Epoch: 2 [28800/60000 (48%)]\tLoss: 0.116791\n",
            "Train Epoch: 2 [29440/60000 (49%)]\tLoss: 0.061852\n",
            "Train Epoch: 2 [30080/60000 (50%)]\tLoss: 0.073884\n",
            "Train Epoch: 2 [30720/60000 (51%)]\tLoss: 0.052092\n",
            "Train Epoch: 2 [31360/60000 (52%)]\tLoss: 0.102455\n",
            "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 0.056015\n",
            "Train Epoch: 2 [32640/60000 (54%)]\tLoss: 0.182831\n",
            "Train Epoch: 2 [33280/60000 (55%)]\tLoss: 0.176110\n",
            "Train Epoch: 2 [33920/60000 (57%)]\tLoss: 0.237974\n",
            "Train Epoch: 2 [34560/60000 (58%)]\tLoss: 0.081260\n",
            "Train Epoch: 2 [35200/60000 (59%)]\tLoss: 0.030758\n",
            "Train Epoch: 2 [35840/60000 (60%)]\tLoss: 0.094665\n",
            "Train Epoch: 2 [36480/60000 (61%)]\tLoss: 0.084123\n",
            "Train Epoch: 2 [37120/60000 (62%)]\tLoss: 0.063712\n",
            "Train Epoch: 2 [37760/60000 (63%)]\tLoss: 0.063024\n",
            "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 0.114328\n",
            "Train Epoch: 2 [39040/60000 (65%)]\tLoss: 0.244359\n",
            "Train Epoch: 2 [39680/60000 (66%)]\tLoss: 0.260734\n",
            "Train Epoch: 2 [40320/60000 (67%)]\tLoss: 0.137302\n",
            "Train Epoch: 2 [40960/60000 (68%)]\tLoss: 0.129305\n",
            "Train Epoch: 2 [41600/60000 (69%)]\tLoss: 0.088332\n",
            "Train Epoch: 2 [42240/60000 (70%)]\tLoss: 0.103455\n",
            "Train Epoch: 2 [42880/60000 (71%)]\tLoss: 0.139182\n",
            "Train Epoch: 2 [43520/60000 (72%)]\tLoss: 0.112156\n",
            "Train Epoch: 2 [44160/60000 (74%)]\tLoss: 0.215405\n",
            "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 0.162673\n",
            "Train Epoch: 2 [45440/60000 (76%)]\tLoss: 0.139971\n",
            "Train Epoch: 2 [46080/60000 (77%)]\tLoss: 0.246534\n",
            "Train Epoch: 2 [46720/60000 (78%)]\tLoss: 0.255697\n",
            "Train Epoch: 2 [47360/60000 (79%)]\tLoss: 0.200296\n",
            "Train Epoch: 2 [48000/60000 (80%)]\tLoss: 0.323837\n",
            "Train Epoch: 2 [48640/60000 (81%)]\tLoss: 0.174747\n",
            "Train Epoch: 2 [49280/60000 (82%)]\tLoss: 0.125006\n",
            "Train Epoch: 2 [49920/60000 (83%)]\tLoss: 0.139354\n",
            "Train Epoch: 2 [50560/60000 (84%)]\tLoss: 0.234191\n",
            "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.044477\n",
            "Train Epoch: 2 [51840/60000 (86%)]\tLoss: 0.136089\n",
            "Train Epoch: 2 [52480/60000 (87%)]\tLoss: 0.212957\n",
            "Train Epoch: 2 [53120/60000 (88%)]\tLoss: 0.120977\n",
            "Train Epoch: 2 [53760/60000 (90%)]\tLoss: 0.233583\n",
            "Train Epoch: 2 [54400/60000 (91%)]\tLoss: 0.276114\n",
            "Train Epoch: 2 [55040/60000 (92%)]\tLoss: 0.086157\n",
            "Train Epoch: 2 [55680/60000 (93%)]\tLoss: 0.070020\n",
            "Train Epoch: 2 [56320/60000 (94%)]\tLoss: 0.152499\n",
            "Train Epoch: 2 [56960/60000 (95%)]\tLoss: 0.283784\n",
            "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 0.151876\n",
            "Train Epoch: 2 [58240/60000 (97%)]\tLoss: 0.167558\n",
            "Train Epoch: 2 [58880/60000 (98%)]\tLoss: 0.161919\n",
            "Train Epoch: 2 [59520/60000 (99%)]\tLoss: 0.078947\n",
            "\n",
            "Test set: Average loss: 0.1142, Accuracy: 9675/10000 (97%)\n",
            "\n",
            "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.134595\n",
            "Train Epoch: 3 [640/60000 (1%)]\tLoss: 0.090193\n",
            "Train Epoch: 3 [1280/60000 (2%)]\tLoss: 0.182638\n",
            "Train Epoch: 3 [1920/60000 (3%)]\tLoss: 0.088030\n",
            "Train Epoch: 3 [2560/60000 (4%)]\tLoss: 0.163274\n",
            "Train Epoch: 3 [3200/60000 (5%)]\tLoss: 0.221890\n",
            "Train Epoch: 3 [3840/60000 (6%)]\tLoss: 0.190609\n",
            "Train Epoch: 3 [4480/60000 (7%)]\tLoss: 0.037326\n",
            "Train Epoch: 3 [5120/60000 (9%)]\tLoss: 0.134987\n",
            "Train Epoch: 3 [5760/60000 (10%)]\tLoss: 0.127169\n",
            "Train Epoch: 3 [6400/60000 (11%)]\tLoss: 0.071643\n",
            "Train Epoch: 3 [7040/60000 (12%)]\tLoss: 0.042128\n",
            "Train Epoch: 3 [7680/60000 (13%)]\tLoss: 0.129434\n",
            "Train Epoch: 3 [8320/60000 (14%)]\tLoss: 0.113733\n",
            "Train Epoch: 3 [8960/60000 (15%)]\tLoss: 0.253405\n",
            "Train Epoch: 3 [9600/60000 (16%)]\tLoss: 0.119743\n",
            "Train Epoch: 3 [10240/60000 (17%)]\tLoss: 0.141451\n",
            "Train Epoch: 3 [10880/60000 (18%)]\tLoss: 0.168042\n",
            "Train Epoch: 3 [11520/60000 (19%)]\tLoss: 0.160579\n",
            "Train Epoch: 3 [12160/60000 (20%)]\tLoss: 0.137183\n",
            "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 0.048699\n",
            "Train Epoch: 3 [13440/60000 (22%)]\tLoss: 0.158978\n",
            "Train Epoch: 3 [14080/60000 (23%)]\tLoss: 0.102776\n",
            "Train Epoch: 3 [14720/60000 (25%)]\tLoss: 0.149317\n",
            "Train Epoch: 3 [15360/60000 (26%)]\tLoss: 0.185924\n",
            "Train Epoch: 3 [16000/60000 (27%)]\tLoss: 0.206779\n",
            "Train Epoch: 3 [16640/60000 (28%)]\tLoss: 0.186892\n",
            "Train Epoch: 3 [17280/60000 (29%)]\tLoss: 0.035960\n",
            "Train Epoch: 3 [17920/60000 (30%)]\tLoss: 0.085995\n",
            "Train Epoch: 3 [18560/60000 (31%)]\tLoss: 0.025815\n",
            "Train Epoch: 3 [19200/60000 (32%)]\tLoss: 0.219605\n",
            "Train Epoch: 3 [19840/60000 (33%)]\tLoss: 0.110810\n",
            "Train Epoch: 3 [20480/60000 (34%)]\tLoss: 0.045209\n",
            "Train Epoch: 3 [21120/60000 (35%)]\tLoss: 0.080433\n",
            "Train Epoch: 3 [21760/60000 (36%)]\tLoss: 0.250600\n",
            "Train Epoch: 3 [22400/60000 (37%)]\tLoss: 0.195346\n",
            "Train Epoch: 3 [23040/60000 (38%)]\tLoss: 0.103971\n",
            "Train Epoch: 3 [23680/60000 (39%)]\tLoss: 0.058464\n",
            "Train Epoch: 3 [24320/60000 (41%)]\tLoss: 0.127045\n",
            "Train Epoch: 3 [24960/60000 (42%)]\tLoss: 0.032214\n",
            "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.019299\n",
            "Train Epoch: 3 [26240/60000 (44%)]\tLoss: 0.170423\n",
            "Train Epoch: 3 [26880/60000 (45%)]\tLoss: 0.154171\n",
            "Train Epoch: 3 [27520/60000 (46%)]\tLoss: 0.143852\n",
            "Train Epoch: 3 [28160/60000 (47%)]\tLoss: 0.090635\n",
            "Train Epoch: 3 [28800/60000 (48%)]\tLoss: 0.041948\n",
            "Train Epoch: 3 [29440/60000 (49%)]\tLoss: 0.109477\n",
            "Train Epoch: 3 [30080/60000 (50%)]\tLoss: 0.047707\n",
            "Train Epoch: 3 [30720/60000 (51%)]\tLoss: 0.056658\n",
            "Train Epoch: 3 [31360/60000 (52%)]\tLoss: 0.084368\n",
            "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 0.075001\n",
            "Train Epoch: 3 [32640/60000 (54%)]\tLoss: 0.057743\n",
            "Train Epoch: 3 [33280/60000 (55%)]\tLoss: 0.152974\n",
            "Train Epoch: 3 [33920/60000 (57%)]\tLoss: 0.246028\n",
            "Train Epoch: 3 [34560/60000 (58%)]\tLoss: 0.176165\n",
            "Train Epoch: 3 [35200/60000 (59%)]\tLoss: 0.111203\n",
            "Train Epoch: 3 [35840/60000 (60%)]\tLoss: 0.210260\n",
            "Train Epoch: 3 [36480/60000 (61%)]\tLoss: 0.148579\n",
            "Train Epoch: 3 [37120/60000 (62%)]\tLoss: 0.074542\n",
            "Train Epoch: 3 [37760/60000 (63%)]\tLoss: 0.021354\n",
            "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 0.092004\n",
            "Train Epoch: 3 [39040/60000 (65%)]\tLoss: 0.067834\n",
            "Train Epoch: 3 [39680/60000 (66%)]\tLoss: 0.122958\n",
            "Train Epoch: 3 [40320/60000 (67%)]\tLoss: 0.274352\n",
            "Train Epoch: 3 [40960/60000 (68%)]\tLoss: 0.091293\n",
            "Train Epoch: 3 [41600/60000 (69%)]\tLoss: 0.028007\n",
            "Train Epoch: 3 [42240/60000 (70%)]\tLoss: 0.077207\n",
            "Train Epoch: 3 [42880/60000 (71%)]\tLoss: 0.110104\n",
            "Train Epoch: 3 [43520/60000 (72%)]\tLoss: 0.176522\n",
            "Train Epoch: 3 [44160/60000 (74%)]\tLoss: 0.121319\n",
            "Train Epoch: 3 [44800/60000 (75%)]\tLoss: 0.307402\n",
            "Train Epoch: 3 [45440/60000 (76%)]\tLoss: 0.049296\n",
            "Train Epoch: 3 [46080/60000 (77%)]\tLoss: 0.032730\n",
            "Train Epoch: 3 [46720/60000 (78%)]\tLoss: 0.182389\n",
            "Train Epoch: 3 [47360/60000 (79%)]\tLoss: 0.018852\n",
            "Train Epoch: 3 [48000/60000 (80%)]\tLoss: 0.193430\n",
            "Train Epoch: 3 [48640/60000 (81%)]\tLoss: 0.236819\n",
            "Train Epoch: 3 [49280/60000 (82%)]\tLoss: 0.145790\n",
            "Train Epoch: 3 [49920/60000 (83%)]\tLoss: 0.223472\n",
            "Train Epoch: 3 [50560/60000 (84%)]\tLoss: 0.118085\n",
            "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.173218\n",
            "Train Epoch: 3 [51840/60000 (86%)]\tLoss: 0.065847\n",
            "Train Epoch: 3 [52480/60000 (87%)]\tLoss: 0.091887\n",
            "Train Epoch: 3 [53120/60000 (88%)]\tLoss: 0.059544\n",
            "Train Epoch: 3 [53760/60000 (90%)]\tLoss: 0.116552\n",
            "Train Epoch: 3 [54400/60000 (91%)]\tLoss: 0.034339\n",
            "Train Epoch: 3 [55040/60000 (92%)]\tLoss: 0.164468\n",
            "Train Epoch: 3 [55680/60000 (93%)]\tLoss: 0.075676\n",
            "Train Epoch: 3 [56320/60000 (94%)]\tLoss: 0.094449\n",
            "Train Epoch: 3 [56960/60000 (95%)]\tLoss: 0.166851\n",
            "Train Epoch: 3 [57600/60000 (96%)]\tLoss: 0.048000\n",
            "Train Epoch: 3 [58240/60000 (97%)]\tLoss: 0.128367\n",
            "Train Epoch: 3 [58880/60000 (98%)]\tLoss: 0.197781\n",
            "Train Epoch: 3 [59520/60000 (99%)]\tLoss: 0.260894\n",
            "\n",
            "Test set: Average loss: 0.0849, Accuracy: 9743/10000 (97%)\n",
            "\n",
            "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.122657\n",
            "Train Epoch: 4 [640/60000 (1%)]\tLoss: 0.252797\n",
            "Train Epoch: 4 [1280/60000 (2%)]\tLoss: 0.120497\n",
            "Train Epoch: 4 [1920/60000 (3%)]\tLoss: 0.126474\n",
            "Train Epoch: 4 [2560/60000 (4%)]\tLoss: 0.096165\n",
            "Train Epoch: 4 [3200/60000 (5%)]\tLoss: 0.217247\n",
            "Train Epoch: 4 [3840/60000 (6%)]\tLoss: 0.014054\n",
            "Train Epoch: 4 [4480/60000 (7%)]\tLoss: 0.050371\n",
            "Train Epoch: 4 [5120/60000 (9%)]\tLoss: 0.098433\n",
            "Train Epoch: 4 [5760/60000 (10%)]\tLoss: 0.041971\n",
            "Train Epoch: 4 [6400/60000 (11%)]\tLoss: 0.065980\n",
            "Train Epoch: 4 [7040/60000 (12%)]\tLoss: 0.140196\n",
            "Train Epoch: 4 [7680/60000 (13%)]\tLoss: 0.071811\n",
            "Train Epoch: 4 [8320/60000 (14%)]\tLoss: 0.150250\n",
            "Train Epoch: 4 [8960/60000 (15%)]\tLoss: 0.038571\n",
            "Train Epoch: 4 [9600/60000 (16%)]\tLoss: 0.132591\n",
            "Train Epoch: 4 [10240/60000 (17%)]\tLoss: 0.019596\n",
            "Train Epoch: 4 [10880/60000 (18%)]\tLoss: 0.215486\n",
            "Train Epoch: 4 [11520/60000 (19%)]\tLoss: 0.143717\n",
            "Train Epoch: 4 [12160/60000 (20%)]\tLoss: 0.230199\n",
            "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 0.026811\n",
            "Train Epoch: 4 [13440/60000 (22%)]\tLoss: 0.060521\n",
            "Train Epoch: 4 [14080/60000 (23%)]\tLoss: 0.076222\n",
            "Train Epoch: 4 [14720/60000 (25%)]\tLoss: 0.063711\n",
            "Train Epoch: 4 [15360/60000 (26%)]\tLoss: 0.058094\n",
            "Train Epoch: 4 [16000/60000 (27%)]\tLoss: 0.177111\n",
            "Train Epoch: 4 [16640/60000 (28%)]\tLoss: 0.103993\n",
            "Train Epoch: 4 [17280/60000 (29%)]\tLoss: 0.177493\n",
            "Train Epoch: 4 [17920/60000 (30%)]\tLoss: 0.024038\n",
            "Train Epoch: 4 [18560/60000 (31%)]\tLoss: 0.078577\n",
            "Train Epoch: 4 [19200/60000 (32%)]\tLoss: 0.051365\n",
            "Train Epoch: 4 [19840/60000 (33%)]\tLoss: 0.028963\n",
            "Train Epoch: 4 [20480/60000 (34%)]\tLoss: 0.216742\n",
            "Train Epoch: 4 [21120/60000 (35%)]\tLoss: 0.062623\n",
            "Train Epoch: 4 [21760/60000 (36%)]\tLoss: 0.017927\n",
            "Train Epoch: 4 [22400/60000 (37%)]\tLoss: 0.288666\n",
            "Train Epoch: 4 [23040/60000 (38%)]\tLoss: 0.030649\n",
            "Train Epoch: 4 [23680/60000 (39%)]\tLoss: 0.099449\n",
            "Train Epoch: 4 [24320/60000 (41%)]\tLoss: 0.084457\n",
            "Train Epoch: 4 [24960/60000 (42%)]\tLoss: 0.020662\n",
            "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 0.041448\n",
            "Train Epoch: 4 [26240/60000 (44%)]\tLoss: 0.185407\n",
            "Train Epoch: 4 [26880/60000 (45%)]\tLoss: 0.084934\n",
            "Train Epoch: 4 [27520/60000 (46%)]\tLoss: 0.114341\n",
            "Train Epoch: 4 [28160/60000 (47%)]\tLoss: 0.059362\n",
            "Train Epoch: 4 [28800/60000 (48%)]\tLoss: 0.098138\n",
            "Train Epoch: 4 [29440/60000 (49%)]\tLoss: 0.135689\n",
            "Train Epoch: 4 [30080/60000 (50%)]\tLoss: 0.057433\n",
            "Train Epoch: 4 [30720/60000 (51%)]\tLoss: 0.278697\n",
            "Train Epoch: 4 [31360/60000 (52%)]\tLoss: 0.124026\n",
            "Train Epoch: 4 [32000/60000 (53%)]\tLoss: 0.172906\n",
            "Train Epoch: 4 [32640/60000 (54%)]\tLoss: 0.059666\n",
            "Train Epoch: 4 [33280/60000 (55%)]\tLoss: 0.119114\n",
            "Train Epoch: 4 [33920/60000 (57%)]\tLoss: 0.071996\n",
            "Train Epoch: 4 [34560/60000 (58%)]\tLoss: 0.073322\n",
            "Train Epoch: 4 [35200/60000 (59%)]\tLoss: 0.087608\n",
            "Train Epoch: 4 [35840/60000 (60%)]\tLoss: 0.090454\n",
            "Train Epoch: 4 [36480/60000 (61%)]\tLoss: 0.029372\n",
            "Train Epoch: 4 [37120/60000 (62%)]\tLoss: 0.066414\n",
            "Train Epoch: 4 [37760/60000 (63%)]\tLoss: 0.030128\n",
            "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 0.061899\n",
            "Train Epoch: 4 [39040/60000 (65%)]\tLoss: 0.203461\n",
            "Train Epoch: 4 [39680/60000 (66%)]\tLoss: 0.078375\n",
            "Train Epoch: 4 [40320/60000 (67%)]\tLoss: 0.019102\n",
            "Train Epoch: 4 [40960/60000 (68%)]\tLoss: 0.127985\n",
            "Train Epoch: 4 [41600/60000 (69%)]\tLoss: 0.078032\n",
            "Train Epoch: 4 [42240/60000 (70%)]\tLoss: 0.056710\n",
            "Train Epoch: 4 [42880/60000 (71%)]\tLoss: 0.124231\n",
            "Train Epoch: 4 [43520/60000 (72%)]\tLoss: 0.032722\n",
            "Train Epoch: 4 [44160/60000 (74%)]\tLoss: 0.396111\n",
            "Train Epoch: 4 [44800/60000 (75%)]\tLoss: 0.273297\n",
            "Train Epoch: 4 [45440/60000 (76%)]\tLoss: 0.052131\n",
            "Train Epoch: 4 [46080/60000 (77%)]\tLoss: 0.044879\n",
            "Train Epoch: 4 [46720/60000 (78%)]\tLoss: 0.062187\n",
            "Train Epoch: 4 [47360/60000 (79%)]\tLoss: 0.049979\n",
            "Train Epoch: 4 [48000/60000 (80%)]\tLoss: 0.027491\n",
            "Train Epoch: 4 [48640/60000 (81%)]\tLoss: 0.141539\n",
            "Train Epoch: 4 [49280/60000 (82%)]\tLoss: 0.050638\n",
            "Train Epoch: 4 [49920/60000 (83%)]\tLoss: 0.171425\n",
            "Train Epoch: 4 [50560/60000 (84%)]\tLoss: 0.086883\n",
            "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 0.133808\n",
            "Train Epoch: 4 [51840/60000 (86%)]\tLoss: 0.084961\n",
            "Train Epoch: 4 [52480/60000 (87%)]\tLoss: 0.083546\n",
            "Train Epoch: 4 [53120/60000 (88%)]\tLoss: 0.102997\n",
            "Train Epoch: 4 [53760/60000 (90%)]\tLoss: 0.170527\n",
            "Train Epoch: 4 [54400/60000 (91%)]\tLoss: 0.019667\n",
            "Train Epoch: 4 [55040/60000 (92%)]\tLoss: 0.043238\n",
            "Train Epoch: 4 [55680/60000 (93%)]\tLoss: 0.255112\n",
            "Train Epoch: 4 [56320/60000 (94%)]\tLoss: 0.144106\n",
            "Train Epoch: 4 [56960/60000 (95%)]\tLoss: 0.065065\n",
            "Train Epoch: 4 [57600/60000 (96%)]\tLoss: 0.033967\n",
            "Train Epoch: 4 [58240/60000 (97%)]\tLoss: 0.097290\n",
            "Train Epoch: 4 [58880/60000 (98%)]\tLoss: 0.137351\n",
            "Train Epoch: 4 [59520/60000 (99%)]\tLoss: 0.124303\n",
            "\n",
            "Test set: Average loss: 0.0752, Accuracy: 9764/10000 (98%)\n",
            "\n",
            "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.100871\n",
            "Train Epoch: 5 [640/60000 (1%)]\tLoss: 0.078859\n",
            "Train Epoch: 5 [1280/60000 (2%)]\tLoss: 0.024567\n",
            "Train Epoch: 5 [1920/60000 (3%)]\tLoss: 0.021121\n",
            "Train Epoch: 5 [2560/60000 (4%)]\tLoss: 0.102720\n",
            "Train Epoch: 5 [3200/60000 (5%)]\tLoss: 0.189849\n",
            "Train Epoch: 5 [3840/60000 (6%)]\tLoss: 0.078919\n",
            "Train Epoch: 5 [4480/60000 (7%)]\tLoss: 0.123580\n",
            "Train Epoch: 5 [5120/60000 (9%)]\tLoss: 0.083616\n",
            "Train Epoch: 5 [5760/60000 (10%)]\tLoss: 0.031619\n",
            "Train Epoch: 5 [6400/60000 (11%)]\tLoss: 0.031194\n",
            "Train Epoch: 5 [7040/60000 (12%)]\tLoss: 0.021839\n",
            "Train Epoch: 5 [7680/60000 (13%)]\tLoss: 0.032418\n",
            "Train Epoch: 5 [8320/60000 (14%)]\tLoss: 0.047667\n",
            "Train Epoch: 5 [8960/60000 (15%)]\tLoss: 0.062781\n",
            "Train Epoch: 5 [9600/60000 (16%)]\tLoss: 0.128983\n",
            "Train Epoch: 5 [10240/60000 (17%)]\tLoss: 0.027791\n",
            "Train Epoch: 5 [10880/60000 (18%)]\tLoss: 0.032424\n",
            "Train Epoch: 5 [11520/60000 (19%)]\tLoss: 0.040882\n",
            "Train Epoch: 5 [12160/60000 (20%)]\tLoss: 0.146135\n",
            "Train Epoch: 5 [12800/60000 (21%)]\tLoss: 0.017218\n",
            "Train Epoch: 5 [13440/60000 (22%)]\tLoss: 0.171511\n",
            "Train Epoch: 5 [14080/60000 (23%)]\tLoss: 0.045425\n",
            "Train Epoch: 5 [14720/60000 (25%)]\tLoss: 0.017622\n",
            "Train Epoch: 5 [15360/60000 (26%)]\tLoss: 0.077327\n",
            "Train Epoch: 5 [16000/60000 (27%)]\tLoss: 0.107998\n",
            "Train Epoch: 5 [16640/60000 (28%)]\tLoss: 0.102621\n",
            "Train Epoch: 5 [17280/60000 (29%)]\tLoss: 0.048390\n",
            "Train Epoch: 5 [17920/60000 (30%)]\tLoss: 0.060573\n",
            "Train Epoch: 5 [18560/60000 (31%)]\tLoss: 0.125652\n",
            "Train Epoch: 5 [19200/60000 (32%)]\tLoss: 0.047726\n",
            "Train Epoch: 5 [19840/60000 (33%)]\tLoss: 0.006047\n",
            "Train Epoch: 5 [20480/60000 (34%)]\tLoss: 0.088660\n",
            "Train Epoch: 5 [21120/60000 (35%)]\tLoss: 0.031537\n",
            "Train Epoch: 5 [21760/60000 (36%)]\tLoss: 0.067748\n",
            "Train Epoch: 5 [22400/60000 (37%)]\tLoss: 0.076766\n",
            "Train Epoch: 5 [23040/60000 (38%)]\tLoss: 0.038226\n",
            "Train Epoch: 5 [23680/60000 (39%)]\tLoss: 0.022796\n",
            "Train Epoch: 5 [24320/60000 (41%)]\tLoss: 0.088520\n",
            "Train Epoch: 5 [24960/60000 (42%)]\tLoss: 0.149578\n",
            "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 0.138460\n",
            "Train Epoch: 5 [26240/60000 (44%)]\tLoss: 0.078757\n",
            "Train Epoch: 5 [26880/60000 (45%)]\tLoss: 0.281445\n",
            "Train Epoch: 5 [27520/60000 (46%)]\tLoss: 0.092381\n",
            "Train Epoch: 5 [28160/60000 (47%)]\tLoss: 0.092739\n",
            "Train Epoch: 5 [28800/60000 (48%)]\tLoss: 0.031021\n",
            "Train Epoch: 5 [29440/60000 (49%)]\tLoss: 0.039529\n",
            "Train Epoch: 5 [30080/60000 (50%)]\tLoss: 0.088245\n",
            "Train Epoch: 5 [30720/60000 (51%)]\tLoss: 0.073942\n",
            "Train Epoch: 5 [31360/60000 (52%)]\tLoss: 0.105121\n",
            "Train Epoch: 5 [32000/60000 (53%)]\tLoss: 0.172627\n",
            "Train Epoch: 5 [32640/60000 (54%)]\tLoss: 0.070041\n",
            "Train Epoch: 5 [33280/60000 (55%)]\tLoss: 0.052050\n",
            "Train Epoch: 5 [33920/60000 (57%)]\tLoss: 0.071721\n",
            "Train Epoch: 5 [34560/60000 (58%)]\tLoss: 0.061239\n",
            "Train Epoch: 5 [35200/60000 (59%)]\tLoss: 0.130294\n",
            "Train Epoch: 5 [35840/60000 (60%)]\tLoss: 0.008503\n",
            "Train Epoch: 5 [36480/60000 (61%)]\tLoss: 0.095634\n",
            "Train Epoch: 5 [37120/60000 (62%)]\tLoss: 0.284784\n",
            "Train Epoch: 5 [37760/60000 (63%)]\tLoss: 0.055112\n",
            "Train Epoch: 5 [38400/60000 (64%)]\tLoss: 0.110535\n",
            "Train Epoch: 5 [39040/60000 (65%)]\tLoss: 0.026413\n",
            "Train Epoch: 5 [39680/60000 (66%)]\tLoss: 0.062970\n",
            "Train Epoch: 5 [40320/60000 (67%)]\tLoss: 0.089603\n",
            "Train Epoch: 5 [40960/60000 (68%)]\tLoss: 0.185267\n",
            "Train Epoch: 5 [41600/60000 (69%)]\tLoss: 0.183403\n",
            "Train Epoch: 5 [42240/60000 (70%)]\tLoss: 0.127347\n",
            "Train Epoch: 5 [42880/60000 (71%)]\tLoss: 0.130974\n",
            "Train Epoch: 5 [43520/60000 (72%)]\tLoss: 0.049231\n",
            "Train Epoch: 5 [44160/60000 (74%)]\tLoss: 0.044775\n",
            "Train Epoch: 5 [44800/60000 (75%)]\tLoss: 0.075300\n",
            "Train Epoch: 5 [45440/60000 (76%)]\tLoss: 0.192296\n",
            "Train Epoch: 5 [46080/60000 (77%)]\tLoss: 0.031233\n",
            "Train Epoch: 5 [46720/60000 (78%)]\tLoss: 0.091359\n",
            "Train Epoch: 5 [47360/60000 (79%)]\tLoss: 0.015885\n",
            "Train Epoch: 5 [48000/60000 (80%)]\tLoss: 0.024274\n",
            "Train Epoch: 5 [48640/60000 (81%)]\tLoss: 0.083316\n",
            "Train Epoch: 5 [49280/60000 (82%)]\tLoss: 0.057929\n",
            "Train Epoch: 5 [49920/60000 (83%)]\tLoss: 0.007398\n",
            "Train Epoch: 5 [50560/60000 (84%)]\tLoss: 0.040918\n",
            "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 0.073262\n",
            "Train Epoch: 5 [51840/60000 (86%)]\tLoss: 0.137811\n",
            "Train Epoch: 5 [52480/60000 (87%)]\tLoss: 0.064175\n",
            "Train Epoch: 5 [53120/60000 (88%)]\tLoss: 0.111859\n",
            "Train Epoch: 5 [53760/60000 (90%)]\tLoss: 0.015231\n",
            "Train Epoch: 5 [54400/60000 (91%)]\tLoss: 0.032147\n",
            "Train Epoch: 5 [55040/60000 (92%)]\tLoss: 0.029257\n",
            "Train Epoch: 5 [55680/60000 (93%)]\tLoss: 0.069802\n",
            "Train Epoch: 5 [56320/60000 (94%)]\tLoss: 0.046398\n",
            "Train Epoch: 5 [56960/60000 (95%)]\tLoss: 0.027755\n",
            "Train Epoch: 5 [57600/60000 (96%)]\tLoss: 0.198999\n",
            "Train Epoch: 5 [58240/60000 (97%)]\tLoss: 0.189255\n",
            "Train Epoch: 5 [58880/60000 (98%)]\tLoss: 0.042614\n",
            "Train Epoch: 5 [59520/60000 (99%)]\tLoss: 0.050658\n",
            "\n",
            "Test set: Average loss: 0.0651, Accuracy: 9796/10000 (98%)\n",
            "\n",
            "Train Epoch: 6 [0/60000 (0%)]\tLoss: 0.042471\n",
            "Train Epoch: 6 [640/60000 (1%)]\tLoss: 0.098668\n",
            "Train Epoch: 6 [1280/60000 (2%)]\tLoss: 0.032321\n",
            "Train Epoch: 6 [1920/60000 (3%)]\tLoss: 0.060394\n",
            "Train Epoch: 6 [2560/60000 (4%)]\tLoss: 0.038476\n",
            "Train Epoch: 6 [3200/60000 (5%)]\tLoss: 0.016241\n",
            "Train Epoch: 6 [3840/60000 (6%)]\tLoss: 0.032927\n",
            "Train Epoch: 6 [4480/60000 (7%)]\tLoss: 0.014091\n",
            "Train Epoch: 6 [5120/60000 (9%)]\tLoss: 0.086501\n",
            "Train Epoch: 6 [5760/60000 (10%)]\tLoss: 0.080012\n",
            "Train Epoch: 6 [6400/60000 (11%)]\tLoss: 0.022216\n",
            "Train Epoch: 6 [7040/60000 (12%)]\tLoss: 0.273740\n",
            "Train Epoch: 6 [7680/60000 (13%)]\tLoss: 0.032101\n",
            "Train Epoch: 6 [8320/60000 (14%)]\tLoss: 0.081491\n",
            "Train Epoch: 6 [8960/60000 (15%)]\tLoss: 0.017051\n",
            "Train Epoch: 6 [9600/60000 (16%)]\tLoss: 0.027214\n",
            "Train Epoch: 6 [10240/60000 (17%)]\tLoss: 0.039932\n",
            "Train Epoch: 6 [10880/60000 (18%)]\tLoss: 0.055075\n",
            "Train Epoch: 6 [11520/60000 (19%)]\tLoss: 0.050578\n",
            "Train Epoch: 6 [12160/60000 (20%)]\tLoss: 0.112167\n",
            "Train Epoch: 6 [12800/60000 (21%)]\tLoss: 0.023674\n",
            "Train Epoch: 6 [13440/60000 (22%)]\tLoss: 0.088531\n",
            "Train Epoch: 6 [14080/60000 (23%)]\tLoss: 0.024051\n",
            "Train Epoch: 6 [14720/60000 (25%)]\tLoss: 0.092902\n",
            "Train Epoch: 6 [15360/60000 (26%)]\tLoss: 0.078109\n",
            "Train Epoch: 6 [16000/60000 (27%)]\tLoss: 0.148607\n",
            "Train Epoch: 6 [16640/60000 (28%)]\tLoss: 0.100220\n",
            "Train Epoch: 6 [17280/60000 (29%)]\tLoss: 0.061159\n",
            "Train Epoch: 6 [17920/60000 (30%)]\tLoss: 0.028563\n",
            "Train Epoch: 6 [18560/60000 (31%)]\tLoss: 0.092933\n",
            "Train Epoch: 6 [19200/60000 (32%)]\tLoss: 0.040408\n",
            "Train Epoch: 6 [19840/60000 (33%)]\tLoss: 0.018007\n",
            "Train Epoch: 6 [20480/60000 (34%)]\tLoss: 0.118520\n",
            "Train Epoch: 6 [21120/60000 (35%)]\tLoss: 0.051180\n",
            "Train Epoch: 6 [21760/60000 (36%)]\tLoss: 0.047383\n",
            "Train Epoch: 6 [22400/60000 (37%)]\tLoss: 0.017456\n",
            "Train Epoch: 6 [23040/60000 (38%)]\tLoss: 0.022237\n",
            "Train Epoch: 6 [23680/60000 (39%)]\tLoss: 0.112481\n",
            "Train Epoch: 6 [24320/60000 (41%)]\tLoss: 0.026667\n",
            "Train Epoch: 6 [24960/60000 (42%)]\tLoss: 0.041321\n",
            "Train Epoch: 6 [25600/60000 (43%)]\tLoss: 0.025281\n",
            "Train Epoch: 6 [26240/60000 (44%)]\tLoss: 0.040850\n",
            "Train Epoch: 6 [26880/60000 (45%)]\tLoss: 0.113968\n",
            "Train Epoch: 6 [27520/60000 (46%)]\tLoss: 0.068516\n",
            "Train Epoch: 6 [28160/60000 (47%)]\tLoss: 0.133371\n",
            "Train Epoch: 6 [28800/60000 (48%)]\tLoss: 0.022620\n",
            "Train Epoch: 6 [29440/60000 (49%)]\tLoss: 0.042235\n",
            "Train Epoch: 6 [30080/60000 (50%)]\tLoss: 0.112809\n",
            "Train Epoch: 6 [30720/60000 (51%)]\tLoss: 0.037453\n",
            "Train Epoch: 6 [31360/60000 (52%)]\tLoss: 0.222726\n",
            "Train Epoch: 6 [32000/60000 (53%)]\tLoss: 0.093125\n",
            "Train Epoch: 6 [32640/60000 (54%)]\tLoss: 0.197257\n",
            "Train Epoch: 6 [33280/60000 (55%)]\tLoss: 0.022521\n",
            "Train Epoch: 6 [33920/60000 (57%)]\tLoss: 0.066581\n",
            "Train Epoch: 6 [34560/60000 (58%)]\tLoss: 0.175012\n",
            "Train Epoch: 6 [35200/60000 (59%)]\tLoss: 0.135964\n",
            "Train Epoch: 6 [35840/60000 (60%)]\tLoss: 0.113020\n",
            "Train Epoch: 6 [36480/60000 (61%)]\tLoss: 0.034923\n",
            "Train Epoch: 6 [37120/60000 (62%)]\tLoss: 0.063117\n",
            "Train Epoch: 6 [37760/60000 (63%)]\tLoss: 0.105159\n",
            "Train Epoch: 6 [38400/60000 (64%)]\tLoss: 0.024090\n",
            "Train Epoch: 6 [39040/60000 (65%)]\tLoss: 0.083993\n",
            "Train Epoch: 6 [39680/60000 (66%)]\tLoss: 0.116810\n",
            "Train Epoch: 6 [40320/60000 (67%)]\tLoss: 0.100066\n",
            "Train Epoch: 6 [40960/60000 (68%)]\tLoss: 0.129956\n",
            "Train Epoch: 6 [41600/60000 (69%)]\tLoss: 0.292261\n",
            "Train Epoch: 6 [42240/60000 (70%)]\tLoss: 0.045525\n",
            "Train Epoch: 6 [42880/60000 (71%)]\tLoss: 0.019226\n",
            "Train Epoch: 6 [43520/60000 (72%)]\tLoss: 0.014528\n",
            "Train Epoch: 6 [44160/60000 (74%)]\tLoss: 0.015109\n",
            "Train Epoch: 6 [44800/60000 (75%)]\tLoss: 0.034160\n",
            "Train Epoch: 6 [45440/60000 (76%)]\tLoss: 0.035825\n",
            "Train Epoch: 6 [46080/60000 (77%)]\tLoss: 0.092461\n",
            "Train Epoch: 6 [46720/60000 (78%)]\tLoss: 0.096032\n",
            "Train Epoch: 6 [47360/60000 (79%)]\tLoss: 0.009393\n",
            "Train Epoch: 6 [48000/60000 (80%)]\tLoss: 0.054078\n",
            "Train Epoch: 6 [48640/60000 (81%)]\tLoss: 0.029475\n",
            "Train Epoch: 6 [49280/60000 (82%)]\tLoss: 0.067678\n",
            "Train Epoch: 6 [49920/60000 (83%)]\tLoss: 0.030224\n",
            "Train Epoch: 6 [50560/60000 (84%)]\tLoss: 0.009993\n",
            "Train Epoch: 6 [51200/60000 (85%)]\tLoss: 0.058474\n",
            "Train Epoch: 6 [51840/60000 (86%)]\tLoss: 0.036630\n",
            "Train Epoch: 6 [52480/60000 (87%)]\tLoss: 0.132441\n",
            "Train Epoch: 6 [53120/60000 (88%)]\tLoss: 0.116155\n",
            "Train Epoch: 6 [53760/60000 (90%)]\tLoss: 0.043001\n",
            "Train Epoch: 6 [54400/60000 (91%)]\tLoss: 0.288444\n",
            "Train Epoch: 6 [55040/60000 (92%)]\tLoss: 0.076333\n",
            "Train Epoch: 6 [55680/60000 (93%)]\tLoss: 0.040280\n",
            "Train Epoch: 6 [56320/60000 (94%)]\tLoss: 0.040181\n",
            "Train Epoch: 6 [56960/60000 (95%)]\tLoss: 0.070288\n",
            "Train Epoch: 6 [57600/60000 (96%)]\tLoss: 0.115086\n",
            "Train Epoch: 6 [58240/60000 (97%)]\tLoss: 0.269692\n",
            "Train Epoch: 6 [58880/60000 (98%)]\tLoss: 0.009461\n",
            "Train Epoch: 6 [59520/60000 (99%)]\tLoss: 0.091115\n",
            "\n",
            "Test set: Average loss: 0.0632, Accuracy: 9798/10000 (98%)\n",
            "\n",
            "Train Epoch: 7 [0/60000 (0%)]\tLoss: 0.047764\n",
            "Train Epoch: 7 [640/60000 (1%)]\tLoss: 0.060102\n",
            "Train Epoch: 7 [1280/60000 (2%)]\tLoss: 0.067484\n",
            "Train Epoch: 7 [1920/60000 (3%)]\tLoss: 0.057946\n",
            "Train Epoch: 7 [2560/60000 (4%)]\tLoss: 0.026861\n",
            "Train Epoch: 7 [3200/60000 (5%)]\tLoss: 0.037234\n",
            "Train Epoch: 7 [3840/60000 (6%)]\tLoss: 0.059976\n",
            "Train Epoch: 7 [4480/60000 (7%)]\tLoss: 0.014505\n",
            "Train Epoch: 7 [5120/60000 (9%)]\tLoss: 0.075181\n",
            "Train Epoch: 7 [5760/60000 (10%)]\tLoss: 0.190521\n",
            "Train Epoch: 7 [6400/60000 (11%)]\tLoss: 0.112156\n",
            "Train Epoch: 7 [7040/60000 (12%)]\tLoss: 0.084477\n",
            "Train Epoch: 7 [7680/60000 (13%)]\tLoss: 0.043780\n",
            "Train Epoch: 7 [8320/60000 (14%)]\tLoss: 0.039471\n",
            "Train Epoch: 7 [8960/60000 (15%)]\tLoss: 0.027936\n",
            "Train Epoch: 7 [9600/60000 (16%)]\tLoss: 0.066389\n",
            "Train Epoch: 7 [10240/60000 (17%)]\tLoss: 0.082795\n",
            "Train Epoch: 7 [10880/60000 (18%)]\tLoss: 0.030713\n",
            "Train Epoch: 7 [11520/60000 (19%)]\tLoss: 0.039820\n",
            "Train Epoch: 7 [12160/60000 (20%)]\tLoss: 0.086264\n",
            "Train Epoch: 7 [12800/60000 (21%)]\tLoss: 0.115567\n",
            "Train Epoch: 7 [13440/60000 (22%)]\tLoss: 0.021740\n",
            "Train Epoch: 7 [14080/60000 (23%)]\tLoss: 0.015193\n",
            "Train Epoch: 7 [14720/60000 (25%)]\tLoss: 0.022530\n",
            "Train Epoch: 7 [15360/60000 (26%)]\tLoss: 0.073231\n",
            "Train Epoch: 7 [16000/60000 (27%)]\tLoss: 0.028805\n",
            "Train Epoch: 7 [16640/60000 (28%)]\tLoss: 0.054346\n",
            "Train Epoch: 7 [17280/60000 (29%)]\tLoss: 0.033124\n",
            "Train Epoch: 7 [17920/60000 (30%)]\tLoss: 0.036131\n",
            "Train Epoch: 7 [18560/60000 (31%)]\tLoss: 0.070942\n",
            "Train Epoch: 7 [19200/60000 (32%)]\tLoss: 0.080474\n",
            "Train Epoch: 7 [19840/60000 (33%)]\tLoss: 0.053109\n",
            "Train Epoch: 7 [20480/60000 (34%)]\tLoss: 0.112317\n",
            "Train Epoch: 7 [21120/60000 (35%)]\tLoss: 0.085756\n",
            "Train Epoch: 7 [21760/60000 (36%)]\tLoss: 0.024981\n",
            "Train Epoch: 7 [22400/60000 (37%)]\tLoss: 0.046077\n",
            "Train Epoch: 7 [23040/60000 (38%)]\tLoss: 0.048448\n",
            "Train Epoch: 7 [23680/60000 (39%)]\tLoss: 0.094705\n",
            "Train Epoch: 7 [24320/60000 (41%)]\tLoss: 0.058377\n",
            "Train Epoch: 7 [24960/60000 (42%)]\tLoss: 0.102865\n",
            "Train Epoch: 7 [25600/60000 (43%)]\tLoss: 0.038411\n",
            "Train Epoch: 7 [26240/60000 (44%)]\tLoss: 0.055892\n",
            "Train Epoch: 7 [26880/60000 (45%)]\tLoss: 0.110627\n",
            "Train Epoch: 7 [27520/60000 (46%)]\tLoss: 0.191828\n",
            "Train Epoch: 7 [28160/60000 (47%)]\tLoss: 0.154462\n",
            "Train Epoch: 7 [28800/60000 (48%)]\tLoss: 0.035550\n",
            "Train Epoch: 7 [29440/60000 (49%)]\tLoss: 0.008954\n",
            "Train Epoch: 7 [30080/60000 (50%)]\tLoss: 0.067663\n",
            "Train Epoch: 7 [30720/60000 (51%)]\tLoss: 0.014070\n",
            "Train Epoch: 7 [31360/60000 (52%)]\tLoss: 0.048648\n",
            "Train Epoch: 7 [32000/60000 (53%)]\tLoss: 0.024598\n",
            "Train Epoch: 7 [32640/60000 (54%)]\tLoss: 0.073455\n",
            "Train Epoch: 7 [33280/60000 (55%)]\tLoss: 0.035231\n",
            "Train Epoch: 7 [33920/60000 (57%)]\tLoss: 0.063552\n",
            "Train Epoch: 7 [34560/60000 (58%)]\tLoss: 0.041463\n",
            "Train Epoch: 7 [35200/60000 (59%)]\tLoss: 0.012546\n",
            "Train Epoch: 7 [35840/60000 (60%)]\tLoss: 0.024106\n",
            "Train Epoch: 7 [36480/60000 (61%)]\tLoss: 0.013428\n",
            "Train Epoch: 7 [37120/60000 (62%)]\tLoss: 0.103298\n",
            "Train Epoch: 7 [37760/60000 (63%)]\tLoss: 0.058462\n",
            "Train Epoch: 7 [38400/60000 (64%)]\tLoss: 0.143410\n",
            "Train Epoch: 7 [39040/60000 (65%)]\tLoss: 0.008315\n",
            "Train Epoch: 7 [39680/60000 (66%)]\tLoss: 0.040618\n",
            "Train Epoch: 7 [40320/60000 (67%)]\tLoss: 0.010531\n",
            "Train Epoch: 7 [40960/60000 (68%)]\tLoss: 0.056805\n",
            "Train Epoch: 7 [41600/60000 (69%)]\tLoss: 0.124885\n",
            "Train Epoch: 7 [42240/60000 (70%)]\tLoss: 0.048703\n",
            "Train Epoch: 7 [42880/60000 (71%)]\tLoss: 0.034030\n",
            "Train Epoch: 7 [43520/60000 (72%)]\tLoss: 0.034960\n",
            "Train Epoch: 7 [44160/60000 (74%)]\tLoss: 0.078179\n",
            "Train Epoch: 7 [44800/60000 (75%)]\tLoss: 0.058196\n",
            "Train Epoch: 7 [45440/60000 (76%)]\tLoss: 0.043603\n",
            "Train Epoch: 7 [46080/60000 (77%)]\tLoss: 0.062340\n",
            "Train Epoch: 7 [46720/60000 (78%)]\tLoss: 0.033960\n",
            "Train Epoch: 7 [47360/60000 (79%)]\tLoss: 0.101149\n",
            "Train Epoch: 7 [48000/60000 (80%)]\tLoss: 0.029706\n",
            "Train Epoch: 7 [48640/60000 (81%)]\tLoss: 0.052719\n",
            "Train Epoch: 7 [49280/60000 (82%)]\tLoss: 0.024070\n",
            "Train Epoch: 7 [49920/60000 (83%)]\tLoss: 0.044765\n",
            "Train Epoch: 7 [50560/60000 (84%)]\tLoss: 0.101158\n",
            "Train Epoch: 7 [51200/60000 (85%)]\tLoss: 0.026525\n",
            "Train Epoch: 7 [51840/60000 (86%)]\tLoss: 0.013954\n",
            "Train Epoch: 7 [52480/60000 (87%)]\tLoss: 0.065753\n",
            "Train Epoch: 7 [53120/60000 (88%)]\tLoss: 0.109335\n",
            "Train Epoch: 7 [53760/60000 (90%)]\tLoss: 0.081642\n",
            "Train Epoch: 7 [54400/60000 (91%)]\tLoss: 0.070914\n",
            "Train Epoch: 7 [55040/60000 (92%)]\tLoss: 0.097769\n",
            "Train Epoch: 7 [55680/60000 (93%)]\tLoss: 0.120957\n",
            "Train Epoch: 7 [56320/60000 (94%)]\tLoss: 0.021854\n",
            "Train Epoch: 7 [56960/60000 (95%)]\tLoss: 0.029164\n",
            "Train Epoch: 7 [57600/60000 (96%)]\tLoss: 0.095628\n",
            "Train Epoch: 7 [58240/60000 (97%)]\tLoss: 0.035528\n",
            "Train Epoch: 7 [58880/60000 (98%)]\tLoss: 0.042767\n",
            "Train Epoch: 7 [59520/60000 (99%)]\tLoss: 0.017950\n",
            "\n",
            "Test set: Average loss: 0.0566, Accuracy: 9822/10000 (98%)\n",
            "\n",
            "Train Epoch: 8 [0/60000 (0%)]\tLoss: 0.054169\n",
            "Train Epoch: 8 [640/60000 (1%)]\tLoss: 0.033415\n",
            "Train Epoch: 8 [1280/60000 (2%)]\tLoss: 0.070563\n",
            "Train Epoch: 8 [1920/60000 (3%)]\tLoss: 0.054414\n",
            "Train Epoch: 8 [2560/60000 (4%)]\tLoss: 0.180855\n",
            "Train Epoch: 8 [3200/60000 (5%)]\tLoss: 0.042587\n",
            "Train Epoch: 8 [3840/60000 (6%)]\tLoss: 0.006810\n",
            "Train Epoch: 8 [4480/60000 (7%)]\tLoss: 0.098430\n",
            "Train Epoch: 8 [5120/60000 (9%)]\tLoss: 0.031624\n",
            "Train Epoch: 8 [5760/60000 (10%)]\tLoss: 0.034145\n",
            "Train Epoch: 8 [6400/60000 (11%)]\tLoss: 0.037381\n",
            "Train Epoch: 8 [7040/60000 (12%)]\tLoss: 0.034627\n",
            "Train Epoch: 8 [7680/60000 (13%)]\tLoss: 0.054003\n",
            "Train Epoch: 8 [8320/60000 (14%)]\tLoss: 0.091935\n",
            "Train Epoch: 8 [8960/60000 (15%)]\tLoss: 0.073672\n",
            "Train Epoch: 8 [9600/60000 (16%)]\tLoss: 0.025720\n",
            "Train Epoch: 8 [10240/60000 (17%)]\tLoss: 0.094540\n",
            "Train Epoch: 8 [10880/60000 (18%)]\tLoss: 0.185793\n",
            "Train Epoch: 8 [11520/60000 (19%)]\tLoss: 0.021757\n",
            "Train Epoch: 8 [12160/60000 (20%)]\tLoss: 0.083799\n",
            "Train Epoch: 8 [12800/60000 (21%)]\tLoss: 0.014801\n",
            "Train Epoch: 8 [13440/60000 (22%)]\tLoss: 0.024962\n",
            "Train Epoch: 8 [14080/60000 (23%)]\tLoss: 0.024963\n",
            "Train Epoch: 8 [14720/60000 (25%)]\tLoss: 0.033077\n",
            "Train Epoch: 8 [15360/60000 (26%)]\tLoss: 0.018838\n",
            "Train Epoch: 8 [16000/60000 (27%)]\tLoss: 0.089948\n",
            "Train Epoch: 8 [16640/60000 (28%)]\tLoss: 0.124512\n",
            "Train Epoch: 8 [17280/60000 (29%)]\tLoss: 0.011722\n",
            "Train Epoch: 8 [17920/60000 (30%)]\tLoss: 0.042812\n",
            "Train Epoch: 8 [18560/60000 (31%)]\tLoss: 0.157020\n",
            "Train Epoch: 8 [19200/60000 (32%)]\tLoss: 0.073639\n",
            "Train Epoch: 8 [19840/60000 (33%)]\tLoss: 0.017353\n",
            "Train Epoch: 8 [20480/60000 (34%)]\tLoss: 0.018937\n",
            "Train Epoch: 8 [21120/60000 (35%)]\tLoss: 0.075740\n",
            "Train Epoch: 8 [21760/60000 (36%)]\tLoss: 0.060934\n",
            "Train Epoch: 8 [22400/60000 (37%)]\tLoss: 0.074384\n",
            "Train Epoch: 8 [23040/60000 (38%)]\tLoss: 0.109669\n",
            "Train Epoch: 8 [23680/60000 (39%)]\tLoss: 0.065160\n",
            "Train Epoch: 8 [24320/60000 (41%)]\tLoss: 0.130178\n",
            "Train Epoch: 8 [24960/60000 (42%)]\tLoss: 0.121435\n",
            "Train Epoch: 8 [25600/60000 (43%)]\tLoss: 0.045143\n",
            "Train Epoch: 8 [26240/60000 (44%)]\tLoss: 0.016144\n",
            "Train Epoch: 8 [26880/60000 (45%)]\tLoss: 0.030561\n",
            "Train Epoch: 8 [27520/60000 (46%)]\tLoss: 0.166854\n",
            "Train Epoch: 8 [28160/60000 (47%)]\tLoss: 0.022786\n",
            "Train Epoch: 8 [28800/60000 (48%)]\tLoss: 0.099005\n",
            "Train Epoch: 8 [29440/60000 (49%)]\tLoss: 0.055255\n",
            "Train Epoch: 8 [30080/60000 (50%)]\tLoss: 0.032918\n",
            "Train Epoch: 8 [30720/60000 (51%)]\tLoss: 0.144337\n",
            "Train Epoch: 8 [31360/60000 (52%)]\tLoss: 0.016118\n",
            "Train Epoch: 8 [32000/60000 (53%)]\tLoss: 0.027028\n",
            "Train Epoch: 8 [32640/60000 (54%)]\tLoss: 0.045851\n",
            "Train Epoch: 8 [33280/60000 (55%)]\tLoss: 0.048905\n",
            "Train Epoch: 8 [33920/60000 (57%)]\tLoss: 0.066046\n",
            "Train Epoch: 8 [34560/60000 (58%)]\tLoss: 0.059808\n",
            "Train Epoch: 8 [35200/60000 (59%)]\tLoss: 0.110321\n",
            "Train Epoch: 8 [35840/60000 (60%)]\tLoss: 0.082171\n",
            "Train Epoch: 8 [36480/60000 (61%)]\tLoss: 0.042900\n",
            "Train Epoch: 8 [37120/60000 (62%)]\tLoss: 0.022048\n",
            "Train Epoch: 8 [37760/60000 (63%)]\tLoss: 0.019251\n",
            "Train Epoch: 8 [38400/60000 (64%)]\tLoss: 0.055170\n",
            "Train Epoch: 8 [39040/60000 (65%)]\tLoss: 0.020844\n",
            "Train Epoch: 8 [39680/60000 (66%)]\tLoss: 0.035255\n",
            "Train Epoch: 8 [40320/60000 (67%)]\tLoss: 0.081918\n",
            "Train Epoch: 8 [40960/60000 (68%)]\tLoss: 0.089255\n",
            "Train Epoch: 8 [41600/60000 (69%)]\tLoss: 0.144127\n",
            "Train Epoch: 8 [42240/60000 (70%)]\tLoss: 0.045325\n",
            "Train Epoch: 8 [42880/60000 (71%)]\tLoss: 0.032203\n",
            "Train Epoch: 8 [43520/60000 (72%)]\tLoss: 0.116933\n",
            "Train Epoch: 8 [44160/60000 (74%)]\tLoss: 0.092849\n",
            "Train Epoch: 8 [44800/60000 (75%)]\tLoss: 0.160111\n",
            "Train Epoch: 8 [45440/60000 (76%)]\tLoss: 0.027483\n",
            "Train Epoch: 8 [46080/60000 (77%)]\tLoss: 0.069903\n",
            "Train Epoch: 8 [46720/60000 (78%)]\tLoss: 0.057290\n",
            "Train Epoch: 8 [47360/60000 (79%)]\tLoss: 0.121278\n",
            "Train Epoch: 8 [48000/60000 (80%)]\tLoss: 0.013645\n",
            "Train Epoch: 8 [48640/60000 (81%)]\tLoss: 0.112152\n",
            "Train Epoch: 8 [49280/60000 (82%)]\tLoss: 0.167304\n",
            "Train Epoch: 8 [49920/60000 (83%)]\tLoss: 0.035434\n",
            "Train Epoch: 8 [50560/60000 (84%)]\tLoss: 0.077201\n",
            "Train Epoch: 8 [51200/60000 (85%)]\tLoss: 0.059686\n",
            "Train Epoch: 8 [51840/60000 (86%)]\tLoss: 0.065387\n",
            "Train Epoch: 8 [52480/60000 (87%)]\tLoss: 0.054985\n",
            "Train Epoch: 8 [53120/60000 (88%)]\tLoss: 0.029944\n",
            "Train Epoch: 8 [53760/60000 (90%)]\tLoss: 0.171540\n",
            "Train Epoch: 8 [54400/60000 (91%)]\tLoss: 0.022899\n",
            "Train Epoch: 8 [55040/60000 (92%)]\tLoss: 0.037241\n",
            "Train Epoch: 8 [55680/60000 (93%)]\tLoss: 0.066757\n",
            "Train Epoch: 8 [56320/60000 (94%)]\tLoss: 0.013722\n",
            "Train Epoch: 8 [56960/60000 (95%)]\tLoss: 0.086803\n",
            "Train Epoch: 8 [57600/60000 (96%)]\tLoss: 0.125336\n",
            "Train Epoch: 8 [58240/60000 (97%)]\tLoss: 0.013315\n",
            "Train Epoch: 8 [58880/60000 (98%)]\tLoss: 0.089002\n",
            "Train Epoch: 8 [59520/60000 (99%)]\tLoss: 0.104848\n",
            "\n",
            "Test set: Average loss: 0.0501, Accuracy: 9830/10000 (98%)\n",
            "\n",
            "Train Epoch: 9 [0/60000 (0%)]\tLoss: 0.039336\n",
            "Train Epoch: 9 [640/60000 (1%)]\tLoss: 0.017452\n",
            "Train Epoch: 9 [1280/60000 (2%)]\tLoss: 0.030770\n",
            "Train Epoch: 9 [1920/60000 (3%)]\tLoss: 0.040520\n",
            "Train Epoch: 9 [2560/60000 (4%)]\tLoss: 0.085023\n",
            "Train Epoch: 9 [3200/60000 (5%)]\tLoss: 0.008787\n",
            "Train Epoch: 9 [3840/60000 (6%)]\tLoss: 0.016978\n",
            "Train Epoch: 9 [4480/60000 (7%)]\tLoss: 0.016317\n",
            "Train Epoch: 9 [5120/60000 (9%)]\tLoss: 0.163067\n",
            "Train Epoch: 9 [5760/60000 (10%)]\tLoss: 0.033200\n",
            "Train Epoch: 9 [6400/60000 (11%)]\tLoss: 0.040266\n",
            "Train Epoch: 9 [7040/60000 (12%)]\tLoss: 0.015635\n",
            "Train Epoch: 9 [7680/60000 (13%)]\tLoss: 0.094692\n",
            "Train Epoch: 9 [8320/60000 (14%)]\tLoss: 0.041800\n",
            "Train Epoch: 9 [8960/60000 (15%)]\tLoss: 0.055655\n",
            "Train Epoch: 9 [9600/60000 (16%)]\tLoss: 0.044281\n",
            "Train Epoch: 9 [10240/60000 (17%)]\tLoss: 0.104607\n",
            "Train Epoch: 9 [10880/60000 (18%)]\tLoss: 0.028472\n",
            "Train Epoch: 9 [11520/60000 (19%)]\tLoss: 0.075396\n",
            "Train Epoch: 9 [12160/60000 (20%)]\tLoss: 0.019240\n",
            "Train Epoch: 9 [12800/60000 (21%)]\tLoss: 0.029778\n",
            "Train Epoch: 9 [13440/60000 (22%)]\tLoss: 0.025602\n",
            "Train Epoch: 9 [14080/60000 (23%)]\tLoss: 0.028412\n",
            "Train Epoch: 9 [14720/60000 (25%)]\tLoss: 0.017637\n",
            "Train Epoch: 9 [15360/60000 (26%)]\tLoss: 0.031281\n",
            "Train Epoch: 9 [16000/60000 (27%)]\tLoss: 0.084481\n",
            "Train Epoch: 9 [16640/60000 (28%)]\tLoss: 0.007890\n",
            "Train Epoch: 9 [17280/60000 (29%)]\tLoss: 0.037575\n",
            "Train Epoch: 9 [17920/60000 (30%)]\tLoss: 0.021466\n",
            "Train Epoch: 9 [18560/60000 (31%)]\tLoss: 0.015641\n",
            "Train Epoch: 9 [19200/60000 (32%)]\tLoss: 0.046711\n",
            "Train Epoch: 9 [19840/60000 (33%)]\tLoss: 0.094873\n",
            "Train Epoch: 9 [20480/60000 (34%)]\tLoss: 0.076975\n",
            "Train Epoch: 9 [21120/60000 (35%)]\tLoss: 0.030273\n",
            "Train Epoch: 9 [21760/60000 (36%)]\tLoss: 0.015134\n",
            "Train Epoch: 9 [22400/60000 (37%)]\tLoss: 0.019964\n",
            "Train Epoch: 9 [23040/60000 (38%)]\tLoss: 0.074737\n",
            "Train Epoch: 9 [23680/60000 (39%)]\tLoss: 0.050900\n",
            "Train Epoch: 9 [24320/60000 (41%)]\tLoss: 0.046657\n",
            "Train Epoch: 9 [24960/60000 (42%)]\tLoss: 0.028200\n",
            "Train Epoch: 9 [25600/60000 (43%)]\tLoss: 0.102996\n",
            "Train Epoch: 9 [26240/60000 (44%)]\tLoss: 0.132638\n",
            "Train Epoch: 9 [26880/60000 (45%)]\tLoss: 0.058367\n",
            "Train Epoch: 9 [27520/60000 (46%)]\tLoss: 0.007400\n",
            "Train Epoch: 9 [28160/60000 (47%)]\tLoss: 0.063573\n",
            "Train Epoch: 9 [28800/60000 (48%)]\tLoss: 0.046079\n",
            "Train Epoch: 9 [29440/60000 (49%)]\tLoss: 0.036826\n",
            "Train Epoch: 9 [30080/60000 (50%)]\tLoss: 0.058948\n",
            "Train Epoch: 9 [30720/60000 (51%)]\tLoss: 0.069850\n",
            "Train Epoch: 9 [31360/60000 (52%)]\tLoss: 0.048964\n",
            "Train Epoch: 9 [32000/60000 (53%)]\tLoss: 0.010035\n",
            "Train Epoch: 9 [32640/60000 (54%)]\tLoss: 0.207901\n",
            "Train Epoch: 9 [33280/60000 (55%)]\tLoss: 0.074425\n",
            "Train Epoch: 9 [33920/60000 (57%)]\tLoss: 0.126084\n",
            "Train Epoch: 9 [34560/60000 (58%)]\tLoss: 0.055252\n",
            "Train Epoch: 9 [35200/60000 (59%)]\tLoss: 0.013235\n",
            "Train Epoch: 9 [35840/60000 (60%)]\tLoss: 0.018350\n",
            "Train Epoch: 9 [36480/60000 (61%)]\tLoss: 0.113075\n",
            "Train Epoch: 9 [37120/60000 (62%)]\tLoss: 0.081475\n",
            "Train Epoch: 9 [37760/60000 (63%)]\tLoss: 0.054294\n",
            "Train Epoch: 9 [38400/60000 (64%)]\tLoss: 0.110283\n",
            "Train Epoch: 9 [39040/60000 (65%)]\tLoss: 0.014928\n",
            "Train Epoch: 9 [39680/60000 (66%)]\tLoss: 0.105036\n",
            "Train Epoch: 9 [40320/60000 (67%)]\tLoss: 0.014472\n",
            "Train Epoch: 9 [40960/60000 (68%)]\tLoss: 0.216056\n",
            "Train Epoch: 9 [41600/60000 (69%)]\tLoss: 0.009564\n",
            "Train Epoch: 9 [42240/60000 (70%)]\tLoss: 0.071708\n",
            "Train Epoch: 9 [42880/60000 (71%)]\tLoss: 0.055342\n",
            "Train Epoch: 9 [43520/60000 (72%)]\tLoss: 0.027623\n",
            "Train Epoch: 9 [44160/60000 (74%)]\tLoss: 0.112558\n",
            "Train Epoch: 9 [44800/60000 (75%)]\tLoss: 0.038202\n",
            "Train Epoch: 9 [45440/60000 (76%)]\tLoss: 0.013059\n",
            "Train Epoch: 9 [46080/60000 (77%)]\tLoss: 0.065771\n",
            "Train Epoch: 9 [46720/60000 (78%)]\tLoss: 0.011325\n",
            "Train Epoch: 9 [47360/60000 (79%)]\tLoss: 0.042295\n",
            "Train Epoch: 9 [48000/60000 (80%)]\tLoss: 0.012334\n",
            "Train Epoch: 9 [48640/60000 (81%)]\tLoss: 0.080964\n",
            "Train Epoch: 9 [49280/60000 (82%)]\tLoss: 0.064506\n",
            "Train Epoch: 9 [49920/60000 (83%)]\tLoss: 0.057896\n",
            "Train Epoch: 9 [50560/60000 (84%)]\tLoss: 0.104973\n",
            "Train Epoch: 9 [51200/60000 (85%)]\tLoss: 0.061634\n",
            "Train Epoch: 9 [51840/60000 (86%)]\tLoss: 0.009187\n",
            "Train Epoch: 9 [52480/60000 (87%)]\tLoss: 0.164268\n",
            "Train Epoch: 9 [53120/60000 (88%)]\tLoss: 0.097050\n",
            "Train Epoch: 9 [53760/60000 (90%)]\tLoss: 0.060276\n",
            "Train Epoch: 9 [54400/60000 (91%)]\tLoss: 0.044544\n",
            "Train Epoch: 9 [55040/60000 (92%)]\tLoss: 0.017982\n",
            "Train Epoch: 9 [55680/60000 (93%)]\tLoss: 0.036109\n",
            "Train Epoch: 9 [56320/60000 (94%)]\tLoss: 0.061766\n",
            "Train Epoch: 9 [56960/60000 (95%)]\tLoss: 0.017922\n",
            "Train Epoch: 9 [57600/60000 (96%)]\tLoss: 0.091588\n",
            "Train Epoch: 9 [58240/60000 (97%)]\tLoss: 0.051261\n",
            "Train Epoch: 9 [58880/60000 (98%)]\tLoss: 0.020022\n",
            "Train Epoch: 9 [59520/60000 (99%)]\tLoss: 0.045657\n",
            "\n",
            "Test set: Average loss: 0.0485, Accuracy: 9838/10000 (98%)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MstdPhXOYtIg"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}