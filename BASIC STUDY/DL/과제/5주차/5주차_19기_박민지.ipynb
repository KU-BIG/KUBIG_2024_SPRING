{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "nmQ5F7UAeKB_"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task1\n",
        "\n",
        "빈 칸을 채워주세요!\n",
        "\n",
        "단계별 output이 github 파일에는 남아있으니 그 output과 동일한 형태인지 확인하면서 진행해주시면 됩니다~"
      ],
      "metadata": {
        "id": "Sgxd6SxmeVcP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#1. 생성할 문장 데이터\n",
        "\n",
        "sentence = (\"Brick walls are there for a reason and you must not think \"\n",
        "            \"that the brick walls aren't there to keep us out, but rather \"\n",
        "            \"in this way that the brick walls are there to show us how badly we want things.\")"
      ],
      "metadata": {
        "id": "NDvUeC8BoUb6"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#2. 문자 집합 만들기\n",
        "world_set = list(set(sentence))\n",
        "\n",
        "## 문제(1): 각 문자에 정수 인코딩 (공백도 하나의 원소로 포함)\n",
        "vocab = {char: i for i, char in enumerate((world_set))} #인덱스와 해당 인덱스의 값(문자)를 가져와서 dict형태로 설정"
      ],
      "metadata": {
        "id": "b9lkrKyZf8ie"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(vocab)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F_0we5Y-gYDq",
        "outputId": "3e76f043-ae22-4cf6-ffb6-205eedce5ba7"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'w': 0, \"'\": 1, 'l': 2, 'y': 3, 'c': 4, 's': 5, 'a': 6, 'h': 7, 'B': 8, 'f': 9, 'p': 10, ',': 11, 'e': 12, 'i': 13, 'u': 14, 'g': 15, 'r': 16, '.': 17, 'n': 18, 'o': 19, ' ': 20, 'm': 21, 'b': 22, 't': 23, 'd': 24, 'k': 25}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#3. 문자 집합 크기 확인\n",
        "\n",
        "vocab_size = len(vocab)\n",
        "print('문자 집합 크기 : {}'.format(vocab_size))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TpKupU6lgpfT",
        "outputId": "ce1d6586-5e3a-40b9-c031-1463dd83266a"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "문자 집합 크기 : 26\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#4. 하이퍼 파라미터 설정(자유롭게 수정해보세요!)\n",
        "\n",
        "hidden_size = vocab_size # 같아야 하는 것 확인!\n",
        "sequence_length = 15  # 너무 길거나 너무 짧게 잡으면 안됩니다!\n",
        "learning_rate = 0.01"
      ],
      "metadata": {
        "id": "wFDZJHSMg9In"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# hidden size  : hidden layer의 각 노드가 처리할 수 있는 정보의 차원 수\n",
        "\n",
        "- hidden_size가 크면 **더 많은 정보**를 내부 상태에 저장할 수 있어서 더 복잡한 패턴을 학습할 수 있지만, **과적합 발생** 가능성 있음.\n",
        "- hidden_size가 작으면 모델의 **계산 효율성**은 높아지지만, 너무 적은 정보만을 저장할 수 있어서 **학습 능력이 제한**될 수 있음.\n",
        "\n",
        "\n",
        "--> 어휘 크기가 상대적으로 작기 때문에 vocab_size와 같게 설정해서 모델이 충분한 정보를 저장할 수 있도록 함.\n",
        "\n"
      ],
      "metadata": {
        "id": "-U4tJ2IKI10y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#5. seqence 길이 단위 자르기\n",
        "\n",
        "# 데이터 구성을 위한 리스트\n",
        "x_data = []\n",
        "y_data = []"
      ],
      "metadata": {
        "id": "RbDcmJmghN7V"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## 문제(2): 반복문 내에서의 인덱싱을 사용하여 sequence_length 값 단위로 샘플을 잘라 데이터 만들기, y_str은 x_str은 한 칸씩 쉬프트된 sequnce\n",
        "\n",
        "for i in range(0, len(sentence) - sequence_length):\n",
        "  x_str = sentence[i:i+sequence_length]\n",
        "  y_str = sentence[i+1:i+sequence_length+1]\n",
        "\n",
        "  # x_str과 y_str이 문자집합에 해당하는 인덱스를 각각 x_data, y_data에 append\n",
        "  x_data.append([vocab[c] for c in x_str])\n",
        "  y_data.append([vocab[d] for d in y_str])"
      ],
      "metadata": {
        "id": "wxIaEgasYJ-k"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 출력해서 한 칸씩 쉬프트된 것 확인하기!\n",
        "\n",
        "print(x_data[0])\n",
        "print(y_data[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XVFlILiOixdc",
        "outputId": "0c7e5a4a-a6a8-47f1-9106-3d798cd32f2b"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[8, 16, 13, 4, 25, 20, 0, 6, 2, 2, 5, 20, 6, 16, 12]\n",
            "[16, 13, 4, 25, 20, 0, 6, 2, 2, 5, 20, 6, 16, 12, 20]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##6. 입력 시퀀스에 대해 원핫인코딩 수행\n",
        "\n",
        "## 문제(4) : x_data를 원핫인코딩 > numpy의 eye를 쓸 수 있지 않을까?\n",
        "N = vocab_size\n",
        "x_one_hot= [np.eye(N)[x] for x in x_data]\n",
        "\n",
        "##7. 입력 데이터, 레이블데이터 텐서로 변환\n",
        "\n",
        "## 문제(5) : x_one_hot과 y_data 텐서로 변환 : 둘 다 같은 형식의 텐서로 변환하면 될까?? (FloatTensor, LongTesor 중 맞는 것은?)\n",
        "X = torch.FloatTensor(x_one_hot)\n",
        "Y = torch.LongTensor(y_data)"
      ],
      "metadata": {
        "id": "5lPes1dvjlNb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cd0f7144-67d3-4ba5-f76c-0adfdd80d3c3"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-10-15539bf880af>:10: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:261.)\n",
            "  X = torch.FloatTensor(x_one_hot)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##8. 크기 확인\n",
        "print('훈련 데이터의 크기 : {}'.format(X.shape))\n",
        "print('레이블의 크기 : {}'.format(Y.shape))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lMZzZlaymMk8",
        "outputId": "829efe9c-0a4f-4b0f-b2f0-a1fb4adc75da"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "훈련 데이터의 크기 : torch.Size([183, 15, 26])\n",
            "레이블의 크기 : torch.Size([183, 15])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##9.원핫인코딩 결과 샘플 확인하기\n",
        "print(X[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "knx1DE_AmSFB",
        "outputId": "f4566f92-c709-426d-d287-95d16ee6e8b9"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 1.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 1., 0., 0., 0., 0., 0.],\n",
            "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 1., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0.]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##10. 레이블 데이터 샘플 확인하기\n",
        "print(Y[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6pWDiH1SmYT_",
        "outputId": "7a78496b-3277-4b04-d06c-8611b72a0c98"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([16, 13,  4, 25, 20,  0,  6,  2,  2,  5, 20,  6, 16, 12, 20])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##11. RNN 모델 구현\n",
        "\n",
        "##문제(6) : 기본 pytorch 인자 넣기 연습 + forward 채우기\n",
        "### 조건 : rnn layer 2개 쌓기 + 마지막은 fc layer\n",
        "### batch_fisrt 설정 필요할까? (유튜브 강의 참고)\n",
        "\n",
        "class Net(torch.nn.Module):\n",
        "  def __init__(self, input_dim, hidden_dim, layers):\n",
        "    super(Net, self).__init__()\n",
        "    self.rnn = torch.nn.RNN(input_size=input_dim,\n",
        "                            hidden_size=hidden_dim,\n",
        "                            num_layers=layers,\n",
        "                            batch_first=True)\n",
        "    self.fc = torch.nn.Linear(hidden_dim, vocab_size)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x, hidden  = self.rnn(x)\n",
        "    x = self.fc(x)\n",
        "    return x"
      ],
      "metadata": {
        "id": "-Ww22xu8mfUc"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "net = Net(vocab_size, hidden_size, 2)"
      ],
      "metadata": {
        "id": "No2GRvTpnLBl"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##12. loss function\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "##13. optimizer\n",
        "optimizer = optim.Adam(net.parameters(), learning_rate)"
      ],
      "metadata": {
        "id": "9-zuJLeUnQLB"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##14. 출력 크기 점검\n",
        "outputs = net(X)\n",
        "print(outputs.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z-RxRaiHnh9U",
        "outputId": "47eb646a-3289-4ba4-9b72-19917218fd61"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([183, 15, 26])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##15. Training 시작\n",
        "\n",
        "for i in range(100):\n",
        "    optimizer.zero_grad()\n",
        "    outputs = net(X)\n",
        "    ##문제(7) : outputs, Y 형태 그대로 넣으면 안되죠. view 함수를 이용해 loss값을 계산해봅시다.\n",
        "    loss = criterion(outputs.view(-1, vocab_size), Y.view(-1))\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "#16. 예측결과 확인\n",
        "results = outputs.argmax(dim=2) # 텐서의 마지막 차원(클래스 차원)을 따라 최댓값의 인덱스\n",
        "predict_str = \"\"\n",
        "for j, result in enumerate(results): #각 문자에 대한 예측 결과와 인덱스 추출\n",
        "    if j == 0: # 처음에는 예측 결과를 전부 가져오기\n",
        "        predict_str += ''.join([world_set[t] for t in result])\n",
        "    else: # 그 다음에는 마지막 글자만 반복 추가\n",
        "        predict_str += world_set[result[-1]]\n",
        "\n",
        "print(predict_str)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uxxrxCd2nwoo",
        "outputId": "38537823-7265-420d-8826-84859e17356f"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "rick walls are there tor a reason and you must not think that the brick walls are 't there to keep us out, but rather in this way that the brick walls are there to show us how badly we want thinksl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "results.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3xO3IOGrIyMD",
        "outputId": "21fe27ec-e1b2-42de-b3fa-232fe97c7fb3"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([183, 15])"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "predict_str"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "8qUkbiw2t0Il",
        "outputId": "f63cb92c-14a8-4b61-a0bf-46fd564781ce"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"rick walls are there tor a reason and you must not think that the brick walls are 't there to keep us out, but rather in this way that the brick walls are there to show us how badly we want thinksl\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task2\n",
        "\n",
        "위 sentence는 제가 임의로 생성한 문장들입니다.\n",
        "\n",
        "마음에 드시는 문구 가져오셔서 문장이 어떻게 생성되는지 확인해보세요!\n",
        "\n",
        "영어가 아닌 한국어로 시도해보는 것도 좋겠죠?\n",
        "\n",
        "수정이 많이 필요(토큰화 등) 할 수 있으나 한번 시도해보시는 것 권장드립니다 :)\n",
        "\n",
        "위 베이스라인은 어디든 수정하셔도 좋고 조금 더 자연스러운 문장이 나올 수 있게 다양한 시도를 해보세요!\n",
        "\n",
        "조건 : 문장 3개 이상, 연결성이 있는 문장을 \" \" 으로 구분하여 ( )에 넣기"
      ],
      "metadata": {
        "id": "kN1zL8Dpvane"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 연설문 데이터\n",
        "\n",
        "안녕하십니까.\n",
        "부총리 겸 교육부장관 이주호입니다.\n",
        "\n",
        "오늘 오전, 대통령께서 주재하는 ‘국민과 함께하는 민생토론회’에 참석하여, 학부모, 교원 등 다양한 국민들의 의견을 들었습니다.\n",
        "\n",
        "대통령께서는 이 자리에서 이제는 ‘부모 돌봄에서 국가 돌봄으로 나아가야 한다.’라고 하시면서 늘봄학교의 중요성을 강조하셨습니다.\n",
        "\n",
        "또한, 국가 돌봄이 정착하려면 ‘무엇보다 공교육의 중심인 학교의 역할이 확대돼야 한다.’라고도 하셨습니다.\n",
        "\n",
        "출처 : https://www.korea.kr/briefing/speechView.do?newsId=132036217&pageIndex=1&srchType=&period=&startDate=2023-02-12&endDate=2024-02-12&srchKeyword="
      ],
      "metadata": {
        "id": "dUa3VU1FKfSM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### sequence_length = 20 + layer 3 + 에폭100\n"
      ],
      "metadata": {
        "id": "tuiohevkZO5f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. 텍스트 전처리"
      ],
      "metadata": {
        "id": "lt6uvoCWMu3K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#1. 데이터 정의 및 토큰화\n",
        "\n",
        "# 연설문 데이터\n",
        "data = \"안녕하십니까. 부총리 겸 교육부장관 이주호입니다. 오늘 오전, 대통령께서 주재하는 ‘국민과 함께하는 민생토론회’에 참석하여, 학부모, 교원 등 다양한 국민들의 의견을 들었습니다. 대통령께서는 이 자리에서 이제는 ‘부모 돌봄에서 국가 돌봄으로 나아가야 한다.’라고 하시면서 늘봄학교의 중요성을 강조하셨습니다. 또한, 국가 돌봄이 정착하려면 ‘무엇보다 공교육의 중심인 학교의 역할이 확대돼야 한다.’라고도 하셨습니다.\"\n"
      ],
      "metadata": {
        "id": "IKp-lKrjvXR9"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 간단한 토큰화 : 공백으로 분리\n",
        "tokens = data.split()\n",
        "\n",
        "# 고유 토큰 집합 생성 및 정수 인코딩\n",
        "vocab = {tkn: i for i, tkn in enumerate(set(tokens))}\n",
        "vocab_size = len(vocab)\n",
        "\n",
        "print(vocab)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c78DuNesSYC4",
        "outputId": "f86f6e43-71d0-4785-e927-6e3aa2bcdcd8"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'또한,': 0, '오전,': 1, '함께하는': 2, '이': 3, '‘무엇보다': 4, '중심인': 5, '국가': 6, '학부모,': 7, '확대돼야': 8, '돌봄이': 9, '정착하려면': 10, '늘봄학교의': 11, '안녕하십니까.': 12, '참석하여,': 13, '학교의': 14, '의견을': 15, '민생토론회’에': 16, '국민들의': 17, '등': 18, '하시면서': 19, '대통령께서': 20, '교원': 21, '주재하는': 22, '중요성을': 23, '들었습니다.': 24, '역할이': 25, '자리에서': 26, '부총리': 27, '다양한': 28, '이주호입니다.': 29, '교육부장관': 30, '강조하셨습니다.': 31, '‘국민과': 32, '한다.’라고도': 33, '오늘': 34, '돌봄으로': 35, '하셨습니다.': 36, '이제는': 37, '나아가야': 38, '공교육의': 39, '겸': 40, '한다.’라고': 41, '대통령께서는': 42, '돌봄에서': 43, '‘부모': 44}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 데이터를 시퀀스로 변환 ()\n",
        "sequences = [vocab[tkn] for tkn in tokens]"
      ],
      "metadata": {
        "id": "OnGnYgizLuh6"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#문자 집합 크기 확인\n",
        "vocab_size = len(vocab) #26개의 고유한 문자가 있음\n",
        "print('문자 집합 크기 : {}'.format(vocab_size))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R9_j7Je0Lz1C",
        "outputId": "4b458693-f37a-490d-b2b7-ed22d59aaae8"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "문자 집합 크기 : 45\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 하이퍼 파라미터 설정(자유롭게 수정해보세요!)\n",
        "\n",
        "hidden_size = vocab_size\n",
        "sequence_length = 20\n",
        "learning_rate = 0.01"
      ],
      "metadata": {
        "id": "PqO3NJXBMBr1"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# eqence 길이 단위 자르기\n",
        "\n",
        "# 데이터 구성을 위한 리스트\n",
        "x_data = [] #입력 시퀀스\n",
        "y_data = [] #각 입력 시퀀스에 대응하는 타깃 문자(입력 시퀀스 바로 다음에 오는 문자)\n",
        "\n",
        "for i in range(len(sequences) - sequence_length):\n",
        "    x_data.append(sequences[i:i+sequence_length])\n",
        "    y_data.append(sequences[i+1:i+sequence_length+1])\n",
        "\n",
        "##입력 시퀀스에 대해 원핫인코딩 수행\n",
        "N = vocab_size  #191\n",
        "x_one_hot= [np.eye(N)[x] for x in x_data]\n",
        "\n",
        "# 텐서로 변환\n",
        "X = torch.FloatTensor(x_one_hot)\n",
        "Y = torch.LongTensor(y_data)"
      ],
      "metadata": {
        "id": "9YOGHYnaMPXH"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 출력해서 한 칸씩 쉬프트된 것 확인하기!\n",
        "\n",
        "print(x_data[0])\n",
        "print(y_data[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SwqLieqeMnck",
        "outputId": "54d798ac-32df-4624-9ea0-5f56ec1e32c9"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[12, 27, 40, 30, 29, 34, 1, 20, 22, 32, 2, 16, 13, 7, 21, 18, 28, 17, 15, 24]\n",
            "[27, 40, 30, 29, 34, 1, 20, 22, 32, 2, 16, 13, 7, 21, 18, 28, 17, 15, 24, 42]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##8. 크기 확인\n",
        "print('훈련 데이터의 크기 : {}'.format(X.shape))\n",
        "print('레이블의 크기 : {}'.format(Y.shape))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q9vlQNlXNLx2",
        "outputId": "bd59f18d-692a-4f11-c9d0-49c92622961c"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "훈련 데이터의 크기 : torch.Size([26, 20, 45])\n",
            "레이블의 크기 : torch.Size([26, 20])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##10. 레이블 데이터 샘플 확인하기\n",
        "print(Y[0]) #4개 각각 문자의 바로 다음 문자에 대한 인덱스 (정답)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Ekf_c82NUiv",
        "outputId": "e57cb9f4-bcae-4c14-97d1-050a065505eb"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([27, 40, 30, 29, 34,  1, 20, 22, 32,  2, 16, 13,  7, 21, 18, 28, 17, 15,\n",
            "        24, 42])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. 모델 정의"
      ],
      "metadata": {
        "id": "cv7IFtwtKZrM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "##11. RNN 모델 구현\n",
        "\n",
        "class Net(torch.nn.Module):\n",
        "  def __init__(self, input_dim, hidden_dim, layers):\n",
        "    super(Net, self).__init__()\n",
        "    self.rnn = torch.nn.RNN(input_size=input_dim,\n",
        "                            hidden_size=hidden_dim,\n",
        "                            num_layers=layers,\n",
        "                            batch_first=True)\n",
        "    self.fc = torch.nn.Linear(hidden_dim, vocab_size)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x, hidden  = self.rnn(x)\n",
        "    x = self.fc(x)\n",
        "    return x"
      ],
      "metadata": {
        "id": "rSkxLomgMxnp"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "net = Net(vocab_size, hidden_size, 3)"
      ],
      "metadata": {
        "id": "hFCTLYLJNtrc"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##12. loss function\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "##13. optimizer\n",
        "optimizer = optim.Adam(net.parameters(), learning_rate)"
      ],
      "metadata": {
        "id": "4IkVEJIDNwi7"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##14. 출력 크기 점검\n",
        "outputs = net(X)\n",
        "print(outputs.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hoKAFjvgNxRn",
        "outputId": "401db3f8-a5fa-405f-f0ea-6559fef1bf7b"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([26, 20, 45])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. 모델 학습"
      ],
      "metadata": {
        "id": "xkOmc7ZVN3IS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "index_to_token = {index: token for token, index in vocab.items()}\n",
        "\n",
        "##15. Training 시작\n",
        "\n",
        "for i in range(100): #에폭 100\n",
        "    optimizer.zero_grad()\n",
        "    outputs = net(X)\n",
        "    ##문제(7) : outputs, Y 형태 그대로 넣으면 안되죠. view 함수를 이용해 loss값을 계산해봅시다.\n",
        "    loss = criterion(outputs.view(-1, vocab_size), Y.view(-1))\n",
        "    loss.backward() #역전파를 수행하여 각 파라미터의 그라디언트를 계산\n",
        "    optimizer.step() #파라미터 업데이트\n",
        "\n",
        "    #16. 예측결과 확인\n",
        "results = outputs.argmax(dim=2) # 텐서의 마지막 차원(클래스 차원)을 따라 최댓값의 인덱스 -> 183 * 15개의 인덱스\n",
        "predict_str = \"\"\n",
        "for j, result in enumerate(results):\n",
        "  if j == 0:  # 첫 번째 결과에 대해서는 전체 문자열을 생성\n",
        "    predict_str += ''.join([index_to_token[t.item()] for t in result])\n",
        "else:  # 그 다음부터는 마지막 글자만 추가\n",
        "    predict_str += index_to_token[result[-1].item()]\n"
      ],
      "metadata": {
        "id": "eVjlvq8ytwcu"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. 예측 결과 확인\n"
      ],
      "metadata": {
        "id": "kZQD5wgHN8kF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "predict_str"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "U7luPWF8N81S",
        "outputId": "8118970d-6e7d-4208-b7fd-7cb064d2ae57"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'부총리겸교육부장관이주호입니다.오늘오전,대통령께서주재하는‘국민과함께하는민생토론회’에참석하여,학부모,교원등다양한국민들의의견을들었습니다.대통령께서는하셨습니다.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    }
  ]
}