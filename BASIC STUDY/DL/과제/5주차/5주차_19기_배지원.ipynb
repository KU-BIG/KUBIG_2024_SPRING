{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "nmQ5F7UAeKB_"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Sgxd6SxmeVcP"
   },
   "source": [
    "## Task1\n",
    "\n",
    "빈 칸을 채워주세요!\n",
    "\n",
    "단계별 output이 github 파일에는 남아있으니 그 output과 동일한 형태인지 확인하면서 진행해주시면 됩니다~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "NDvUeC8BoUb6"
   },
   "outputs": [],
   "source": [
    "#1. 생성할 문장 데이터\n",
    "\n",
    "sentence = (\"Brick walls are there for a reason and you must not think \"\n",
    "            \"that the brick walls aren't there to keep us out, but rather \"\n",
    "            \"in this way that the brick walls are there to show us how badly we want things.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "b9lkrKyZf8ie"
   },
   "outputs": [],
   "source": [
    "#2. 문자 집합 만들기\n",
    "world_set = list(set(sentence))\n",
    "\n",
    "## 문제(1): 각 문자에 정수 인코딩 (공백도 하나의 원소로 포함)\n",
    "vocab = {char: i for i, char in enumerate(world_set)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"'\": 0, 'n': 1, 'y': 2, 'r': 3, 't': 4, 'c': 5, 'k': 6, 'f': 7, '.': 8, 's': 9, 'h': 10, 'a': 11, 'p': 12, 'i': 13, 'w': 14, 'e': 15, ' ': 16, 'd': 17, 'o': 18, 'b': 19, ',': 20, 'u': 21, 'B': 22, 'l': 23, 'g': 24, 'm': 25}\n"
     ]
    }
   ],
   "source": [
    "print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TpKupU6lgpfT",
    "outputId": "8aa615b8-e7e7-4ded-89bd-0a30ea2b7963"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "문자 집합 크기 : 26\n"
     ]
    }
   ],
   "source": [
    "#3. 문자 집합 크기 확인\n",
    "\n",
    "vocab_size = len(vocab)\n",
    "print('문자 집합 크기 : {}'.format(vocab_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "wFDZJHSMg9In"
   },
   "outputs": [],
   "source": [
    "#4. 하이퍼 파라미터 설정(자유롭게 수정해보세요!)\n",
    "\n",
    "hidden_size = vocab_size # 같아야 하는 것 확인!\n",
    "sequence_length = 15  # 너무 길거나 너무 짧게 잡으면 안됩니다!\n",
    "learning_rate = 0.01\n",
    "\n",
    "# sequence_length에 따라 다른 결과?\n",
    "# 너무 길면 under fit, 너무 짧으면 over fit 가능성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Brick walls are -> rick walls are \n",
      "1 rick walls are  -> ick walls are t\n",
      "2 ick walls are t -> ck walls are th\n",
      "3 ck walls are th -> k walls are the\n",
      "4 k walls are the ->  walls are ther\n",
      "5  walls are ther -> walls are there\n",
      "6 walls are there -> alls are there \n",
      "7 alls are there  -> lls are there f\n",
      "8 lls are there f -> ls are there fo\n",
      "9 ls are there fo -> s are there for\n",
      "10 s are there for ->  are there for \n",
      "11  are there for  -> are there for a\n",
      "12 are there for a -> re there for a \n",
      "13 re there for a  -> e there for a r\n",
      "14 e there for a r ->  there for a re\n",
      "15  there for a re -> there for a rea\n",
      "16 there for a rea -> here for a reas\n",
      "17 here for a reas -> ere for a reaso\n",
      "18 ere for a reaso -> re for a reason\n",
      "19 re for a reason -> e for a reason \n",
      "20 e for a reason  ->  for a reason a\n",
      "21  for a reason a -> for a reason an\n",
      "22 for a reason an -> or a reason and\n",
      "23 or a reason and -> r a reason and \n",
      "24 r a reason and  ->  a reason and y\n",
      "25  a reason and y -> a reason and yo\n",
      "26 a reason and yo ->  reason and you\n",
      "27  reason and you -> reason and you \n",
      "28 reason and you  -> eason and you m\n",
      "29 eason and you m -> ason and you mu\n",
      "30 ason and you mu -> son and you mus\n",
      "31 son and you mus -> on and you must\n",
      "32 on and you must -> n and you must \n",
      "33 n and you must  ->  and you must n\n",
      "34  and you must n -> and you must no\n",
      "35 and you must no -> nd you must not\n",
      "36 nd you must not -> d you must not \n",
      "37 d you must not  ->  you must not t\n",
      "38  you must not t -> you must not th\n",
      "39 you must not th -> ou must not thi\n",
      "40 ou must not thi -> u must not thin\n",
      "41 u must not thin ->  must not think\n",
      "42  must not think -> must not think \n",
      "43 must not think  -> ust not think t\n",
      "44 ust not think t -> st not think th\n",
      "45 st not think th -> t not think tha\n",
      "46 t not think tha ->  not think that\n",
      "47  not think that -> not think that \n",
      "48 not think that  -> ot think that t\n",
      "49 ot think that t -> t think that th\n",
      "50 t think that th ->  think that the\n",
      "51  think that the -> think that the \n",
      "52 think that the  -> hink that the b\n",
      "53 hink that the b -> ink that the br\n",
      "54 ink that the br -> nk that the bri\n",
      "55 nk that the bri -> k that the bric\n",
      "56 k that the bric ->  that the brick\n",
      "57  that the brick -> that the brick \n",
      "58 that the brick  -> hat the brick w\n",
      "59 hat the brick w -> at the brick wa\n",
      "60 at the brick wa -> t the brick wal\n",
      "61 t the brick wal ->  the brick wall\n",
      "62  the brick wall -> the brick walls\n",
      "63 the brick walls -> he brick walls \n",
      "64 he brick walls  -> e brick walls a\n",
      "65 e brick walls a ->  brick walls ar\n",
      "66  brick walls ar -> brick walls are\n",
      "67 brick walls are -> rick walls aren\n",
      "68 rick walls aren -> ick walls aren'\n",
      "69 ick walls aren' -> ck walls aren't\n",
      "70 ck walls aren't -> k walls aren't \n",
      "71 k walls aren't  ->  walls aren't t\n",
      "72  walls aren't t -> walls aren't th\n",
      "73 walls aren't th -> alls aren't the\n",
      "74 alls aren't the -> lls aren't ther\n",
      "75 lls aren't ther -> ls aren't there\n",
      "76 ls aren't there -> s aren't there \n",
      "77 s aren't there  ->  aren't there t\n",
      "78  aren't there t -> aren't there to\n",
      "79 aren't there to -> ren't there to \n",
      "80 ren't there to  -> en't there to k\n",
      "81 en't there to k -> n't there to ke\n",
      "82 n't there to ke -> 't there to kee\n",
      "83 't there to kee -> t there to keep\n",
      "84 t there to keep ->  there to keep \n",
      "85  there to keep  -> there to keep u\n",
      "86 there to keep u -> here to keep us\n",
      "87 here to keep us -> ere to keep us \n",
      "88 ere to keep us  -> re to keep us o\n",
      "89 re to keep us o -> e to keep us ou\n",
      "90 e to keep us ou ->  to keep us out\n",
      "91  to keep us out -> to keep us out,\n",
      "92 to keep us out, -> o keep us out, \n",
      "93 o keep us out,  ->  keep us out, b\n",
      "94  keep us out, b -> keep us out, bu\n",
      "95 keep us out, bu -> eep us out, but\n",
      "96 eep us out, but -> ep us out, but \n",
      "97 ep us out, but  -> p us out, but r\n",
      "98 p us out, but r ->  us out, but ra\n",
      "99  us out, but ra -> us out, but rat\n",
      "100 us out, but rat -> s out, but rath\n",
      "101 s out, but rath ->  out, but rathe\n",
      "102  out, but rathe -> out, but rather\n",
      "103 out, but rather -> ut, but rather \n",
      "104 ut, but rather  -> t, but rather i\n",
      "105 t, but rather i -> , but rather in\n",
      "106 , but rather in ->  but rather in \n",
      "107  but rather in  -> but rather in t\n",
      "108 but rather in t -> ut rather in th\n",
      "109 ut rather in th -> t rather in thi\n",
      "110 t rather in thi ->  rather in this\n",
      "111  rather in this -> rather in this \n",
      "112 rather in this  -> ather in this w\n",
      "113 ather in this w -> ther in this wa\n",
      "114 ther in this wa -> her in this way\n",
      "115 her in this way -> er in this way \n",
      "116 er in this way  -> r in this way t\n",
      "117 r in this way t ->  in this way th\n",
      "118  in this way th -> in this way tha\n",
      "119 in this way tha -> n this way that\n",
      "120 n this way that ->  this way that \n",
      "121  this way that  -> this way that t\n",
      "122 this way that t -> his way that th\n",
      "123 his way that th -> is way that the\n",
      "124 is way that the -> s way that the \n",
      "125 s way that the  ->  way that the b\n",
      "126  way that the b -> way that the br\n",
      "127 way that the br -> ay that the bri\n",
      "128 ay that the bri -> y that the bric\n",
      "129 y that the bric ->  that the brick\n",
      "130  that the brick -> that the brick \n",
      "131 that the brick  -> hat the brick w\n",
      "132 hat the brick w -> at the brick wa\n",
      "133 at the brick wa -> t the brick wal\n",
      "134 t the brick wal ->  the brick wall\n",
      "135  the brick wall -> the brick walls\n",
      "136 the brick walls -> he brick walls \n",
      "137 he brick walls  -> e brick walls a\n",
      "138 e brick walls a ->  brick walls ar\n",
      "139  brick walls ar -> brick walls are\n",
      "140 brick walls are -> rick walls are \n",
      "141 rick walls are  -> ick walls are t\n",
      "142 ick walls are t -> ck walls are th\n",
      "143 ck walls are th -> k walls are the\n",
      "144 k walls are the ->  walls are ther\n",
      "145  walls are ther -> walls are there\n",
      "146 walls are there -> alls are there \n",
      "147 alls are there  -> lls are there t\n",
      "148 lls are there t -> ls are there to\n",
      "149 ls are there to -> s are there to \n",
      "150 s are there to  ->  are there to s\n",
      "151  are there to s -> are there to sh\n",
      "152 are there to sh -> re there to sho\n",
      "153 re there to sho -> e there to show\n",
      "154 e there to show ->  there to show \n",
      "155  there to show  -> there to show u\n",
      "156 there to show u -> here to show us\n",
      "157 here to show us -> ere to show us \n",
      "158 ere to show us  -> re to show us h\n",
      "159 re to show us h -> e to show us ho\n",
      "160 e to show us ho ->  to show us how\n",
      "161  to show us how -> to show us how \n",
      "162 to show us how  -> o show us how b\n",
      "163 o show us how b ->  show us how ba\n",
      "164  show us how ba -> show us how bad\n",
      "165 show us how bad -> how us how badl\n",
      "166 how us how badl -> ow us how badly\n",
      "167 ow us how badly -> w us how badly \n",
      "168 w us how badly  ->  us how badly w\n",
      "169  us how badly w -> us how badly we\n",
      "170 us how badly we -> s how badly we \n",
      "171 s how badly we  ->  how badly we w\n",
      "172  how badly we w -> how badly we wa\n",
      "173 how badly we wa -> ow badly we wan\n",
      "174 ow badly we wan -> w badly we want\n",
      "175 w badly we want ->  badly we want \n",
      "176  badly we want  -> badly we want t\n",
      "177 badly we want t -> adly we want th\n",
      "178 adly we want th -> dly we want thi\n",
      "179 dly we want thi -> ly we want thin\n",
      "180 ly we want thin -> y we want thing\n",
      "181 y we want thing ->  we want things\n",
      "182  we want things -> we want things.\n"
     ]
    }
   ],
   "source": [
    "#5. seqence 길이 단위 자르기\n",
    "\n",
    "# 데이터 구성을 위한 리스트\n",
    "x_data = []\n",
    "y_data = []\n",
    "\n",
    "## 문제(2): 반복문 내에서의 인덱싱을 사용하여 sequence_length 값 단위로 샘플을 잘라 데이터 만들기, \n",
    "##         y_str은 x_str은 한 칸씩 쉬프트된 sequnce\n",
    "\n",
    "for i in range(0, len(sentence) - sequence_length):\n",
    "  x_str = sentence[i:i + sequence_length]\n",
    "  y_str = sentence[i+1 : i+1 + sequence_length]\n",
    "  print(i, x_str, \"->\", y_str)\n",
    "\n",
    "  # x_str과 y_str이 문자집합에 해당하는 인덱스를 각각 x_data, y_data에 append\n",
    "  x_data.append([vocab[c] for c in x_str])\n",
    "  y_data.append([vocab[d] for d in y_str])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XVFlILiOixdc",
    "outputId": "504a1c71-f8f5-446f-f9fb-8fb1e24d41fc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[22, 3, 13, 5, 6, 16, 14, 11, 23, 23, 9, 16, 11, 3, 15]\n",
      "[3, 13, 5, 6, 16, 14, 11, 23, 23, 9, 16, 11, 3, 15, 16]\n"
     ]
    }
   ],
   "source": [
    "# 출력해서 한 칸씩 쉬프트된 것 확인하기!\n",
    "\n",
    "print(x_data[0])\n",
    "print(y_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[22, 3, 13, 5, 6, 16, 14, 11, 23, 23, 9, 16, 11, 3, 15],\n",
       " [3, 13, 5, 6, 16, 14, 11, 23, 23, 9, 16, 11, 3, 15, 16],\n",
       " [13, 5, 6, 16, 14, 11, 23, 23, 9, 16, 11, 3, 15, 16, 4],\n",
       " [5, 6, 16, 14, 11, 23, 23, 9, 16, 11, 3, 15, 16, 4, 10],\n",
       " [6, 16, 14, 11, 23, 23, 9, 16, 11, 3, 15, 16, 4, 10, 15],\n",
       " [16, 14, 11, 23, 23, 9, 16, 11, 3, 15, 16, 4, 10, 15, 3],\n",
       " [14, 11, 23, 23, 9, 16, 11, 3, 15, 16, 4, 10, 15, 3, 15],\n",
       " [11, 23, 23, 9, 16, 11, 3, 15, 16, 4, 10, 15, 3, 15, 16],\n",
       " [23, 23, 9, 16, 11, 3, 15, 16, 4, 10, 15, 3, 15, 16, 7],\n",
       " [23, 9, 16, 11, 3, 15, 16, 4, 10, 15, 3, 15, 16, 7, 18],\n",
       " [9, 16, 11, 3, 15, 16, 4, 10, 15, 3, 15, 16, 7, 18, 3],\n",
       " [16, 11, 3, 15, 16, 4, 10, 15, 3, 15, 16, 7, 18, 3, 16],\n",
       " [11, 3, 15, 16, 4, 10, 15, 3, 15, 16, 7, 18, 3, 16, 11],\n",
       " [3, 15, 16, 4, 10, 15, 3, 15, 16, 7, 18, 3, 16, 11, 16],\n",
       " [15, 16, 4, 10, 15, 3, 15, 16, 7, 18, 3, 16, 11, 16, 3],\n",
       " [16, 4, 10, 15, 3, 15, 16, 7, 18, 3, 16, 11, 16, 3, 15],\n",
       " [4, 10, 15, 3, 15, 16, 7, 18, 3, 16, 11, 16, 3, 15, 11],\n",
       " [10, 15, 3, 15, 16, 7, 18, 3, 16, 11, 16, 3, 15, 11, 9],\n",
       " [15, 3, 15, 16, 7, 18, 3, 16, 11, 16, 3, 15, 11, 9, 18],\n",
       " [3, 15, 16, 7, 18, 3, 16, 11, 16, 3, 15, 11, 9, 18, 1],\n",
       " [15, 16, 7, 18, 3, 16, 11, 16, 3, 15, 11, 9, 18, 1, 16],\n",
       " [16, 7, 18, 3, 16, 11, 16, 3, 15, 11, 9, 18, 1, 16, 11],\n",
       " [7, 18, 3, 16, 11, 16, 3, 15, 11, 9, 18, 1, 16, 11, 1],\n",
       " [18, 3, 16, 11, 16, 3, 15, 11, 9, 18, 1, 16, 11, 1, 17],\n",
       " [3, 16, 11, 16, 3, 15, 11, 9, 18, 1, 16, 11, 1, 17, 16],\n",
       " [16, 11, 16, 3, 15, 11, 9, 18, 1, 16, 11, 1, 17, 16, 2],\n",
       " [11, 16, 3, 15, 11, 9, 18, 1, 16, 11, 1, 17, 16, 2, 18],\n",
       " [16, 3, 15, 11, 9, 18, 1, 16, 11, 1, 17, 16, 2, 18, 21],\n",
       " [3, 15, 11, 9, 18, 1, 16, 11, 1, 17, 16, 2, 18, 21, 16],\n",
       " [15, 11, 9, 18, 1, 16, 11, 1, 17, 16, 2, 18, 21, 16, 25],\n",
       " [11, 9, 18, 1, 16, 11, 1, 17, 16, 2, 18, 21, 16, 25, 21],\n",
       " [9, 18, 1, 16, 11, 1, 17, 16, 2, 18, 21, 16, 25, 21, 9],\n",
       " [18, 1, 16, 11, 1, 17, 16, 2, 18, 21, 16, 25, 21, 9, 4],\n",
       " [1, 16, 11, 1, 17, 16, 2, 18, 21, 16, 25, 21, 9, 4, 16],\n",
       " [16, 11, 1, 17, 16, 2, 18, 21, 16, 25, 21, 9, 4, 16, 1],\n",
       " [11, 1, 17, 16, 2, 18, 21, 16, 25, 21, 9, 4, 16, 1, 18],\n",
       " [1, 17, 16, 2, 18, 21, 16, 25, 21, 9, 4, 16, 1, 18, 4],\n",
       " [17, 16, 2, 18, 21, 16, 25, 21, 9, 4, 16, 1, 18, 4, 16],\n",
       " [16, 2, 18, 21, 16, 25, 21, 9, 4, 16, 1, 18, 4, 16, 4],\n",
       " [2, 18, 21, 16, 25, 21, 9, 4, 16, 1, 18, 4, 16, 4, 10],\n",
       " [18, 21, 16, 25, 21, 9, 4, 16, 1, 18, 4, 16, 4, 10, 13],\n",
       " [21, 16, 25, 21, 9, 4, 16, 1, 18, 4, 16, 4, 10, 13, 1],\n",
       " [16, 25, 21, 9, 4, 16, 1, 18, 4, 16, 4, 10, 13, 1, 6],\n",
       " [25, 21, 9, 4, 16, 1, 18, 4, 16, 4, 10, 13, 1, 6, 16],\n",
       " [21, 9, 4, 16, 1, 18, 4, 16, 4, 10, 13, 1, 6, 16, 4],\n",
       " [9, 4, 16, 1, 18, 4, 16, 4, 10, 13, 1, 6, 16, 4, 10],\n",
       " [4, 16, 1, 18, 4, 16, 4, 10, 13, 1, 6, 16, 4, 10, 11],\n",
       " [16, 1, 18, 4, 16, 4, 10, 13, 1, 6, 16, 4, 10, 11, 4],\n",
       " [1, 18, 4, 16, 4, 10, 13, 1, 6, 16, 4, 10, 11, 4, 16],\n",
       " [18, 4, 16, 4, 10, 13, 1, 6, 16, 4, 10, 11, 4, 16, 4],\n",
       " [4, 16, 4, 10, 13, 1, 6, 16, 4, 10, 11, 4, 16, 4, 10],\n",
       " [16, 4, 10, 13, 1, 6, 16, 4, 10, 11, 4, 16, 4, 10, 15],\n",
       " [4, 10, 13, 1, 6, 16, 4, 10, 11, 4, 16, 4, 10, 15, 16],\n",
       " [10, 13, 1, 6, 16, 4, 10, 11, 4, 16, 4, 10, 15, 16, 19],\n",
       " [13, 1, 6, 16, 4, 10, 11, 4, 16, 4, 10, 15, 16, 19, 3],\n",
       " [1, 6, 16, 4, 10, 11, 4, 16, 4, 10, 15, 16, 19, 3, 13],\n",
       " [6, 16, 4, 10, 11, 4, 16, 4, 10, 15, 16, 19, 3, 13, 5],\n",
       " [16, 4, 10, 11, 4, 16, 4, 10, 15, 16, 19, 3, 13, 5, 6],\n",
       " [4, 10, 11, 4, 16, 4, 10, 15, 16, 19, 3, 13, 5, 6, 16],\n",
       " [10, 11, 4, 16, 4, 10, 15, 16, 19, 3, 13, 5, 6, 16, 14],\n",
       " [11, 4, 16, 4, 10, 15, 16, 19, 3, 13, 5, 6, 16, 14, 11],\n",
       " [4, 16, 4, 10, 15, 16, 19, 3, 13, 5, 6, 16, 14, 11, 23],\n",
       " [16, 4, 10, 15, 16, 19, 3, 13, 5, 6, 16, 14, 11, 23, 23],\n",
       " [4, 10, 15, 16, 19, 3, 13, 5, 6, 16, 14, 11, 23, 23, 9],\n",
       " [10, 15, 16, 19, 3, 13, 5, 6, 16, 14, 11, 23, 23, 9, 16],\n",
       " [15, 16, 19, 3, 13, 5, 6, 16, 14, 11, 23, 23, 9, 16, 11],\n",
       " [16, 19, 3, 13, 5, 6, 16, 14, 11, 23, 23, 9, 16, 11, 3],\n",
       " [19, 3, 13, 5, 6, 16, 14, 11, 23, 23, 9, 16, 11, 3, 15],\n",
       " [3, 13, 5, 6, 16, 14, 11, 23, 23, 9, 16, 11, 3, 15, 1],\n",
       " [13, 5, 6, 16, 14, 11, 23, 23, 9, 16, 11, 3, 15, 1, 0],\n",
       " [5, 6, 16, 14, 11, 23, 23, 9, 16, 11, 3, 15, 1, 0, 4],\n",
       " [6, 16, 14, 11, 23, 23, 9, 16, 11, 3, 15, 1, 0, 4, 16],\n",
       " [16, 14, 11, 23, 23, 9, 16, 11, 3, 15, 1, 0, 4, 16, 4],\n",
       " [14, 11, 23, 23, 9, 16, 11, 3, 15, 1, 0, 4, 16, 4, 10],\n",
       " [11, 23, 23, 9, 16, 11, 3, 15, 1, 0, 4, 16, 4, 10, 15],\n",
       " [23, 23, 9, 16, 11, 3, 15, 1, 0, 4, 16, 4, 10, 15, 3],\n",
       " [23, 9, 16, 11, 3, 15, 1, 0, 4, 16, 4, 10, 15, 3, 15],\n",
       " [9, 16, 11, 3, 15, 1, 0, 4, 16, 4, 10, 15, 3, 15, 16],\n",
       " [16, 11, 3, 15, 1, 0, 4, 16, 4, 10, 15, 3, 15, 16, 4],\n",
       " [11, 3, 15, 1, 0, 4, 16, 4, 10, 15, 3, 15, 16, 4, 18],\n",
       " [3, 15, 1, 0, 4, 16, 4, 10, 15, 3, 15, 16, 4, 18, 16],\n",
       " [15, 1, 0, 4, 16, 4, 10, 15, 3, 15, 16, 4, 18, 16, 6],\n",
       " [1, 0, 4, 16, 4, 10, 15, 3, 15, 16, 4, 18, 16, 6, 15],\n",
       " [0, 4, 16, 4, 10, 15, 3, 15, 16, 4, 18, 16, 6, 15, 15],\n",
       " [4, 16, 4, 10, 15, 3, 15, 16, 4, 18, 16, 6, 15, 15, 12],\n",
       " [16, 4, 10, 15, 3, 15, 16, 4, 18, 16, 6, 15, 15, 12, 16],\n",
       " [4, 10, 15, 3, 15, 16, 4, 18, 16, 6, 15, 15, 12, 16, 21],\n",
       " [10, 15, 3, 15, 16, 4, 18, 16, 6, 15, 15, 12, 16, 21, 9],\n",
       " [15, 3, 15, 16, 4, 18, 16, 6, 15, 15, 12, 16, 21, 9, 16],\n",
       " [3, 15, 16, 4, 18, 16, 6, 15, 15, 12, 16, 21, 9, 16, 18],\n",
       " [15, 16, 4, 18, 16, 6, 15, 15, 12, 16, 21, 9, 16, 18, 21],\n",
       " [16, 4, 18, 16, 6, 15, 15, 12, 16, 21, 9, 16, 18, 21, 4],\n",
       " [4, 18, 16, 6, 15, 15, 12, 16, 21, 9, 16, 18, 21, 4, 20],\n",
       " [18, 16, 6, 15, 15, 12, 16, 21, 9, 16, 18, 21, 4, 20, 16],\n",
       " [16, 6, 15, 15, 12, 16, 21, 9, 16, 18, 21, 4, 20, 16, 19],\n",
       " [6, 15, 15, 12, 16, 21, 9, 16, 18, 21, 4, 20, 16, 19, 21],\n",
       " [15, 15, 12, 16, 21, 9, 16, 18, 21, 4, 20, 16, 19, 21, 4],\n",
       " [15, 12, 16, 21, 9, 16, 18, 21, 4, 20, 16, 19, 21, 4, 16],\n",
       " [12, 16, 21, 9, 16, 18, 21, 4, 20, 16, 19, 21, 4, 16, 3],\n",
       " [16, 21, 9, 16, 18, 21, 4, 20, 16, 19, 21, 4, 16, 3, 11],\n",
       " [21, 9, 16, 18, 21, 4, 20, 16, 19, 21, 4, 16, 3, 11, 4],\n",
       " [9, 16, 18, 21, 4, 20, 16, 19, 21, 4, 16, 3, 11, 4, 10],\n",
       " [16, 18, 21, 4, 20, 16, 19, 21, 4, 16, 3, 11, 4, 10, 15],\n",
       " [18, 21, 4, 20, 16, 19, 21, 4, 16, 3, 11, 4, 10, 15, 3],\n",
       " [21, 4, 20, 16, 19, 21, 4, 16, 3, 11, 4, 10, 15, 3, 16],\n",
       " [4, 20, 16, 19, 21, 4, 16, 3, 11, 4, 10, 15, 3, 16, 13],\n",
       " [20, 16, 19, 21, 4, 16, 3, 11, 4, 10, 15, 3, 16, 13, 1],\n",
       " [16, 19, 21, 4, 16, 3, 11, 4, 10, 15, 3, 16, 13, 1, 16],\n",
       " [19, 21, 4, 16, 3, 11, 4, 10, 15, 3, 16, 13, 1, 16, 4],\n",
       " [21, 4, 16, 3, 11, 4, 10, 15, 3, 16, 13, 1, 16, 4, 10],\n",
       " [4, 16, 3, 11, 4, 10, 15, 3, 16, 13, 1, 16, 4, 10, 13],\n",
       " [16, 3, 11, 4, 10, 15, 3, 16, 13, 1, 16, 4, 10, 13, 9],\n",
       " [3, 11, 4, 10, 15, 3, 16, 13, 1, 16, 4, 10, 13, 9, 16],\n",
       " [11, 4, 10, 15, 3, 16, 13, 1, 16, 4, 10, 13, 9, 16, 14],\n",
       " [4, 10, 15, 3, 16, 13, 1, 16, 4, 10, 13, 9, 16, 14, 11],\n",
       " [10, 15, 3, 16, 13, 1, 16, 4, 10, 13, 9, 16, 14, 11, 2],\n",
       " [15, 3, 16, 13, 1, 16, 4, 10, 13, 9, 16, 14, 11, 2, 16],\n",
       " [3, 16, 13, 1, 16, 4, 10, 13, 9, 16, 14, 11, 2, 16, 4],\n",
       " [16, 13, 1, 16, 4, 10, 13, 9, 16, 14, 11, 2, 16, 4, 10],\n",
       " [13, 1, 16, 4, 10, 13, 9, 16, 14, 11, 2, 16, 4, 10, 11],\n",
       " [1, 16, 4, 10, 13, 9, 16, 14, 11, 2, 16, 4, 10, 11, 4],\n",
       " [16, 4, 10, 13, 9, 16, 14, 11, 2, 16, 4, 10, 11, 4, 16],\n",
       " [4, 10, 13, 9, 16, 14, 11, 2, 16, 4, 10, 11, 4, 16, 4],\n",
       " [10, 13, 9, 16, 14, 11, 2, 16, 4, 10, 11, 4, 16, 4, 10],\n",
       " [13, 9, 16, 14, 11, 2, 16, 4, 10, 11, 4, 16, 4, 10, 15],\n",
       " [9, 16, 14, 11, 2, 16, 4, 10, 11, 4, 16, 4, 10, 15, 16],\n",
       " [16, 14, 11, 2, 16, 4, 10, 11, 4, 16, 4, 10, 15, 16, 19],\n",
       " [14, 11, 2, 16, 4, 10, 11, 4, 16, 4, 10, 15, 16, 19, 3],\n",
       " [11, 2, 16, 4, 10, 11, 4, 16, 4, 10, 15, 16, 19, 3, 13],\n",
       " [2, 16, 4, 10, 11, 4, 16, 4, 10, 15, 16, 19, 3, 13, 5],\n",
       " [16, 4, 10, 11, 4, 16, 4, 10, 15, 16, 19, 3, 13, 5, 6],\n",
       " [4, 10, 11, 4, 16, 4, 10, 15, 16, 19, 3, 13, 5, 6, 16],\n",
       " [10, 11, 4, 16, 4, 10, 15, 16, 19, 3, 13, 5, 6, 16, 14],\n",
       " [11, 4, 16, 4, 10, 15, 16, 19, 3, 13, 5, 6, 16, 14, 11],\n",
       " [4, 16, 4, 10, 15, 16, 19, 3, 13, 5, 6, 16, 14, 11, 23],\n",
       " [16, 4, 10, 15, 16, 19, 3, 13, 5, 6, 16, 14, 11, 23, 23],\n",
       " [4, 10, 15, 16, 19, 3, 13, 5, 6, 16, 14, 11, 23, 23, 9],\n",
       " [10, 15, 16, 19, 3, 13, 5, 6, 16, 14, 11, 23, 23, 9, 16],\n",
       " [15, 16, 19, 3, 13, 5, 6, 16, 14, 11, 23, 23, 9, 16, 11],\n",
       " [16, 19, 3, 13, 5, 6, 16, 14, 11, 23, 23, 9, 16, 11, 3],\n",
       " [19, 3, 13, 5, 6, 16, 14, 11, 23, 23, 9, 16, 11, 3, 15],\n",
       " [3, 13, 5, 6, 16, 14, 11, 23, 23, 9, 16, 11, 3, 15, 16],\n",
       " [13, 5, 6, 16, 14, 11, 23, 23, 9, 16, 11, 3, 15, 16, 4],\n",
       " [5, 6, 16, 14, 11, 23, 23, 9, 16, 11, 3, 15, 16, 4, 10],\n",
       " [6, 16, 14, 11, 23, 23, 9, 16, 11, 3, 15, 16, 4, 10, 15],\n",
       " [16, 14, 11, 23, 23, 9, 16, 11, 3, 15, 16, 4, 10, 15, 3],\n",
       " [14, 11, 23, 23, 9, 16, 11, 3, 15, 16, 4, 10, 15, 3, 15],\n",
       " [11, 23, 23, 9, 16, 11, 3, 15, 16, 4, 10, 15, 3, 15, 16],\n",
       " [23, 23, 9, 16, 11, 3, 15, 16, 4, 10, 15, 3, 15, 16, 4],\n",
       " [23, 9, 16, 11, 3, 15, 16, 4, 10, 15, 3, 15, 16, 4, 18],\n",
       " [9, 16, 11, 3, 15, 16, 4, 10, 15, 3, 15, 16, 4, 18, 16],\n",
       " [16, 11, 3, 15, 16, 4, 10, 15, 3, 15, 16, 4, 18, 16, 9],\n",
       " [11, 3, 15, 16, 4, 10, 15, 3, 15, 16, 4, 18, 16, 9, 10],\n",
       " [3, 15, 16, 4, 10, 15, 3, 15, 16, 4, 18, 16, 9, 10, 18],\n",
       " [15, 16, 4, 10, 15, 3, 15, 16, 4, 18, 16, 9, 10, 18, 14],\n",
       " [16, 4, 10, 15, 3, 15, 16, 4, 18, 16, 9, 10, 18, 14, 16],\n",
       " [4, 10, 15, 3, 15, 16, 4, 18, 16, 9, 10, 18, 14, 16, 21],\n",
       " [10, 15, 3, 15, 16, 4, 18, 16, 9, 10, 18, 14, 16, 21, 9],\n",
       " [15, 3, 15, 16, 4, 18, 16, 9, 10, 18, 14, 16, 21, 9, 16],\n",
       " [3, 15, 16, 4, 18, 16, 9, 10, 18, 14, 16, 21, 9, 16, 10],\n",
       " [15, 16, 4, 18, 16, 9, 10, 18, 14, 16, 21, 9, 16, 10, 18],\n",
       " [16, 4, 18, 16, 9, 10, 18, 14, 16, 21, 9, 16, 10, 18, 14],\n",
       " [4, 18, 16, 9, 10, 18, 14, 16, 21, 9, 16, 10, 18, 14, 16],\n",
       " [18, 16, 9, 10, 18, 14, 16, 21, 9, 16, 10, 18, 14, 16, 19],\n",
       " [16, 9, 10, 18, 14, 16, 21, 9, 16, 10, 18, 14, 16, 19, 11],\n",
       " [9, 10, 18, 14, 16, 21, 9, 16, 10, 18, 14, 16, 19, 11, 17],\n",
       " [10, 18, 14, 16, 21, 9, 16, 10, 18, 14, 16, 19, 11, 17, 23],\n",
       " [18, 14, 16, 21, 9, 16, 10, 18, 14, 16, 19, 11, 17, 23, 2],\n",
       " [14, 16, 21, 9, 16, 10, 18, 14, 16, 19, 11, 17, 23, 2, 16],\n",
       " [16, 21, 9, 16, 10, 18, 14, 16, 19, 11, 17, 23, 2, 16, 14],\n",
       " [21, 9, 16, 10, 18, 14, 16, 19, 11, 17, 23, 2, 16, 14, 15],\n",
       " [9, 16, 10, 18, 14, 16, 19, 11, 17, 23, 2, 16, 14, 15, 16],\n",
       " [16, 10, 18, 14, 16, 19, 11, 17, 23, 2, 16, 14, 15, 16, 14],\n",
       " [10, 18, 14, 16, 19, 11, 17, 23, 2, 16, 14, 15, 16, 14, 11],\n",
       " [18, 14, 16, 19, 11, 17, 23, 2, 16, 14, 15, 16, 14, 11, 1],\n",
       " [14, 16, 19, 11, 17, 23, 2, 16, 14, 15, 16, 14, 11, 1, 4],\n",
       " [16, 19, 11, 17, 23, 2, 16, 14, 15, 16, 14, 11, 1, 4, 16],\n",
       " [19, 11, 17, 23, 2, 16, 14, 15, 16, 14, 11, 1, 4, 16, 4],\n",
       " [11, 17, 23, 2, 16, 14, 15, 16, 14, 11, 1, 4, 16, 4, 10],\n",
       " [17, 23, 2, 16, 14, 15, 16, 14, 11, 1, 4, 16, 4, 10, 13],\n",
       " [23, 2, 16, 14, 15, 16, 14, 11, 1, 4, 16, 4, 10, 13, 1],\n",
       " [2, 16, 14, 15, 16, 14, 11, 1, 4, 16, 4, 10, 13, 1, 24],\n",
       " [16, 14, 15, 16, 14, 11, 1, 4, 16, 4, 10, 13, 1, 24, 9]]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5lPes1dvjlNb",
    "outputId": "0705e61b-779f-4c71-8303-dc6c275bb401"
   },
   "outputs": [],
   "source": [
    "##6. 입력 시퀀스에 대해 원핫인코딩 수행\n",
    "\n",
    "## 문제(4) : x_data를 원핫인코딩 > numpy의 eye를 쓸 수 있지 않을까?\n",
    "x_one_hot = [np.eye(vocab_size)[x] for x in x_data]\n",
    "# np.eye : 주 대각선이 모두 1이고 나머지는 0인 2차원 배열을 생성\n",
    "\n",
    "##7. 입력 데이터, 레이블데이터 텐서로 변환\n",
    "\n",
    "## 문제(5) : x_one_hot과 y_data 텐서로 변환 : 둘 다 같은 형식의 텐서로 변환하면 될까??\n",
    "X = torch.FloatTensor(x_one_hot)\n",
    "Y = torch.LongTensor(y_data)\n",
    "# torch.FloatTensor : 32비트 부동 소수점 숫자를 저장\n",
    "# torch.LongTensor : 64비트 정수(부호 있는 정수)를 저장\n",
    "\n",
    "# 왜 X data 원핫인코딩은 0. 1. 이런 형태?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "print(type(x_one_hot))\n",
    "print(type(y_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lMZzZlaymMk8",
    "outputId": "9d64e989-9ca6-4c8c-830a-7460a4f3a47b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "훈련 데이터의 크기 : torch.Size([183, 15, 26])\n",
      "레이블의 크기 : torch.Size([183, 15])\n"
     ]
    }
   ],
   "source": [
    "##8. 크기 확인\n",
    "print('훈련 데이터의 크기 : {}'.format(X.shape))\n",
    "print('레이블의 크기 : {}'.format(Y.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "knx1DE_AmSFB",
    "outputId": "5e46adcc-8fec-4ea2-ea1f-e5f0766eb161"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 1., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "##9.원핫인코딩 결과 샘플 확인하기\n",
    "print(X[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6pWDiH1SmYT_",
    "outputId": "744a60f6-c22b-47e1-8ff5-dee57ac8e706"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 3, 13,  5,  6, 16, 14, 11, 23, 23,  9, 16, 11,  3, 15, 16])\n"
     ]
    }
   ],
   "source": [
    "##10. 레이블 데이터 샘플 확인하기\n",
    "print(Y[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "-Ww22xu8mfUc"
   },
   "outputs": [],
   "source": [
    "##11. RNN 모델 구현\n",
    "\n",
    "##문제(6) : 기본 pytorch 인자 넣기 연습 + forward 채우기\n",
    "### 조건 : rnn layer 2개 쌓기 + 마지막은 fc layer\n",
    "### batch_first 설정 필요할까? (유튜브 강의 참고)\n",
    "\n",
    "class Net(torch.nn.Module):\n",
    "  def __init__(self, input_dim, hidden_dim, layers, output_dim):\n",
    "    super(Net, self).__init__()\n",
    "    self.rnn = torch.nn.RNN(input_size = input_dim,\n",
    "                           hidden_size = hidden_dim,\n",
    "                           num_layers = layers,\n",
    "                           batch_first = True)\n",
    "    # batch_first = True : 필요하다. x input 형태 (batch_size, seq_length, features)로 인식하기 위함.\n",
    "    self.fc = torch.nn.Linear(hidden_dim, output_dim)\n",
    "    # output_dim 없이 어떻게 선형 레이어 완성?\n",
    "    # output이 글자를 예측하는 것, output_dim이 같은 input_dim\n",
    "    # Task 2에 생략\n",
    "  \n",
    "  def forward(self, x):\n",
    "    # RNN 레이어를 통해 x 처리\n",
    "    # 여기서 x의 차원은 (batch_size, seq_length, input_dim)\n",
    "    x, hidden = self.rnn(x)\n",
    "    \n",
    "    # Reshape output for fc layer\n",
    "    # fc layer는 2D input 받기 때문에\n",
    "    batch_size, sequence_length, hidden_dim = x.shape\n",
    "    \n",
    "    x = x.contiguous().view(batch_size * sequence_length, hidden_dim)\n",
    "    # contiguous() : batch_size * seq_length를 유지하면서 나머지 차원을 평탄화\n",
    "    \n",
    "    # 선형 레이어 통과\n",
    "    x = self.fc(x)\n",
    "    \n",
    "    # Reshape back to (batch size, sequence length, output dimension)\n",
    "    x = x.view(batch_size, sequence_length, -1)\n",
    "    \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "No2GRvTpnLBl"
   },
   "outputs": [],
   "source": [
    "# net = Net(vocab_size, hidden_size, 2)\n",
    "\n",
    "net = Net(vocab_size, hidden_size, 2, vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "9-zuJLeUnQLB"
   },
   "outputs": [],
   "source": [
    "##12. loss function\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "##13. optimizer\n",
    "optimizer = optim.Adam(net.parameters(), learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "z-RxRaiHnh9U",
    "outputId": "91348b4b-28bb-469b-85a0-8b9cc85e0454"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([183, 15, 26])\n"
     ]
    }
   ],
   "source": [
    "##14. 출력 크기 점검\n",
    "outputs = net(X)\n",
    "print(outputs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ss''''s's'ssss's'sss's'ssssss's'ss''s's'ss''''''s''''''s''s'ss'''ssss'ss's's'ssss's'''s'ss's's's's'ss'ss''''ss''sss'ss'ss''s'ss''sss'ss'''ssss'ss's's'ssss's'sss's's's's'ss'sss''ss's'ss's'ss'''s's's\n",
      "a' '' ''   a'' ''t'' ''t''''''''tta't' '  t'' '' t ''t t'  '' ' t'''' '' ''   a'' 't ''t'' ''t ' t'a'tatt'' '  ''' '''' '' ' ''''  't' ' t'''' '' ''   a'' ''t'' ''t tt' '''a't '''' ' 't'''  ' t t  \n",
      "a          t                                                                  t                                                                        t                                             \n",
      "a                                                                                                                                                                                                    \n",
      "                                                                                                                                                                                                     \n",
      "                                                                                                                                                                                                     \n",
      "                                                                                                                                                                                                     \n",
      "                                                                                                                                                                                                     \n",
      "                                                                                                                                                                                                     \n",
      "                                                                                                                                                                                                     \n",
      "                                                                                                                                                                                                     \n",
      "                                                                                                                                                                                                     \n",
      "                                                                                                                                                                                                     \n",
      "                 e                                   e                                 e                                            e                        e                                  e    \n",
      "                 e e                                 e     e    e   e         t        e     e                             e        e    e   e         t     e e                      a         e    \n",
      "           a e t e e e e t e            e e        t e   e e    e e e   e     a e      e e a e e   t               e e     e        e    e e e   e     a e t e e e e         e        a       t e    \n",
      "     t     a e t e e a e t e      t     e e e      t e e e e    e e e   a e   a e    t e e a e e   t           t e e e e a e   a    e    e e e   a e   a e t e e a e a   t   e    e   a e     t e    \n",
      "     t     a e t e e a e t e      a     e t    t   t e e a e    e e e   a e   a e    t e e a e e   t e     tt  t e e e t t e   a    e    e e e   a e   a e t e e a e a   t  ta    e   a e     t e    \n",
      "     t ll  ate t e e a e t e      a     e t    t   t e e t e    e e e   atll  ate    t e e a e e   t e     tt  t e e e t t e   a  t e    e e e   atll  ate t e e a e at  t  ta  t e   a e     t e    \n",
      "     atll  ate t e e a e tte    t a     e a    a e a e   a e  t e t e   atll  ate    a e e a e a   t  t    ta  tte e e t t e  tae t e  t e a e   atll  ate t e e a e at  t  tae t e   a e     a e    \n",
      "     aall  ate t e e a e ate  t t a     e at   a e a e   a e  t e t e  taall  ate  t a e e a e a   t  t    ta  tte e e t t e  tae t e  t e a e  taall  ate t e e a e aa  t  tae t e   at      a e    \n",
      "     aalls ate t e e a e ate  t t a     e aa   a e a e   a e  t e are   aalls ate  t a e e a e at  t  t    ta  tte e e t t e  tae t e  t e are   aalls ate t e e a e aa  t  tae t e   at      a e    \n",
      " e   aalls are t e e aoe ate  t t a    oe aa   a e a e   a e  t e are   aalls are  t a e e a e aa  t  t    ta  toe e e t t e  tae t e  t e are   aalls are t e e a e aa  t  tae tre   ar  al  a e    \n",
      " e   aalls are t e e aoe tre  t t ar   oe tr   a e a e   a e  t e are   aalls are  t t e e a e ar  t  t    ta  the e e t t e  tae t e  t e are   aalls are t e e a e aa  t  tae tre   ar tal  a e    \n",
      " e   aalls are t e e aoe tre  t t ar   oe tr   a e a e   ahe  t e tre   aalls are  t t e e t e are t  t    tae the e e t t e  tae t e  t e tre   aalls are t e e a e aa  tr tae trl s ar tal  the    \n",
      " e   aalls are t e e tre tres   t ar   oe tr   are the   are  the tre   aalls are  t t e e t e tre t  t    tae the e e t t e  tal t e  the tre   aalls are t e e t e aa  ts tae trlll ar tal  the    \n",
      " e   aalls are the e toe tren   t tr   oe tr   are the   tre  the tre   aalls are  t the e t e tre ts tr   tae the e e t t er tal t er the tre   aalls are the e t e aar ts tae trlll aa tal  ther   \n",
      " e   aalls are there to  tren t   ar   oe tr   are ther  ther the tre   aalls are  t there the are ts ta   tae the e e n t er tal ther the tre   aalls are there the aat tr tae talll aa tal  ther   \n",
      " e   talls are there to  tren t   ar   oe ta   aoe ther  ther thertre   aalls are  t there the art ts ta   tae the ere n then tal ther thertre   aalls are there the aat tr tae talls aa tal  ther   \n",
      " e   talls aae there to  tree     ar   ot ta   aoe then  ther thertre   talls aae  t there the art ts ta   tae toe ere n then tal ther thertre   talls aae there the aat ta tae talll aa tal  ther   \n",
      " el  talls are there to  tree n   ar  tot ta   aoe then  then the tre   talls are tt there th  art ts ta   tae to  e e n then tal ther the tre   talls are there th  aat ta tar talll aa tal  then   \n",
      "rel  talls are there to  tree s   ar  tot tr   ah  then  then the trec  talls are tt the e th  art ts ta   tae tot e e n then tal then the trec  talls are there th  aat ta taw talll aa tal  then   \n",
      "rel  talls are there to  tree s   ar  tot tr   th  then  then the treck talls are tt the e th  art ts ta   tae tot e ets then tal then the treck talls are there th  aat ts taw talll aa tal  then   \n",
      "relk talls are there to  arae s   ar  tot tr   tot then  then the trelk talls are tt the e th  art ts ta   tae tot e ets then tal then the trelk talls are there th  aat ts taw talll aa tal  then   \n",
      "relk talls are there to  arae s   an  tow tr   tot then  then the trelk talls are tt the e th  art ts ta   tae tot e ets then tal then the trelk talls are there th  aat ts waw talll aa tal  then   \n",
      "reck talls are there to  araets   an  tow tr e tot then  then the treck talls are tt there th  art ts ha   tae tot e ets then tal then the treck talls are there th  aat ts waw talll aa tal  then   \n",
      "reck talls are there to  araets   an  tow tr e tot then  then the trick talls are tt there th  art ts ha   tae tot erets then tal then the trick talls are there th  aat ts oaw talll aa tal  then   \n",
      "reck talls are there to  araets   as  tow ar e tot then  then the trick talls are tt there th  art ts ha   tae tat erets then wal ther the trick talls are there th  aat ts haw talll aa tal  then   \n",
      "reck talls are there to  araets   an  tot ar e tot ahen  then the trick talls are tt there th  art ts ha   tae tat eretn then wal ther the trick talls are there th  aat ts haw talll aa talk then  s\n",
      "reck talls are there to  artews   ane tot brse tot ahen  thet the trick talls are tt there th  a t ts ha   tae tat eretn then wal ther the trick talls are there th  aot ts haw balll aa talk then  s\n",
      "reck talls are there to  artews   ane tor base tot then  thet the trick talls are tt there th  a t ts ha e tae tat er tn then wal ther the trick talls are there th  aot ts haw balll ae ta k thenk s\n",
      "reck talls are there tor araeas   ane tor base tot then  thet the trick talls are kt there th  a t ts ha e tae tet er tn then wal thet the trick talls are there th  aor ts oaw balll ae ta k thenk s\n",
      "reck talls are there tor araeas n ani tor base tor thenk thet the trick talls are kt there th  a t ts hu e tae tether tn then wal thet the trick talls are there th  aor ts oaw balll ae ta t thenk s\n",
      "reck walls are there tor araeason ani yor base tor thenk thet the trick walls are kt there th  a t ts hu e tat tether tn then wal thet the trick walls are there th  aor ts oow badll ae ta t thenk s\n",
      "reck walls are there tor araeason ani yor base wor thenk thet the trick walls are kt there th  a t ts ou e tat tether tn then wal thet the trick walls are there th  aor ts oow tadll ae ta t thenk s\n",
      "rick walls are there tor arteason ani yor base wor thenk thet the trick walls are kt there th  a t ts ou e tat tether tn then wal thet the trick walls are there th  aor ts oow talll ae ta t thenk s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rick walls are there tor arteason anl yor base wor then  thet the trick walls are kt there th  a t ts ou e tat tether tn thin wal thet the trick walls are there th  aor ts oow talll ae ta t thenk s\n",
      "rick walls are there tor arteason anl yot base wor then  thet the trick walls are kt there th  a t us ou e tat tether tn thin wal thet the brick walls are there th saor us oow talll ae ta t thenk s\n",
      "rick walls are there tor arteason anl yot base wor then  thit the brick walls are kt there th se t us ou e tat tetheretn thin wal thet the brick walls are there th soor us oow talll we ta t thenk s\n",
      "rick walls are there tor arteason anl yot buse wor then  thit the brick walls are kt there th se t us oute tat tether tn thin wal thet the brick walls are there th soor us oow tadll we ta t thenkns\n",
      "rick walls are there tor arreason anl yow buse wor then  that the brick walls are kt there th  ert us oute tat tether tn thin wal thet the brick walls are there th soow us oow tadll we ta t thenkns\n",
      "rick walls are there tor arreason anl yow bust wot then  that the brick walls are kt there th sert us oute tut tether tn thin wal thet the brick walls are there th soow us oow talll we tant thenkns\n",
      "rick walls are there tor arreason and yow bust wot thin  that the brick walls are kt there th se t us oute tut tether tn thin wal thet the brick walls are there th soow us how bally we tant thenknl\n",
      "rick walls are there tor arreason and yow bust wot thin  that the brick walls are kt there th se t us hute tut tether tn thin wal thet the brick walls are there th soow us how bally we tant thenknl\n",
      "rick walls are there tor arreason and yow bust wot then  that the brick walls are kt there th  eep us hute tut tether tn thin wal thet the brick walls are there th soow us how bally we tant thenknl\n",
      "rick walls are there tor arreason and you bust wot think that the brick walls are kt there to  eep us hute tut tather wn thin wal thet the brick walls are there th  oow us how bally we tant thenknl\n",
      "rick walls are there tor arreason and you bust wot think that the brick walls are kt there to  eep us hut, tut tather wn thin wal that the brick walls are there to soow us how badly we tant thinknl\n",
      "rick walls are there tor arreason and you bust wot think that the brick walls are kt there to  eep us hut, tut rather wn thin wal that the brick walls are there to soow us how badly we tant thinknl\n",
      "rick walls are there tor arreason and you bust wot think that the brick walls are kt there to seep us out, tut rather wn thin way that the brick walls are there to soow us how badly we tant thinknl\n",
      "rick walls are there tor arreason and you bust wot think that the brick walls are kt there to seep us out, tut rather wn this way that the brick walls are there to soow us how badly we tant thinktl\n",
      "rick walls are there tor arreason and you bust wot think that the brick walls are 't there to seep us out, but rather wn this way that the brick walls are there to soow us how badly we tant thinktl\n",
      "rick walls are there tor arreason and you bust wot think that the brick walls are 't there to keep us out, but rather wn this way that the brick walls are there to koow us how badly we want thinktl\n",
      "rick walls are there tor a reason and you bust wot think that the brick walls are 't there to keep us out, but rather in this way that the brick walls are there to koow us how badly we want thinkty\n",
      "rick walls are there tor a reason and you must wot think that the brick walls are 't there to keep us out, but rather in this way that the brick walls are there to koow us how badly we want thinkty\n",
      "rick walls are there tor a reason and you must wot think that the brick walls are 't there to keep us out, but rather in this way that the brick walls are there to koow us how badly we want thinktk\n",
      "rick walls are there tor a reason and you must wot think that the brick walls are 't there to keep us out, but rather in this way that the brick walls are there to khow us how badly we want thinktk\n",
      "rick walls are there tor a reason and you must wot think that the brick walls are 't there to keep us out, but rather in this way that the brick walls are there to khow us how badly we want thinklk\n",
      "rick walls are there tor a reason and you must wot think that the brick walls are 't there to keep us out, but rather in this way that the brick walls are there to khow us how badly we want thinklk\n",
      "rick walls are there tor a reason and you must not think that the brick walls are 't there to keep us out, but rather in this way that the brick walls are there to khow us how badly we want thinklk\n",
      "rick walls are there tor a reason and you must not think that the brick walls are 't there to keep us out, but rather in this way that the brick walls are there to khow us how badly we want thinklk\n",
      "rick walls are there tor a reason and you must not think that the brick walls are 't there to keep us out, but rather in this way that the brick walls are there to khow us how badly we want thinklk\n",
      "rick walls are there tor a reason and you must not think that the brick walls are 't there to keep us out, but rather in this way that the brick walls are there to khow us how badly we want thinklk\n",
      "rick walls are there tor a reason and you must not think that the brick walls are 't there to keep us out, but rather in this way that the brick walls are there to khow us how badly we want thinklk\n",
      "rick walls are there tor a reason and you must not think that the brick walls are 't there to keep us out, but rather in this way that the brick walls are there to khow us how badly we want thinksk\n",
      "rick walls are there tor a reason and you must not think that the brick walls are 't there to keep us out, but rather in this way that the brick walls are there to khow us how badly we want thinksk\n",
      "rick walls are there tor a reason and you must not think that the brick walls are 't there to keep us out, but rather in this way that the brick walls are there to khow us how badly we want thinksk\n",
      "rick walls are there tor a reason and you must not think that the brick walls are 't there to keep us out, but rather in this way that the brick walls are there to khow us how badly we want thinksk\n",
      "rick walls are there tor a reason and you must not think that the brick walls are 't there to keep us out, but rather in this way that the brick walls are there to khow us how badly we want thinksk\n",
      "rick walls are there tor a reason and you must not think that the brick walls are 't there to keep us out, but rather in this way that the brick walls are there to khow us how badly we want thinksk\n",
      "rick walls are there tor a reason and you must not think that the brick walls are 't there to keep us out, but rather in this way that the brick walls are there to khow us how badly we want thinksk\n",
      "rick walls are there tor a reason and you must not think that the brick walls are 't there to keep us out, but rather in this way that the brick walls are there to khow us how badly we want thinksk\n",
      "rick walls are there tor a reason and you must not think that the brick walls are 't there to keep us out, but rather in this way that the brick walls are there to khow us how badly we want thinksk\n",
      "rick walls are there tor a reason and you must not think that the brick walls are 't there to keep us out, but rather in this way that the brick walls are there to khow us how badly we want thinksk\n",
      "rick walls are there tor a reason and you must not think that the brick walls are 't there to keep us out, but rather in this way that the brick walls are there to khow us how badly we want thinksk\n",
      "rick walls are there tor a reason and you must not think that the brick walls are 't there to keep us out, but rather in this way that the brick walls are there to khow us how badly we want thinksm\n",
      "rick walls are there tor a reason and you must not think that the brick walls are 't there to keep us out, but rather in this way that the brick walls are there to khow us how badly we want thinksm\n",
      "rick walls are there tor a reason and you must not think that the brick walls are 't there to keep us out, but rather in this way that the brick walls are there to show us how badly we want thinksm\n",
      "rick walls are there tor a reason and you must not think that the brick walls are 't there to keep us out, but rather in this way that the brick walls are there to show us how badly we want thinksm\n",
      "rick walls are there tor a reason and you must not think that the brick walls are 't there to keep us out, but rather in this way that the brick walls are there to show us how badly we want thinksk\n",
      "rick walls are there tor a reason and you must not think that the brick walls are 't there to keep us out, but rather in this way that the brick walls are there to show us how badly we want thinksk\n",
      "rick walls are there tor a reason and you must not think that the brick walls are 't there to keep us out, but rather in this way that the brick walls are there to show us how badly we want thinksk\n",
      "rick walls are there tor a reason and you must not think that the brick walls are 't there to keep us out, but rather in this way that the brick walls are there to show us how badly we want thinks.\n",
      "rick walls are there tor a reason and you must not think that the brick walls are 't there to keep us out, but rather in this way that the brick walls are there to show us how badly we want thinks.\n",
      "rick walls are there tor a reason and you must not think that the brick walls are 't there to keep us out, but rather in this way that the brick walls are there to show us how badly we want thinks.\n",
      "rick walls are there tor a reason and you must not think that the brick walls are 't there to keep us out, but rather in this way that the brick walls are there to show us how badly we want thinks.\n",
      "rick walls are there tor a reason and you must not think that the brick walls are 't there to keep us out, but rather in this way that the brick walls are there to show us how badly we want thinks.\n",
      "rick walls are there tor a reason and you must not think that the brick walls are 't there to keep us out, but rather in this way that the brick walls are there to show us how badly we want thinks.\n",
      "rick walls are there tor a reason and you must not think that the brick walls are 't there to keep us out, but rather in this way that the brick walls are there to show us how badly we want thinks.\n",
      "rick walls are there tor a reason and you must not think that the brick walls are 't there to keep us out, but rather in this way that the brick walls are there to show us how badly we want thinks.\n",
      "rick walls are there tor a reason and you must not think that the brick walls are 't there to keep us out, but rather in this way that the brick walls are there to show us how badly we want thinks.\n"
     ]
    }
   ],
   "source": [
    "##15. Training 시작\n",
    "\n",
    "for i in range(100):\n",
    "    optimizer.zero_grad()\n",
    "    outputs = net(X)\n",
    "    ##문제(7) : outputs, Y 형태 그대로 넣으면 안되죠. view 함수를 이용해 loss값을 계산해봅시다.\n",
    "    loss = criterion(outputs.view(-1, vocab_size), Y.view(-1))\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    #16. 예측결과 확인\n",
    "    results = outputs.argmax(dim=2)\n",
    "    predict_str = \"\"\n",
    "    for j, result in enumerate(results):\n",
    "        if j == 0: # 처음에는 예측 결과를 전부 가져오기\n",
    "            predict_str += ''.join([world_set[t] for t in result])\n",
    "        else: # 그 다음에는 마지막 글자만 반복 추가\n",
    "            predict_str += world_set[result[-1]]\n",
    "\n",
    "    print(predict_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"rick walls are there tor a reason and you must not think that the brick walls are 't there to keep us out, but rather in this way that the brick walls are there to show us how badly we want thinks.\""
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_str"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PkIzDTdyvTHz"
   },
   "source": [
    "결과가 어떤가요?? 마지막 에폭의 문장이 그럴싸한가요?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kN1zL8Dpvane"
   },
   "source": [
    "## Task2\n",
    "\n",
    "위 sentence는 제가 임의로 생성한 문장들입니다.\n",
    "\n",
    "마음에 드시는 문구 가져오셔서 문장이 어떻게 생성되는지 확인해보세요! \n",
    "\n",
    "영어가 아닌 한국어로 시도해보는 것도 좋겠죠? \n",
    "\n",
    "수정이 많이 필요(토큰화 등) 할 수 있으나 한번 시도해보시는 것 권장드립니다 :)\n",
    "\n",
    "위 베이스라인은 어디든 수정하셔도 좋고 조금 더 자연스러운 문장이 나올 수 있게 다양한 시도를 해보세요!\n",
    "\n",
    "조건 : 문장 3개 이상, 연결성이 있는 문장을 \" \" 으로 구분하여 ( )에 넣기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'직 정산요청 금액을 송금하지 않으셨나요? 금액 확인 후 송금해주세요. 정산요청 메시지나 정산현황을 통해 송금하지 않은 경우 알림 메시지가 발송될 수 있어요.'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#1. 생성할 문장 데이터\n",
    "\n",
    "sentence = (\"아직 정산요청 금액을 송금하지 않으셨나요? \"\n",
    "            \"금액 확인 후 송금해주세요. \"\n",
    "            \"정산요청 메시지나 정산현황을 통해 송금하지 않은 경우 알림 메시지가 발송될 수 있어요.\")\n",
    "\n",
    "#2. 문자 집합 만들기\n",
    "world_set = list(set(sentence))\n",
    "\n",
    "vocab = {char: i for i, char in enumerate(world_set)}\n",
    "\n",
    "#3. 문자 집합 크기 확인\n",
    "\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "#4. 하이퍼 파라미터 설정(자유롭게 수정해보세요!)\n",
    "\n",
    "hidden_size = vocab_size # 같아야 하는 것 확인!\n",
    "sequence_length = 10\n",
    "learning_rate = 0.01\n",
    "\n",
    "#5. seqence 길이 단위 자르기\n",
    "\n",
    "# 데이터 구성을 위한 리스트\n",
    "x_data = []\n",
    "y_data = []\n",
    "\n",
    "for i in range(0, len(sentence) - sequence_length):\n",
    "  x_str = sentence[i:i + sequence_length]\n",
    "  y_str = sentence[i+1 : i+1 + sequence_length]\n",
    "#   print(i, x_str, \"->\", y_str)\n",
    "\n",
    "  # x_str과 y_str이 문자집합에 해당하는 인덱스를 각각 x_data, y_data에 append\n",
    "  x_data.append([vocab[c] for c in x_str])\n",
    "  y_data.append([vocab[d] for d in y_str])\n",
    "    \n",
    "##6. 입력 시퀀스에 대해 원핫인코딩 수행\n",
    "\n",
    "x_one_hot = [np.eye(vocab_size)[x] for x in x_data]\n",
    "\n",
    "##7. 입력 데이터, 레이블데이터 텐서로 변환\n",
    "\n",
    "X = torch.FloatTensor(x_one_hot)\n",
    "Y = torch.LongTensor(y_data)\n",
    "\n",
    "##11. RNN 모델 구현\n",
    "\n",
    "##문제(6) : 기본 pytorch 인자 넣기 연습 + forward 채우기\n",
    "### 조건 : rnn layer 2개 쌓기 + 마지막은 fc layer\n",
    "### batch_first 설정 필요할까? (유튜브 강의 참고)\n",
    "\n",
    "class Net(torch.nn.Module):\n",
    "  def __init__(self, input_dim, hidden_dim, layers):\n",
    "    super(Net, self).__init__()\n",
    "    self.rnn = torch.nn.RNN(input_size = input_dim,\n",
    "                           hidden_size = hidden_dim,\n",
    "                           num_layers = layers,\n",
    "                           batch_first = True)\n",
    "    self.fc = torch.nn.Linear(hidden_dim, input_dim)\n",
    "  \n",
    "  def forward(self, x):\n",
    "    # RNN 레이어를 통해 x 처리\n",
    "    # 여기서 x의 차원은 (batch_size, seq_length, input_dim)\n",
    "    x, hidden = self.rnn(x)\n",
    "    \n",
    "    # Reshape output for fc layer\n",
    "    # fc layer는 2D input 받기 때문에\n",
    "    batch_size, sequence_length, hidden_dim = x.shape\n",
    "    x = x.contiguous().view(batch_size * sequence_length, hidden_dim)\n",
    "    # contiguous() : batch_size * seq_length를 유지하면서 나머지 차원을 평탄화\n",
    "    \n",
    "    # 선형 레이어 통과\n",
    "    x = self.fc(x)\n",
    "    \n",
    "    # Reshape back to (batch size, sequence length, output dimension)\n",
    "    x = x.view(batch_size, sequence_length, -1)\n",
    "    return x\n",
    "\n",
    "net = Net(vocab_size, hidden_size, 2)\n",
    "\n",
    "##12. loss function\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "##13. optimizer\n",
    "optimizer = optim.Adam(net.parameters(), learning_rate)\n",
    "\n",
    "##14. 출력 크기 점검\n",
    "outputs = net(X)\n",
    "\n",
    "##15. Training 시작\n",
    "\n",
    "for i in range(100):\n",
    "    optimizer.zero_grad()\n",
    "    outputs = net(X)\n",
    "    loss = criterion(outputs.view(-1, vocab_size), Y.view(-1))\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    #16. 예측결과 확인\n",
    "    results = outputs.argmax(dim=2)\n",
    "    predict_str = \"\"\n",
    "    for j, result in enumerate(results):\n",
    "        if j == 0: # 처음에는 예측 결과를 전부 가져오기\n",
    "            predict_str += ''.join([world_set[t] for t in result])\n",
    "        else: # 그 다음에는 마지막 글자만 반복 추가\n",
    "            predict_str += world_set[result[-1]]\n",
    "\n",
    "predict_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['은',\n",
       " '수',\n",
       " '금',\n",
       " '해',\n",
       " '주',\n",
       " '경',\n",
       " '하',\n",
       " '현',\n",
       " '후',\n",
       " '.',\n",
       " '정',\n",
       " '으',\n",
       " '발',\n",
       " '셨',\n",
       " '어',\n",
       " '있',\n",
       " '직',\n",
       " '산',\n",
       " '세',\n",
       " '우',\n",
       " ' ',\n",
       " '청',\n",
       " '인',\n",
       " '황',\n",
       " '될',\n",
       " '시',\n",
       " '확',\n",
       " '않',\n",
       " '통',\n",
       " '아',\n",
       " '가',\n",
       " '?',\n",
       " '액',\n",
       " '알',\n",
       " '요',\n",
       " '송',\n",
       " '나',\n",
       " '지',\n",
       " '메',\n",
       " '림',\n",
       " '을']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "world_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
