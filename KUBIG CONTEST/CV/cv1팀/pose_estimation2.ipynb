{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "UF5suHpCOZjg"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "import os\n",
        "from PIL import Image\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "5LR7GBbeOpv5"
      },
      "outputs": [],
      "source": [
        "def getKeypoints(probMap, threshold=0.1):\n",
        "    mapSmooth = cv2.GaussianBlur(probMap, (3,3), 0, 0)\n",
        "    mapMask = np.uint8(mapSmooth > threshold)\n",
        "    keypoints = []\n",
        "\n",
        "    contours, _ = cv2.findContours(mapMask, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
        "\n",
        "    for cnt in contours:\n",
        "        blobMask = np.zeros(mapMask.shape)\n",
        "        blobMask = cv2.fillConvexPoly(blobMask, cnt, 1)\n",
        "        maskedProbMap = mapSmooth * blobMask\n",
        "        _, maxVal, _, maxLoc = cv2.minMaxLoc(maskedProbMap)\n",
        "        keypoints.append((*maxLoc, maxVal))  # (x, y, confidence)\n",
        "\n",
        "    return keypoints"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "NZdXRMjTO0_k"
      },
      "outputs": [],
      "source": [
        "import numbers\n",
        "import logging\n",
        "logging.basicConfig(level=logging.DEBUG)\n",
        "def keypoints_to_vector(detected_keypoints, nPoints=18):\n",
        "    \"\"\"\n",
        "    OpenPose에서 탐지된 관절 위치 데이터를 특징 벡터로 변환\n",
        "\n",
        "    Parameters:\n",
        "    detected_keypoints (list): 각 관절의 탐지된 위치 정보를 담고 있는 list\n",
        "                               각 요소는 (x, y, confidence) 형태의t tuple\n",
        "    nPoints (int): 모델에서 탐지하는 관절의 총 개수 - coco 18개\n",
        "\n",
        "    Returns:\n",
        "    numpy.ndarray: 모든 관절 위치를 표현하는 1D 벡터. 관절이 탐지되지 않은 경우 (0, 0)처리\n",
        "    \"\"\"\n",
        "\n",
        "    # 각 관절의 x, y, confidence를 위한 공간을 확보\n",
        "    feature_vector = np.zeros(nPoints * 3)  # nPoints에 맞는 크기 확인\n",
        "\n",
        "    for i, keypoint in enumerate(detected_keypoints):\n",
        "        # i가 nPoints를 넘지 않도록 합니다. 루프는 0부터 nPoints-1까지만 돌아야 합니다.\n",
        "        if i >= nPoints:\n",
        "            break  # nPoints 이상의 인덱스에 대해서는 처리하지 않고 루프 종료\n",
        "\n",
        "        if keypoint:  # 관절이 탐지된 경우\n",
        "            x, y, confidence = keypoint\n",
        "            index = i * 3\n",
        "            feature_vector[index] = x\n",
        "            feature_vector[index + 1] = y\n",
        "            feature_vector[index + 2] = confidence\n",
        "\n",
        "    return feature_vector\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "qUCC4gsXO10a"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "def process_image_with_openpose(image, net, inHeight=368, nPoints=18):\n",
        "    frameWidth = image.shape[1]\n",
        "    frameHeight = image.shape[0]\n",
        "    inWidth = int((inHeight / frameHeight) * frameWidth)\n",
        "\n",
        "    inpBlob = cv2.dnn.blobFromImage(image, 1.0 / 255, (inWidth, inHeight), (0, 0, 0), swapRB=False, crop=False)\n",
        "    net.setInput(inpBlob)\n",
        "    output = net.forward()\n",
        "\n",
        "    detected_keypoints = []\n",
        "    threshold = 0.1\n",
        "    logging.debug(\"Processing image for keypoints...\")\n",
        "    for part in range(nPoints):\n",
        "        probMap = output[0, part, :, :]\n",
        "        probMap = cv2.resize(probMap, (frameWidth, frameHeight))\n",
        "        keypoints = getKeypoints(probMap, threshold)\n",
        "        logging.debug(f\"Part {part}: Detected {len(keypoints)} keypoints.\")\n",
        "        for keypoint in keypoints:\n",
        "            # getKeypoints 함수로부터 (x, y, confidence) 형태로 추출된 각 관절 정보를 detected_keypoints에 추가\n",
        "            logging.debug(f\"Keypoint: {keypoint}\")\n",
        "            detected_keypoints.append(keypoint)\n",
        "\n",
        "    # 모든 관절 정보를 특징 벡터로 변환\n",
        "    feature_vector = keypoints_to_vector(detected_keypoints, nPoints)\n",
        "    logging.debug(f\"Feature vector size: {len(feature_vector)}\")\n",
        "    logging.debug(f\"Feature vector: {feature_vector}\")\n",
        "    return feature_vector"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "5qmdmExrO6Q6"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "import os\n",
        "from PIL import Image\n",
        "\n",
        "class YogaPoseDataset(Dataset):\n",
        "    def __init__(self, csv_file, img_dir, net, transform=None):\n",
        "        self.annotations = pd.read_csv(csv_file)\n",
        "        self.annotations = self.annotations[self.annotations['pose_name'].str.lower().str.startswith('a')]\n",
        "        self.img_dir = img_dir\n",
        "        self.net = net\n",
        "        self.transform = transform\n",
        "\n",
        "        unique_pose_ids = sorted(self.annotations['pose_id'].unique())\n",
        "        self.pose_id_to_index = {pose_id: idx for idx, pose_id in enumerate(unique_pose_ids)}\n",
        "        print(f\"Filtered dataset size: {len(self.annotations)}\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.annotations)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        img_path = os.path.join(self.img_dir, self.annotations.iloc[index, 0])\n",
        "        image = Image.open(img_path).convert('RGB')\n",
        "        pose_id = self.annotations.iloc[index, 1]\n",
        "        pose_id = self.pose_id_to_index[pose_id]  # pose_id를 인덱스로\n",
        "        pose_id = torch.tensor(pose_id, dtype=torch.long)\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        # OpenPose에서 특징 벡터 추출\n",
        "        feature_vector = process_image_with_openpose(cv2.imread(img_path), self.net)\n",
        "\n",
        "        # 특징 벡터를 Tensor로 변환\n",
        "        feature_vector = torch.tensor(feature_vector, dtype=torch.float)\n",
        "\n",
        "        pose_id = torch.tensor(int(pose_id), dtype=torch.long)\n",
        "\n",
        "        return image, feature_vector, pose_id"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ICa92SKXPEgu",
        "outputId": "69210d96-aeaf-47e0-afaf-3fa6f153ffcd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Filtered dataset size: 171\n"
          ]
        }
      ],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "from torchvision import transforms\n",
        "\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    #transforms.ToPILImage(),\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # 정규화\n",
        "])\n",
        "\n",
        "\n",
        "protoFile = \"/content/drive/MyDrive/openpose/pose_deploy_linevec_faster_4_stages.prototxt\"\n",
        "weightsFile = \"/content/drive/MyDrive/openpose/pose_iter_160000.caffemodel\"\n",
        "net = cv2.dnn.readNetFromCaffe(protoFile, weightsFile)\n",
        "\n",
        "train_dataset = YogaPoseDataset(\n",
        "    csv_file='/content/drive/MyDrive/openpose/dataset/train_dataset.csv',\n",
        "    img_dir='/content/drive/MyDrive/openpose/dataset',\n",
        "    net=net,\n",
        "    transform=transform\n",
        ")\n",
        "\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "vOyKAzMWP9nF",
        "outputId": "5b4cf8f3-b097-47aa-9a9c-9b66938e0c30"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                file_name  pose_id      pose_name\n",
              "0  yoganidrasana/46-0.png       94  Yoganidrasana\n",
              "1   marjaryasana/32-0.png        7   Marjaryasana\n",
              "2  yoganidrasana/45-0.png       94  Yoganidrasana\n",
              "3   vasisthasana/30-0.png       73   Vasisthasana\n",
              "4       halasana/51-1.png       59       Halasana"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-d5f8158a-7e91-450f-9c63-e645add998f1\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>file_name</th>\n",
              "      <th>pose_id</th>\n",
              "      <th>pose_name</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>yoganidrasana/46-0.png</td>\n",
              "      <td>94</td>\n",
              "      <td>Yoganidrasana</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>marjaryasana/32-0.png</td>\n",
              "      <td>7</td>\n",
              "      <td>Marjaryasana</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>yoganidrasana/45-0.png</td>\n",
              "      <td>94</td>\n",
              "      <td>Yoganidrasana</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>vasisthasana/30-0.png</td>\n",
              "      <td>73</td>\n",
              "      <td>Vasisthasana</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>halasana/51-1.png</td>\n",
              "      <td>59</td>\n",
              "      <td>Halasana</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-d5f8158a-7e91-450f-9c63-e645add998f1')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-d5f8158a-7e91-450f-9c63-e645add998f1 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-d5f8158a-7e91-450f-9c63-e645add998f1');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-42271bf0-dd02-44c9-b97a-d7c8e0922f8d\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-42271bf0-dd02-44c9-b97a-d7c8e0922f8d')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-42271bf0-dd02-44c9-b97a-d7c8e0922f8d button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "data",
              "summary": "{\n  \"name\": \"data\",\n  \"rows\": 2471,\n  \"fields\": [\n    {\n      \"column\": \"file_name\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 2471,\n        \"samples\": [\n          \"dandasana/12. dandasana_yoga_posture.jpg\",\n          \"phalakasana/13-2.png\",\n          \"phalakasana/13-1.png\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"pose_id\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 31,\n        \"min\": 1,\n        \"max\": 105,\n        \"num_unique_values\": 55,\n        \"samples\": [\n          12,\n          82,\n          104\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"pose_name\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 55,\n        \"samples\": [\n          \"Bhujangasana\",\n          \"Vriksasana\",\n          \"Bhekasana\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# CSV 파일 경로\n",
        "csv_file_path = '/content/drive/MyDrive/openpose/dataset/train_dataset.csv'\n",
        "\n",
        "# 데이터프레임으로 읽기\n",
        "data = pd.read_csv(csv_file_path)\n",
        "data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EXCHGiXqVIyA",
        "outputId": "f3b41b11-c450-4209-a6c5-3f61a299100e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda', index=0)"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "import cv2\n",
        "import time\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import matplotlib\n",
        "\n",
        "from random import randint\n",
        "from pathlib import Path\n",
        "import json\n",
        "\n",
        "from collections import defaultdict\n",
        "\n",
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torch.nn as nn\n",
        "from torchvision.models import resnet34\n",
        "device = torch.device('cuda:0')\n",
        "device"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### RandomForest Classifier\n"
      ],
      "metadata": {
        "id": "NiiFSw6Xxdnw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Random Forest Classifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "model=RandomForestClassifier(n_estimators=100,\n",
        "                             criterion='gini',\n",
        "                             min_samples_split=2,\n",
        "                             min_samples_leaf=1,\n",
        "                             max_features='sqrt',\n",
        "                             bootstrap=False,\n",
        "                             random_state=1)\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "# 데이터 로드 및 준비\n",
        "features = []\n",
        "labels = []\n",
        "for i in range(len(train_dataset)):\n",
        "    image, feature_vector, pose_id = train_dataset[i]\n",
        "    features.append(feature_vector.numpy())\n",
        "    labels.append(int(pose_id))\n",
        "features = np.array(features)\n",
        "labels = np.array(labels)\n",
        "# 학습 및 테스트 데이터 분할\n",
        "X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.2, random_state=42)\n",
        "# 모델 정의\n",
        "model = RandomForestClassifier(n_estimators=100, criterion='gini', min_samples_split=2,\n",
        "                               min_samples_leaf=1, max_features='sqrt', bootstrap=False, random_state=1)\n",
        "# 모델 학습\n",
        "model.fit(X_train, y_train)\n",
        "# 예측 및 평가\n",
        "predictions = model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, predictions)\n",
        "print(f\"Accuracy: {accuracy * 100:.2f}%\")"
      ],
      "metadata": {
        "id": "PDBnHUKixzk0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### KNN classifier"
      ],
      "metadata": {
        "id": "dyI2GKe0xkUY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "features = []\n",
        "labels = []\n",
        "for i in range(len(train_dataset)):\n",
        "    _, feature_vector, pose_id = train_dataset[i]\n",
        "    features.append(feature_vector.numpy())\n",
        "    labels.append(int(pose_id))\n",
        "features = np.array(features)\n",
        "labels = np.array(labels)\n",
        "X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.2, random_state=42)\n",
        "knn = KNeighborsClassifier(n_neighbors=3)\n",
        "knn.fit(X_train, y_train)\n",
        "predictions = knn.predict(X_test)\n",
        "# 평가\n",
        "accuracy = accuracy_score(y_test, predictions)\n",
        "print(f\"KNN Accuracy: {accuracy * 100:.2f}%\")"
      ],
      "metadata": {
        "id": "226ZYiD4wsZF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.optim as optim\n",
        "def calculate_accuracy(outputs, targets):\n",
        "    _, predictions = torch.max(outputs, 1)\n",
        "    correct = (predictions == targets).sum().item()\n",
        "    total = targets.size(0)\n",
        "    accuracy = 100 * correct / total\n",
        "    return accuracy\n",
        "\n",
        "def validate(model, valid_loader, criterion):\n",
        "    model.eval()\n",
        "    valid_loss = 0\n",
        "    valid_accuracy = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, feature_vectors, pose_ids in valid_loader:\n",
        "            feature_vectors = feature_vectors.to(device)\n",
        "            pose_ids = pose_ids.to(device)\n",
        "            outputs = model(feature_vectors)\n",
        "            loss = criterion(outputs, pose_ids)\n",
        "            valid_loss += loss.item()\n",
        "            valid_accuracy += calculate_accuracy(outputs, pose_ids)\n",
        "\n",
        "    valid_loss /= len(valid_loader)\n",
        "    valid_accuracy /= len(valid_loader)\n",
        "    return valid_loss, valid_accuracy\n",
        "\n",
        "def train(model, train_loader, valid_loader, criterion, optimizer, epochs=10, print_every=20):\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        train_loss = 0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        for i, (images, feature_vectors, pose_ids) in enumerate(train_loader):\n",
        "            feature_vectors = feature_vectors.to(device)\n",
        "            pose_ids = pose_ids.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(feature_vectors)\n",
        "            loss = criterion(outputs, pose_ids)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            train_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += pose_ids.size(0)\n",
        "            correct += (predicted == pose_ids).sum().item()\n",
        "\n",
        "            if (i + 1) % print_every == 0:\n",
        "                print(f'Epoch {epoch+1}, Batch {i+1}, Loss: {train_loss / (i+1):.4f}')\n",
        "\n",
        "        train_accuracy = 100 * correct / total\n",
        "        valid_loss, valid_accuracy = validate(model, valid_loader, criterion)\n",
        "        print(f'End of Epoch {epoch+1}, '\n",
        "              f'Train Loss: {train_loss / len(train_loader):.4f}, '\n",
        "              f'Train Accuracy: {train_accuracy:.2f}%, '\n",
        "              f'Valid Loss: {valid_loss:.4f}, Valid Accuracy: {valid_accuracy:.2f}%\\n')\n",
        "\n",
        "\n",
        "\n",
        "valid_dataset = YogaPoseDataset(\n",
        "    csv_file='/content/drive/MyDrive/openpose/dataset/valid_dataset.csv',\n",
        "    img_dir='/content/drive/MyDrive/openpose/dataset',\n",
        "    net=net,\n",
        "    transform=transform\n",
        ")\n",
        "valid_loader = DataLoader(valid_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "train(model, train_loader, valid_loader, criterion, optimizer, epochs=10, print_every=20)"
      ],
      "metadata": {
        "id": "II3aEuMiwUDq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2 layer MLP"
      ],
      "metadata": {
        "id": "9R0ZPRWWxrE3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qEiLWt09Rujx",
        "outputId": "67f083f4-a439-463f-a783-fae237dd1366"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "device(type='cuda', index=0)"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class PoseClassifier(nn.Module):\n",
        "    def __init__(self, input_size, num_classes):\n",
        "        super(PoseClassifier, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_size, 64)\n",
        "        self.bn1 = nn.BatchNorm1d(64)\n",
        "        self.activation = nn.ReLU()\n",
        "        self.dropout = nn.Dropout(0.2)\n",
        "        self.fc2 = nn.Linear(64, 64)\n",
        "        self.bn2 = nn.BatchNorm1d(64)\n",
        "        self.outlayer = nn.Linear(64, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.activation(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.bn2(x)\n",
        "        x = self.activation(x)\n",
        "        x = self.outlayer(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "#(18개 관절 * 3 좌표)\n",
        "model = PoseClassifier(input_size=54, num_classes=13)\n",
        "model = model.to(device)\n",
        "device"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "enYXdBl75Sib"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fGBXmI0LSvqF",
        "outputId": "bfa321ae-6e7b-473c-aea1-ab693574221d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Filtered dataset size: 41\n",
            "End of Epoch 1, Train Loss: 13.3535, Train Accuracy: 25.15%, Valid Loss: 5.7393, Valid Accuracy: 43.40%\n",
            "\n",
            "End of Epoch 2, Train Loss: 7.1545, Train Accuracy: 36.84%, Valid Loss: 4.9700, Valid Accuracy: 29.17%\n",
            "\n",
            "End of Epoch 3, Train Loss: 4.9706, Train Accuracy: 42.11%, Valid Loss: 3.9035, Valid Accuracy: 42.53%\n",
            "\n",
            "End of Epoch 4, Train Loss: 3.0009, Train Accuracy: 47.95%, Valid Loss: 3.5891, Valid Accuracy: 40.10%\n",
            "\n",
            "End of Epoch 5, Train Loss: 3.4937, Train Accuracy: 51.46%, Valid Loss: 3.2360, Valid Accuracy: 40.10%\n",
            "\n",
            "End of Epoch 6, Train Loss: 2.6861, Train Accuracy: 54.39%, Valid Loss: 2.9362, Valid Accuracy: 48.78%\n",
            "\n",
            "End of Epoch 7, Train Loss: 2.4243, Train Accuracy: 57.31%, Valid Loss: 2.4216, Valid Accuracy: 47.22%\n",
            "\n",
            "End of Epoch 8, Train Loss: 2.2075, Train Accuracy: 57.31%, Valid Loss: 2.2772, Valid Accuracy: 48.78%\n",
            "\n",
            "End of Epoch 9, Train Loss: 1.7808, Train Accuracy: 57.31%, Valid Loss: 2.3315, Valid Accuracy: 48.78%\n",
            "\n",
            "End of Epoch 10, Train Loss: 1.2896, Train Accuracy: 60.82%, Valid Loss: 2.4097, Valid Accuracy: 41.67%\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.optim as optim\n",
        "def calculate_accuracy(outputs, targets):\n",
        "    _, predictions = torch.max(outputs, 1)\n",
        "    correct = (predictions == targets).sum().item()\n",
        "    total = targets.size(0)\n",
        "    accuracy = 100 * correct / total\n",
        "    return accuracy\n",
        "\n",
        "def validate(model, valid_loader, criterion):\n",
        "    model.eval()\n",
        "    valid_loss = 0\n",
        "    valid_accuracy = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, feature_vectors, pose_ids in valid_loader:\n",
        "            feature_vectors = feature_vectors.to(device)\n",
        "            pose_ids = pose_ids.to(device)\n",
        "            outputs = model(feature_vectors)\n",
        "            loss = criterion(outputs, pose_ids)\n",
        "            valid_loss += loss.item()\n",
        "            valid_accuracy += calculate_accuracy(outputs, pose_ids)\n",
        "\n",
        "    valid_loss /= len(valid_loader)\n",
        "    valid_accuracy /= len(valid_loader)\n",
        "    return valid_loss, valid_accuracy\n",
        "\n",
        "def train(model, train_loader, valid_loader, criterion, optimizer, epochs=10, print_every=20):\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        train_loss = 0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        for i, (images, feature_vectors, pose_ids) in enumerate(train_loader):\n",
        "            feature_vectors = feature_vectors.to(device)\n",
        "            pose_ids = pose_ids.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(feature_vectors)\n",
        "            loss = criterion(outputs, pose_ids)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            train_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += pose_ids.size(0)\n",
        "            correct += (predicted == pose_ids).sum().item()\n",
        "\n",
        "            if (i + 1) % print_every == 0:\n",
        "                print(f'Epoch {epoch+1}, Batch {i+1}, Loss: {train_loss / (i+1):.4f}')\n",
        "\n",
        "        train_accuracy = 100 * correct / total\n",
        "        valid_loss, valid_accuracy = validate(model, valid_loader, criterion)\n",
        "        print(f'End of Epoch {epoch+1}, '\n",
        "              f'Train Loss: {train_loss / len(train_loader):.4f}, '\n",
        "              f'Train Accuracy: {train_accuracy:.2f}%, '\n",
        "              f'Valid Loss: {valid_loss:.4f}, Valid Accuracy: {valid_accuracy:.2f}%\\n')\n",
        "\n",
        "\n",
        "\n",
        "valid_dataset = YogaPoseDataset(\n",
        "    csv_file='/content/drive/MyDrive/openpose/dataset/valid_dataset.csv',\n",
        "    img_dir='/content/drive/MyDrive/openpose/dataset',\n",
        "    net=net,\n",
        "    transform=transform\n",
        ")\n",
        "valid_loader = DataLoader(valid_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "train(model, train_loader, valid_loader, criterion, optimizer, epochs=10, print_every=20)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}