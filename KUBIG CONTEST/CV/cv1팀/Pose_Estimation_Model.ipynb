{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5sXPKceYsnQd"
      },
      "source": [
        "### **Install all**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3bu2VSwxIzf_"
      },
      "outputs": [],
      "source": [
        "!pip install opencv-python\n",
        "!pip install torch\n",
        "!pip install requests"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WDqrK0NHV7Z0"
      },
      "outputs": [],
      "source": [
        "!pip install timm\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_PRMoOd2ZE3_"
      },
      "outputs": [],
      "source": [
        "!pip install torch-geometric\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4ZJG-sO4Kq8d"
      },
      "outputs": [],
      "source": [
        "cd drive/MyDrive/OpenPose-Pose-Estimation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "aKQNbx7uKcdd"
      },
      "outputs": [],
      "source": [
        "#import packages\n",
        "\n",
        "import cv2\n",
        "import time\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import matplotlib\n",
        "\n",
        "from random import randint\n",
        "from pathlib import Path\n",
        "import json\n",
        "\n",
        "from collections import defaultdict\n",
        "\n",
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torch.nn as nn\n",
        "from torchvision.models import resnet34\n",
        "#device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "from custom_datasets import YogaPoseDataset\n",
        "\n",
        "import model_utils\n",
        "import plot_utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-2KfxcCPVJfs"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "9L7UpDsiKeIE"
      },
      "outputs": [],
      "source": [
        "#import model file and define pairs of pose\n",
        "protoFile = \"/content/drive/MyDrive/OpenPose-Pose-Estimation/pose/pose_deploy_linevec.prototxt\"\n",
        "weightsFile = \"/content/drive/MyDrive/OpenPose-Pose-Estimation/pose/pose_iter_440000.caffemodel\"\n",
        "\n",
        "nPoints = 18\n",
        "# COCO Output Format\n",
        "keypointsMapping = ['Nose', 'Neck', 'R-Sho', 'R-Elb', 'R-Wr', 'L-Sho',\n",
        "                    'L-Elb', 'L-Wr', 'R-Hip', 'R-Knee', 'R-Ank', 'L-Hip',\n",
        "                    'L-Knee', 'L-Ank', 'R-Eye', 'L-Eye', 'R-Ear', 'L-Ear']\n",
        "\n",
        "POSE_PAIRS = [[1,2], [1,5], [2,3], [3,4], [5,6], [6,7],\n",
        "              [1,8], [8,9], [9,10], [1,11], [11,12], [12,13],\n",
        "              [1,0], [0,14], [14,16], [0,15], [15,17],\n",
        "              [2,17], [5,16] ]\n",
        "\n",
        "mapIdx = [[31,32], [39,40], [33,34], [35,36], [41,42], [43,44],\n",
        "          [19,20], [21,22], [23,24], [25,26], [27,28], [29,30],\n",
        "          [47,48], [49,50], [53,54], [51,52], [55,56],\n",
        "          [37,38], [45,46]]\n",
        "\n",
        "colors = [ [0,100,255], [0,100,255], [0,255,255], [0,100,255], [0,255,255], [0,100,255],\n",
        "         [0,255,0], [255,200,100], [255,0,255], [0,255,0], [255,200,100], [255,0,255],\n",
        "         [0,0,255], [255,0,0], [200,200,0], [255,0,0], [200,200,0], [0,0,0]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "yDpMvQFGMU7J"
      },
      "outputs": [],
      "source": [
        "#Reading JSON file\n",
        "pose_list = json.load(Path('pose-list-with-meta.json').open())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "guE0hXCNyevs"
      },
      "outputs": [],
      "source": [
        "#Read classifier csv file\n",
        "val_csv = pd.read_csv('/content/drive/MyDrive/OpenPose-Pose-Estimation/val_data.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "07u6F7s7MWrj"
      },
      "outputs": [],
      "source": [
        "#Encode pose\n",
        "pose_id_to_name = {0: 'Bharadvajasana I', 1: 'Padangusthasana', 2: 'Paripurna Navasana', 3: 'Baddha Konasana', 4: 'Dhanurasana', 5: 'Setu Bandha Sarvangasana', 6: 'Ustrasana', 7: 'Marjaryasana', 8: 'Chakravakasana', 9: 'Ashtanga Namaskara', 10: 'Utkatasana', 11: 'Balasana', 12: 'Bhujangasana', 13: 'Savasana', 14: 'Gomukhasana', 15: 'Bitilasana', 16: 'Bakasana', 17: 'Makara Adho Mukha Svanasana', 18: 'Ardha Pincha Mayurasana', 19: 'Adho Mukha Svanasana', 20: 'Garudasana', 21: 'Sukhasana', 22: 'Astavakrasana', 23: 'Utthita Hasta Padangustasana', 24: 'Uttana Shishosana', 25: 'Utthita Parsvakonasana', 26: 'Utthita Trikonasana', 27: 'Pincha Mayurasana', 28: 'Agnistambhasana', 29: 'Tittibhasana', 30: 'Matsyasana', 31: 'Chaturanga Dandasana', 32: 'Malasana', 33: 'Parighasana', 34: 'Ardha Bhekasana', 35: 'Ardha Matsyendrasana', 36: 'Supta Matsyendrasana', 37: 'Ardha Chandrasana', 38: 'Adho Mukha Vriksasana', 39: 'Ananda Balasana', 40: 'Janu Sirsasana', 41: 'Virasana', 42: 'Krounchasana', 43: 'Utthita Ashwa Sanchalanasana', 44: 'Parsvottanasana', 45: 'Viparita Karani', 46: 'Salabhasana', 47: 'Natarajasana', 48: 'Padmasana', 49: 'Anjaneyasana', 50: 'Marichyasana III', 51: 'Hanumanasana', 52: 'Tadasana', 53: 'Pasasana', 54: 'Eka Pada Rajakapotasana', 55: 'Eka Pada Rajakapotasana II', 56: 'Mayurasana', 57: 'Kapotasana', 58: 'Phalakasana', 59: 'Halasana', 60: 'Eka Pada Koundinyanasana I', 61: 'Eka Pada Koundinyanasana II', 62: 'Marichyasana I', 63: 'Supta Baddha Konasana', 64: 'Supta Padangusthasana', 65: 'Supta Virasana', 66: 'Parivrtta Janu Sirsasana', 67: 'Parivrtta Parsvakonasana', 68: 'Parivrtta Trikonasana', 69: 'Tolasana', 70: 'Paschimottanasana', 72: 'Parsva Bakasana', 73: 'Vasisthasana', 74: 'Anantasana', 75: 'Salamba Bhujangasana', 76: 'Dandasana', 77: 'Uttanasana', 78: 'Ardha Uttanasana', 79: 'Urdhva Prasarita Eka Padasana', 80: 'Salamba Sirsasana', 81: 'Salamba Sarvangasana', 82: 'Vriksasana', 83: 'Urdhva Dhanurasana', 84: 'Dwi Pada Viparita Dandasana', 85: 'Purvottanasana', 86: 'Urdhva Hastasana', 87: 'Urdhva Mukha Svanasana', 88: 'Virabhadrasana I', 89: 'Virabhadrasana II', 90: 'Virabhadrasana III', 91: 'Upavistha Konasana', 92: 'Prasarita Padottanasana', 93: 'Camatkarasana', 94: 'Yoganidrasana', 95: 'Vrischikasana', 96: 'Vajrasana', 97: 'Tulasana', 98: 'Simhasana', 99: 'Makarasana', 100: 'Lolasana', 101: 'Kurmasana', 102: 'Garbha Pindasana', 103: 'Durvasasana', 71: 'Bhujapidasana', 104: 'Bhekasana', 105: 'Bhairavasana', 106: 'Ganda Bherundasana'}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "zvd8EU_SMXuo"
      },
      "outputs": [],
      "source": [
        "#Decode pose\n",
        "pose_name_to_id = {'bharadvajasana i': 0, 'padangusthasana': 1, 'paripurna navasana': 2, 'baddha konasana': 3, 'dhanurasana': 4, 'setu bandha sarvangasana': 5, 'ustrasana': 6, 'marjaryasana': 7, 'chakravakasana': 8, 'ashtanga namaskara': 9, 'utkatasana': 10, 'balasana': 11, 'bhujangasana': 12, 'savasana': 13, 'gomukhasana': 14, 'bitilasana': 15, 'bakasana': 16, 'makara adho mukha svanasana': 17, 'ardha pincha mayurasana': 18, 'adho mukha svanasana': 19, 'garudasana': 20, 'sukhasana': 21, 'astavakrasana': 22, 'utthita hasta padangustasana': 23, 'uttana shishosana': 24, 'utthita parsvakonasana': 25, 'utthita trikonasana': 26, 'pincha mayurasana': 27, 'agnistambhasana': 28, 'tittibhasana': 29, 'matsyasana': 30, 'chaturanga dandasana': 31, 'malasana': 32, 'parighasana': 33, 'ardha bhekasana': 34, 'ardha matsyendrasana': 35, 'supta matsyendrasana': 36, 'ardha chandrasana': 37, 'adho mukha vriksasana': 38, 'ananda balasana': 39, 'janu sirsasana': 40, 'virasana': 41, 'krounchasana': 42, 'utthita ashwa sanchalanasana': 43, 'parsvottanasana': 44, 'viparita karani': 45, 'salabhasana': 46, 'natarajasana': 47, 'padmasana': 48, 'anjaneyasana': 49, 'marichyasana iii': 50, 'hanumanasana': 51, 'tadasana': 52, 'pasasana': 53, 'eka pada rajakapotasana': 54, 'eka pada rajakapotasana ii': 55, 'mayurasana': 56, 'kapotasana': 57, 'phalakasana': 58, 'halasana': 59, 'eka pada koundinyanasana i': 60, 'eka pada koundinyanasana ii': 61, 'marichyasana i': 62, 'supta baddha konasana': 63, 'supta padangusthasana': 64, 'supta virasana': 65, 'parivrtta janu sirsasana': 66, 'parivrtta parsvakonasana': 67, 'parivrtta trikonasana': 68, 'tolasana': 69, 'paschimottanasana': 70, 'parsva bakasana': 72, 'vasisthasana': 73, 'anantasana': 74, 'salamba bhujangasana': 75, 'dandasana': 76, 'uttanasana': 77, 'ardha uttanasana': 78, 'urdhva prasarita eka padasana': 79, 'salamba sirsasana': 80, 'salamba sarvangasana': 81, 'vriksasana': 82, 'urdhva dhanurasana': 83, 'dwi pada viparita dandasana': 84, 'purvottanasana': 85, 'urdhva hastasana': 86, 'urdhva mukha svanasana': 87, 'virabhadrasana i': 88, 'virabhadrasana ii': 89, 'virabhadrasana iii': 90, 'upavistha konasana': 91, 'prasarita padottanasana': 92, 'camatkarasana': 93, 'yoganidrasana': 94, 'vrischikasana': 95, 'vajrasana': 96, 'tulasana': 97, 'simhasana': 98, 'makarasana': 99, 'lolasana': 100, 'kurmasana': 101, 'garbha pindasana': 102, 'durvasasana': 103, 'bhujapidasana': 71, 'bhekasana': 104, 'bhairavasana': 105, 'ganda bherundasana': 106}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "o8QF_oSSMZ4w"
      },
      "outputs": [],
      "source": [
        "#Read classifier csv file\n",
        "train_csv = pd.read_csv('/content/drive/MyDrive/OpenPose-Pose-Estimation/train_data.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gTaw5yYlrDKO"
      },
      "source": [
        "### **Openpose에서 keypoint 받아와서 vector 로 변환**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "mzlalDyGwiH3"
      },
      "outputs": [],
      "source": [
        "\n",
        "def getKeypoints(probMap, threshold=0.1):\n",
        "    mapSmooth = cv2.GaussianBlur(probMap, (3,3), 0, 0)\n",
        "    mapMask = np.uint8(mapSmooth > threshold)\n",
        "    keypoints = []\n",
        "\n",
        "    contours, _ = cv2.findContours(mapMask, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
        "\n",
        "    for cnt in contours:\n",
        "        blobMask = np.zeros(mapMask.shape)\n",
        "        blobMask = cv2.fillConvexPoly(blobMask, cnt, 1)\n",
        "        maskedProbMap = mapSmooth * blobMask\n",
        "        _, maxVal, _, maxLoc = cv2.minMaxLoc(maskedProbMap)\n",
        "        keypoints.append((*maxLoc, maxVal))  # (x, y, confidence)\n",
        "\n",
        "    return keypoints\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "BRw-eqSR3b06"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import numbers\n",
        "import logging\n",
        "logging.basicConfig(level=logging.DEBUG)\n",
        "def keypoints_to_vector(detected_keypoints, nPoints=18):\n",
        "    \"\"\"\n",
        "    OpenPose에서 탐지된 관절 위치 데이터를 특징 벡터로 변환\n",
        "\n",
        "    Parameters:\n",
        "    detected_keypoints (list): 각 관절의 탐지된 위치 정보를 담고 있는 list\n",
        "                               각 요소는 (x, y, confidence) 형태의t tuple\n",
        "    nPoints (int): 모델에서 탐지하는 관절의 총 개수 - coco 18개\n",
        "\n",
        "    Returns:\n",
        "    numpy.ndarray: 모든 관절 위치를 표현하는 1D 벡터. 관절이 탐지되지 않은 경우 (0, 0)처리\n",
        "    \"\"\"\n",
        "\n",
        "    # 각 관절의 x, y, confidence를 위한 공간을 확보\n",
        "    feature_vector = np.zeros(nPoints * 3)  # nPoints에 맞는 크기 확인\n",
        "\n",
        "    for i, keypoint in enumerate(detected_keypoints):\n",
        "        if i >= nPoints:\n",
        "            break  # nPoints 이상의 인덱스에 대해서는 처리하지 않고 종료\n",
        "\n",
        "        if keypoint:  # 관절이 탐지된 경우\n",
        "            x, y, confidence = keypoint\n",
        "            index = i * 3\n",
        "            feature_vector[index] = x\n",
        "            feature_vector[index + 1] = y\n",
        "            feature_vector[index + 2] = confidence\n",
        "\n",
        "    return feature_vector\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9_k66g5Jwrdu"
      },
      "outputs": [],
      "source": [
        "\n",
        "# 확인용\n",
        "detected_keypoints = [\n",
        "    (100, 200, 0.9),  # Nose\n",
        "    None,             # Neck (탐지되지 않음)\n",
        "    (150, 250, 0.8),  # R-Sho\n",
        "    # ... 나머지 관절들\n",
        "]\n",
        "\n",
        "# 특징 벡터 변환\n",
        "feature_vector = keypoints_to_vector(detected_keypoints)\n",
        "print(\"특징 벡터:\", feature_vector)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "WzQ4FoXZ_vl8"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "def process_image_with_openpose(image, net, inHeight=368, nPoints=18):\n",
        "    frameWidth = image.shape[1]\n",
        "    frameHeight = image.shape[0]\n",
        "    inWidth = int((inHeight / frameHeight) * frameWidth)\n",
        "\n",
        "    inpBlob = cv2.dnn.blobFromImage(image, 1.0 / 255, (inWidth, inHeight), (0, 0, 0), swapRB=False, crop=False)\n",
        "    net.setInput(inpBlob)\n",
        "    output = net.forward()\n",
        "\n",
        "    detected_keypoints = []\n",
        "    threshold = 0.1\n",
        "    logging.debug(\"Processing image for keypoints...\")\n",
        "    for part in range(nPoints):\n",
        "        probMap = output[0, part, :, :]\n",
        "        probMap = cv2.resize(probMap, (frameWidth, frameHeight))\n",
        "        keypoints = getKeypoints(probMap, threshold)\n",
        "        logging.debug(f\"Part {part}: Detected {len(keypoints)} keypoints.\")\n",
        "        for keypoint in keypoints:\n",
        "            # getKeypoints 함수로부터 (x, y, confidence) 형태로 추출된 각 관절 정보를 detected_keypoints에 추가\n",
        "            logging.debug(f\"Keypoint: {keypoint}\")\n",
        "            detected_keypoints.append(keypoint)\n",
        "\n",
        "    # 모든 관절 정보를 특징 벡터로 변환\n",
        "    feature_vector = keypoints_to_vector(detected_keypoints, nPoints)\n",
        "    logging.debug(f\"Feature vector size: {len(feature_vector)}\")\n",
        "    logging.debug(f\"Feature vector: {feature_vector}\")\n",
        "    return feature_vector\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XwYRsJsNq1hb"
      },
      "source": [
        "## **DATA PREPROCESS**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "ayZywfZFBjiR"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "import os\n",
        "from PIL import Image\n",
        "\n",
        "class YogaPoseDataset(Dataset):\n",
        "    '''\n",
        "    #전체데이터\n",
        "    def __init__(self, csv_file, img_dir, net, transform=None):\n",
        "        self.annotations = pd.read_csv(csv_file)\n",
        "        self.img_dir = img_dir\n",
        "        self.net = net\n",
        "        self.transform = transform\n",
        "\n",
        "        unique_pose_ids = sorted(self.annotations['pose_id'].unique())\n",
        "        self.pose_id_to_index = {pose_id: idx for idx, pose_id in enumerate(unique_pose_ids)}\n",
        "    '''\n",
        "\n",
        "    #a로시작하는 데이터만\n",
        "    def __init__(self, csv_file, img_dir, net, transform=None):\n",
        "        self.annotations = pd.read_csv(csv_file)\n",
        "        self.annotations = self.annotations[self.annotations['pose_name'].str.lower().str.startswith('a')]\n",
        "        self.img_dir = img_dir\n",
        "        self.net = net\n",
        "        self.transform = transform\n",
        "\n",
        "        unique_pose_ids = sorted(self.annotations['pose_id'].unique())\n",
        "        self.pose_id_to_index = {pose_id: idx for idx, pose_id in enumerate(unique_pose_ids)}\n",
        "        print(f\"Filtered dataset size: {len(self.annotations)}\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.annotations)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        img_path = os.path.join(self.img_dir, self.annotations.iloc[index, 0])\n",
        "        image = Image.open(img_path).convert('RGB')\n",
        "        pose_id = self.annotations.iloc[index, 1]\n",
        "        pose_id = self.pose_id_to_index[pose_id]  # pose_id를 인덱스로\n",
        "        pose_id = torch.tensor(pose_id, dtype=torch.long)\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        # OpenPose에서 피쳐 벡터 추출\n",
        "        feature_vector = process_image_with_openpose(cv2.imread(img_path), self.net)\n",
        "\n",
        "        # 피쳐 벡터를 Tensor로 변환\n",
        "        feature_vector = torch.tensor(feature_vector, dtype=torch.float)\n",
        "\n",
        "        pose_id = torch.tensor(int(pose_id), dtype=torch.long)\n",
        "\n",
        "\n",
        "        return image, feature_vector, pose_id\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l8LEK5kpDEgG",
        "outputId": "c84ab388-6d3d-4ab1-c05d-f3d6bc52bfc8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Filtered dataset size: 171\n"
          ]
        }
      ],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "from torchvision import transforms\n",
        "\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    #transforms.ToPILImage(),\n",
        "    transforms.Resize((256, 256)),\n",
        "    #transforms.Resize((112, 112)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # 정규화\n",
        "])\n",
        "\n",
        "\n",
        "net = cv2.dnn.readNetFromCaffe(protoFile, weightsFile)\n",
        "\n",
        "train_dataset = YogaPoseDataset(\n",
        "    csv_file='/content/drive/MyDrive/dataset/train_dataset.csv',\n",
        "    img_dir='/content/drive/MyDrive/dataset',\n",
        "    net=net,\n",
        "    transform=transform\n",
        ")\n",
        "\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dI9ik-f2rRAF"
      },
      "source": [
        "### **1. Simple MLP Model** (INPUT - 관절 피쳐벡터 + pose_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9ET-NNaNDHOy"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class PoseClassifier(nn.Module):\n",
        "    def __init__(self, input_size, num_classes):\n",
        "        super(PoseClassifier, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_size, 256)\n",
        "        self.bn1 = nn.BatchNorm1d(256)\n",
        "        self.dropout1 = nn.Dropout(0.5)\n",
        "\n",
        "        self.fc2 = nn.Linear(256, 512)\n",
        "        self.bn2 = nn.BatchNorm1d(512)\n",
        "        self.dropout2 = nn.Dropout(0.5)\n",
        "\n",
        "        self.fc3 = nn.Linear(512, 1024)\n",
        "        self.bn3 = nn.BatchNorm1d(1024)\n",
        "        self.dropout3 = nn.Dropout(0.5)\n",
        "\n",
        "        self.fc4 = nn.Linear(1024, 512)\n",
        "        self.bn4 = nn.BatchNorm1d(512)\n",
        "        self.dropout4 = nn.Dropout(0.5)\n",
        "\n",
        "        self.fc5 = nn.Linear(512, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.bn1(x)\n",
        "        x = self.dropout1(x)\n",
        "\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.bn2(x)\n",
        "        x = self.dropout2(x)\n",
        "\n",
        "        x = F.relu(self.fc3(x))\n",
        "        x = self.bn3(x)\n",
        "        x = self.dropout3(x)\n",
        "\n",
        "        x = F.relu(self.fc4(x))\n",
        "        x = self.bn4(x)\n",
        "        x = self.dropout4(x)\n",
        "\n",
        "        x = self.fc5(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "#(18개 관절 * 3 좌표)\n",
        "model = PoseClassifier(input_size=54, num_classes=13)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = model.to(device)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "msqa3DZ4VRUA"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.optim as optim\n",
        "def calculate_accuracy(outputs, targets):\n",
        "    _, predictions = torch.max(outputs, 1)\n",
        "    correct = (predictions == targets).sum().item()\n",
        "    total = targets.size(0)\n",
        "    accuracy = 100 * correct / total\n",
        "    return accuracy\n",
        "\n",
        "def validate(model, valid_loader, criterion):\n",
        "    model.eval()\n",
        "    valid_loss = 0\n",
        "    valid_accuracy = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, feature_vectors, pose_ids in valid_loader:\n",
        "            feature_vectors = feature_vectors.to(device)\n",
        "            pose_ids = pose_ids.to(device)\n",
        "            outputs = model(feature_vectors)\n",
        "            loss = criterion(outputs, pose_ids)\n",
        "            valid_loss += loss.item()\n",
        "            valid_accuracy += calculate_accuracy(outputs, pose_ids)\n",
        "\n",
        "    valid_loss /= len(valid_loader)\n",
        "    valid_accuracy /= len(valid_loader)\n",
        "    return valid_loss, valid_accuracy\n",
        "\n",
        "def train(model, train_loader, valid_loader, criterion, optimizer, epochs=10, print_every=20):\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        train_loss = 0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        for i, (images, feature_vectors, pose_ids) in enumerate(train_loader):\n",
        "            feature_vectors = feature_vectors.to(device)\n",
        "            pose_ids = pose_ids.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(feature_vectors)\n",
        "            loss = criterion(outputs, pose_ids)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            train_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += pose_ids.size(0)\n",
        "            correct += (predicted == pose_ids).sum().item()\n",
        "\n",
        "            if (i + 1) % print_every == 0:\n",
        "                print(f'Epoch {epoch+1}, Batch {i+1}, Loss: {train_loss / (i+1):.4f}')\n",
        "\n",
        "        train_accuracy = 100 * correct / total\n",
        "        valid_loss, valid_accuracy = validate(model, valid_loader, criterion)\n",
        "        print(f'End of Epoch {epoch+1}, '\n",
        "              f'Train Loss: {train_loss / len(train_loader):.4f}, '\n",
        "              f'Train Accuracy: {train_accuracy:.2f}%, '\n",
        "              f'Valid Loss: {valid_loss:.4f}, Valid Accuracy: {valid_accuracy:.2f}%\\n')\n",
        "\n",
        "\n",
        "\n",
        "valid_dataset = YogaPoseDataset(\n",
        "    csv_file='/content/drive/MyDrive/dataset/valid_dataset.csv',\n",
        "    img_dir='/content/drive/MyDrive/dataset',\n",
        "    net=net,\n",
        "    transform=transform\n",
        ")\n",
        "valid_loader = DataLoader(valid_dataset, batch_size=32, shuffle=True)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.0008)\n",
        "\n",
        "train(model, train_loader, valid_loader, criterion, optimizer, epochs=20, print_every=20)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **2. **4. ResNetFeatures(image) +MLP CLASSIFIER (I관절 피쳐벡터)**"
      ],
      "metadata": {
        "id": "jQRA_wrz-qHz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchvision import models\n",
        "\n",
        "class ImageVectorCombinedModel(nn.Module):\n",
        "    def __init__(self, vector_input_size, num_classes):\n",
        "        super(ImageVectorCombinedModel, self).__init__()\n",
        "        self.resnet = models.resnet18(pretrained=True)\n",
        "        self.resnet.fc = nn.Identity()  # 마지막 fc layer -> nn.identity로 교체해서 feature 그대로 받아옴\n",
        "        # 벡터 MLP\n",
        "        self.vector_processor = nn.Sequential(\n",
        "            nn.Linear(vector_input_size, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(256, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, 1024),\n",
        "            nn.ReLU(),\n",
        "        )\n",
        "\n",
        "        # IMAGE Feature 차원 줄이고 Vector Feature 차원 늘림\n",
        "        self.image_dim_reducer = nn.Linear(512, 128)\n",
        "        self.combined_fc = nn.Sequential(\n",
        "            nn.Linear(128 + 1024, 512),  # Combine\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(256, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, images, vectors):\n",
        "        image_out = self.resnet(images)\n",
        "        image_out = self.image_dim_reducer(image_out)\n",
        "\n",
        "        vector_out = self.vector_processor(vectors)\n",
        "\n",
        "        combined_out = torch.cat((image_out, vector_out), dim=1)  # Concatenate\n",
        "        out = self.combined_fc(combined_out)  # Final classification\n",
        "        return out\n",
        "model = ImageVectorCombinedModel(vector_input_size=54, num_classes=13)\n"
      ],
      "metadata": {
        "id": "QWl_r_wR-uZ-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.optim as optim\n",
        "\n",
        "\n",
        "def calculate_accuracy(outputs, targets):\n",
        "    _, predictions = torch.max(outputs, 1)\n",
        "    correct = (predictions == targets).sum().item()\n",
        "    total = targets.size(0)\n",
        "    accuracy = 100 * correct / total\n",
        "    return accuracy\n",
        "\n",
        "def validate(model, valid_loader, criterion, device):\n",
        "    model.eval()\n",
        "    valid_loss = 0\n",
        "    valid_accuracy = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, vectors, pose_ids in valid_loader:\n",
        "            images = images.to(device)\n",
        "            vectors = vectors.to(device)\n",
        "            pose_ids = pose_ids.to(device)\n",
        "\n",
        "            outputs = model(images, vectors)\n",
        "            loss = criterion(outputs, pose_ids)\n",
        "            valid_loss += loss.item()\n",
        "            valid_accuracy += calculate_accuracy(outputs, pose_ids)\n",
        "\n",
        "    valid_loss /= len(valid_loader)\n",
        "    valid_accuracy /= len(valid_loader)\n",
        "    return valid_loss, valid_accuracy\n",
        "\n",
        "\n",
        "import os\n",
        "\n",
        "def train(model, train_loader, valid_loader, criterion, optimizer, device, epochs, print_every=20):\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        train_loss = 0.0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        for i, (images, vectors, pose_ids) in enumerate(train_loader):\n",
        "            images, vectors, pose_ids = images.to(device), vectors.to(device), pose_ids.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(images, vectors)\n",
        "            loss = criterion(outputs, pose_ids)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            train_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += pose_ids.size(0)\n",
        "            correct += (predicted == pose_ids).sum().item()\n",
        "\n",
        "            if (i + 1) % print_every == 0:\n",
        "                print(f'Epoch {epoch+1}/{epochs}, Step {i+1}/{len(train_loader)}, Loss: {train_loss / (i+1):.4f}')\n",
        "        train_accuracy = 100 * correct / total\n",
        "        valid_loss, valid_accuracy = validate(model, valid_loader, criterion, device)\n",
        "        print(f'End of Epoch {epoch+1}, '\n",
        "              f'Train Loss: {train_loss / len(train_loader):.4f}, '\n",
        "              f'Train Accuracy: {train_accuracy:.2f}%, '\n",
        "              f'Valid Loss: {valid_loss:.4f}, Valid Accuracy: {valid_accuracy:.2f}%\\n')\n",
        "\n",
        "\n",
        "\n",
        "valid_dataset = YogaPoseDataset(\n",
        "    csv_file='/content/drive/MyDrive/dataset/valid_dataset.csv',\n",
        "    img_dir='/content/drive/MyDrive/dataset',\n",
        "    net=net,\n",
        "    transform=transform\n",
        ")\n",
        "valid_loader = DataLoader(valid_dataset, batch_size=32, shuffle=True)\n",
        "\n",
        "\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.0008)\n",
        "\n",
        "\n",
        "train(model, train_loader, valid_loader, criterion, optimizer, device = device, epochs=20, print_every=20)\n"
      ],
      "metadata": {
        "id": "vCiV6Uqg-05r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HHzfBCRESRt-"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0vSl8JB8rmID"
      },
      "source": [
        "## **3 LSTM CLASSIFIER** (Input : 관절 피쳐벡터+pose name)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "03d0z76oQ5OG"
      },
      "outputs": [],
      "source": [
        "class PoseLSTMClassifier(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
        "        super(PoseLSTMClassifier, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out, _ = self.lstm(x)\n",
        "        out = self.fc(out) if out.dim() == 2 else self.fc(out[:, -1, :])\n",
        "        return out\n",
        "\n",
        "\n",
        "\n",
        "# input_size: 관절 벡터의 차원 (18개 관절 * 3 좌표 = 54)\n",
        "# hidden_size: LSTM 셀의 hidden state의 크기\n",
        "# num_layers: LSTM 층의 수\n",
        "# num_classes: 요가 포즈수\n",
        "LSTMmodel = PoseLSTMClassifier(input_size=54, hidden_size=128, num_layers=2, num_classes=107).to(device)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "LSTMmodel = LSTMmodel.to(device)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VnCWmwJdRmKC"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.optim as optim\n",
        "def calculate_accuracy(outputs, targets):\n",
        "    _, predictions = torch.max(outputs, 1)\n",
        "    correct = (predictions == targets).sum().item()\n",
        "    total = targets.size(0)\n",
        "    accuracy = 100 * correct / total\n",
        "    return accuracy\n",
        "\n",
        "def validate(model, valid_loader, criterion):\n",
        "    model.eval()\n",
        "    valid_loss = 0\n",
        "    valid_accuracy = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, feature_vectors, pose_ids in valid_loader:\n",
        "            feature_vectors = feature_vectors.to(device)\n",
        "            pose_ids = pose_ids.to(device)\n",
        "            outputs = model(feature_vectors)\n",
        "            loss = criterion(outputs, pose_ids)\n",
        "            valid_loss += loss.item()\n",
        "            valid_accuracy += calculate_accuracy(outputs, pose_ids)\n",
        "\n",
        "    valid_loss /= len(valid_loader)\n",
        "    valid_accuracy /= len(valid_loader)\n",
        "    return valid_loss, valid_accuracy\n",
        "\n",
        "def train(model, train_loader, valid_loader, criterion, optimizer, epochs=10, print_every=20):\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        train_loss = 0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        for i, (images, feature_vectors, pose_ids) in enumerate(train_loader):\n",
        "            feature_vectors = feature_vectors.to(device)\n",
        "            pose_ids = pose_ids.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(feature_vectors)\n",
        "            loss = criterion(outputs, pose_ids)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            train_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += pose_ids.size(0)\n",
        "            correct += (predicted == pose_ids).sum().item()\n",
        "\n",
        "            if (i + 1) % print_every == 0:\n",
        "                print(f'Epoch {epoch+1}, Batch {i+1}, Loss: {train_loss / (i+1):.4f}')\n",
        "\n",
        "        train_accuracy = 100 * correct / total\n",
        "        valid_loss, valid_accuracy = validate(model, valid_loader, criterion)\n",
        "        print(f'End of Epoch {epoch+1}, '\n",
        "              f'Train Loss: {train_loss / len(train_loader):.4f}, '\n",
        "              f'Train Accuracy: {train_accuracy:.2f}%, '\n",
        "              f'Valid Loss: {valid_loss:.4f}, Valid Accuracy: {valid_accuracy:.2f}%\\n')\n",
        "\n",
        "\n",
        "\n",
        "valid_dataset = YogaPoseDataset(\n",
        "    csv_file='/content/drive/MyDrive/dataset/valid_dataset.csv',\n",
        "    img_dir='/content/drive/MyDrive/dataset',\n",
        "    net=net,\n",
        "    transform=transform\n",
        ")\n",
        "valid_loader = DataLoader(valid_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "train(LSTMmodel, train_loader, valid_loader, criterion, optimizer, epochs=10, print_every=20)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w4RpLiQk5hUs"
      },
      "source": [
        "## **4. ResNetFeatures(image) + LSTM CLASSIFIER (I관절 피쳐벡터)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sUceXOsl506b"
      },
      "outputs": [],
      "source": [
        "import torchvision.models as models\n",
        "\n",
        "class ResNetFeatures(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ResNetFeatures, self).__init__()\n",
        "        original_model = models.resnet18(pretrained=True)\n",
        "        self.features = nn.Sequential(*list(original_model.children())[:-1])\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "\n",
        "        x = x.view(x.size(0), -1)\n",
        "        return x\n",
        "\n",
        "resnet_features = ResNetFeatures().to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oEryQ-x755mW"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class ImageVectorCombinedModel(nn.Module):\n",
        "    def __init__(self, vector_input_size, hidden_size, num_layers, num_classes):\n",
        "        super(ImageVectorCombinedModel, self).__init__()\n",
        "        self.resnet = models.resnet18(pretrained=True)\n",
        "        self.resnet.fc = nn.Identity()\n",
        "\n",
        "        self.lstm = nn.LSTM(vector_input_size, hidden_size, num_layers, batch_first=True)\n",
        "\n",
        "        self.combined_fc1 = nn.Linear(512 + hidden_size, 128)\n",
        "        self.combined_fc2 = nn.Linear(128, num_classes)\n",
        "\n",
        "    def forward(self, images, vectors):\n",
        "\n",
        "        image_out = self.resnet(images)\n",
        "\n",
        "        lstm_out, _ = self.lstm(vectors)\n",
        "        lstm_out = lstm_out.squeeze(1)\n",
        "\n",
        "\n",
        "        combined_out = torch.cat((image_out, lstm_out), dim=1)\n",
        "\n",
        "        combined_out = F.relu(self.combined_fc1(combined_out))\n",
        "        out = self.combined_fc2(combined_out)\n",
        "        return out\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mfsXboYV7pPs"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.optim as optim\n",
        "def calculate_accuracy(outputs, targets):\n",
        "    _, predictions = torch.max(outputs, 1)\n",
        "    correct = (predictions == targets).sum().item()\n",
        "    total = targets.size(0)\n",
        "    accuracy = 100 * correct / total\n",
        "    return accuracy\n",
        "\n",
        "def validate(model, valid_loader, criterion, device):\n",
        "    model.eval()\n",
        "    valid_loss = 0\n",
        "    valid_accuracy = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, vectors, pose_ids in valid_loader:\n",
        "            images = images.to(device)\n",
        "            vectors = vectors.to(device)\n",
        "            pose_ids = pose_ids.to(device)\n",
        "\n",
        "            outputs = model(images, vectors)\n",
        "            loss = criterion(outputs, pose_ids)\n",
        "            valid_loss += loss.item()\n",
        "            valid_accuracy += calculate_accuracy(outputs, pose_ids)\n",
        "\n",
        "    valid_loss /= len(valid_loader)\n",
        "    valid_accuracy /= len(valid_loader)\n",
        "    return valid_loss, valid_accuracy\n",
        "\n",
        "\n",
        "import os\n",
        "\n",
        "def train(model, train_loader, valid_loader, criterion, optimizer, device, epochs, model_save_path, print_every=20):\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        train_loss = 0.0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        for i, (images, vectors, pose_ids) in enumerate(train_loader):\n",
        "            images, vectors, pose_ids = images.to(device), vectors.to(device), pose_ids.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(images, vectors)\n",
        "            loss = criterion(outputs, pose_ids)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            train_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += pose_ids.size(0)\n",
        "            correct += (predicted == pose_ids).sum().item()\n",
        "\n",
        "            if (i + 1) % print_every == 0:\n",
        "                print(f'Epoch {epoch+1}/{epochs}, Step {i+1}/{len(train_loader)}, Loss: {train_loss / (i+1):.4f}')\n",
        "        train_accuracy = 100 * correct / total\n",
        "        valid_loss, valid_accuracy = validate(model, valid_loader, criterion, device)\n",
        "        print(f'End of Epoch {epoch+1}, '\n",
        "              f'Train Loss: {train_loss / len(train_loader):.4f}, '\n",
        "              f'Train Accuracy: {train_accuracy:.2f}%, '\n",
        "              f'Valid Loss: {valid_loss:.4f}, Valid Accuracy: {valid_accuracy:.2f}%\\n')\n",
        "\n",
        "        save_path = os.path.join(model_save_path, f'model_epoch_{epoch+1}.pth')\n",
        "        torch.save(model.state_dict(), save_path)\n",
        "        print(f'Model saved to {save_path}')\n",
        "\n",
        "\n",
        "\n",
        "valid_dataset = YogaPoseDataset(\n",
        "    csv_file='/content/drive/MyDrive/dataset/valid_dataset.csv',\n",
        "    img_dir='/content/drive/MyDrive/dataset',\n",
        "    net=net,\n",
        "    transform=transform\n",
        ")\n",
        "valid_loader = DataLoader(valid_dataset, batch_size=32, shuffle=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_JuJyDpR56Ou"
      },
      "outputs": [],
      "source": [
        "\n",
        "image_input_size = 112 #changed 224 to 112\n",
        "vector_input_size = 54\n",
        "hidden_size = 128        # LSTM의 hidden state 크기\n",
        "num_layers = 2           # LSTM 층의 수\n",
        "num_classes = 13\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = ImageVectorCombinedModel(vector_input_size, hidden_size, num_layers, num_classes)\n",
        "model = model.to(device)\n",
        "\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.0005)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "\n",
        "\n",
        "model_save_path = '/content/drive/MyDrive/OpenPose-Pose-Estimation'\n",
        "train(model, train_loader, valid_loader, criterion, optimizer, device, epochs=10, model_save_path=model_save_path, print_every=20)\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "V100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}