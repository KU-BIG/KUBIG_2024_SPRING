# -*- coding: utf-8 -*-
"""LSTM_train_50epoch_prediction

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1X5Jm50SA-HTNhGxkLJnnuK8vgqwYluEW

# 중요: README

본 코드는 LSTM 모델을 training 하는 코드이다.

# 전처리 Scheme
이 ipynb파일을 바탕으로 딥러닝 모델을 만들면 된다.

만약 처음부터 끝까지 정보가 필요하다면 원본 코드를 참조하면 된다.

https://colab.research.google.com/drive/1_89gAFgKXN3J1FzdLcUExPpK2eli3mY7?authuser=1#scrollTo=Eh_LVBG7crR9


전처리 scheme은 다음과 같다.
- X-CA, Y-CA, Z-CA는 모두 정규화
- structure_detail의 경우 각 열마다 의미가 있기 때문에 one-hot encoding을 중심으로 진행. 우선 structure_detail에 등장하는 고유 문자의 종류는 (https://www.mankier.com/1/mkdssp#Synopsis 참고; 공백은 전부 해당 없다는 뜻):
  - `{' ', '<', '>', 'P'}`: **`structure_detail1`**-> helix 종류
  - `{' ', '3', 'X', '<', '>'}`: **`structure_detail2`** -> helix 종류
  - `{' ', '4', 'X', '<', '>'}`: **`structure_detail3`** -> helix 종류
  - `{' ', 'X', '<', '>', '5'}`: **`structure_detail4`** -> helix 종류
  - `{' ', 'S'}`: **`structure_detail5`** -> Bend 위치
  - `{' ', '-', '+'}`: **`structure_detail6`** -> chirality
  - `{'U', 's', 'Q', 'O', 'e', 'g', 'R', 'd', 'D', 'Z', 'H', 'w', 'v', 'V', 'G', 'a', 'p', 'W', 'l', 'M', 'C', 'o', 'S', 'j', 'F', 'i', 'u', 'L', 'K', 'q', 'N', 'm', 'k', 'X', 'z', 'c', 'A', 'Y', 'y', 'b', 't', 'P', ' ', 'f', 'I', 'h', 'n', 'r', 'J', 'E', 'B', 'x', 'T'}`: : **`structure_detail7`** -> beta-bridge label
  - `{'U', 's', 'e', 'O', 'Q', 'g', 'R', 'd', 'D', 'Z', 'H', 'w', 'v', 'V', 'G', 'p', 'a', 'W', 'l', 'M', 'C', 'o', 'S', 'j', 'F', 'i', 'u', 'L', 'K', 'q', 'm', 'N', 'k', 'X', 'z', 'c', 'Y', 'A', 'y', 'b', 't', 'P', ' ', 'f', 'I', 'h', 'n', 'r', 'J', 'E', 'B', 'x', 'T'}` **`structure_detail8`**-> beta bridge label

여기서 helix이 종류는 P, 3, 4, 5이고, `>`는 시작점, `>`는 끝점이라는 뜻이다. beta-bridge label은 같은 알파벳끼리 bridge를 형성한다는 뜻이다. 예를 들어 `structure_detail7`에서 1-5 redsidue에 A, 60-75 residue에 A가 온다면 이 위치에 bridge가 온다는 뜻이다. 즉 알파벳은 일종의 cluster라고 볼 수 있고, 등장한 순서대로 A-Z가 붙는다. 소문자의 경우 parallel이 다른 경우. 그러나 bridge가 26개가 넘어가는 경우 어떤 일이 일어나는지는 데이터상 알 수 없었다.

- structure_detail1-6 one hot encoding (범주형 변수), structure_detail7-8 제거 (cluster를 반영하는 방법을 모르겠음)
- ACC열 제거: 인접한 물 분자의 수. 단백질 구조와 관련이 없기 때문에 제거

# 라이브러리 불러오기
"""

import pandas as pd
import numpy as np
import torch
import matplotlib.pyplot as plt
import tqdm
from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import OneHotEncoder

from torch.utils.data import Dataset, DataLoader
from sklearn.preprocessing import StandardScaler

# 구글 드라이브 마운트
from google.colab import drive
drive.mount('/content/drive')

"""# 파일 경로 설정하기"""

# Commented out IPython magic to ensure Python compatibility.
# 작업 폴더를 해당 경로로 수정
# %cd /content/drive/MyDrive/Datasets/PDB Raw

"""# 쿼리 종류 불러오기"""

# legacy 데이터베이스에 있는 4글자 코드를 모두 리스트에 저장
four_letter_code_legacy = [] # 4글자 코드 리스트

filename = 'pdb_four_letter_legacy.txt'
f = open(filename, 'r')
lines = f.readlines()

for line in lines:
    four_letter_code_legacy.append(line[-10:-6])

print(four_letter_code_legacy)

# cif 데이터베이서에 있는 4글자 코드를 모두 리스트에 저장
# 워낙 양이 많기 때문에 10개만 print (전부 print 하려면 오류남)
four_letter_code_cif = []

filename = 'pdb_four_letter_cif.txt'
f = open(filename, 'r')
lines = f.readlines()

for line in lines:
    four_letter_code_cif.append(line[-12:-8])

print(four_letter_code_cif[0:10])

# 몇개의 query가 있는가? legacy
len(four_letter_code_legacy)

# 몇개의 query가 있는가? cif - 확실히 최근 데이터라 이게 더 많다
len(four_letter_code_cif)

"""# 데이터 다운로드 함수"""

import subprocess
import os

# 데이터 다운로드 받는 함수
# https://www.scrapingbee.com/blog/python-wget/
def runcmd(cmd, verbose = False, *args, **kwargs):

    process = subprocess.Popen(
        cmd,
        stdout = subprocess.PIPE,
        stderr = subprocess.PIPE,
        text = True,
        shell = True
    )
    std_out, std_err = process.communicate()
    if verbose:
        print(std_out.strip(), std_err)
    pass

"""# Header, Data 분리 함수"""

# DSSP 파일은 처음에 데이터 설명 및 Flag가 많기 때문에, 이것을 제외하고 Tidy dataframe 만 추출
def read_useful_data(file_path):
    obsolete_data = []
    header = []
    useful_data = []
    output_data = []
    with open(file_path, 'r') as f:
        # Skip lines until you find the line starting with '#'
        for line in f:
          if '#  RESIDUE' in line: # description에 '#'이 있는 경우가 있어서 RESIDUE 까지 붙여줌
            header = line.strip().split()
          elif '!' in line: # ! is missing residue
            pass
          elif not header:
            obsolete_data.append(line.strip())
          else:
            useful_data.append(line)


    return header, useful_data

"""# 전체 전처리 함수"""

def preprocess_dssp(data):
  # 각 line을 인덱싱하여 리스트로 저장
  residue_num=[]
  AA=[]
  structure_x=[]
  structure_detail=[]
  BP1=[]
  BP2=[]
  ACC=[]
  N_H__O_11=[]
  N_H__O_12=[]
  O__H_N_11=[]
  O__H_N_12=[]
  N_H__O_21=[]
  N_H__O_22=[]
  O__H_N_21=[]
  O__H_N_22=[]
  TCO=[]
  KAPPA=[]
  ALPHA=[]
  PHI=[]
  PSI=[]
  X_CA=[]
  Y_CA=[]
  Z_CA=[]

  variables_name=['residue_num', 'AA', 'structure_x', 'structure_detail', 'BP1', 'BP2', 'ACC',
           'N_H__O_11', 'N_H__O_12', 'O__H_N_11','O__H_N_12', 'N_H__O_21', 'N_H__O_22', 'O__H_N_21', 'O__H_N_22',
           'TCO', 'KAPPA', 'ALPHA', 'PHI', 'PSI', 'X_CA', 'Y_CA', 'Z_CA']
  variables_value=[residue_num, AA, structure_x, structure_detail, BP1, BP2, ACC,
           N_H__O_11, N_H__O_12, O__H_N_11,O__H_N_12, N_H__O_21, N_H__O_22, O__H_N_21, O__H_N_22,
           TCO, KAPPA, ALPHA, PHI, PSI, X_CA, Y_CA, Z_CA]

  for i in data:
    line=i.split()
    residue_num.append(i[6:10]) # line[1]->i[6:10] 변경
    AA.append(i[13]) # line[3] -> i[13] 변경
    X_CA.append(line[-3])
    Y_CA.append(line[-2])
    Z_CA.append(line[-1])

    structure_x.append(i[16])
    structure_detail.append(i[17:25])
    BP1.append(i[25:29]) # 30->29 변경
    BP2.append(i[29:34]) # 31->29 , 35->34 변경
    ACC.append(i[35:38]) # 35->34, 39->38 변경
    N_H__O_11.append(i[40:45])
    N_H__O_12.append(i[46:50]) # 52->50 변경
    O__H_N_11.append(i[53:56])
    O__H_N_12.append(i[57:61]) # 63->61 변경
    N_H__O_21.append(i[62:67]) # 63->62 변경
    N_H__O_22.append(i[68:72]) # 74->72 변경
    O__H_N_21.append(i[73:78]) # 75-_73 변경
    O__H_N_22.append(i[79:83]) # 85->83 변경
    TCO.append(i[85:91])
    KAPPA.append(i[91:97])
    ALPHA.append(i[97:103])
    PHI.append(i[103:109])
    PSI.append(i[109:116])

  # 모든 리스트를 데이터프레임에 저장
  cleaned_data=pd.DataFrame()

  for i in range(len(variables_name)):
    df=pd.DataFrame({variables_name[i]: variables_value[i]})
    cleaned_data=pd.concat([cleaned_data, df], axis=1)

  # structure label이 결측일 경우 'O' 로 대체
  cleaned_data['structure_x_1']=cleaned_data['structure_x'].replace(' ', 'O')
  cleaned_data['structure_x']=cleaned_data['structure_x'].replace(' ', 'O')
  cleaned_data = cleaned_data.drop(columns=['structure_x_1'])

  ## Structure Detail 전처리
  # 새로운 열 만들기
  for i in range(8):
    cleaned_data[f'structure_detail{i+1}'] = cleaned_data['structure_detail'].str[i]

  # structure_detail7, structure_detail8 제거
  cleaned_data = cleaned_data.drop(columns=['structure_detail7', 'structure_detail8'])

  # change datatype for each column
  cleaned_data['residue_num'] = cleaned_data['residue_num'].astype(float).astype('int32')
  cleaned_data['ACC'] = cleaned_data['ACC'].astype(float)
  cleaned_data['N_H__O_11'] = cleaned_data['N_H__O_11'].astype(float)
  cleaned_data['N_H__O_12'] = cleaned_data['N_H__O_12'].astype(float)
  cleaned_data['O__H_N_11'] = cleaned_data['O__H_N_11'].astype(float)
  cleaned_data['O__H_N_12'] = cleaned_data['O__H_N_12'].astype(float)
  cleaned_data['N_H__O_21'] = cleaned_data['N_H__O_21'].astype(float)
  cleaned_data['N_H__O_22'] = cleaned_data['N_H__O_22'].astype(float)
  cleaned_data['O__H_N_21'] = cleaned_data['O__H_N_21'].astype(float)
  cleaned_data['O__H_N_22'] = cleaned_data['O__H_N_22'].astype(float)
  cleaned_data['TCO'] = cleaned_data['TCO'].astype(float)
  cleaned_data['KAPPA'] = cleaned_data['KAPPA'].astype(float)
  cleaned_data['ALPHA'] = cleaned_data['ALPHA'].astype(float)
  cleaned_data['PHI'] = cleaned_data['PHI'].astype(float)
  cleaned_data['PSI'] = cleaned_data['PSI'].astype(float)
  cleaned_data['X_CA'] = cleaned_data['X_CA'].astype(float)
  cleaned_data['Y_CA'] = cleaned_data['Y_CA'].astype(float)
  cleaned_data['Z_CA'] = cleaned_data['Z_CA'].astype(float)

  # 불필요한 열 제거: ACC, BP1, BP2, residue_num, structure_detail, AA
  cleaned_data = cleaned_data.drop(columns=['ACC', 'BP1', 'BP2', 'residue_num', 'structure_detail'])

  # X-CA, Y-CA, Z-CA 정규화
  coordinate_col = ['X_CA', 'Y_CA', 'Z_CA']


  # Initialize the StandardScaler
  scaler = StandardScaler()

  # Normalize the numerical columns
  cleaned_data[coordinate_col] = scaler.fit_transform(cleaned_data[coordinate_col])

  return cleaned_data

"""# 아미노산 one-hot encoding 함수

총 20개 열
"""

amino_acids = list('ACDEFGHIKLMNPQRSTVWY')  # 20 standard amino acids

def one_hot_encode(df, column, amino_acids):
    one_hot_df = pd.DataFrame(0, index=df.index, columns=amino_acids)
    for aa in amino_acids:
        one_hot_df[aa] = (df[column] == aa).astype(int)
    return pd.concat([df, one_hot_df], axis=1).drop(columns=['AA']) # 원래 'AA' 열 드랍

"""# `structure_x` one-hot encoding 함수

- H = α-helix
- B = residue in isolated β-bridge
- E = extended strand, participates in β ladder
- G = 3-helix (310 helix)
- I = 5 helix (π-helix)
- T = hydrogen bonded turn
- S = bend

여기에 우리가 그외를 O로 만들었었다.

총 8개 열
"""

structures = list('HBEGITSO')

def one_hot_encode_structure(df, column, structures):
  one_hot_df = pd.DataFrame(0, index = df.index, columns = structures)
  for st in structures:
    one_hot_df[st] = (df[column] == st).astype(int) # True = 1, False = 0

  one_hot_df.columns = ['ss_'+i for i in structures]

  return pd.concat([df, one_hot_df], axis=1).drop(columns=['structure_x']) # 원래 'structure_x' 열 드랍

structures

['ss_'+i for i in structures]

"""# `structure_detail` one-hot encoding 함수

각 단백질마다 structure_detail에 있는 정보가 다르기 때문에 이것도 one-hot encoding 통일해줘야 함.

- `{' ', '<', '>', 'P'}`: structure_detail1-> helix 종류
- `{' ', '3', 'X', '<', '>'}`: structure_detail2 -> helix 종류
- `{' ', '4', 'X', '<', '>'}`: structure_detail3 -> helix 종류
- `{' ', 'X', '<', '>', '5'}`: structure_detail4 -> helix 종류
- `{' ', 'S'}`: structure_detail5 -> Bend 위치
- `{' ', '-', '+'}`: structure_detail6 -> chirality

총 24개 열
"""

def encode_structure_details(df):
    # Define mapping for each structure detail column
    mapping = {
        'structure_detail1': {' ': 'b', '<': '<', '>': '>', 'P': 'P'},
        'structure_detail2': {' ': 'b', '3': '3', 'X': 'X', '<': '<', '>': '>'},
        'structure_detail3': {' ': 'b', '4': '4', 'X': 'X', '<': '<', '>': '>'},
        'structure_detail4': {' ': 'b', 'X': 'X', '<': '<', '>': '>', '5': '5'},
        'structure_detail5': {' ': 'b', 'S': 'S'},
        'structure_detail6': {' ': 'b', '-': '-', '+': '+'}
    }

    # Iterate over each structure detail column and encode values
    for col, mapping_dict in mapping.items():
        # Create new columns for each category in the mapping
        for category, code in mapping_dict.items():
            new_col = col + '_' + code
            df[new_col] = (df[col] == category).astype(int)

    # Drop the original structure detail columns
    df.drop(columns=['structure_detail1', 'structure_detail2',
                     'structure_detail3', 'structure_detail4',
                     'structure_detail5', 'structure_detail6'], inplace=True)

    return df

# Example usage
# Assuming df is your dataframe containing structure details columns
# encoded_df = encode_structure_details(df)

"""# 예시"""

# 예시 쿼리
query = '7dmg'

runcmd(f"wget https://pdb-redo.eu/dssp/db/{query}/legacy", verbose = True)
os.rename('legacy', f"{query}.dssp")

header, data = read_useful_data(f"{query}.dssp")

dssp_7dmg = preprocess_dssp(data=data)
dssp_7dmg

dssp_7dmg = encode_structure_details(dssp_7dmg)
dssp_7dmg

dssp_7dmg = one_hot_encode(dssp_7dmg, 'AA', amino_acids)
dssp_7dmg

dssp_7dmg.columns

len(dssp_7dmg.columns) # 61개가 나와야 한다.

dssp_7dmg.head()

dssp_7dmg.to_csv('7dmg.tsv', sep="\t")
dssp_7dmg.to_csv('/content/drive/MyDrive/7dmg.tsv', sep = '\t')

"""여기서 `7dmg.tsv`를 보면 파일 데이터 형식을 알 수 있다. 이제 이 형식을 가지고 딥러닝 모델을 학습시켜보자.

# `for` loop으로 전부 작업해보기

중간중간 pkl 파일로 체크포인트 만들어야 마지막으로 가서 큰 데이터프레임을 얻을 수 있다.
"""

four_letter_code_legacy[0:10]

four_letter_code_legacy.index("7dv2")

"""### ideal 다운로드 코드"""

# # 이게 제일 이상적인 코드이지만 이미 데이터를 많이 다운받아서 바로 아랫 코드로 끝까지 다운받아보자

# import os
# import tqdm
# import pickle
# from IPython.display import clear_output  # Import the function to clear the output


# # Define directories and paths
# download_directory = "/content/drive/MyDrive/Datasets/PDB Raw/"
# checkpoint_directory = "/content/drive/MyDrive/Model/checkpoints/"
# checkpoint_path = os.path.join(checkpoint_directory, 'checkpoint_data.pkl')  # Main checkpoint path

# dataframes = []

# # Function to load the checkpoint safely
# def safe_load_checkpoint(checkpoint_path):
#     if os.path.exists(checkpoint_path) and os.path.getsize(checkpoint_path) > 0:
#         try:
#             with open(checkpoint_path, 'rb') as f:
#                 checkpoint = pickle.load(f)
#                 return checkpoint
#         except (EOFError, pickle.UnpicklingError) as e:
#             print(f"Error loading checkpoint: {e}")
#             return None
#     return None

# # Load the checkpoint if it exists and is valid
# checkpoint = safe_load_checkpoint(checkpoint_path)
# if checkpoint:
#     start_index = checkpoint['start_index']
#     chunk_counter = checkpoint['chunk_counter']
#     chunk_dataframes = checkpoint['chunk_dataframes']
# else:
#     start_index = 0
#     chunk_counter = 1
#     chunk_dataframes = []

# # Define chunk size and save interval
# chunk_size = 2000
# save_interval = 100

# # Ensure the checkpoint directory exists
# os.makedirs(checkpoint_directory, exist_ok=True)

# for i, x in enumerate(tqdm.tqdm(four_letter_code_legacy[start_index:], initial=start_index, total=len(four_letter_code_legacy)), start=start_index):
#     print(f"Query {x} is running...........")
#     runcmd(f"wget https://pdb-redo.eu/dssp/db/{x}/legacy", verbose=True)
#     os.rename('legacy', f"{x}.dssp")

#     header, data = read_useful_data(f"{x}.dssp")

#     if not data:
#         if os.path.exists(download_directory + f"/{x}.dssp"):
#             # Remove the file
#             os.remove(download_directory + f"/{x}.dssp")
#             print(f'File {x} has been deleted.')
#         else:
#             print(f'File {x} does not exist.')
#         continue  # Skip to the next iteration if data is not available

#     else:
#         globals()[f'dssp_{x}'] = one_hot_encode(preprocess_dssp(data=data), 'AA', amino_acids)
#         globals()[f'dssp_{x}'] = encode_structure_details(globals()[f'dssp_{x}'])
#         globals()[f'dssp_{x}'] = one_hot_encode_structure(globals()[f'dssp_{x}'], 'structure_x', structures)
#         dataframes.append(globals()[f'dssp_{x}'])

#         # Delete the DataFrame from globals to free up memory
#         del globals()[f'dssp_{x}']

#     # Save dataframe as numpy array - 데이터가 부족하기 때문에 일단 스킵
#     # np.save(save_path + f'{x}.npy', globals()[f'dssp_{x}'].to_numpy())

#     # delete file
#     # Check if the file exists
#     if os.path.exists(download_directory + f"/{x}.dssp"):
#         # Remove the file
#         os.remove(download_directory + f"/{x}.dssp")
#         print(f'File {x} has been deleted.')
#     else:
#         print(f'File {x} does not exist.')

#     # Append to chunk dataframes
#     chunk_dataframes.append(dataframes[-1])

#     # Save progress every save_interval iterations
#     if (i + 1) % save_interval == 0:
#         current_checkpoint = {'start_index': i + 1, 'chunk_counter': chunk_counter, 'chunk_dataframes': chunk_dataframes}
#         with open(checkpoint_path, 'wb') as f:
#             pickle.dump(current_checkpoint, f)
#         print(f'Main checkpoint saved at iteration {i + 1}')

#         # Clear output to manage memory usage in Google Colab
#         clear_output(wait=True)
#         print('Cleared all outputs')

#     # Save chunk dataframes when it reaches chunk_size
#     if len(chunk_dataframes) >= chunk_size:
#         chunk_checkpoint_path = os.path.join(checkpoint_directory, f'checkpoint_data_chunk_{chunk_counter}.pkl')
#         with open(chunk_checkpoint_path, 'wb') as f:
#             pickle.dump(chunk_dataframes[:chunk_size], f)
#         print(f'Checkpoint chunk saved with {chunk_size} dataframes at iteration {i + 1}')

#         # Update chunk_dataframes to remove the saved data
#         chunk_dataframes = chunk_dataframes[chunk_size:]

#         # Increment chunk counter
#         chunk_counter += 1

# # Save any remaining dataframes
# if chunk_dataframes:  # Check if there are remaining dataframes to save
#     final_chunk_path = os.path.join(checkpoint_directory, f'checkpoint_data_chunk_{chunk_counter}.pkl')
#     with open(final_chunk_path, 'wb') as f:
#         pickle.dump(chunk_dataframes, f)
#     print('Final chunk checkpoint saved')

# # Save the final progress in the main checkpoint
# current_checkpoint = {'start_index': i + 1, 'chunk_counter': chunk_counter, 'chunk_dataframes': chunk_dataframes}
# with open(checkpoint_path, 'wb') as f:
#     pickle.dump(current_checkpoint, f)
# print('Main checkpoint updated and saved')

"""### 실질적으로 다운로드에 이용된 코드"""

# # 이걸 주력으로 했다.

# import os
# import tqdm
# import pickle
# from IPython.display import clear_output  # Import the function to clear the output


# # Define directories and paths
# download_directory = "/content/drive/MyDrive/Datasets/PDB Raw/"
# checkpoint_directory = "/content/drive/MyDrive/Model/checkpoints/"
# checkpoint_path = os.path.join(checkpoint_directory, 'checkpoint_data.pkl')  # Main checkpoint path

# dataframes = []

# # Function to load the checkpoint safely
# def safe_load_checkpoint(checkpoint_path):
#     if os.path.exists(checkpoint_path) and os.path.getsize(checkpoint_path) > 0:
#         try:
#             with open(checkpoint_path, 'rb') as f:
#                 checkpoint = pickle.load(f)
#                 return checkpoint
#         except (EOFError, pickle.UnpicklingError) as e:
#             print(f"Error loading checkpoint: {e}")
#             return None
#     return None

# # Load the checkpoint if it exists and is valid
# checkpoint = safe_load_checkpoint(checkpoint_path)
# if checkpoint:
#     start_index = checkpoint['start_index']
#     chunk_counter = checkpoint['chunk_counter']
# else:
#     start_index = 0
#     chunk_counter = 1

# # Define chunk size and save interval
# chunk_size = 2000
# save_interval = 100

# for i, x in enumerate(tqdm.tqdm(four_letter_code_legacy[start_index:], initial=start_index, total=len(four_letter_code_legacy)), start=start_index):
#     print(f"Query {x} is running...........")
#     runcmd(f"wget https://pdb-redo.eu/dssp/db/{x}/legacy", verbose=True)
#     os.rename('legacy', f"{x}.dssp")

#     header, data = read_useful_data(f"{x}.dssp")

#     if not data:
#         if os.path.exists(download_directory + f"/{x}.dssp"):
#             # Remove the file
#             os.remove(download_directory + f"/{x}.dssp")
#             print(f'File {x} has been deleted.')
#         else:
#             print(f'File {x} does not exist.')
#         continue  # Skip to the next iteration if data is not available

#     else:
#         globals()[f'dssp_{x}'] = one_hot_encode(preprocess_dssp(data=data), 'AA', amino_acids)
#         globals()[f'dssp_{x}'] = encode_structure_details(globals()[f'dssp_{x}'])
#         globals()[f'dssp_{x}'] = one_hot_encode_structure(globals()[f'dssp_{x}'], 'structure_x', structures)
#         dataframes.append(globals()[f'dssp_{x}'])

#         # Delete the DataFrame from globals to free up memory
#         del globals()[f'dssp_{x}']

#     # Save dataframe as numpy array - 데이터가 부족하기 때문에 일단 스킵
#     # np.save(save_path + f'{x}.npy', globals()[f'dssp_{x}'].to_numpy())

#     # delete file
#     # Check if the file exists
#     if os.path.exists(download_directory + f"/{x}.dssp"):
#         # Remove the file
#         os.remove(download_directory + f"/{x}.dssp")
#         print(f'File {x} has been deleted.')
#     else:
#         print(f'File {x} does not exist.')

#     # Save progress every save_interval iterations
#     if (i + 1) % save_interval == 0:
#         current_checkpoint = {'start_index': i + 1, 'chunk_counter': chunk_counter}
#         with open(checkpoint_path, 'wb') as f:
#             pickle.dump(current_checkpoint, f)
#         print(f'Main checkpoint saved at iteration {i + 1}')

#         # Clear output to manage memory usage in Google Colab
#         clear_output(wait=True)
#         print('Cleared all outputs')

#     # Save chunk every chunk_size iterations
#     if (i + 1) % chunk_size == 0:
#         chunk_checkpoint_path = os.path.join(checkpoint_directory, f'checkpoint_data_chunk_{chunk_counter}.pkl')
#         chunk_checkpoint = {'start_index': i + 1, 'dataframes': dataframes}
#         with open(chunk_checkpoint_path, 'wb') as f:
#             pickle.dump(chunk_checkpoint, f)
#         print(f'Checkpoint chunk saved at iteration {i + 1}')

#         # Clear dataframes list after saving
#         dataframes = []

#         # Increment chunk counter
#         chunk_counter += 1

# # Save any remaining dataframes
# if dataframes:  # Check if there are remaining dataframes to save
#     final_chunk_path = os.path.join(checkpoint_directory, f'checkpoint_data_chunk_{chunk_counter}.pkl')
#     final_checkpoint = {'start_index': i + 1, 'dataframes': dataframes}
#     with open(final_chunk_path, 'wb') as f:
#         pickle.dump(final_checkpoint, f)
#     print('Final chunk checkpoint saved')

# # Save the final progress in the main checkpoint
# current_checkpoint = {'start_index': i + 1, 'chunk_counter': chunk_counter}
# with open(checkpoint_path, 'wb') as f:
#     pickle.dump(current_checkpoint, f)
# print('Main checkpoint updated and saved')

# dataframes

# # save the dataframes object
# import pickle
# import pandas as pd

# # Save the list of DataFrames to a file
# with open('dataframes.pkl', 'wb') as f:
#     pickle.dump(dataframes, f)

# print("Dataframes saved successfully!")

# import pickle

# # Load the list of DataFrames from the file
# with open('dataframes.pkl', 'rb') as f:
#     loaded_dataframes = pickle.load(f)

# print("Dataframes loaded successfully!")
# print(loaded_dataframes)

# dssp_7dmg.columns

# dataframes[0].columns

"""# Model Training

## Train Test Split

## Save and Load Checkpoint

지금 우리가 다루고 있는 것이 200,000개 단백질이기 때문에 중간에 튕길 가능성이 매우 큼. 그래서 중간중간 checkpoint을 만들어서 모델을 저장하도록 하자.
"""

# checkpoint 저장 함수
def save_checkpoint(model, optimizer, epoch, loss, filename='checkpoint.pth.tar'):
    state = {
        'epoch': epoch,
        'state_dict': model.state_dict(),
        'optimizer': optimizer.state_dict(),
        'loss': loss,
    }
    torch.save(state, filename)
    print(f'Checkpoint saved: {filename}')

def load_checkpoint(model, optimizer, filename='checkpoint.pth.tar'):
    checkpoint = torch.load(filename)
    model.load_state_dict(checkpoint['state_dict'])
    optimizer.load_state_dict(checkpoint['optimizer'])
    epoch = checkpoint['epoch']
    loss = checkpoint['loss']
    print(f'Checkpoint loaded: {filename} (Epoch {epoch}, Loss {loss})')
    return epoch, loss

"""## Custom Dataset and DataLoader"""

class ProteinDataset(Dataset):
    def __init__(self, dataframes):
        self.dataframes = dataframes

    def __len__(self):
        return len(self.dataframes)

    def __getitem__(self, idx):
        df = self.dataframes[idx]
        inputs = df.drop(columns=['ss_B', 'ss_E', 'ss_G', 'ss_H', 'ss_I', 'ss_O', 'ss_S', 'ss_T']).values
        targets = df[['ss_B', 'ss_E', 'ss_G', 'ss_H', 'ss_I', 'ss_O', 'ss_S', 'ss_T']].values
        return torch.tensor(inputs, dtype=torch.float32), torch.tensor(targets, dtype=torch.float32)

def collate_fn(batch):
    inputs = [item[0] for item in batch]
    targets = [item[1] for item in batch]

    inputs_padded = torch.nn.utils.rnn.pad_sequence(inputs, batch_first=True)
    targets_padded = torch.nn.utils.rnn.pad_sequence(targets, batch_first=True)

    return inputs_padded, targets_padded

"""## Define LSTM and Transformer Models"""

import torch.nn as nn

class LSTMModel(nn.Module):
    def __init__(self, input_size, hidden_size, num_layers, output_size):
        super(LSTMModel, self).__init__()
        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)
        self.fc = nn.Linear(hidden_size, output_size)

    def forward(self, x):
        h0 = torch.zeros(num_layers, x.size(0), hidden_size).to(x.device)
        c0 = torch.zeros(num_layers, x.size(0), hidden_size).to(x.device)

        out, _ = self.lstm(x, (h0, c0))
        out = self.fc(out)
        return out

class TransformerModel(nn.Module):
    def __init__(self, input_size, d_model, nhead, num_encoder_layers, num_decoder_layers, dim_feedforward, output_size):
        super(TransformerModel, self).__init__()
        self.embedding = nn.Linear(input_size, d_model)
        self.transformer = nn.Transformer(d_model, nhead, num_encoder_layers, num_decoder_layers, dim_feedforward)
        self.fc = nn.Linear(d_model, output_size)

    def forward(self, src):
        src = self.embedding(src)

        src_mask = self.transformer.generate_square_subsequent_mask(src.size(1)).to(src.device)
        out = self.transformer(src, src, src_mask)
        out = self.fc(out)

        return out

"""## Train loop"""

def train_model_on_chunk(chunk_file, model, criterion, optimizer, device, checkpoint_path='checkpoint.pth.tar'):
    with open(chunk_file, 'rb') as f:
        dataframes = pickle.load(f)

    print(f"{chunk_file} loaded successfully!")
    dataset = ProteinDataset(dataframes['dataframes'])
    dataloader = DataLoader(dataset, batch_size=16, collate_fn=collate_fn, shuffle=True)

    model.train()
    total_loss = 0
    for inputs, targets in dataloader:
        inputs, targets = inputs.to(device), targets.to(device)
        outputs = model(inputs)

        # Mask for padding
        mask = targets != 0
        outputs = outputs[mask]
        targets = targets[mask]

        loss = criterion(outputs, targets)

        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        total_loss += loss.item()

    avg_loss = total_loss / len(dataloader)
    print(f'Chunk {chunk_file}, Loss: {avg_loss:.4f}')

    # Save checkpoint
    save_checkpoint(model, optimizer, epoch+1, avg_loss, checkpoint_path)

# Model Parameters
input_size = 60  # 총 column (68개) 'structure_x' and target columns (8개)
hidden_size = 128
num_layers = 2
output_size = 8  # Number of secondary structure categories (ss_B to ss_T)

# Device configuration
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

# Initialize the model, criterion, and optimizer
lstm_model = LSTMModel(input_size, hidden_size, num_layers, output_size).to(device)
criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(lstm_model.parameters(), lr=0.001)

"""## EPOCH 50 Trial
1-70 chunk 파일로 trainig loop

"""

# reset memory from GPU
torch.cuda.empty_cache()

import os
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
import pickle

# Checkpoint save function
def save_checkpoint(model, optimizer, epoch, loss, filename='checkpoint.pth.tar'):
    state = {
        'epoch': epoch,
        'state_dict': model.state_dict(),
        'optimizer': optimizer.state_dict(),
        'loss': loss,
    }
    torch.save(state, filename)
    print(f'Checkpoint saved: {filename}')

# Checkpoint load function
def load_checkpoint(model, optimizer, filename='checkpoint.pth.tar'):
    checkpoint = torch.load(filename)
    model.load_state_dict(checkpoint['state_dict'])
    optimizer.load_state_dict(checkpoint['optimizer'])
    epoch = checkpoint['epoch']
    loss = checkpoint['loss']
    print(f'Checkpoint loaded: {filename} (Epoch {epoch}, Loss {loss})')
    return epoch, loss

# Dataset class
class ProteinDataset(Dataset):
    def __init__(self, dataframes):
        self.dataframes = dataframes

    def __len__(self):
        return len(self.dataframes)

    def __getitem__(self, idx):
        df = self.dataframes[idx]
        inputs = df.drop(columns=['ss_B', 'ss_E', 'ss_G', 'ss_H', 'ss_I', 'ss_O', 'ss_S', 'ss_T']).values
        targets = df[['ss_B', 'ss_E', 'ss_G', 'ss_H', 'ss_I', 'ss_O', 'ss_S', 'ss_T']].values.argmax(axis=1)  # Convert one-hot to class index
        return torch.tensor(inputs, dtype=torch.float32), torch.tensor(targets, dtype=torch.long)

# Collate function for DataLoader
def collate_fn(batch):
    inputs = [item[0] for item in batch]
    targets = [item[1] for item in batch]

    inputs_padded = torch.nn.utils.rnn.pad_sequence(inputs, batch_first=True)
    targets_padded = torch.nn.utils.rnn.pad_sequence(targets, batch_first=True, padding_value=-1)  # Use -1 as padding value

    return inputs_padded, targets_padded

# Model definition
class LSTMModel(nn.Module):
    def __init__(self, input_size, hidden_size, num_layers, output_size):
        super(LSTMModel, self).__init__()
        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)
        self.fc = nn.Linear(hidden_size, output_size)

    def forward(self, x):
        h0 = torch.zeros(num_layers, x.size(0), hidden_size).to(x.device)
        c0 = torch.zeros(num_layers, x.size(0), hidden_size).to(x.device)

        out, _ = self.lstm(x, (h0, c0))
        out = self.fc(out)
        return out

# Training function for each chunk
def train_model_on_chunk(chunk_file, model, criterion, optimizer, device, epoch, checkpoint_path='checkpoint.pth.tar'):
    with open(chunk_file, 'rb') as f:
        dataframes = pickle.load(f)

    print(f"{chunk_file} loaded successfully!")
    dataset = ProteinDataset(dataframes['dataframes'])
    dataloader = DataLoader(dataset, batch_size=8, collate_fn=collate_fn, shuffle=True)  # Reduce batch size

    model.train()
    total_loss = 0
    for inputs, targets in dataloader:
        inputs, targets = inputs.to(device), targets.to(device)
        optimizer.zero_grad()  # Reset gradients

        outputs = model(inputs)

        # Flatten outputs and targets
        outputs = outputs.view(-1, outputs.size(-1))
        targets = targets.view(-1)

        # Mask for padding
        mask = targets != -1
        outputs = outputs[mask]
        targets = targets[mask]

        loss = criterion(outputs, targets)

        loss.backward()
        optimizer.step()

        total_loss += loss.item()

        torch.cuda.empty_cache()  # Clear GPU cache to free up memory

    avg_loss = total_loss / len(dataloader)
    print(f'Chunk {chunk_file}, Loss: {avg_loss:.4f}')

    # Save checkpoint
    save_checkpoint(model, optimizer, epoch, avg_loss, checkpoint_path)

# Model Parameters
input_size = 60  # Total columns (68) - target columns (8)
hidden_size = 128
num_layers = 2
output_size = 8  # Number of secondary structure categories

# Device configuration
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

# Initialize the model, criterion, and optimizer
lstm_model = LSTMModel(input_size, hidden_size, num_layers, output_size).to(device)
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(lstm_model.parameters(), lr=0.001)

# # Training over each chunk
# num_epochs = 50
# checkpoint_path = '/content/drive/MyDrive/Model/checkpoint.pth.tar'

# start_epoch = 0
# # Load checkpoint if it exists
# if os.path.exists(checkpoint_path):
#     start_epoch, _ = load_checkpoint(lstm_model, optimizer, checkpoint_path)

# for epoch in range(start_epoch, num_epochs):
#     for i in range(1, 70):
#         chunk_file = f'/content/drive/MyDrive/Model/checkpoints/checkpoint_data_chunk_{i}.pkl'
#         train_model_on_chunk(chunk_file, lstm_model, criterion, optimizer, device, epoch, checkpoint_path)

"""# Test set

지금 데이터파일이 99개이므로 위 코드에서 볼 수 있듯이 70개가 training set에 들어가 있다. 나며지 71-99번째 파일을 test로 해보자.
"""

chunk_file = '/content/drive/MyDrive/Model/checkpoints/checkpoint_data_chunk_70.pkl'

with open(chunk_file, 'rb') as f:
        test = pickle.load(f)

first = test['dataframes'][50]

import torch.nn.functional as F

def prepare_input(new_df):
    inputs = new_df.drop(columns=['ss_B', 'ss_E', 'ss_G', 'ss_H', 'ss_I', 'ss_O', 'ss_S', 'ss_T']).values
    return torch.tensor(inputs, dtype=torch.float32).unsqueeze(0)  # Adding batch dimension

def predict(model, inputs, device):
    model.eval()
    inputs = inputs.to(device)
    with torch.no_grad():
        outputs = model(inputs)
    return outputs

# Make predictions
inputs = prepare_input(first)
outputs = predict(lstm_model, inputs, device)

# Apply softmax to get probabilities
probabilities = F.softmax(outputs, dim=-1)

# Convert to numpy array if needed
predictions = probabilities.cpu().numpy()

print(predictions)

len(first.columns)

predictions[0][0]

# Assuming predictions is your numpy array of shape (n, 8)
predictions = np.array(predictions)

# Column names
column_names = ["B", "E", "G", "H", "I", "O", "S", "T"]

# Convert to DataFrame
df = pd.DataFrame(np.array(predictions).squeeze(axis=0), columns=column_names)

# Add a 'predict' column with the highest probability label
df['predict'] = df.idxmax(axis=1)

print(df)

''.join(list(df["predict"]))

first2 = first[['ss_H', 'ss_B', 'ss_E', 'ss_G', 'ss_I', 'ss_T', 'ss_S', 'ss_O']]
first2.columns = ['H', 'B', 'E', 'G', 'I', 'T', 'S', 'O']
''.join(list(first2.idxmax(axis=1)))

first

# Example strings (replace with your actual strings)
predicted_string = ''.join(list(df["predict"]))
original_string = ''.join(list(first2.idxmax(axis=1)))

# Calculate total length
total_length = len(predicted_string)

# Calculate number of mismatches and indices
mismatches = sum(1 for i in range(total_length) if predicted_string[i] != original_string[i])
error_percentage = (mismatches / total_length) * 100

# Calculate accuracy
accuracy = (total_length - mismatches) / total_length * 100

# Indices of mismatches
mismatch_indices = [i for i in range(total_length) if predicted_string[i] != original_string[i]]

print(f"Accuracy: {accuracy:.2f}%")
print(f"Error percentage: {error_percentage:.2f}%")
print(f"Mismatch indices: {mismatch_indices}")

"""### 50 epoch testing"""

import torch.nn.functional as F

def prepare_input(new_df):
    inputs = new_df.drop(columns=['ss_B', 'ss_E', 'ss_G', 'ss_H', 'ss_I', 'ss_O', 'ss_S', 'ss_T']).values
    return torch.tensor(inputs, dtype=torch.float32).unsqueeze(0)  # Adding batch dimension

def predict(model, inputs, device):
    model.eval()
    inputs = inputs.to(device)
    with torch.no_grad():
        outputs = model(inputs)
    return outputs

basic_path='/content/drive/MyDrive/Model/checkpoints/checkpoint_data_chunk_{}.pkl'
column_names = ["B", "E", "G", "H", "I", "O", "S", "T"]

results = []
under_threshold = []
threshold = 90

for i in range(70, 100):     # for 70-99 chunk
  chunk_file = basic_path.format(i)
  chunk_results = []

  with open(chunk_file, 'rb') as f:
        test = pickle.load(f)

  for j in range(len(test['dataframes'])):
    test_df = test['dataframes'][j]
    print(test_df.columns)

    # Make predictions
    inputs = prepare_input(test_df)
    outputs = predict(lstm_model, inputs, device)

    # Apply softmax to get probabilities
    probabilities = F.softmax(outputs, dim=-1)

    # Convert to numpy array if needed
    predictions = probabilities.cpu().numpy()

    # Assuming predictions is your numpy array of shape (n, 8)
    predictions = np.array(predictions)

    # Convert to DataFrame
    df = pd.DataFrame(np.array(predictions).squeeze(axis=0), columns=column_names)

    # Add a 'predict' column with the highest probability label
    df['predict'] = df.idxmax(axis=1)

    test_df_2 = test_df[['ss_H', 'ss_B', 'ss_E', 'ss_G', 'ss_I', 'ss_T', 'ss_S', 'ss_O']]
    test_df_2.columns = ['H', 'B', 'E', 'G', 'I', 'T', 'S', 'O']

    # Example strings (replace with your actual strings)
    predicted_string = ''.join(list(df["predict"]))
    original_string = ''.join(list(test_df_2.idxmax(axis=1)))

    # Calculate total length
    total_length = len(predicted_string)

    # Calculate number of mismatches and indices
    mismatches = sum(1 for i in range(total_length) if predicted_string[i] != original_string[i])
    error_percentage = (mismatches / total_length) * 100

    # Calculate accuracy
    accuracy = (total_length - mismatches) / total_length * 100

    # Indices of mismatches
    mismatch_indices = [i for i in range(total_length) if predicted_string[i] != original_string[i]]

    print(f"Accuracy: {accuracy:.2f}%")
    print(f"Error percentage: {error_percentage:.2f}%")
    print(f"Mismatch indices: {mismatch_indices}")

    if accuracy < threshold:
      under_threshold.append({
            'chunck': i,
            'dataset': j,
            'accuracy': accuracy,
            'error_percentage': error_percentage,
            'mismatch_indices': mismatch_indices
        })

    chunk_results.append({
            'accuracy': accuracy,
            'error_percentage': error_percentage,
            'mismatch_indices': mismatch_indices
        })

  results.append({'chunk':i, 'results': chunk_results})

results[0]['results']    # Results of the first chunck

# Calculate average accuracy
total_accuracy = 0
total_dataframes = 0
accuracy_list = []

for result in results:
    for res in result['results']:
        total_accuracy += res['accuracy']
        total_dataframes += 1
        accuracy_list.append(res['accuracy'])

average_accuracy = total_accuracy / total_dataframes
print(f"Average Accuracy: {average_accuracy:.2f}% for {num_epochs} epochs")

"""# Threshold"""

len(under_threshold)/total_dataframes

under_threshold=pd.DataFrame(under_threshold)
under_threshold

min(under_threshold['accuracy'])

import seaborn as sns
sns.displot(under_threshold['accuracy'], kde=True)

"""# Load Saved Model"""

import pickle

chunk_file = '/content/drive/MyDrive/Model/checkpoints/checkpoint_data_chunk_70.pkl'

with open(chunk_file, 'rb') as f:
        test = pickle.load(f)

first = test['dataframes'][50]

# Define the model class again if it's not already in the current context
class LSTMModel(nn.Module):
    def __init__(self, input_size, hidden_size, num_layers, output_size):
        super(LSTMModel, self).__init__()
        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)
        self.fc = nn.Linear(hidden_size, output_size)

    def forward(self, x):
        h0 = torch.zeros(num_layers, x.size(0), hidden_size).to(x.device)
        c0 = torch.zeros(num_layers, x.size(0), hidden_size).to(x.device)

        out, _ = self.lstm(x, (h0, c0))
        out = self.fc(out)
        return out

def load_checkpoint(model, optimizer, filename='checkpoint.pth.tar'):
    checkpoint = torch.load(filename)
    model.load_state_dict(checkpoint['state_dict'])
    optimizer.load_state_dict(checkpoint['optimizer'])
    epoch = checkpoint['epoch']
    loss = checkpoint['loss']
    print(f'Checkpoint loaded: {filename} (Epoch {epoch}, Loss {loss})')
    return epoch, loss

def prepare_input(new_df):
    inputs = new_df.drop(columns=['ss_B', 'ss_E', 'ss_G', 'ss_H', 'ss_I', 'ss_O', 'ss_S', 'ss_T']).values
    return torch.tensor(inputs, dtype=torch.float32).unsqueeze(0)  # Adding batch dimension

def predict(model, inputs, device):
    model.eval()
    inputs = inputs.to(device)
    with torch.no_grad():
        outputs = model(inputs)
    return outputs

# Model Parameters
input_size = 60  # Total columns (68) - target columns (8)
hidden_size = 128
num_layers = 2
output_size = 8  # Number of secondary structure categories

# Device configuration
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

# Initialize the model and optimizer
lstm_model = LSTMModel(input_size, hidden_size, num_layers, output_size).to(device)
optimizer = torch.optim.Adam(lstm_model.parameters(), lr=0.001)

# Load the model from the checkpoint
checkpoint_path = '/content/drive/MyDrive/Model/checkpoint.pth.tar'
load_checkpoint(lstm_model, optimizer, checkpoint_path)

import torch.nn.functional as F

def prepare_input(new_df):
    inputs = new_df.drop(columns=['ss_B', 'ss_E', 'ss_G', 'ss_H', 'ss_I', 'ss_O', 'ss_S', 'ss_T']).values
    return torch.tensor(inputs, dtype=torch.float32).unsqueeze(0)  # Adding batch dimension

def predict(model, inputs, device):
    model.eval()
    inputs = inputs.to(device)
    with torch.no_grad():
        outputs = model(inputs)
    return outputs

import torch
import torch.nn.functional as F
import pandas as pd
import numpy as np
import pickle

def calculate_accuracy(predicted_string, original_string):
    total_length = len(predicted_string)
    mismatches = sum(1 for i in range(total_length) if predicted_string[i] != original_string[i])
    accuracy = (total_length - mismatches) / total_length * 100
    error_percentage = (mismatches / total_length) * 100
    mismatch_indices = [i for i in range(total_length) if predicted_string[i] != original_string[i]]
    return accuracy, error_percentage, mismatches, mismatch_indices

def process_chunk_file(chunk_file, lstm_model, device, column_names, threshold, under_threshold, error_stats, aa_error_stats, protein_counter):
    with open(chunk_file, 'rb') as f:
        test = pickle.load(f)

    chunk_results = []
    for j in range(len(test['dataframes'])):
        test_df = test['dataframes'][j]
        inputs = prepare_input(test_df)
        outputs = predict(lstm_model, inputs, device)
        probabilities = F.softmax(outputs, dim=-1)
        predictions = probabilities.cpu().numpy()

        df = pd.DataFrame(np.array(predictions).squeeze(axis=0), columns=column_names)
        df['predict'] = df.idxmax(axis=1)

        test_df_2 = test_df[['ss_H', 'ss_B', 'ss_E', 'ss_G', 'ss_I', 'ss_T', 'ss_S', 'ss_O']]
        test_df_2.columns = ['H', 'B', 'E', 'G', 'I', 'T', 'S', 'O']

        predicted_string = ''.join(list(df["predict"]))
        original_string = ''.join(list(test_df_2.idxmax(axis=1)))

        accuracy, error_percentage, mismatches, mismatch_indices = calculate_accuracy(predicted_string, original_string)

        # Update error statistics
        for idx in mismatch_indices:
            error_stats[original_string[idx]] += 1
            amino_acid = test_df.iloc[idx]['A':'Y'].idxmax()  # Find the correct amino acid
            aa_error_stats[amino_acid] += 1  # Update amino acid error count

        if accuracy < threshold:
            under_threshold.append({
                'chunk': i,
                'dataset': j,
                'accuracy': accuracy,
                'error_percentage': error_percentage,
                'mismatch_indices': mismatch_indices
            })

        chunk_results.append({
            'accuracy': accuracy,
            'error_percentage': error_percentage,
            'mismatches': mismatches,
            'mismatch_indices': mismatch_indices
        })

        # Decode the one-hot encoded amino acid sequence
        amino_acids = 'ACDEFGHIKLMNPQRSTVWY'
        amino_acid_sequence = ''.join([amino_acids[np.argmax(row['A':'Y'])] for _, row in test_df.iterrows()])

        # Print results neatly
        print(f"Protein {protein_counter}:")
        print(f"Amino Acid Sequence: {amino_acid_sequence}")
        print(f"Accuracy: {accuracy:.2f}%")
        print(f"Error percentage: {error_percentage:.2f}%")
        print(f"Mismatch indices: {mismatch_indices}")
        print()

        protein_counter += 1

    return chunk_results, protein_counter

# Main loop
basic_path = '/content/drive/MyDrive/Model/checkpoints/checkpoint_data_chunk_{}.pkl'
column_names = ["B", "E", "G", "H", "I", "O", "S", "T"]

results = []
under_threshold = []
threshold = 90

# Initialize error statistics dictionary
error_stats = {key: 0 for key in column_names}

# Initialize amino acid error statistics dictionary
amino_acids = 'ACDEFGHIKLMNPQRSTVWY'
aa_error_stats = {aa: 0 for aa in amino_acids}

# Initialize protein counter
protein_counter = 1

for i in range(70, 100):  # for 70-99 chunk
    chunk_file = basic_path.format(i)
    chunk_results, protein_counter = process_chunk_file(chunk_file, lstm_model, device, column_names, threshold, under_threshold, error_stats, aa_error_stats, protein_counter)
    results.append({'chunk': i, 'results': chunk_results})

# Print error statistics
print("Error Statistics:")
for key, value in error_stats.items():
    print(f"{key}: {value}")
print()

# Print amino acid error statistics
print("Amino Acid Error Statistics:")
for key, value in aa_error_stats.items():
    print(f"{key}: {value}")
print()

# Plotting Amino Acid Error Statistics:
import matplotlib.pyplot as plt

# Function to plot the bar graph for amino acid error frequency
def plot_aa_error_frequency(aa_error_stats):
    # Extract keys and values from the aa_error_stats dictionary
    amino_acids = list(aa_error_stats.keys())
    frequencies = list(aa_error_stats.values())

    # Plotting the bar graph
    plt.figure(figsize=(12, 6))
    plt.bar(amino_acids, frequencies, color='lightgreen')
    plt.xlabel('Amino Acids')
    plt.ylabel('Frequency of Errors')
    plt.title('Frequency of Mispredicted Amino Acids')
    plt.show()

# Plot the amino acid error frequency
plot_aa_error_frequency(aa_error_stats)

import matplotlib.pyplot as plt

# Function to plot the bar graph for error frequency
def plot_error_frequency(error_stats):
    # Extract keys and values from the error_stats dictionary
    secondary_structures = list(error_stats.keys())
    frequencies = list(error_stats.values())

    # Plotting the bar graph
    plt.figure(figsize=(10, 6))
    plt.bar(secondary_structures, frequencies, color='skyblue')
    plt.xlabel('Secondary Structure')
    plt.ylabel('Frequency')
    plt.title('Frequency of Errors by Secondary Structure')
    plt.show()

# Function to plot the relative frequency bar graph
def plot_relative_frequency(error_stats):
    # Extract keys and values from the error_stats dictionary
    secondary_structures = list(error_stats.keys())
    frequencies = list(error_stats.values())

    # Calculate the total number of errors
    total_errors = sum(frequencies)

    # Calculate relative frequencies
    relative_frequencies = [freq / total_errors for freq in frequencies]

    # Plotting the bar graph for relative frequency
    plt.figure(figsize=(10, 6))
    plt.bar(secondary_structures, relative_frequencies, color='salmon')
    plt.xlabel('Secondary Structure')
    plt.ylabel('Relative Frequency')
    plt.title('Relative Frequency of Errors by Secondary Structure')
    plt.ylim(0, 1)  # Ensure y-axis is scaled between 0 and 1
    plt.show()

# Plotting the graphs
plot_error_frequency(error_stats)
plot_relative_frequency(error_stats)

import matplotlib.pyplot as plt

# Function to plot the bar graph for amino acid error frequency
def plot_aa_error_frequency(aa_error_stats):
    # Extract keys and values from the aa_error_stats dictionary
    amino_acids = list(aa_error_stats.keys())
    frequencies = list(aa_error_stats.values())

    # Plotting the bar graph
    plt.figure(figsize=(12, 6))
    plt.bar(amino_acids, frequencies, color='lightgreen')
    plt.xlabel('Amino Acids')
    plt.ylabel('Frequency of Errors')
    plt.title('Frequency of Mispredicted Amino Acids')
    plt.show()

# Plot the amino acid error frequency
plot_aa_error_frequency(aa_error_stats)

"""# Computational Graph"""

!pip install torchviz

import torch.optim as optim
from torchviz import make_dot

# Create a dummy input
x = torch.randn(1, 1, input_size).to(device)

# Generate a visualization of the computational graph
y = lstm_model(x)
dot = make_dot(y, params=dict(lstm_model.named_parameters()))

# Adjust figure size to see the whole graph
plt.figure(figsize=(20, 20))  # Adjust the size as needed
dot.render('/content/drive/MyDrive/Model/lstm_model_graph', format='png')  # Save the graph as an image