{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: fineGrained).\n",
      "Your token has been saved to /c1/yumin/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import os\n",
    "\n",
    "import tool_pool as tp\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain import hub\n",
    "from langchain.agents import AgentExecutor, create_openai_tools_agent\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain import hub\n",
    "from langchain.agents import AgentExecutor\n",
    "from langchain.memory import ChatMessageHistory\n",
    "\n",
    "from IPython.display import Markdown, display\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "본 노트북을 실행하기 위해선 Openai api key가 필요합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_key = 'sk-' # Your openai API key here\n",
    "# openai_key = os.getenv(\"OPENAI_API_KEY\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tool List\n",
    "loadpaper : HTML 기반으로 Section list를 받고, Tool을 한 번 더 실행하는 방식으로 동작  \n",
    "loadpaper_wo_figure : loadpaper 에서 figure 기능이 제외된 Tool  \n",
    "loadpaper_wo_figure_wo_section : loadpaper에서 figure, section 기능이 제외된 Tool  \n",
    "recommendpaper : 관련 논문을 찾아주는 Tool. citation, reference mode 두 가지로 동작  \n",
    "code_matching : 논문 깃허브에서 관련 코드를 찾아주는 Tool  \n",
    "  \n",
    "tool_pool.py에서 직접 tool을 custom 할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tp.loadpaper, tp.recommendpaper, tp.code_matching, tp.loadpaper_wo_figure, tp.loadpaper_wo_figure_wo_section\n",
    "\n",
    "# default tools\n",
    "# tools = [tp.loadpaper, tp.recommendpaper, tp.code_matching]\n",
    "\n",
    "# light version. Customize it as you wish!\n",
    "# tools = [tp.loadpaper_wo_figure_wo_section, tp.recommendpaper]\n",
    "\n",
    "# Recommend tool combination.\n",
    "tools = [tp.loadpaper_wo_figure, tp.recommendpaper]\n",
    "\n",
    "# load Agent prompt\n",
    "prompt = hub.pull(\"hwchase17/openai-tools-agent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### base model 선택"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(model=\"gpt-4o\", temperature=0)\n",
    "# llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Real Use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PaperAssistant:\n",
    "    def __init__(self, llm, verbose):\n",
    "        self.memory = None\n",
    "        self.llm = llm\n",
    "        self.verbose = verbose\n",
    "\n",
    "        self.agent_executor = self.build_agent_executor()\n",
    "        self.agent_with_chat_history = self.bulid_agent_with_chat_history()\n",
    "\n",
    "\n",
    "    def build_agent_executor(self):\n",
    "        # Construct the OpenAI Tools agent\n",
    "        agent = create_openai_tools_agent(self.llm, tools, prompt)\n",
    "        # Create an agent executor by passing in the agent and tools\n",
    "        agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=self.verbose)\n",
    "        return agent_executor\n",
    "    \n",
    "    def bulid_agent_with_chat_history(self):\n",
    "        memory = ChatMessageHistory(session_id=\"test-session\")\n",
    "\n",
    "        agent_with_chat_history = RunnableWithMessageHistory(\n",
    "            self.agent_executor,\n",
    "            # This is needed because in most real world scenarios, a session id is needed\n",
    "            # It isn't really used here because we are using a simple in memory ChatMessageHistory\n",
    "            lambda session_id: memory,\n",
    "            input_messages_key=\"input\",\n",
    "            history_messages_key=\"chat_history\",\n",
    "        )\n",
    "        return agent_with_chat_history\n",
    "\n",
    "    def reset_memory(self, session_id='temp'):\n",
    "        self.agent_with_chat_history = self.bulid_agent_with_chat_history()\n",
    "\n",
    "    def send_message(self, message, session_id='temp'):\n",
    "        output = self.agent_with_chat_history.invoke(\n",
    "            {\"input\": message},\n",
    "            config={\"configurable\": {\"session_id\": session_id}},\n",
    "        )\n",
    "        display(Markdown(output['output']))\n",
    "\n",
    "    def __call__(self, message):\n",
    "        self.send_message(message)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tool 실행 과정을 생략하려면 verbose = False로 설정하세요\n",
    "assistant = PaperAssistant(llm, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `loadpaper` with `{'title': 'Attention is all you need', 'arxiv_id': '1706.03762'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3mHere is the title and section of the paper in HTML\n",
      "title\n",
      "Attention Is All You Need\n",
      "sections\n",
      "          Abstract\n",
      "  1 Introduction\n",
      "  2 Background\n",
      "  3 Model Architecture\n",
      "    3.1 Encoder and Decoder Stacks\n",
      "        Encoder:\n",
      "        Decoder:\n",
      "    3.2 Attention\n",
      "      3.2.1 Scaled Dot-Product Attention\n",
      "      3.2.2 Multi-Head Attention\n",
      "      3.2.3 Applications of Attention in our Model\n",
      "    3.3 Position-wise Feed-Forward Networks\n",
      "    3.4 Embeddings and Softmax\n",
      "    3.5 Positional Encoding\n",
      "  4 Why Self-Attention\n",
      "  5 Training\n",
      "    5.1 Training Data and Batching\n",
      "    5.2 Hardware and Schedule\n",
      "    5.3 Optimizer\n",
      "    5.4 Regularization\n",
      "        Residual Dropout\n",
      "        Label Smoothing\n",
      "  6 Results\n",
      "    6.1 Machine Translation\n",
      "    6.2 Model Variations\n",
      "    6.3 English Constituency Parsing\n",
      "  7 Conclusion\n",
      "        Acknowledgements\n",
      "  References\n",
      "  Attention Visualizations\n",
      "\n",
      "\n",
      " Use the 'loadpaper' tool again, specifying the section list you want to view in detail.\u001b[0m\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `loadpaper` with `{'title': 'Attention is all you need', 'arxiv_id': '1706.03762', 'sections': ['3.5 Positional Encoding']}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3m\n",
      "3.5 Positional Encoding\n",
      "Since our model contains no recurrence and no convolution, in order for the model to make use of the order of the sequence, we must inject some information about the relative or absolute position of the tokens in the sequence. To this end, we add \"positional encodings\" to the input embeddings at the bottoms of the encoder and decoder stacks. The positional encodings have the same dimension dmodelsubscript𝑑modeld_{\\text{model}} as the embeddings, so that the two can be summed. There are many choices of positional encodings, learned and fixed [9].\n",
      "\n",
      "In this work, we use sine and cosine functions of different frequencies:\n",
      "\n",
      "P​E(p​o​s,2​i)=s​i​n​(p​o​s/100002​i/dmodel)𝑃subscript𝐸𝑝𝑜𝑠2𝑖𝑠𝑖𝑛𝑝𝑜𝑠superscript100002𝑖subscript𝑑model\\displaystyle PE_{(pos,2i)}=sin(pos/10000^{2i/d_{\\text{model}}})\n",
      "\n",
      "P​E(p​o​s,2​i+1)=c​o​s​(p​o​s/100002​i/dmodel)𝑃subscript𝐸𝑝𝑜𝑠2𝑖1𝑐𝑜𝑠𝑝𝑜𝑠superscript100002𝑖subscript𝑑model\\displaystyle PE_{(pos,2i+1)}=cos(pos/10000^{2i/d_{\\text{model}}})\n",
      "\n",
      "where p​o​s𝑝𝑜𝑠pos is the position and i𝑖i is the dimension. That is, each dimension of the positional encoding corresponds to a sinusoid. The wavelengths form a geometric progression from 2​π2𝜋2\\pi to 10000⋅2​π⋅100002𝜋10000\\cdot 2\\pi. We chose this function because we hypothesized it would allow the model to easily learn to attend by relative positions, since for any fixed offset k𝑘k, P​Ep​o​s+k𝑃subscript𝐸𝑝𝑜𝑠𝑘PE_{pos+k} can be represented as a linear function of P​Ep​o​s𝑃subscript𝐸𝑝𝑜𝑠PE_{pos}.\n",
      "\n",
      "We also experimented with using learned positional embeddings [9] instead, and found that the two versions produced nearly identical results (see Table 3 row (E)). We chose the sinusoidal version because it may allow the model to extrapolate to sequence lengths longer than the ones encountered during training.\n",
      "\u001b[0m\u001b[32;1m\u001b[1;3m\"Attention is All You Need\" 논문에서 positional encoding에 대해 설명한 부분은 다음과 같습니다:\n",
      "\n",
      "### Positional Encoding\n",
      "\n",
      "모델에 recurrence(순환)이나 convolution(합성곱)이 없기 때문에, 시퀀스의 순서를 모델이 활용할 수 있도록 하기 위해서는 토큰의 상대적 또는 절대적 위치에 대한 정보를 주입해야 합니다. 이를 위해, 인코더와 디코더 스택의 입력 임베딩에 \"positional encodings\"를 추가합니다. positional encodings는 임베딩과 동일한 차원 \\(d_{\\text{model}}\\)을 가지며, 두 값을 더할 수 있습니다. positional encodings는 학습된 것과 고정된 것 등 여러 가지 선택지가 있습니다.\n",
      "\n",
      "이 논문에서는 다양한 주파수의 사인(sine) 함수와 코사인(cosine) 함수를 사용합니다:\n",
      "\n",
      "\\[ PE_{(pos, 2i)} = \\sin\\left(\\frac{pos}{10000^{2i/d_{\\text{model}}}}\\right) \\]\n",
      "\n",
      "\\[ PE_{(pos, 2i+1)} = \\cos\\left(\\frac{pos}{10000^{2i/d_{\\text{model}}}}\\right) \\]\n",
      "\n",
      "여기서 \\(pos\\)는 위치를, \\(i\\)는 차원을 나타냅니다. 즉, positional encoding의 각 차원은 사인파에 해당합니다. 파장의 길이는 \\(2\\pi\\)에서 \\(10000 \\cdot 2\\pi\\)까지 기하급수적으로 증가합니다. 이 함수를 선택한 이유는 모델이 상대적 위치에 따라 쉽게 주목(attend)할 수 있도록 하기 위해서입니다. 고정된 오프셋 \\(k\\)에 대해 \\(PE_{pos+k}\\)는 \\(PE_{pos}\\)의 선형 함수로 표현될 수 있기 때문입니다.\n",
      "\n",
      "또한, 학습된 positional embeddings를 사용하는 실험도 진행했으며, 두 버전이 거의 동일한 결과를 낳는다는 것을 발견했습니다. (표 3의 (E) 행 참조) 우리는 사인파 버전을 선택했는데, 이는 모델이 학습 중에 접한 시퀀스 길이보다 더 긴 시퀀스 길이로 외삽할 수 있게 할 수 있기 때문입니다.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "\"Attention is All You Need\" 논문에서 positional encoding에 대해 설명한 부분은 다음과 같습니다:\n",
       "\n",
       "### Positional Encoding\n",
       "\n",
       "모델에 recurrence(순환)이나 convolution(합성곱)이 없기 때문에, 시퀀스의 순서를 모델이 활용할 수 있도록 하기 위해서는 토큰의 상대적 또는 절대적 위치에 대한 정보를 주입해야 합니다. 이를 위해, 인코더와 디코더 스택의 입력 임베딩에 \"positional encodings\"를 추가합니다. positional encodings는 임베딩과 동일한 차원 \\(d_{\\text{model}}\\)을 가지며, 두 값을 더할 수 있습니다. positional encodings는 학습된 것과 고정된 것 등 여러 가지 선택지가 있습니다.\n",
       "\n",
       "이 논문에서는 다양한 주파수의 사인(sine) 함수와 코사인(cosine) 함수를 사용합니다:\n",
       "\n",
       "\\[ PE_{(pos, 2i)} = \\sin\\left(\\frac{pos}{10000^{2i/d_{\\text{model}}}}\\right) \\]\n",
       "\n",
       "\\[ PE_{(pos, 2i+1)} = \\cos\\left(\\frac{pos}{10000^{2i/d_{\\text{model}}}}\\right) \\]\n",
       "\n",
       "여기서 \\(pos\\)는 위치를, \\(i\\)는 차원을 나타냅니다. 즉, positional encoding의 각 차원은 사인파에 해당합니다. 파장의 길이는 \\(2\\pi\\)에서 \\(10000 \\cdot 2\\pi\\)까지 기하급수적으로 증가합니다. 이 함수를 선택한 이유는 모델이 상대적 위치에 따라 쉽게 주목(attend)할 수 있도록 하기 위해서입니다. 고정된 오프셋 \\(k\\)에 대해 \\(PE_{pos+k}\\)는 \\(PE_{pos}\\)의 선형 함수로 표현될 수 있기 때문입니다.\n",
       "\n",
       "또한, 학습된 positional embeddings를 사용하는 실험도 진행했으며, 두 버전이 거의 동일한 결과를 낳는다는 것을 발견했습니다. (표 3의 (E) 행 참조) 우리는 사인파 버전을 선택했는데, 이는 모델이 학습 중에 접한 시퀀스 길이보다 더 긴 시퀀스 길이로 외삽할 수 있게 할 수 있기 때문입니다."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "assistant('\"Attention is all you need\" arxiv id 1706.03762 에서 positional encoding 에 대해 설명해줘')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `loadpaper` with `{'title': 'Attention is all you need', 'arxiv_id': '1706.03762'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3mHere is the title and section of the paper in HTML\n",
      "title\n",
      "Attention Is All You Need\n",
      "sections\n",
      "          Abstract\n",
      "  1 Introduction\n",
      "  2 Background\n",
      "  3 Model Architecture\n",
      "    3.1 Encoder and Decoder Stacks\n",
      "        Encoder:\n",
      "        Decoder:\n",
      "    3.2 Attention\n",
      "      3.2.1 Scaled Dot-Product Attention\n",
      "      3.2.2 Multi-Head Attention\n",
      "      3.2.3 Applications of Attention in our Model\n",
      "    3.3 Position-wise Feed-Forward Networks\n",
      "    3.4 Embeddings and Softmax\n",
      "    3.5 Positional Encoding\n",
      "  4 Why Self-Attention\n",
      "  5 Training\n",
      "    5.1 Training Data and Batching\n",
      "    5.2 Hardware and Schedule\n",
      "    5.3 Optimizer\n",
      "    5.4 Regularization\n",
      "        Residual Dropout\n",
      "        Label Smoothing\n",
      "  6 Results\n",
      "    6.1 Machine Translation\n",
      "    6.2 Model Variations\n",
      "    6.3 English Constituency Parsing\n",
      "  7 Conclusion\n",
      "        Acknowledgements\n",
      "  References\n",
      "  Attention Visualizations\n",
      "\n",
      "\n",
      " Use the 'loadpaper' tool again, specifying the section list you want to view in detail.\u001b[0m\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `loadpaper` with `{'title': 'Attention is all you need', 'arxiv_id': '1706.03762', 'sections': ['3 Model Architecture']}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3m\n",
      "3 Model Architecture\n",
      "Figure 1: The Transformer - model architecture.\n",
      "\n",
      "Most competitive neural sequence transduction models have an encoder-decoder structure [5, 2, 35]. Here, the encoder maps an input sequence of symbol representations (x1,…,xn)subscript𝑥1…subscript𝑥𝑛(x_{1},...,x_{n}) to a sequence of continuous representations 𝐳=(z1,…,zn)𝐳subscript𝑧1…subscript𝑧𝑛\\mathbf{z}=(z_{1},...,z_{n}). Given 𝐳𝐳\\mathbf{z}, the decoder then generates an output sequence (y1,…,ym)subscript𝑦1…subscript𝑦𝑚(y_{1},...,y_{m}) of symbols one element at a time. At each step the model is auto-regressive [10], consuming the previously generated symbols as additional input when generating the next.\n",
      "\n",
      "The Transformer follows this overall architecture using stacked self-attention and point-wise, fully connected layers for both the encoder and decoder, shown in the left and right halves of Figure 1, respectively.\n",
      "\n",
      "3.1 Encoder and Decoder Stacks\n",
      "\n",
      "Encoder:\n",
      "\n",
      "The encoder is composed of a stack of N=6𝑁6N=6 identical layers. Each layer has two sub-layers. The first is a multi-head self-attention mechanism, and the second is a simple, position-wise fully connected feed-forward network. We employ a residual connection [11] around each of the two sub-layers, followed by layer normalization [1]. That is, the output of each sub-layer is LayerNorm​(x+Sublayer​(x))LayerNorm𝑥Sublayer𝑥\\mathrm{LayerNorm}(x+\\mathrm{Sublayer}(x)), where Sublayer​(x)Sublayer𝑥\\mathrm{Sublayer}(x) is the function implemented by the sub-layer itself. To facilitate these residual connections, all sub-layers in the model, as well as the embedding layers, produce outputs of dimension dmodel=512subscript𝑑model512d_{\\text{model}}=512.\n",
      "\n",
      "Decoder:\n",
      "\n",
      "The decoder is also composed of a stack of N=6𝑁6N=6 identical layers. In addition to the two sub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head attention over the output of the encoder stack. Similar to the encoder, we employ residual connections around each of the sub-layers, followed by layer normalization. We also modify the self-attention sub-layer in the decoder stack to prevent positions from attending to subsequent positions. This masking, combined with fact that the output embeddings are offset by one position, ensures that the predictions for position i𝑖i can depend only on the known outputs at positions less than i𝑖i.\n",
      "\n",
      "3.2 Attention\n",
      "\n",
      "An attention function can be described as mapping a query and a set of key-value pairs to an output, where the query, keys, values, and output are all vectors. The output is computed as a weighted sum of the values, where the weight assigned to each value is computed by a compatibility function of the query with the corresponding key.\n",
      "\n",
      "3.2.1 Scaled Dot-Product Attention\n",
      "\n",
      "We call our particular attention \"Scaled Dot-Product Attention\" (Figure 2). The input consists of queries and keys of dimension dksubscript𝑑𝑘d_{k}, and values of dimension dvsubscript𝑑𝑣d_{v}. We compute the dot products of the query with all keys, divide each by dksubscript𝑑𝑘\\sqrt{d_{k}}, and apply a softmax function to obtain the weights on the values.\n",
      "\n",
      "In practice, we compute the attention function on a set of queries simultaneously, packed together into a matrix Q𝑄Q. The keys and values are also packed together into matrices K𝐾K and V𝑉V. We compute the matrix of outputs as:\n",
      "\n",
      "Attention​(Q,K,V)=softmax​(Q​KTdk)​VAttention𝑄𝐾𝑉softmax𝑄superscript𝐾𝑇subscript𝑑𝑘𝑉\\mathrm{Attention}(Q,K,V)=\\mathrm{softmax}(\\frac{QK^{T}}{\\sqrt{d_{k}}})V\n",
      "\n",
      "(1)\n",
      "\n",
      "The two most commonly used attention functions are additive attention [2], and dot-product (multiplicative) attention. Dot-product attention is identical to our algorithm, except for the scaling factor of 1dk1subscript𝑑𝑘\\frac{1}{\\sqrt{d_{k}}}. Additive attention computes the compatibility function using a feed-forward network with a single hidden layer. While the two are similar in theoretical complexity, dot-product attention is much faster and more space-efficient in practice, since it can be implemented using highly optimized matrix multiplication code.\n",
      "\n",
      "While for small values of dksubscript𝑑𝑘d_{k} the two mechanisms perform similarly, additive attention outperforms dot product attention without scaling for larger values of dksubscript𝑑𝑘d_{k} [3]. We suspect that for large values of dksubscript𝑑𝑘d_{k}, the dot products grow large in magnitude, pushing the softmax function into regions where it has extremely small gradients 111To illustrate why the dot products get large, assume that the components of q𝑞q and k𝑘k are independent random variables with mean 00 and variance 111. Then their dot product, q⋅k=∑i=1dkqi​ki⋅𝑞𝑘superscriptsubscript𝑖1subscript𝑑𝑘subscript𝑞𝑖subscript𝑘𝑖q\\cdot k=\\sum_{i=1}^{d_{k}}q_{i}k_{i}, has mean 00 and variance dksubscript𝑑𝑘d_{k}.. To counteract this effect, we scale the dot products by 1dk1subscript𝑑𝑘\\frac{1}{\\sqrt{d_{k}}}.\n",
      "\n",
      "3.2.2 Multi-Head Attention\n",
      "\n",
      "Scaled Dot-Product Attention\n",
      "\n",
      "Multi-Head Attention\n",
      "\n",
      "Figure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several attention layers running in parallel.\n",
      "\n",
      "Instead of performing a single attention function with dmodelsubscript𝑑modeld_{\\text{model}}-dimensional keys, values and queries, we found it beneficial to linearly project the queries, keys and values hℎh times with different, learned linear projections to dksubscript𝑑𝑘d_{k}, dksubscript𝑑𝑘d_{k} and dvsubscript𝑑𝑣d_{v} dimensions, respectively.\n",
      "On each of these projected versions of queries, keys and values we then perform the attention function in parallel, yielding dvsubscript𝑑𝑣d_{v}-dimensional output values. These are concatenated and once again projected, resulting in the final values, as depicted in Figure 2.\n",
      "\n",
      "Multi-head attention allows the model to jointly attend to information from different representation subspaces at different positions. With a single attention head, averaging inhibits this.\n",
      "\n",
      "MultiHead​(Q,K,V)MultiHead𝑄𝐾𝑉\\displaystyle\\mathrm{MultiHead}(Q,K,V)\n",
      "=Concat​(head1,…,headh)​WOabsentConcatsubscripthead1…subscriptheadhsuperscript𝑊𝑂\\displaystyle=\\mathrm{Concat}(\\mathrm{head_{1}},...,\\mathrm{head_{h}})W^{O}\n",
      "\n",
      "where​headiwheresubscriptheadi\\displaystyle\\text{where}~{}\\mathrm{head_{i}}\n",
      "=Attention​(Q​WiQ,K​WiK,V​WiV)absentAttention𝑄subscriptsuperscript𝑊𝑄𝑖𝐾subscriptsuperscript𝑊𝐾𝑖𝑉subscriptsuperscript𝑊𝑉𝑖\\displaystyle=\\mathrm{Attention}(QW^{Q}_{i},KW^{K}_{i},VW^{V}_{i})\n",
      "\n",
      "Where the projections are parameter matrices WiQ∈ℝdmodel×dksubscriptsuperscript𝑊𝑄𝑖superscriptℝsubscript𝑑modelsubscript𝑑𝑘W^{Q}_{i}\\in\\mathbb{R}^{d_{\\text{model}}\\times d_{k}}, WiK∈ℝdmodel×dksubscriptsuperscript𝑊𝐾𝑖superscriptℝsubscript𝑑modelsubscript𝑑𝑘W^{K}_{i}\\in\\mathbb{R}^{d_{\\text{model}}\\times d_{k}}, WiV∈ℝdmodel×dvsubscriptsuperscript𝑊𝑉𝑖superscriptℝsubscript𝑑modelsubscript𝑑𝑣W^{V}_{i}\\in\\mathbb{R}^{d_{\\text{model}}\\times d_{v}} and WO∈ℝh​dv×dmodelsuperscript𝑊𝑂superscriptℝℎsubscript𝑑𝑣subscript𝑑modelW^{O}\\in\\mathbb{R}^{hd_{v}\\times d_{\\text{model}}}.\n",
      "\n",
      "In this work we employ h=8ℎ8h=8 parallel attention layers, or heads. For each of these we use dk=dv=dmodel/h=64subscript𝑑𝑘subscript𝑑𝑣subscript𝑑modelℎ64d_{k}=d_{v}=d_{\\text{model}}/h=64.\n",
      "Due to the reduced dimension of each head, the total computational cost is similar to that of single-head attention with full dimensionality.\n",
      "\n",
      "3.2.3 Applications of Attention in our Model\n",
      "\n",
      "The Transformer uses multi-head attention in three different ways:\n",
      "\n",
      "•\n",
      "\n",
      "In \"encoder-decoder attention\" layers, the queries come from the previous decoder layer, and the memory keys and values come from the output of the encoder. This allows every position in the decoder to attend over all positions in the input sequence. This mimics the typical encoder-decoder attention mechanisms in sequence-to-sequence models such as [38, 2, 9].\n",
      "\n",
      "•\n",
      "\n",
      "The encoder contains self-attention layers. In a self-attention layer all of the keys, values and queries come from the same place, in this case, the output of the previous layer in the encoder. Each position in the encoder can attend to all positions in the previous layer of the encoder.\n",
      "\n",
      "•\n",
      "\n",
      "Similarly, self-attention layers in the decoder allow each position in the decoder to attend to all positions in the decoder up to and including that position. We need to prevent leftward information flow in the decoder to preserve the auto-regressive property. We implement this inside of scaled dot-product attention by masking out (setting to −∞-\\infty) all values in the input of the softmax which correspond to illegal connections. See Figure 2.\n",
      "\n",
      "3.3 Position-wise Feed-Forward Networks\n",
      "\n",
      "In addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully connected feed-forward network, which is applied to each position separately and identically. This consists of two linear transformations with a ReLU activation in between.\n",
      "\n",
      "FFN​(x)=max⁡(0,x​W1+b1)​W2+b2FFN𝑥0𝑥subscript𝑊1subscript𝑏1subscript𝑊2subscript𝑏2\\mathrm{FFN}(x)=\\max(0,xW_{1}+b_{1})W_{2}+b_{2}\n",
      "\n",
      "(2)\n",
      "\n",
      "While the linear transformations are the same across different positions, they use different parameters from layer to layer. Another way of describing this is as two convolutions with kernel size 1. The dimensionality of input and output is dmodel=512subscript𝑑model512d_{\\text{model}}=512, and the inner-layer has dimensionality df​f=2048subscript𝑑𝑓𝑓2048d_{ff}=2048.\n",
      "\n",
      "3.4 Embeddings and Softmax\n",
      "\n",
      "Similarly to other sequence transduction models, we use learned embeddings to convert the input tokens and output tokens to vectors of dimension dmodelsubscript𝑑modeld_{\\text{model}}. We also use the usual learned linear transformation and softmax function to convert the decoder output to predicted next-token probabilities. In our model, we share the same weight matrix between the two embedding layers and the pre-softmax linear transformation, similar to [30]. In the embedding layers, we multiply those weights by dmodelsubscript𝑑model\\sqrt{d_{\\text{model}}}.\n",
      "\n",
      "3.5 Positional Encoding\n",
      "\n",
      "Since our model contains no recurrence and no convolution, in order for the model to make use of the order of the sequence, we must inject some information about the relative or absolute position of the tokens in the sequence. To this end, we add \"positional encodings\" to the input embeddings at the bottoms of the encoder and decoder stacks. The positional encodings have the same dimension dmodelsubscript𝑑modeld_{\\text{model}} as the embeddings, so that the two can be summed. There are many choices of positional encodings, learned and fixed [9].\n",
      "\n",
      "In this work, we use sine and cosine functions of different frequencies:\n",
      "\n",
      "P​E(p​o​s,2​i)=s​i​n​(p​o​s/100002​i/dmodel)𝑃subscript𝐸𝑝𝑜𝑠2𝑖𝑠𝑖𝑛𝑝𝑜𝑠superscript100002𝑖subscript𝑑model\\displaystyle PE_{(pos,2i)}=sin(pos/10000^{2i/d_{\\text{model}}})\n",
      "\n",
      "P​E(p​o​s,2​i+1)=c​o​s​(p​o​s/100002​i/dmodel)𝑃subscript𝐸𝑝𝑜𝑠2𝑖1𝑐𝑜𝑠𝑝𝑜𝑠superscript100002𝑖subscript𝑑model\\displaystyle PE_{(pos,2i+1)}=cos(pos/10000^{2i/d_{\\text{model}}})\n",
      "\n",
      "where p​o​s𝑝𝑜𝑠pos is the position and i𝑖i is the dimension. That is, each dimension of the positional encoding corresponds to a sinusoid. The wavelengths form a geometric progression from 2​π2𝜋2\\pi to 10000⋅2​π⋅100002𝜋10000\\cdot 2\\pi. We chose this function because we hypothesized it would allow the model to easily learn to attend by relative positions, since for any fixed offset k𝑘k, P​Ep​o​s+k𝑃subscript𝐸𝑝𝑜𝑠𝑘PE_{pos+k} can be represented as a linear function of P​Ep​o​s𝑃subscript𝐸𝑝𝑜𝑠PE_{pos}.\n",
      "\n",
      "We also experimented with using learned positional embeddings [9] instead, and found that the two versions produced nearly identical results (see Table 3 row (E)). We chose the sinusoidal version because it may allow the model to extrapolate to sequence lengths longer than the ones encountered during training.\n",
      "\n",
      "\u001b[0m\u001b[32;1m\u001b[1;3mThe \"Method\" section of the paper \"Attention is All You Need\" is detailed under the \"Model Architecture\" section. Here is a summary of each element:\n",
      "\n",
      "### 3 Model Architecture\n",
      "\n",
      "#### 3.1 Encoder and Decoder Stacks\n",
      "\n",
      "- **Encoder**: \n",
      "  - Composed of 6 identical layers.\n",
      "  - Each layer has two sub-layers: a multi-head self-attention mechanism and a position-wise fully connected feed-forward network.\n",
      "  - Residual connections and layer normalization are applied around each sub-layer.\n",
      "  - All sub-layers and embedding layers produce outputs of dimension \\( d_{\\text{model}} = 512 \\).\n",
      "\n",
      "- **Decoder**: \n",
      "  - Also composed of 6 identical layers.\n",
      "  - Each layer has an additional third sub-layer for multi-head attention over the encoder's output.\n",
      "  - Similar to the encoder, residual connections and layer normalization are applied.\n",
      "  - Self-attention sub-layer is modified to prevent positions from attending to subsequent positions, ensuring auto-regressive property.\n",
      "\n",
      "#### 3.2 Attention\n",
      "\n",
      "- **Attention Function**: \n",
      "  - Maps a query and a set of key-value pairs to an output.\n",
      "  - Output is a weighted sum of the values, with weights computed by a compatibility function of the query with the corresponding key.\n",
      "\n",
      "##### 3.2.1 Scaled Dot-Product Attention\n",
      "\n",
      "- Computes dot products of the query with all keys, divides by \\(\\sqrt{d_k}\\), and applies a softmax function to obtain weights on the values.\n",
      "- Efficiently implemented using matrix multiplication.\n",
      "\n",
      "##### 3.2.2 Multi-Head Attention\n",
      "\n",
      "- Projects queries, keys, and values multiple times with different learned linear projections.\n",
      "- Performs attention function in parallel on these projections.\n",
      "- Concatenates and projects the results to form the final values.\n",
      "- Allows the model to attend to information from different representation subspaces at different positions.\n",
      "- Uses 8 parallel attention layers (heads), each with \\( d_k = d_v = d_{\\text{model}} / h = 64 \\).\n",
      "\n",
      "##### 3.2.3 Applications of Attention in the Model\n",
      "\n",
      "- **Encoder-Decoder Attention**: Queries from the decoder, keys, and values from the encoder.\n",
      "- **Self-Attention in Encoder**: Keys, values, and queries from the same layer output.\n",
      "- **Self-Attention in Decoder**: Similar to encoder but prevents leftward information flow to maintain auto-regressive property.\n",
      "\n",
      "#### 3.3 Position-wise Feed-Forward Networks\n",
      "\n",
      "- Each layer in the encoder and decoder contains a fully connected feed-forward network.\n",
      "- Applied to each position separately and identically.\n",
      "- Consists of two linear transformations with a ReLU activation in between.\n",
      "- Input and output dimensionality is \\( d_{\\text{model}} = 512 \\), and inner-layer dimensionality is \\( d_{ff} = 2048 \\).\n",
      "\n",
      "#### 3.4 Embeddings and Softmax\n",
      "\n",
      "- Uses learned embeddings to convert input and output tokens to vectors of dimension \\( d_{\\text{model}} \\).\n",
      "- Uses a learned linear transformation and softmax function to convert decoder output to predicted next-token probabilities.\n",
      "- Shares the same weight matrix between embedding layers and pre-softmax linear transformation, scaled by \\( \\sqrt{d_{\\text{model}}} \\).\n",
      "\n",
      "#### 3.5 Positional Encoding\n",
      "\n",
      "- Adds positional encodings to input embeddings to incorporate sequence order.\n",
      "- Positional encodings have the same dimension \\( d_{\\text{model}} \\) as embeddings.\n",
      "- Uses sine and cosine functions of different frequencies for encoding.\n",
      "- Allows the model to learn to attend by relative positions.\n",
      "- Experimented with learned positional embeddings but found sinusoidal version to be equally effective and potentially better for extrapolation to longer sequences.\n",
      "\n",
      "This summary encapsulates the key components and mechanisms of the Transformer model as described in the \"Attention is All You Need\" paper.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "The \"Method\" section of the paper \"Attention is All You Need\" is detailed under the \"Model Architecture\" section. Here is a summary of each element:\n",
       "\n",
       "### 3 Model Architecture\n",
       "\n",
       "#### 3.1 Encoder and Decoder Stacks\n",
       "\n",
       "- **Encoder**: \n",
       "  - Composed of 6 identical layers.\n",
       "  - Each layer has two sub-layers: a multi-head self-attention mechanism and a position-wise fully connected feed-forward network.\n",
       "  - Residual connections and layer normalization are applied around each sub-layer.\n",
       "  - All sub-layers and embedding layers produce outputs of dimension \\( d_{\\text{model}} = 512 \\).\n",
       "\n",
       "- **Decoder**: \n",
       "  - Also composed of 6 identical layers.\n",
       "  - Each layer has an additional third sub-layer for multi-head attention over the encoder's output.\n",
       "  - Similar to the encoder, residual connections and layer normalization are applied.\n",
       "  - Self-attention sub-layer is modified to prevent positions from attending to subsequent positions, ensuring auto-regressive property.\n",
       "\n",
       "#### 3.2 Attention\n",
       "\n",
       "- **Attention Function**: \n",
       "  - Maps a query and a set of key-value pairs to an output.\n",
       "  - Output is a weighted sum of the values, with weights computed by a compatibility function of the query with the corresponding key.\n",
       "\n",
       "##### 3.2.1 Scaled Dot-Product Attention\n",
       "\n",
       "- Computes dot products of the query with all keys, divides by \\(\\sqrt{d_k}\\), and applies a softmax function to obtain weights on the values.\n",
       "- Efficiently implemented using matrix multiplication.\n",
       "\n",
       "##### 3.2.2 Multi-Head Attention\n",
       "\n",
       "- Projects queries, keys, and values multiple times with different learned linear projections.\n",
       "- Performs attention function in parallel on these projections.\n",
       "- Concatenates and projects the results to form the final values.\n",
       "- Allows the model to attend to information from different representation subspaces at different positions.\n",
       "- Uses 8 parallel attention layers (heads), each with \\( d_k = d_v = d_{\\text{model}} / h = 64 \\).\n",
       "\n",
       "##### 3.2.3 Applications of Attention in the Model\n",
       "\n",
       "- **Encoder-Decoder Attention**: Queries from the decoder, keys, and values from the encoder.\n",
       "- **Self-Attention in Encoder**: Keys, values, and queries from the same layer output.\n",
       "- **Self-Attention in Decoder**: Similar to encoder but prevents leftward information flow to maintain auto-regressive property.\n",
       "\n",
       "#### 3.3 Position-wise Feed-Forward Networks\n",
       "\n",
       "- Each layer in the encoder and decoder contains a fully connected feed-forward network.\n",
       "- Applied to each position separately and identically.\n",
       "- Consists of two linear transformations with a ReLU activation in between.\n",
       "- Input and output dimensionality is \\( d_{\\text{model}} = 512 \\), and inner-layer dimensionality is \\( d_{ff} = 2048 \\).\n",
       "\n",
       "#### 3.4 Embeddings and Softmax\n",
       "\n",
       "- Uses learned embeddings to convert input and output tokens to vectors of dimension \\( d_{\\text{model}} \\).\n",
       "- Uses a learned linear transformation and softmax function to convert decoder output to predicted next-token probabilities.\n",
       "- Shares the same weight matrix between embedding layers and pre-softmax linear transformation, scaled by \\( \\sqrt{d_{\\text{model}}} \\).\n",
       "\n",
       "#### 3.5 Positional Encoding\n",
       "\n",
       "- Adds positional encodings to input embeddings to incorporate sequence order.\n",
       "- Positional encodings have the same dimension \\( d_{\\text{model}} \\) as embeddings.\n",
       "- Uses sine and cosine functions of different frequencies for encoding.\n",
       "- Allows the model to learn to attend by relative positions.\n",
       "- Experimented with learned positional embeddings but found sinusoidal version to be equally effective and potentially better for extrapolation to longer sequences.\n",
       "\n",
       "This summary encapsulates the key components and mechanisms of the Transformer model as described in the \"Attention is All You Need\" paper."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "assistant('논문 Method의 각 요소를 요약해줘')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset memory 로 메모리를 초기화합니다.\n",
    "assistant.reset_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m논문의 제목을 알려주시면, 해당 논문의 Method 섹션을 불러와 요약해드리겠습니다.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "논문의 제목을 알려주시면, 해당 논문의 Method 섹션을 불러와 요약해드리겠습니다."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "assistant('논문 Method의 각 요소를 요약해줘')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "assistant.reset_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `recommendpaper` with `{'query': 'PROMPTAGENT: STRATEGIC PLANNING WITH LANGUAGE MODELS ENABLES EXPERT-LEVEL PROMPT OPTIMIZATION', 'rec_type': 'citation'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[33;1m\u001b[1;3m[{'title': 'Agent-Pro: Learning to Evolve via Policy-Level Reflection and Optimization', 'abstract': \"Large Language Models (LLMs) exhibit robust problem-solving capabilities for diverse tasks. However, most LLM-based agents are designed as specific task solvers with sophisticated prompt engineering, rather than agents capable of learning and evolving through interactions. These task solvers necessitate manually crafted prompts to inform task rules and regulate LLM behaviors, inherently incapacitating to address complex dynamic scenarios e.g., large interactive games. In light of this, we propose Agent-Pro: an LLM-based Agent with Policy-level Reflection and Optimization that can learn a wealth of expertise from interactive experiences and progressively elevate its behavioral policy. Specifically, it involves a dynamic belief generation and reflection process for policy evolution. Rather than action-level reflection, Agent-Pro iteratively reflects on past trajectories and beliefs, fine-tuning its irrational beliefs for a better policy. Moreover, a depth-first search is employed for policy optimization, ensuring continual enhancement in policy payoffs. Agent-Pro is evaluated across two games: Blackjack and Texas Hold'em, outperforming vanilla LLM and specialized models. Our results show Agent-Pro can learn and evolve in complex and dynamic scenes, which also benefits numerous LLM-based applications.\", 'influentialCitationCount': 1, 'publicationDate': '2024-02-27'}, {'title': 'RoT: Enhancing Large Language Models with Reflection on Search Trees', 'abstract': 'Large language models (LLMs) have demonstrated impressive capability in reasoning and planning when integrated with tree-search-based prompting methods. However, since these methods ignore the previous search experiences, they often make the same mistakes in the search process. To address this issue, we introduce Reflection on search Trees (RoT), an LLM reflection framework designed to improve the performance of tree-search-based prompting methods. It uses a strong LLM to summarize guidelines from previous tree search experiences to enhance the ability of a weak LLM. The guidelines are instructions about solving this task through tree search which can prevent the weak LLMs from making similar mistakes in the past search process. In addition, we proposed a novel state selection method, which identifies the critical information from historical search processes to help RoT generate more specific and meaningful guidelines. In our extensive experiments, we find that RoT significantly improves the performance of LLMs in reasoning or planning tasks with various tree-search-based prompting methods (e.g., BFS and MCTS). Non-tree-search-based prompting methods such as Chain-of-Thought (CoT) can also benefit from RoT guidelines since RoT can provide task-specific knowledge collected from the search experience.', 'influentialCitationCount': 1, 'publicationDate': '2024-04-08'}, {'title': 'Fairer Preferences Elicit Improved Human-Aligned Large Language Model Judgments', 'abstract': 'Large language models (LLMs) have shown promising abilities as cost-effective and reference-free evaluators for assessing language generation quality. In particular, pairwise LLM evaluators, which compare two generated texts and determine the preferred one, have been employed in a wide range of applications. However, LLMs exhibit preference biases and worrying sensitivity to prompt designs. In this work, we first reveal that the predictive preference of LLMs can be highly brittle and skewed, even with semantically equivalent instructions. We find that fairer predictive preferences from LLMs consistently lead to judgments that are better aligned with humans. Motivated by this phenomenon, we propose an automatic Zero-shot Evaluation-oriented Prompt Optimization framework, ZEPO, which aims to produce fairer preference decisions and improve the alignment of LLM evaluators with human judgments. To this end, we propose a zero-shot learning objective based on the preference decision fairness. ZEPO demonstrates substantial performance improvements over state-of-the-art LLM evaluators, without requiring labeled data, on representative meta-evaluation benchmarks. Our findings underscore the critical correlation between preference fairness and human alignment, positioning ZEPO as an efficient prompt optimizer for bridging the gap between LLM evaluators and human judgments.', 'influentialCitationCount': 0, 'publicationDate': '2024-06-17'}, {'title': 'Dual-Phase Accelerated Prompt Optimization', 'abstract': 'Gradient-free prompt optimization methods have made significant strides in enhancing the performance of closed-source Large Language Models (LLMs) across a wide range of tasks. However, existing approaches make light of the importance of high-quality prompt initialization and the identification of effective optimization directions, thus resulting in substantial optimization steps to obtain satisfactory performance. In this light, we aim to accelerate prompt optimization process to tackle the challenge of low convergence rate. We propose a dual-phase approach which starts with generating high-quality initial prompts by adopting a well-designed meta-instruction to delve into task-specific information, and iteratively optimize the prompts at the sentence level, leveraging previous tuning experience to expand prompt candidates and accept effective ones. Extensive experiments on eight datasets demonstrate the effectiveness of our proposed method, achieving a consistent accuracy gain over baselines with less than five optimization steps.', 'influentialCitationCount': 0, 'publicationDate': '2024-06-19'}, {'title': 'Trace is the New AutoDiff -- Unlocking Efficient Optimization of Computational Workflows', 'abstract': \"We study a class of optimization problems motivated by automating the design and update of AI systems like coding assistants, robots, and copilots. We propose an end-to-end optimization framework, Trace, which treats the computational workflow of an AI system as a graph akin to neural networks, based on a generalization of back-propagation. Optimization of computational workflows often involves rich feedback (e.g. console output or user's responses), heterogeneous parameters (e.g. prompts, hyper-parameters, codes), and intricate objectives (beyond maximizing a score). Moreover, its computation graph can change dynamically with the inputs and parameters. We frame a new mathematical setup of iterative optimization, Optimization with Trace Oracle (OPTO), to capture and abstract these properties so as to design optimizers that work across many domains. In OPTO, an optimizer receives an execution trace along with feedback on the computed output and updates parameters iteratively. Trace is the tool to implement OPTO in practice. Trace has a Python interface that efficiently converts a computational workflow into an OPTO instance using a PyTorch-like interface. Using Trace, we develop a general-purpose LLM-based optimizer called OptoPrime that can effectively solve OPTO problems. In empirical studies, we find that OptoPrime is capable of first-order numerical optimization, prompt optimization, hyper-parameter tuning, robot controller design, code debugging, etc., and is often competitive with specialized optimizers for each domain. We believe that Trace, OptoPrime and the OPTO framework will enable the next generation of interactive agents that automatically adapt using various kinds of feedback. Website: https://microsoft.github.io/Trace\", 'influentialCitationCount': 0, 'publicationDate': '2024-06-23'}]\u001b[0m\u001b[32;1m\u001b[1;3mHere are some recent papers that cite \"PROMPTAGENT: STRATEGIC PLANNING WITH LANGUAGE MODELS ENABLES EXPERT-LEVEL PROMPT OPTIMIZATION\":\n",
      "\n",
      "1. **Agent-Pro: Learning to Evolve via Policy-Level Reflection and Optimization**\n",
      "   - **Abstract**: Large Language Models (LLMs) exhibit robust problem-solving capabilities for diverse tasks. However, most LLM-based agents are designed as specific task solvers with sophisticated prompt engineering, rather than agents capable of learning and evolving through interactions. These task solvers necessitate manually crafted prompts to inform task rules and regulate LLM behaviors, inherently incapacitating to address complex dynamic scenarios e.g., large interactive games. In light of this, we propose Agent-Pro: an LLM-based Agent with Policy-level Reflection and Optimization that can learn a wealth of expertise from interactive experiences and progressively elevate its behavioral policy. Specifically, it involves a dynamic belief generation and reflection process for policy evolution. Rather than action-level reflection, Agent-Pro iteratively reflects on past trajectories and beliefs, fine-tuning its irrational beliefs for a better policy. Moreover, a depth-first search is employed for policy optimization, ensuring continual enhancement in policy payoffs. Agent-Pro is evaluated across two games: Blackjack and Texas Hold'em, outperforming vanilla LLM and specialized models. Our results show Agent-Pro can learn and evolve in complex and dynamic scenes, which also benefits numerous LLM-based applications.\n",
      "   - **Publication Date**: 2024-02-27\n",
      "   - **Influential Citation Count**: 1\n",
      "\n",
      "2. **RoT: Enhancing Large Language Models with Reflection on Search Trees**\n",
      "   - **Abstract**: Large language models (LLMs) have demonstrated impressive capability in reasoning and planning when integrated with tree-search-based prompting methods. However, since these methods ignore the previous search experiences, they often make the same mistakes in the search process. To address this issue, we introduce Reflection on search Trees (RoT), an LLM reflection framework designed to improve the performance of tree-search-based prompting methods. It uses a strong LLM to summarize guidelines from previous tree search experiences to enhance the ability of a weak LLM. The guidelines are instructions about solving this task through tree search which can prevent the weak LLMs from making similar mistakes in the past search process. In addition, we proposed a novel state selection method, which identifies the critical information from historical search processes to help RoT generate more specific and meaningful guidelines. In our extensive experiments, we find that RoT significantly improves the performance of LLMs in reasoning or planning tasks with various tree-search-based prompting methods (e.g., BFS and MCTS). Non-tree-search-based prompting methods such as Chain-of-Thought (CoT) can also benefit from RoT guidelines since RoT can provide task-specific knowledge collected from the search experience.\n",
      "   - **Publication Date**: 2024-04-08\n",
      "   - **Influential Citation Count**: 1\n",
      "\n",
      "3. **Fairer Preferences Elicit Improved Human-Aligned Large Language Model Judgments**\n",
      "   - **Abstract**: Large language models (LLMs) have shown promising abilities as cost-effective and reference-free evaluators for assessing language generation quality. In particular, pairwise LLM evaluators, which compare two generated texts and determine the preferred one, have been employed in a wide range of applications. However, LLMs exhibit preference biases and worrying sensitivity to prompt designs. In this work, we first reveal that the predictive preference of LLMs can be highly brittle and skewed, even with semantically equivalent instructions. We find that fairer predictive preferences from LLMs consistently lead to judgments that are better aligned with humans. Motivated by this phenomenon, we propose an automatic Zero-shot Evaluation-oriented Prompt Optimization framework, ZEPO, which aims to produce fairer preference decisions and improve the alignment of LLM evaluators with human judgments. To this end, we propose a zero-shot learning objective based on the preference decision fairness. ZEPO demonstrates substantial performance improvements over state-of-the-art LLM evaluators, without requiring labeled data, on representative meta-evaluation benchmarks. Our findings underscore the critical correlation between preference fairness and human alignment, positioning ZEPO as an efficient prompt optimizer for bridging the gap between LLM evaluators and human judgments.\n",
      "   - **Publication Date**: 2024-06-17\n",
      "   - **Influential Citation Count**: 0\n",
      "\n",
      "4. **Dual-Phase Accelerated Prompt Optimization**\n",
      "   - **Abstract**: Gradient-free prompt optimization methods have made significant strides in enhancing the performance of closed-source Large Language Models (LLMs) across a wide range of tasks. However, existing approaches make light of the importance of high-quality prompt initialization and the identification of effective optimization directions, thus resulting in substantial optimization steps to obtain satisfactory performance. In this light, we aim to accelerate prompt optimization process to tackle the challenge of low convergence rate. We propose a dual-phase approach which starts with generating high-quality initial prompts by adopting a well-designed meta-instruction to delve into task-specific information, and iteratively optimize the prompts at the sentence level, leveraging previous tuning experience to expand prompt candidates and accept effective ones. Extensive experiments on eight datasets demonstrate the effectiveness of our proposed method, achieving a consistent accuracy gain over baselines with less than five optimization steps.\n",
      "   - **Publication Date**: 2024-06-19\n",
      "   - **Influential Citation Count**: 0\n",
      "\n",
      "5. **Trace is the New AutoDiff -- Unlocking Efficient Optimization of Computational Workflows**\n",
      "   - **Abstract**: We study a class of optimization problems motivated by automating the design and update of AI systems like coding assistants, robots, and copilots. We propose an end-to-end optimization framework, Trace, which treats the computational workflow of an AI system as a graph akin to neural networks, based on a generalization of back-propagation. Optimization of computational workflows often involves rich feedback (e.g. console output or user's responses), heterogeneous parameters (e.g. prompts, hyper-parameters, codes), and intricate objectives (beyond maximizing a score). Moreover, its computation graph can change dynamically with the inputs and parameters. We frame a new mathematical setup of iterative optimization, Optimization with Trace Oracle (OPTO), to capture and abstract these properties so as to design optimizers that work across many domains. In OPTO, an optimizer receives an execution trace along with feedback on the computed output and updates parameters iteratively. Trace is the tool to implement OPTO in practice. Trace has a Python interface that efficiently converts a computational workflow into an OPTO instance using a PyTorch-like interface. Using Trace, we develop a general-purpose LLM-based optimizer called OptoPrime that can effectively solve OPTO problems. In empirical studies, we find that OptoPrime is capable of first-order numerical optimization, prompt optimization, hyper-parameter tuning, robot controller design, code debugging, etc., and is often competitive with specialized optimizers for each domain. We believe that Trace, OptoPrime and the OPTO framework will enable the next generation of interactive agents that automatically adapt using various kinds of feedback.\n",
      "   - **Publication Date**: 2024-06-23\n",
      "   - **Influential Citation Count**: 0\n",
      "\n",
      "These papers build upon the concepts introduced in \"PROMPTAGENT\" and explore various aspects of prompt optimization, reflection, and learning in large language models.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Here are some recent papers that cite \"PROMPTAGENT: STRATEGIC PLANNING WITH LANGUAGE MODELS ENABLES EXPERT-LEVEL PROMPT OPTIMIZATION\":\n",
       "\n",
       "1. **Agent-Pro: Learning to Evolve via Policy-Level Reflection and Optimization**\n",
       "   - **Abstract**: Large Language Models (LLMs) exhibit robust problem-solving capabilities for diverse tasks. However, most LLM-based agents are designed as specific task solvers with sophisticated prompt engineering, rather than agents capable of learning and evolving through interactions. These task solvers necessitate manually crafted prompts to inform task rules and regulate LLM behaviors, inherently incapacitating to address complex dynamic scenarios e.g., large interactive games. In light of this, we propose Agent-Pro: an LLM-based Agent with Policy-level Reflection and Optimization that can learn a wealth of expertise from interactive experiences and progressively elevate its behavioral policy. Specifically, it involves a dynamic belief generation and reflection process for policy evolution. Rather than action-level reflection, Agent-Pro iteratively reflects on past trajectories and beliefs, fine-tuning its irrational beliefs for a better policy. Moreover, a depth-first search is employed for policy optimization, ensuring continual enhancement in policy payoffs. Agent-Pro is evaluated across two games: Blackjack and Texas Hold'em, outperforming vanilla LLM and specialized models. Our results show Agent-Pro can learn and evolve in complex and dynamic scenes, which also benefits numerous LLM-based applications.\n",
       "   - **Publication Date**: 2024-02-27\n",
       "   - **Influential Citation Count**: 1\n",
       "\n",
       "2. **RoT: Enhancing Large Language Models with Reflection on Search Trees**\n",
       "   - **Abstract**: Large language models (LLMs) have demonstrated impressive capability in reasoning and planning when integrated with tree-search-based prompting methods. However, since these methods ignore the previous search experiences, they often make the same mistakes in the search process. To address this issue, we introduce Reflection on search Trees (RoT), an LLM reflection framework designed to improve the performance of tree-search-based prompting methods. It uses a strong LLM to summarize guidelines from previous tree search experiences to enhance the ability of a weak LLM. The guidelines are instructions about solving this task through tree search which can prevent the weak LLMs from making similar mistakes in the past search process. In addition, we proposed a novel state selection method, which identifies the critical information from historical search processes to help RoT generate more specific and meaningful guidelines. In our extensive experiments, we find that RoT significantly improves the performance of LLMs in reasoning or planning tasks with various tree-search-based prompting methods (e.g., BFS and MCTS). Non-tree-search-based prompting methods such as Chain-of-Thought (CoT) can also benefit from RoT guidelines since RoT can provide task-specific knowledge collected from the search experience.\n",
       "   - **Publication Date**: 2024-04-08\n",
       "   - **Influential Citation Count**: 1\n",
       "\n",
       "3. **Fairer Preferences Elicit Improved Human-Aligned Large Language Model Judgments**\n",
       "   - **Abstract**: Large language models (LLMs) have shown promising abilities as cost-effective and reference-free evaluators for assessing language generation quality. In particular, pairwise LLM evaluators, which compare two generated texts and determine the preferred one, have been employed in a wide range of applications. However, LLMs exhibit preference biases and worrying sensitivity to prompt designs. In this work, we first reveal that the predictive preference of LLMs can be highly brittle and skewed, even with semantically equivalent instructions. We find that fairer predictive preferences from LLMs consistently lead to judgments that are better aligned with humans. Motivated by this phenomenon, we propose an automatic Zero-shot Evaluation-oriented Prompt Optimization framework, ZEPO, which aims to produce fairer preference decisions and improve the alignment of LLM evaluators with human judgments. To this end, we propose a zero-shot learning objective based on the preference decision fairness. ZEPO demonstrates substantial performance improvements over state-of-the-art LLM evaluators, without requiring labeled data, on representative meta-evaluation benchmarks. Our findings underscore the critical correlation between preference fairness and human alignment, positioning ZEPO as an efficient prompt optimizer for bridging the gap between LLM evaluators and human judgments.\n",
       "   - **Publication Date**: 2024-06-17\n",
       "   - **Influential Citation Count**: 0\n",
       "\n",
       "4. **Dual-Phase Accelerated Prompt Optimization**\n",
       "   - **Abstract**: Gradient-free prompt optimization methods have made significant strides in enhancing the performance of closed-source Large Language Models (LLMs) across a wide range of tasks. However, existing approaches make light of the importance of high-quality prompt initialization and the identification of effective optimization directions, thus resulting in substantial optimization steps to obtain satisfactory performance. In this light, we aim to accelerate prompt optimization process to tackle the challenge of low convergence rate. We propose a dual-phase approach which starts with generating high-quality initial prompts by adopting a well-designed meta-instruction to delve into task-specific information, and iteratively optimize the prompts at the sentence level, leveraging previous tuning experience to expand prompt candidates and accept effective ones. Extensive experiments on eight datasets demonstrate the effectiveness of our proposed method, achieving a consistent accuracy gain over baselines with less than five optimization steps.\n",
       "   - **Publication Date**: 2024-06-19\n",
       "   - **Influential Citation Count**: 0\n",
       "\n",
       "5. **Trace is the New AutoDiff -- Unlocking Efficient Optimization of Computational Workflows**\n",
       "   - **Abstract**: We study a class of optimization problems motivated by automating the design and update of AI systems like coding assistants, robots, and copilots. We propose an end-to-end optimization framework, Trace, which treats the computational workflow of an AI system as a graph akin to neural networks, based on a generalization of back-propagation. Optimization of computational workflows often involves rich feedback (e.g. console output or user's responses), heterogeneous parameters (e.g. prompts, hyper-parameters, codes), and intricate objectives (beyond maximizing a score). Moreover, its computation graph can change dynamically with the inputs and parameters. We frame a new mathematical setup of iterative optimization, Optimization with Trace Oracle (OPTO), to capture and abstract these properties so as to design optimizers that work across many domains. In OPTO, an optimizer receives an execution trace along with feedback on the computed output and updates parameters iteratively. Trace is the tool to implement OPTO in practice. Trace has a Python interface that efficiently converts a computational workflow into an OPTO instance using a PyTorch-like interface. Using Trace, we develop a general-purpose LLM-based optimizer called OptoPrime that can effectively solve OPTO problems. In empirical studies, we find that OptoPrime is capable of first-order numerical optimization, prompt optimization, hyper-parameter tuning, robot controller design, code debugging, etc., and is often competitive with specialized optimizers for each domain. We believe that Trace, OptoPrime and the OPTO framework will enable the next generation of interactive agents that automatically adapt using various kinds of feedback.\n",
       "   - **Publication Date**: 2024-06-23\n",
       "   - **Influential Citation Count**: 0\n",
       "\n",
       "These papers build upon the concepts introduced in \"PROMPTAGENT\" and explore various aspects of prompt optimization, reflection, and learning in large language models."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# semantic scholar error 발생 시 다시 한 번 실행하면 됩니다.\n",
    "assistant('PROMPTAGENT: STRATEGIC PLANNING WITH LANGUAGE MODELS ENABLES EXPERT-LEVEL PROMPT OPTIMIZATION 후속 논문 추천해줘') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "assistant.reset_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `loadpaper` with `{'title': 'A Survey on Recent Advances in LLM-Based Multi-turn Dialogue Systems', 'arxiv_id': '2402.18013'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3mHere is the title and section of the paper in HTML\n",
      "title\n",
      "A Survey on Recent Advances in LLM-Based Multi-turn Dialogue Systems\n",
      "sections\n",
      "          Abstract.\n",
      "  1. INTRODUCTION\n",
      "    1.1. What is Multi-turn Dialogue System?\n",
      "    1.2. Why a Survey on LLM-based Multi-turn Dialogue System?\n",
      "    1.3. Contribution of this Survey\n",
      "  2. GENERAL METHODS\n",
      "    2.1. Decoder-only Transformer Architecture\n",
      "      2.1.1. Causal Decoder\n",
      "        GPT Series\n",
      "          GPT-1.\n",
      "          GPT-2.\n",
      "          GPT-3.\n",
      "          GPT-3.5.\n",
      "          GPT-4.\n",
      "          ChatGPT.\n",
      "          GPTs.\n",
      "        LLAMA Series\n",
      "          LLAMA.\n",
      "          LLAMA2.\n",
      "          LLAMA2 CHAT.\n",
      "          CODE LLAMA.\n",
      "      2.1.2. Prefix Decoder\n",
      "        GLM\n",
      "        ChatGLM series\n",
      "          ChatGLM-6B.\n",
      "          ChatGLM2-6B.\n",
      "          ChatGLM3-6B.\n",
      "    2.2. Encoder-only Transformer Architecture\n",
      "      2.2.1. BERT Series\n",
      "        BERT\n",
      "        RoBERTa\n",
      "      2.2.2. UNiLM\n",
      "    2.3. Encoder-decoder Transformer Architecture\n",
      "      2.3.1. BART\n",
      "      2.3.2. Text-to-Text Transfer Transformer\n",
      "  3. Fine-Tuning\n",
      "    3.1. Full Fine-Tuning\n",
      "    3.2. Parameter-efficient Fine-Tuning\n",
      "      3.2.1. Adapters\n",
      "      3.2.2. LoRA\n",
      "      3.2.3. Instruction Fine-Tuning\n",
      "    3.3. Mitigating Fine-Tuning Instabilities\n",
      "  4. Prompt Engineering\n",
      "    4.1. Prompt Tuning\n",
      "      4.1.1. Discrete Prompts\n",
      "        D1:Pattern-Exploiting Training\n",
      "        D2:LM-BFF\n",
      "        D3:Prompt Tuning with Rules\n",
      "        D4:Knowledgeable Prompt-tuning\n",
      "      4.1.2. Continuous Prompts\n",
      "        C1:P-tuning\n",
      "        C2:Prefix Tuning\n",
      "        C3: P-tuning V2\n",
      "    4.2. Tuning-free Prompting\n",
      "      4.2.1. In-Context Learning\n",
      "      4.2.2. Chain-of-Thought\n",
      "  5. LLM Based Task-oriented Dialogue Systems\n",
      "    5.1. Pipeline-based Methods\n",
      "      5.1.1. Natural Language Understanding\n",
      "        Intent detection\n",
      "        Joint intent detection and slot filling\n",
      "      5.1.2. Dialogue State Tracking\n",
      "      5.1.3. Policy Learning\n",
      "      5.1.4. Natural Language Generation\n",
      "    5.2. End-to-End Methods\n",
      "  6. LLM Based Open-Domain Dialogue Systems\n",
      "    6.1. Retrieval-based Methods\n",
      "    6.2. Generation-based Methods\n",
      "      6.2.1. Knowledge-Enhanced Generation\n",
      "      6.2.2. Personalization and Consistency\n",
      "    6.3. Hybrid Methods\n",
      "      6.3.1. Integrating Retrieval and Generation\n",
      "      6.3.2. Enhancing Dialogue with External Knowledge\n",
      "  7. EVALUATION APPROACHES\n",
      "    7.1. Automatic Evaluation\n",
      "      7.1.1. Automatic Evaluation Approaches for Task-oriented Dialogue Systems\n",
      "        Joint Goal Accuracy\n",
      "        Slot Accuracy\n",
      "        Average Goal Accuracy\n",
      "        Requested Slot F1\n",
      "        BLEU\n",
      "        Entity F1\n",
      "      7.1.2. Automatic Evaluation Approaches for Open-domain Dialogue Systems\n",
      "        Perplexity\n",
      "        DIST-n\n",
      "        Recall@K\n",
      "    7.2. Human Evaluation\n",
      "  8. DATASETS\n",
      "    8.1. Datasets for Task-oriented Dialogue Systems\n",
      "        MultiWOZ\n",
      "        RiSAWOZ\n",
      "        CrossWOZ\n",
      "        PersuasionForGood\n",
      "        WOZ 2.0\n",
      "        Stanford Multi-Domain\n",
      "    8.2. Datasets for Open-domain Dialogue Systems\n",
      "        PersonaChat\n",
      "        MMdialog\n",
      "        Dailydialog\n",
      "        Pchatbot\n",
      "        PersonalDialogue\n",
      "        Douban\n",
      "  9. DISCUSSION\n",
      "  10. CONCLUSION\n",
      "  References\n",
      "\n",
      "\n",
      " Use the 'loadpaper' tool again, specifying the section list you want to view in detail.\u001b[0m\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `loadpaper` with `{'title': 'A Survey on Recent Advances in LLM-Based Multi-turn Dialogue Systems', 'sections': ['Abstract'], 'arxiv_id': '2402.18013'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3mAbstract.This survey provides a comprehensive review of research on multi-turn dialogue systems, with a particular focus on multi-turn dialogue systems based on large language models (LLMs). This paper aims to (a) give a summary of existing LLMs and approaches for adapting LLMs to downstream tasks; (b) elaborate recent advances in multi-turn dialogue systems, covering both LLM-based open-domain dialogue (ODD) and task-oriented dialogue (TOD) systems, along with datasets and evaluation metrics; (c) discuss some future emphasis and recent research problems arising from the development of LLMs and the increasing demands on multi-turn dialogue systems.\u001b[0m\u001b[32;1m\u001b[1;3m이 논문의 초록은 다음과 같습니다:\n",
      "\n",
      "이 설문조사는 다중 턴 대화 시스템에 대한 연구를 포괄적으로 검토하며, 특히 대형 언어 모델(LLM)을 기반으로 한 다중 턴 대화 시스템에 중점을 둡니다. 이 논문은 다음을 목표로 합니다:\n",
      "(a) 기존 LLM 및 LLM을 다운스트림 작업에 적응시키기 위한 접근 방식을 요약합니다.\n",
      "(b) LLM 기반의 오픈 도메인 대화(ODD) 및 작업 지향 대화(TOD) 시스템을 포함한 다중 턴 대화 시스템의 최근 발전, 데이터셋 및 평가 지표를 자세히 설명합니다.\n",
      "(c) LLM의 발전과 다중 턴 대화 시스템에 대한 증가하는 요구로 인해 발생하는 미래의 중점 사항과 최근 연구 문제를 논의합니다.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "이 논문의 초록은 다음과 같습니다:\n",
       "\n",
       "이 설문조사는 다중 턴 대화 시스템에 대한 연구를 포괄적으로 검토하며, 특히 대형 언어 모델(LLM)을 기반으로 한 다중 턴 대화 시스템에 중점을 둡니다. 이 논문은 다음을 목표로 합니다:\n",
       "(a) 기존 LLM 및 LLM을 다운스트림 작업에 적응시키기 위한 접근 방식을 요약합니다.\n",
       "(b) LLM 기반의 오픈 도메인 대화(ODD) 및 작업 지향 대화(TOD) 시스템을 포함한 다중 턴 대화 시스템의 최근 발전, 데이터셋 및 평가 지표를 자세히 설명합니다.\n",
       "(c) LLM의 발전과 다중 턴 대화 시스템에 대한 증가하는 요구로 인해 발생하는 미래의 중점 사항과 최근 연구 문제를 논의합니다."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "assistant('A Survey on Recent Advances in LLM-Based Multi-turn Dialogue Systems, arxiv id 2402.18013 논문 초록 요약해줘')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assistant('')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
