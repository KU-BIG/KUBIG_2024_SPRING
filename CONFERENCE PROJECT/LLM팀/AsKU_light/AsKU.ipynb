{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: fineGrained).\n",
      "Your token has been saved to /c1/yumin/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import os\n",
    "\n",
    "import tool_pool as tp\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain import hub\n",
    "from langchain.agents import AgentExecutor, create_openai_tools_agent\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain import hub\n",
    "from langchain.agents import AgentExecutor\n",
    "from langchain.memory import ChatMessageHistory\n",
    "\n",
    "from IPython.display import Markdown, display\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ë³¸ ë…¸íŠ¸ë¶ì„ ì‹¤í–‰í•˜ê¸° ìœ„í•´ì„  Openai api keyê°€ í•„ìš”í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_key = 'sk-' # Your openai API key here\n",
    "# openai_key = os.getenv(\"OPENAI_API_KEY\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tool List\n",
    "loadpaper : HTML ê¸°ë°˜ìœ¼ë¡œ Section listë¥¼ ë°›ê³ , Toolì„ í•œ ë²ˆ ë” ì‹¤í–‰í•˜ëŠ” ë°©ì‹ìœ¼ë¡œ ë™ì‘  \n",
    "loadpaper_wo_figure : loadpaper ì—ì„œ figure ê¸°ëŠ¥ì´ ì œì™¸ëœ Tool  \n",
    "loadpaper_wo_figure_wo_section : loadpaperì—ì„œ figure, section ê¸°ëŠ¥ì´ ì œì™¸ëœ Tool  \n",
    "recommendpaper : ê´€ë ¨ ë…¼ë¬¸ì„ ì°¾ì•„ì£¼ëŠ” Tool. citation, reference mode ë‘ ê°€ì§€ë¡œ ë™ì‘  \n",
    "code_matching : ë…¼ë¬¸ ê¹ƒí—ˆë¸Œì—ì„œ ê´€ë ¨ ì½”ë“œë¥¼ ì°¾ì•„ì£¼ëŠ” Tool  \n",
    "  \n",
    "tool_pool.pyì—ì„œ ì§ì ‘ toolì„ custom í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tp.loadpaper, tp.recommendpaper, tp.code_matching, tp.loadpaper_wo_figure, tp.loadpaper_wo_figure_wo_section\n",
    "\n",
    "# default tools\n",
    "# tools = [tp.loadpaper, tp.recommendpaper, tp.code_matching]\n",
    "\n",
    "# light version. Customize it as you wish!\n",
    "# tools = [tp.loadpaper_wo_figure_wo_section, tp.recommendpaper]\n",
    "\n",
    "# Recommend tool combination.\n",
    "tools = [tp.loadpaper_wo_figure, tp.recommendpaper]\n",
    "\n",
    "# load Agent prompt\n",
    "prompt = hub.pull(\"hwchase17/openai-tools-agent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### base model ì„ íƒ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(model=\"gpt-4o\", temperature=0)\n",
    "# llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Real Use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PaperAssistant:\n",
    "    def __init__(self, llm, verbose):\n",
    "        self.memory = None\n",
    "        self.llm = llm\n",
    "        self.verbose = verbose\n",
    "\n",
    "        self.agent_executor = self.build_agent_executor()\n",
    "        self.agent_with_chat_history = self.bulid_agent_with_chat_history()\n",
    "\n",
    "\n",
    "    def build_agent_executor(self):\n",
    "        # Construct the OpenAI Tools agent\n",
    "        agent = create_openai_tools_agent(self.llm, tools, prompt)\n",
    "        # Create an agent executor by passing in the agent and tools\n",
    "        agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=self.verbose)\n",
    "        return agent_executor\n",
    "    \n",
    "    def bulid_agent_with_chat_history(self):\n",
    "        memory = ChatMessageHistory(session_id=\"test-session\")\n",
    "\n",
    "        agent_with_chat_history = RunnableWithMessageHistory(\n",
    "            self.agent_executor,\n",
    "            # This is needed because in most real world scenarios, a session id is needed\n",
    "            # It isn't really used here because we are using a simple in memory ChatMessageHistory\n",
    "            lambda session_id: memory,\n",
    "            input_messages_key=\"input\",\n",
    "            history_messages_key=\"chat_history\",\n",
    "        )\n",
    "        return agent_with_chat_history\n",
    "\n",
    "    def reset_memory(self, session_id='temp'):\n",
    "        self.agent_with_chat_history = self.bulid_agent_with_chat_history()\n",
    "\n",
    "    def send_message(self, message, session_id='temp'):\n",
    "        output = self.agent_with_chat_history.invoke(\n",
    "            {\"input\": message},\n",
    "            config={\"configurable\": {\"session_id\": session_id}},\n",
    "        )\n",
    "        display(Markdown(output['output']))\n",
    "\n",
    "    def __call__(self, message):\n",
    "        self.send_message(message)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tool ì‹¤í–‰ ê³¼ì •ì„ ìƒëµí•˜ë ¤ë©´ verbose = Falseë¡œ ì„¤ì •í•˜ì„¸ìš”\n",
    "assistant = PaperAssistant(llm, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `loadpaper` with `{'title': 'Attention is all you need', 'arxiv_id': '1706.03762'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3mHere is the title and section of the paper in HTML\n",
      "title\n",
      "Attention Is All You Need\n",
      "sections\n",
      "          Abstract\n",
      "  1 Introduction\n",
      "  2 Background\n",
      "  3 Model Architecture\n",
      "    3.1 Encoder and Decoder Stacks\n",
      "        Encoder:\n",
      "        Decoder:\n",
      "    3.2 Attention\n",
      "      3.2.1 Scaled Dot-Product Attention\n",
      "      3.2.2 Multi-Head Attention\n",
      "      3.2.3 Applications of Attention in our Model\n",
      "    3.3 Position-wise Feed-Forward Networks\n",
      "    3.4 Embeddings and Softmax\n",
      "    3.5 Positional Encoding\n",
      "  4 Why Self-Attention\n",
      "  5 Training\n",
      "    5.1 Training Data and Batching\n",
      "    5.2 Hardware and Schedule\n",
      "    5.3 Optimizer\n",
      "    5.4 Regularization\n",
      "        Residual Dropout\n",
      "        Label Smoothing\n",
      "  6 Results\n",
      "    6.1 Machine Translation\n",
      "    6.2 Model Variations\n",
      "    6.3 English Constituency Parsing\n",
      "  7 Conclusion\n",
      "        Acknowledgements\n",
      "  References\n",
      "  Attention Visualizations\n",
      "\n",
      "\n",
      " Use the 'loadpaper' tool again, specifying the section list you want to view in detail.\u001b[0m\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `loadpaper` with `{'title': 'Attention is all you need', 'arxiv_id': '1706.03762', 'sections': ['3.5 Positional Encoding']}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3m\n",
      "3.5 Positional Encoding\n",
      "Since our model contains no recurrence and no convolution, in order for the model to make use of the order of the sequence, we must inject some information about the relative or absolute position of the tokens in the sequence. To this end, we add \"positional encodings\" to the input embeddings at the bottoms of the encoder and decoder stacks. The positional encodings have the same dimension dmodelsubscriptğ‘‘modeld_{\\text{model}} as the embeddings, so that the two can be summed. There are many choices of positional encodings, learned and fixed [9].\n",
      "\n",
      "In this work, we use sine and cosine functions of different frequencies:\n",
      "\n",
      "Pâ€‹E(pâ€‹oâ€‹s,2â€‹i)=sâ€‹iâ€‹nâ€‹(pâ€‹oâ€‹s/100002â€‹i/dmodel)ğ‘ƒsubscriptğ¸ğ‘ğ‘œğ‘ 2ğ‘–ğ‘ ğ‘–ğ‘›ğ‘ğ‘œğ‘ superscript100002ğ‘–subscriptğ‘‘model\\displaystyle PE_{(pos,2i)}=sin(pos/10000^{2i/d_{\\text{model}}})\n",
      "\n",
      "Pâ€‹E(pâ€‹oâ€‹s,2â€‹i+1)=câ€‹oâ€‹sâ€‹(pâ€‹oâ€‹s/100002â€‹i/dmodel)ğ‘ƒsubscriptğ¸ğ‘ğ‘œğ‘ 2ğ‘–1ğ‘ğ‘œğ‘ ğ‘ğ‘œğ‘ superscript100002ğ‘–subscriptğ‘‘model\\displaystyle PE_{(pos,2i+1)}=cos(pos/10000^{2i/d_{\\text{model}}})\n",
      "\n",
      "where pâ€‹oâ€‹sğ‘ğ‘œğ‘ pos is the position and iğ‘–i is the dimension. That is, each dimension of the positional encoding corresponds to a sinusoid. The wavelengths form a geometric progression from 2â€‹Ï€2ğœ‹2\\pi to 10000â‹…2â€‹Ï€â‹…100002ğœ‹10000\\cdot 2\\pi. We chose this function because we hypothesized it would allow the model to easily learn to attend by relative positions, since for any fixed offset kğ‘˜k, Pâ€‹Epâ€‹oâ€‹s+kğ‘ƒsubscriptğ¸ğ‘ğ‘œğ‘ ğ‘˜PE_{pos+k} can be represented as a linear function of Pâ€‹Epâ€‹oâ€‹sğ‘ƒsubscriptğ¸ğ‘ğ‘œğ‘ PE_{pos}.\n",
      "\n",
      "We also experimented with using learned positional embeddings [9] instead, and found that the two versions produced nearly identical results (see TableÂ 3 row (E)). We chose the sinusoidal version because it may allow the model to extrapolate to sequence lengths longer than the ones encountered during training.\n",
      "\u001b[0m\u001b[32;1m\u001b[1;3m\"Attention is All You Need\" ë…¼ë¬¸ì—ì„œ positional encodingì— ëŒ€í•´ ì„¤ëª…í•œ ë¶€ë¶„ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:\n",
      "\n",
      "### Positional Encoding\n",
      "\n",
      "ëª¨ë¸ì— recurrence(ìˆœí™˜)ì´ë‚˜ convolution(í•©ì„±ê³±)ì´ ì—†ê¸° ë•Œë¬¸ì—, ì‹œí€€ìŠ¤ì˜ ìˆœì„œë¥¼ ëª¨ë¸ì´ í™œìš©í•  ìˆ˜ ìˆë„ë¡ í•˜ê¸° ìœ„í•´ì„œëŠ” í† í°ì˜ ìƒëŒ€ì  ë˜ëŠ” ì ˆëŒ€ì  ìœ„ì¹˜ì— ëŒ€í•œ ì •ë³´ë¥¼ ì£¼ì…í•´ì•¼ í•©ë‹ˆë‹¤. ì´ë¥¼ ìœ„í•´, ì¸ì½”ë”ì™€ ë””ì½”ë” ìŠ¤íƒì˜ ì…ë ¥ ì„ë² ë”©ì— \"positional encodings\"ë¥¼ ì¶”ê°€í•©ë‹ˆë‹¤. positional encodingsëŠ” ì„ë² ë”©ê³¼ ë™ì¼í•œ ì°¨ì› \\(d_{\\text{model}}\\)ì„ ê°€ì§€ë©°, ë‘ ê°’ì„ ë”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. positional encodingsëŠ” í•™ìŠµëœ ê²ƒê³¼ ê³ ì •ëœ ê²ƒ ë“± ì—¬ëŸ¬ ê°€ì§€ ì„ íƒì§€ê°€ ìˆìŠµë‹ˆë‹¤.\n",
      "\n",
      "ì´ ë…¼ë¬¸ì—ì„œëŠ” ë‹¤ì–‘í•œ ì£¼íŒŒìˆ˜ì˜ ì‚¬ì¸(sine) í•¨ìˆ˜ì™€ ì½”ì‚¬ì¸(cosine) í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤:\n",
      "\n",
      "\\[ PE_{(pos, 2i)} = \\sin\\left(\\frac{pos}{10000^{2i/d_{\\text{model}}}}\\right) \\]\n",
      "\n",
      "\\[ PE_{(pos, 2i+1)} = \\cos\\left(\\frac{pos}{10000^{2i/d_{\\text{model}}}}\\right) \\]\n",
      "\n",
      "ì—¬ê¸°ì„œ \\(pos\\)ëŠ” ìœ„ì¹˜ë¥¼, \\(i\\)ëŠ” ì°¨ì›ì„ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤. ì¦‰, positional encodingì˜ ê° ì°¨ì›ì€ ì‚¬ì¸íŒŒì— í•´ë‹¹í•©ë‹ˆë‹¤. íŒŒì¥ì˜ ê¸¸ì´ëŠ” \\(2\\pi\\)ì—ì„œ \\(10000 \\cdot 2\\pi\\)ê¹Œì§€ ê¸°í•˜ê¸‰ìˆ˜ì ìœ¼ë¡œ ì¦ê°€í•©ë‹ˆë‹¤. ì´ í•¨ìˆ˜ë¥¼ ì„ íƒí•œ ì´ìœ ëŠ” ëª¨ë¸ì´ ìƒëŒ€ì  ìœ„ì¹˜ì— ë”°ë¼ ì‰½ê²Œ ì£¼ëª©(attend)í•  ìˆ˜ ìˆë„ë¡ í•˜ê¸° ìœ„í•´ì„œì…ë‹ˆë‹¤. ê³ ì •ëœ ì˜¤í”„ì…‹ \\(k\\)ì— ëŒ€í•´ \\(PE_{pos+k}\\)ëŠ” \\(PE_{pos}\\)ì˜ ì„ í˜• í•¨ìˆ˜ë¡œ í‘œí˜„ë  ìˆ˜ ìˆê¸° ë•Œë¬¸ì…ë‹ˆë‹¤.\n",
      "\n",
      "ë˜í•œ, í•™ìŠµëœ positional embeddingsë¥¼ ì‚¬ìš©í•˜ëŠ” ì‹¤í—˜ë„ ì§„í–‰í–ˆìœ¼ë©°, ë‘ ë²„ì „ì´ ê±°ì˜ ë™ì¼í•œ ê²°ê³¼ë¥¼ ë‚³ëŠ”ë‹¤ëŠ” ê²ƒì„ ë°œê²¬í–ˆìŠµë‹ˆë‹¤. (í‘œ 3ì˜ (E) í–‰ ì°¸ì¡°) ìš°ë¦¬ëŠ” ì‚¬ì¸íŒŒ ë²„ì „ì„ ì„ íƒí–ˆëŠ”ë°, ì´ëŠ” ëª¨ë¸ì´ í•™ìŠµ ì¤‘ì— ì ‘í•œ ì‹œí€€ìŠ¤ ê¸¸ì´ë³´ë‹¤ ë” ê¸´ ì‹œí€€ìŠ¤ ê¸¸ì´ë¡œ ì™¸ì‚½í•  ìˆ˜ ìˆê²Œ í•  ìˆ˜ ìˆê¸° ë•Œë¬¸ì…ë‹ˆë‹¤.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "\"Attention is All You Need\" ë…¼ë¬¸ì—ì„œ positional encodingì— ëŒ€í•´ ì„¤ëª…í•œ ë¶€ë¶„ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:\n",
       "\n",
       "### Positional Encoding\n",
       "\n",
       "ëª¨ë¸ì— recurrence(ìˆœí™˜)ì´ë‚˜ convolution(í•©ì„±ê³±)ì´ ì—†ê¸° ë•Œë¬¸ì—, ì‹œí€€ìŠ¤ì˜ ìˆœì„œë¥¼ ëª¨ë¸ì´ í™œìš©í•  ìˆ˜ ìˆë„ë¡ í•˜ê¸° ìœ„í•´ì„œëŠ” í† í°ì˜ ìƒëŒ€ì  ë˜ëŠ” ì ˆëŒ€ì  ìœ„ì¹˜ì— ëŒ€í•œ ì •ë³´ë¥¼ ì£¼ì…í•´ì•¼ í•©ë‹ˆë‹¤. ì´ë¥¼ ìœ„í•´, ì¸ì½”ë”ì™€ ë””ì½”ë” ìŠ¤íƒì˜ ì…ë ¥ ì„ë² ë”©ì— \"positional encodings\"ë¥¼ ì¶”ê°€í•©ë‹ˆë‹¤. positional encodingsëŠ” ì„ë² ë”©ê³¼ ë™ì¼í•œ ì°¨ì› \\(d_{\\text{model}}\\)ì„ ê°€ì§€ë©°, ë‘ ê°’ì„ ë”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. positional encodingsëŠ” í•™ìŠµëœ ê²ƒê³¼ ê³ ì •ëœ ê²ƒ ë“± ì—¬ëŸ¬ ê°€ì§€ ì„ íƒì§€ê°€ ìˆìŠµë‹ˆë‹¤.\n",
       "\n",
       "ì´ ë…¼ë¬¸ì—ì„œëŠ” ë‹¤ì–‘í•œ ì£¼íŒŒìˆ˜ì˜ ì‚¬ì¸(sine) í•¨ìˆ˜ì™€ ì½”ì‚¬ì¸(cosine) í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤:\n",
       "\n",
       "\\[ PE_{(pos, 2i)} = \\sin\\left(\\frac{pos}{10000^{2i/d_{\\text{model}}}}\\right) \\]\n",
       "\n",
       "\\[ PE_{(pos, 2i+1)} = \\cos\\left(\\frac{pos}{10000^{2i/d_{\\text{model}}}}\\right) \\]\n",
       "\n",
       "ì—¬ê¸°ì„œ \\(pos\\)ëŠ” ìœ„ì¹˜ë¥¼, \\(i\\)ëŠ” ì°¨ì›ì„ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤. ì¦‰, positional encodingì˜ ê° ì°¨ì›ì€ ì‚¬ì¸íŒŒì— í•´ë‹¹í•©ë‹ˆë‹¤. íŒŒì¥ì˜ ê¸¸ì´ëŠ” \\(2\\pi\\)ì—ì„œ \\(10000 \\cdot 2\\pi\\)ê¹Œì§€ ê¸°í•˜ê¸‰ìˆ˜ì ìœ¼ë¡œ ì¦ê°€í•©ë‹ˆë‹¤. ì´ í•¨ìˆ˜ë¥¼ ì„ íƒí•œ ì´ìœ ëŠ” ëª¨ë¸ì´ ìƒëŒ€ì  ìœ„ì¹˜ì— ë”°ë¼ ì‰½ê²Œ ì£¼ëª©(attend)í•  ìˆ˜ ìˆë„ë¡ í•˜ê¸° ìœ„í•´ì„œì…ë‹ˆë‹¤. ê³ ì •ëœ ì˜¤í”„ì…‹ \\(k\\)ì— ëŒ€í•´ \\(PE_{pos+k}\\)ëŠ” \\(PE_{pos}\\)ì˜ ì„ í˜• í•¨ìˆ˜ë¡œ í‘œí˜„ë  ìˆ˜ ìˆê¸° ë•Œë¬¸ì…ë‹ˆë‹¤.\n",
       "\n",
       "ë˜í•œ, í•™ìŠµëœ positional embeddingsë¥¼ ì‚¬ìš©í•˜ëŠ” ì‹¤í—˜ë„ ì§„í–‰í–ˆìœ¼ë©°, ë‘ ë²„ì „ì´ ê±°ì˜ ë™ì¼í•œ ê²°ê³¼ë¥¼ ë‚³ëŠ”ë‹¤ëŠ” ê²ƒì„ ë°œê²¬í–ˆìŠµë‹ˆë‹¤. (í‘œ 3ì˜ (E) í–‰ ì°¸ì¡°) ìš°ë¦¬ëŠ” ì‚¬ì¸íŒŒ ë²„ì „ì„ ì„ íƒí–ˆëŠ”ë°, ì´ëŠ” ëª¨ë¸ì´ í•™ìŠµ ì¤‘ì— ì ‘í•œ ì‹œí€€ìŠ¤ ê¸¸ì´ë³´ë‹¤ ë” ê¸´ ì‹œí€€ìŠ¤ ê¸¸ì´ë¡œ ì™¸ì‚½í•  ìˆ˜ ìˆê²Œ í•  ìˆ˜ ìˆê¸° ë•Œë¬¸ì…ë‹ˆë‹¤."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "assistant('\"Attention is all you need\" arxiv id 1706.03762 ì—ì„œ positional encoding ì— ëŒ€í•´ ì„¤ëª…í•´ì¤˜')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `loadpaper` with `{'title': 'Attention is all you need', 'arxiv_id': '1706.03762'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3mHere is the title and section of the paper in HTML\n",
      "title\n",
      "Attention Is All You Need\n",
      "sections\n",
      "          Abstract\n",
      "  1 Introduction\n",
      "  2 Background\n",
      "  3 Model Architecture\n",
      "    3.1 Encoder and Decoder Stacks\n",
      "        Encoder:\n",
      "        Decoder:\n",
      "    3.2 Attention\n",
      "      3.2.1 Scaled Dot-Product Attention\n",
      "      3.2.2 Multi-Head Attention\n",
      "      3.2.3 Applications of Attention in our Model\n",
      "    3.3 Position-wise Feed-Forward Networks\n",
      "    3.4 Embeddings and Softmax\n",
      "    3.5 Positional Encoding\n",
      "  4 Why Self-Attention\n",
      "  5 Training\n",
      "    5.1 Training Data and Batching\n",
      "    5.2 Hardware and Schedule\n",
      "    5.3 Optimizer\n",
      "    5.4 Regularization\n",
      "        Residual Dropout\n",
      "        Label Smoothing\n",
      "  6 Results\n",
      "    6.1 Machine Translation\n",
      "    6.2 Model Variations\n",
      "    6.3 English Constituency Parsing\n",
      "  7 Conclusion\n",
      "        Acknowledgements\n",
      "  References\n",
      "  Attention Visualizations\n",
      "\n",
      "\n",
      " Use the 'loadpaper' tool again, specifying the section list you want to view in detail.\u001b[0m\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `loadpaper` with `{'title': 'Attention is all you need', 'arxiv_id': '1706.03762', 'sections': ['3 Model Architecture']}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3m\n",
      "3 Model Architecture\n",
      "Figure 1: The Transformer - model architecture.\n",
      "\n",
      "Most competitive neural sequence transduction models have an encoder-decoder structure [5, 2, 35]. Here, the encoder maps an input sequence of symbol representations (x1,â€¦,xn)subscriptğ‘¥1â€¦subscriptğ‘¥ğ‘›(x_{1},...,x_{n}) to a sequence of continuous representations ğ³=(z1,â€¦,zn)ğ³subscriptğ‘§1â€¦subscriptğ‘§ğ‘›\\mathbf{z}=(z_{1},...,z_{n}). Given ğ³ğ³\\mathbf{z}, the decoder then generates an output sequence (y1,â€¦,ym)subscriptğ‘¦1â€¦subscriptğ‘¦ğ‘š(y_{1},...,y_{m}) of symbols one element at a time. At each step the model is auto-regressive [10], consuming the previously generated symbols as additional input when generating the next.\n",
      "\n",
      "The Transformer follows this overall architecture using stacked self-attention and point-wise, fully connected layers for both the encoder and decoder, shown in the left and right halves of FigureÂ 1, respectively.\n",
      "\n",
      "3.1 Encoder and Decoder Stacks\n",
      "\n",
      "Encoder:\n",
      "\n",
      "The encoder is composed of a stack of N=6ğ‘6N=6 identical layers. Each layer has two sub-layers. The first is a multi-head self-attention mechanism, and the second is a simple, position-wise fully connected feed-forward network. We employ a residual connection [11] around each of the two sub-layers, followed by layer normalization [1]. That is, the output of each sub-layer is LayerNormâ€‹(x+Sublayerâ€‹(x))LayerNormğ‘¥Sublayerğ‘¥\\mathrm{LayerNorm}(x+\\mathrm{Sublayer}(x)), where Sublayerâ€‹(x)Sublayerğ‘¥\\mathrm{Sublayer}(x) is the function implemented by the sub-layer itself. To facilitate these residual connections, all sub-layers in the model, as well as the embedding layers, produce outputs of dimension dmodel=512subscriptğ‘‘model512d_{\\text{model}}=512.\n",
      "\n",
      "Decoder:\n",
      "\n",
      "The decoder is also composed of a stack of N=6ğ‘6N=6 identical layers. In addition to the two sub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head attention over the output of the encoder stack. Similar to the encoder, we employ residual connections around each of the sub-layers, followed by layer normalization. We also modify the self-attention sub-layer in the decoder stack to prevent positions from attending to subsequent positions. This masking, combined with fact that the output embeddings are offset by one position, ensures that the predictions for position iğ‘–i can depend only on the known outputs at positions less than iğ‘–i.\n",
      "\n",
      "3.2 Attention\n",
      "\n",
      "An attention function can be described as mapping a query and a set of key-value pairs to an output, where the query, keys, values, and output are all vectors. The output is computed as a weighted sum of the values, where the weight assigned to each value is computed by a compatibility function of the query with the corresponding key.\n",
      "\n",
      "3.2.1 Scaled Dot-Product Attention\n",
      "\n",
      "We call our particular attention \"Scaled Dot-Product Attention\" (FigureÂ 2). The input consists of queries and keys of dimension dksubscriptğ‘‘ğ‘˜d_{k}, and values of dimension dvsubscriptğ‘‘ğ‘£d_{v}. We compute the dot products of the query with all keys, divide each by dksubscriptğ‘‘ğ‘˜\\sqrt{d_{k}}, and apply a softmax function to obtain the weights on the values.\n",
      "\n",
      "In practice, we compute the attention function on a set of queries simultaneously, packed together into a matrix Qğ‘„Q. The keys and values are also packed together into matrices Kğ¾K and Vğ‘‰V. We compute the matrix of outputs as:\n",
      "\n",
      "Attentionâ€‹(Q,K,V)=softmaxâ€‹(Qâ€‹KTdk)â€‹VAttentionğ‘„ğ¾ğ‘‰softmaxğ‘„superscriptğ¾ğ‘‡subscriptğ‘‘ğ‘˜ğ‘‰\\mathrm{Attention}(Q,K,V)=\\mathrm{softmax}(\\frac{QK^{T}}{\\sqrt{d_{k}}})V\n",
      "\n",
      "(1)\n",
      "\n",
      "The two most commonly used attention functions are additive attention [2], and dot-product (multiplicative) attention. Dot-product attention is identical to our algorithm, except for the scaling factor of 1dk1subscriptğ‘‘ğ‘˜\\frac{1}{\\sqrt{d_{k}}}. Additive attention computes the compatibility function using a feed-forward network with a single hidden layer. While the two are similar in theoretical complexity, dot-product attention is much faster and more space-efficient in practice, since it can be implemented using highly optimized matrix multiplication code.\n",
      "\n",
      "While for small values of dksubscriptğ‘‘ğ‘˜d_{k} the two mechanisms perform similarly, additive attention outperforms dot product attention without scaling for larger values of dksubscriptğ‘‘ğ‘˜d_{k} [3]. We suspect that for large values of dksubscriptğ‘‘ğ‘˜d_{k}, the dot products grow large in magnitude, pushing the softmax function into regions where it has extremely small gradients 111To illustrate why the dot products get large, assume that the components of qğ‘q and kğ‘˜k are independent random variables with mean 00 and variance 111. Then their dot product, qâ‹…k=âˆ‘i=1dkqiâ€‹kiâ‹…ğ‘ğ‘˜superscriptsubscriptğ‘–1subscriptğ‘‘ğ‘˜subscriptğ‘ğ‘–subscriptğ‘˜ğ‘–q\\cdot k=\\sum_{i=1}^{d_{k}}q_{i}k_{i}, has mean 00 and variance dksubscriptğ‘‘ğ‘˜d_{k}.. To counteract this effect, we scale the dot products by 1dk1subscriptğ‘‘ğ‘˜\\frac{1}{\\sqrt{d_{k}}}.\n",
      "\n",
      "3.2.2 Multi-Head Attention\n",
      "\n",
      "Scaled Dot-Product Attention\n",
      "\n",
      "Multi-Head Attention\n",
      "\n",
      "Figure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several attention layers running in parallel.\n",
      "\n",
      "Instead of performing a single attention function with dmodelsubscriptğ‘‘modeld_{\\text{model}}-dimensional keys, values and queries, we found it beneficial to linearly project the queries, keys and values hâ„h times with different, learned linear projections to dksubscriptğ‘‘ğ‘˜d_{k}, dksubscriptğ‘‘ğ‘˜d_{k} and dvsubscriptğ‘‘ğ‘£d_{v} dimensions, respectively.\n",
      "On each of these projected versions of queries, keys and values we then perform the attention function in parallel, yielding dvsubscriptğ‘‘ğ‘£d_{v}-dimensional output values. These are concatenated and once again projected, resulting in the final values, as depicted in FigureÂ 2.\n",
      "\n",
      "Multi-head attention allows the model to jointly attend to information from different representation subspaces at different positions. With a single attention head, averaging inhibits this.\n",
      "\n",
      "MultiHeadâ€‹(Q,K,V)MultiHeadğ‘„ğ¾ğ‘‰\\displaystyle\\mathrm{MultiHead}(Q,K,V)\n",
      "=Concatâ€‹(head1,â€¦,headh)â€‹WOabsentConcatsubscripthead1â€¦subscriptheadhsuperscriptğ‘Šğ‘‚\\displaystyle=\\mathrm{Concat}(\\mathrm{head_{1}},...,\\mathrm{head_{h}})W^{O}\n",
      "\n",
      "whereâ€‹headiwheresubscriptheadi\\displaystyle\\text{where}~{}\\mathrm{head_{i}}\n",
      "=Attentionâ€‹(Qâ€‹WiQ,Kâ€‹WiK,Vâ€‹WiV)absentAttentionğ‘„subscriptsuperscriptğ‘Šğ‘„ğ‘–ğ¾subscriptsuperscriptğ‘Šğ¾ğ‘–ğ‘‰subscriptsuperscriptğ‘Šğ‘‰ğ‘–\\displaystyle=\\mathrm{Attention}(QW^{Q}_{i},KW^{K}_{i},VW^{V}_{i})\n",
      "\n",
      "Where the projections are parameter matrices WiQâˆˆâ„dmodelÃ—dksubscriptsuperscriptğ‘Šğ‘„ğ‘–superscriptâ„subscriptğ‘‘modelsubscriptğ‘‘ğ‘˜W^{Q}_{i}\\in\\mathbb{R}^{d_{\\text{model}}\\times d_{k}}, WiKâˆˆâ„dmodelÃ—dksubscriptsuperscriptğ‘Šğ¾ğ‘–superscriptâ„subscriptğ‘‘modelsubscriptğ‘‘ğ‘˜W^{K}_{i}\\in\\mathbb{R}^{d_{\\text{model}}\\times d_{k}}, WiVâˆˆâ„dmodelÃ—dvsubscriptsuperscriptğ‘Šğ‘‰ğ‘–superscriptâ„subscriptğ‘‘modelsubscriptğ‘‘ğ‘£W^{V}_{i}\\in\\mathbb{R}^{d_{\\text{model}}\\times d_{v}} and WOâˆˆâ„hâ€‹dvÃ—dmodelsuperscriptğ‘Šğ‘‚superscriptâ„â„subscriptğ‘‘ğ‘£subscriptğ‘‘modelW^{O}\\in\\mathbb{R}^{hd_{v}\\times d_{\\text{model}}}.\n",
      "\n",
      "In this work we employ h=8â„8h=8 parallel attention layers, or heads. For each of these we use dk=dv=dmodel/h=64subscriptğ‘‘ğ‘˜subscriptğ‘‘ğ‘£subscriptğ‘‘modelâ„64d_{k}=d_{v}=d_{\\text{model}}/h=64.\n",
      "Due to the reduced dimension of each head, the total computational cost is similar to that of single-head attention with full dimensionality.\n",
      "\n",
      "3.2.3 Applications of Attention in our Model\n",
      "\n",
      "The Transformer uses multi-head attention in three different ways:\n",
      "\n",
      "â€¢\n",
      "\n",
      "In \"encoder-decoder attention\" layers, the queries come from the previous decoder layer, and the memory keys and values come from the output of the encoder. This allows every position in the decoder to attend over all positions in the input sequence. This mimics the typical encoder-decoder attention mechanisms in sequence-to-sequence models such as [38, 2, 9].\n",
      "\n",
      "â€¢\n",
      "\n",
      "The encoder contains self-attention layers. In a self-attention layer all of the keys, values and queries come from the same place, in this case, the output of the previous layer in the encoder. Each position in the encoder can attend to all positions in the previous layer of the encoder.\n",
      "\n",
      "â€¢\n",
      "\n",
      "Similarly, self-attention layers in the decoder allow each position in the decoder to attend to all positions in the decoder up to and including that position. We need to prevent leftward information flow in the decoder to preserve the auto-regressive property. We implement this inside of scaled dot-product attention by masking out (setting to âˆ’âˆ-\\infty) all values in the input of the softmax which correspond to illegal connections. See FigureÂ 2.\n",
      "\n",
      "3.3 Position-wise Feed-Forward Networks\n",
      "\n",
      "In addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully connected feed-forward network, which is applied to each position separately and identically. This consists of two linear transformations with a ReLU activation in between.\n",
      "\n",
      "FFNâ€‹(x)=maxâ¡(0,xâ€‹W1+b1)â€‹W2+b2FFNğ‘¥0ğ‘¥subscriptğ‘Š1subscriptğ‘1subscriptğ‘Š2subscriptğ‘2\\mathrm{FFN}(x)=\\max(0,xW_{1}+b_{1})W_{2}+b_{2}\n",
      "\n",
      "(2)\n",
      "\n",
      "While the linear transformations are the same across different positions, they use different parameters from layer to layer. Another way of describing this is as two convolutions with kernel size 1. The dimensionality of input and output is dmodel=512subscriptğ‘‘model512d_{\\text{model}}=512, and the inner-layer has dimensionality dfâ€‹f=2048subscriptğ‘‘ğ‘“ğ‘“2048d_{ff}=2048.\n",
      "\n",
      "3.4 Embeddings and Softmax\n",
      "\n",
      "Similarly to other sequence transduction models, we use learned embeddings to convert the input tokens and output tokens to vectors of dimension dmodelsubscriptğ‘‘modeld_{\\text{model}}. We also use the usual learned linear transformation and softmax function to convert the decoder output to predicted next-token probabilities. In our model, we share the same weight matrix between the two embedding layers and the pre-softmax linear transformation, similar to [30]. In the embedding layers, we multiply those weights by dmodelsubscriptğ‘‘model\\sqrt{d_{\\text{model}}}.\n",
      "\n",
      "3.5 Positional Encoding\n",
      "\n",
      "Since our model contains no recurrence and no convolution, in order for the model to make use of the order of the sequence, we must inject some information about the relative or absolute position of the tokens in the sequence. To this end, we add \"positional encodings\" to the input embeddings at the bottoms of the encoder and decoder stacks. The positional encodings have the same dimension dmodelsubscriptğ‘‘modeld_{\\text{model}} as the embeddings, so that the two can be summed. There are many choices of positional encodings, learned and fixed [9].\n",
      "\n",
      "In this work, we use sine and cosine functions of different frequencies:\n",
      "\n",
      "Pâ€‹E(pâ€‹oâ€‹s,2â€‹i)=sâ€‹iâ€‹nâ€‹(pâ€‹oâ€‹s/100002â€‹i/dmodel)ğ‘ƒsubscriptğ¸ğ‘ğ‘œğ‘ 2ğ‘–ğ‘ ğ‘–ğ‘›ğ‘ğ‘œğ‘ superscript100002ğ‘–subscriptğ‘‘model\\displaystyle PE_{(pos,2i)}=sin(pos/10000^{2i/d_{\\text{model}}})\n",
      "\n",
      "Pâ€‹E(pâ€‹oâ€‹s,2â€‹i+1)=câ€‹oâ€‹sâ€‹(pâ€‹oâ€‹s/100002â€‹i/dmodel)ğ‘ƒsubscriptğ¸ğ‘ğ‘œğ‘ 2ğ‘–1ğ‘ğ‘œğ‘ ğ‘ğ‘œğ‘ superscript100002ğ‘–subscriptğ‘‘model\\displaystyle PE_{(pos,2i+1)}=cos(pos/10000^{2i/d_{\\text{model}}})\n",
      "\n",
      "where pâ€‹oâ€‹sğ‘ğ‘œğ‘ pos is the position and iğ‘–i is the dimension. That is, each dimension of the positional encoding corresponds to a sinusoid. The wavelengths form a geometric progression from 2â€‹Ï€2ğœ‹2\\pi to 10000â‹…2â€‹Ï€â‹…100002ğœ‹10000\\cdot 2\\pi. We chose this function because we hypothesized it would allow the model to easily learn to attend by relative positions, since for any fixed offset kğ‘˜k, Pâ€‹Epâ€‹oâ€‹s+kğ‘ƒsubscriptğ¸ğ‘ğ‘œğ‘ ğ‘˜PE_{pos+k} can be represented as a linear function of Pâ€‹Epâ€‹oâ€‹sğ‘ƒsubscriptğ¸ğ‘ğ‘œğ‘ PE_{pos}.\n",
      "\n",
      "We also experimented with using learned positional embeddings [9] instead, and found that the two versions produced nearly identical results (see TableÂ 3 row (E)). We chose the sinusoidal version because it may allow the model to extrapolate to sequence lengths longer than the ones encountered during training.\n",
      "\n",
      "\u001b[0m\u001b[32;1m\u001b[1;3mThe \"Method\" section of the paper \"Attention is All You Need\" is detailed under the \"Model Architecture\" section. Here is a summary of each element:\n",
      "\n",
      "### 3 Model Architecture\n",
      "\n",
      "#### 3.1 Encoder and Decoder Stacks\n",
      "\n",
      "- **Encoder**: \n",
      "  - Composed of 6 identical layers.\n",
      "  - Each layer has two sub-layers: a multi-head self-attention mechanism and a position-wise fully connected feed-forward network.\n",
      "  - Residual connections and layer normalization are applied around each sub-layer.\n",
      "  - All sub-layers and embedding layers produce outputs of dimension \\( d_{\\text{model}} = 512 \\).\n",
      "\n",
      "- **Decoder**: \n",
      "  - Also composed of 6 identical layers.\n",
      "  - Each layer has an additional third sub-layer for multi-head attention over the encoder's output.\n",
      "  - Similar to the encoder, residual connections and layer normalization are applied.\n",
      "  - Self-attention sub-layer is modified to prevent positions from attending to subsequent positions, ensuring auto-regressive property.\n",
      "\n",
      "#### 3.2 Attention\n",
      "\n",
      "- **Attention Function**: \n",
      "  - Maps a query and a set of key-value pairs to an output.\n",
      "  - Output is a weighted sum of the values, with weights computed by a compatibility function of the query with the corresponding key.\n",
      "\n",
      "##### 3.2.1 Scaled Dot-Product Attention\n",
      "\n",
      "- Computes dot products of the query with all keys, divides by \\(\\sqrt{d_k}\\), and applies a softmax function to obtain weights on the values.\n",
      "- Efficiently implemented using matrix multiplication.\n",
      "\n",
      "##### 3.2.2 Multi-Head Attention\n",
      "\n",
      "- Projects queries, keys, and values multiple times with different learned linear projections.\n",
      "- Performs attention function in parallel on these projections.\n",
      "- Concatenates and projects the results to form the final values.\n",
      "- Allows the model to attend to information from different representation subspaces at different positions.\n",
      "- Uses 8 parallel attention layers (heads), each with \\( d_k = d_v = d_{\\text{model}} / h = 64 \\).\n",
      "\n",
      "##### 3.2.3 Applications of Attention in the Model\n",
      "\n",
      "- **Encoder-Decoder Attention**: Queries from the decoder, keys, and values from the encoder.\n",
      "- **Self-Attention in Encoder**: Keys, values, and queries from the same layer output.\n",
      "- **Self-Attention in Decoder**: Similar to encoder but prevents leftward information flow to maintain auto-regressive property.\n",
      "\n",
      "#### 3.3 Position-wise Feed-Forward Networks\n",
      "\n",
      "- Each layer in the encoder and decoder contains a fully connected feed-forward network.\n",
      "- Applied to each position separately and identically.\n",
      "- Consists of two linear transformations with a ReLU activation in between.\n",
      "- Input and output dimensionality is \\( d_{\\text{model}} = 512 \\), and inner-layer dimensionality is \\( d_{ff} = 2048 \\).\n",
      "\n",
      "#### 3.4 Embeddings and Softmax\n",
      "\n",
      "- Uses learned embeddings to convert input and output tokens to vectors of dimension \\( d_{\\text{model}} \\).\n",
      "- Uses a learned linear transformation and softmax function to convert decoder output to predicted next-token probabilities.\n",
      "- Shares the same weight matrix between embedding layers and pre-softmax linear transformation, scaled by \\( \\sqrt{d_{\\text{model}}} \\).\n",
      "\n",
      "#### 3.5 Positional Encoding\n",
      "\n",
      "- Adds positional encodings to input embeddings to incorporate sequence order.\n",
      "- Positional encodings have the same dimension \\( d_{\\text{model}} \\) as embeddings.\n",
      "- Uses sine and cosine functions of different frequencies for encoding.\n",
      "- Allows the model to learn to attend by relative positions.\n",
      "- Experimented with learned positional embeddings but found sinusoidal version to be equally effective and potentially better for extrapolation to longer sequences.\n",
      "\n",
      "This summary encapsulates the key components and mechanisms of the Transformer model as described in the \"Attention is All You Need\" paper.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "The \"Method\" section of the paper \"Attention is All You Need\" is detailed under the \"Model Architecture\" section. Here is a summary of each element:\n",
       "\n",
       "### 3 Model Architecture\n",
       "\n",
       "#### 3.1 Encoder and Decoder Stacks\n",
       "\n",
       "- **Encoder**: \n",
       "  - Composed of 6 identical layers.\n",
       "  - Each layer has two sub-layers: a multi-head self-attention mechanism and a position-wise fully connected feed-forward network.\n",
       "  - Residual connections and layer normalization are applied around each sub-layer.\n",
       "  - All sub-layers and embedding layers produce outputs of dimension \\( d_{\\text{model}} = 512 \\).\n",
       "\n",
       "- **Decoder**: \n",
       "  - Also composed of 6 identical layers.\n",
       "  - Each layer has an additional third sub-layer for multi-head attention over the encoder's output.\n",
       "  - Similar to the encoder, residual connections and layer normalization are applied.\n",
       "  - Self-attention sub-layer is modified to prevent positions from attending to subsequent positions, ensuring auto-regressive property.\n",
       "\n",
       "#### 3.2 Attention\n",
       "\n",
       "- **Attention Function**: \n",
       "  - Maps a query and a set of key-value pairs to an output.\n",
       "  - Output is a weighted sum of the values, with weights computed by a compatibility function of the query with the corresponding key.\n",
       "\n",
       "##### 3.2.1 Scaled Dot-Product Attention\n",
       "\n",
       "- Computes dot products of the query with all keys, divides by \\(\\sqrt{d_k}\\), and applies a softmax function to obtain weights on the values.\n",
       "- Efficiently implemented using matrix multiplication.\n",
       "\n",
       "##### 3.2.2 Multi-Head Attention\n",
       "\n",
       "- Projects queries, keys, and values multiple times with different learned linear projections.\n",
       "- Performs attention function in parallel on these projections.\n",
       "- Concatenates and projects the results to form the final values.\n",
       "- Allows the model to attend to information from different representation subspaces at different positions.\n",
       "- Uses 8 parallel attention layers (heads), each with \\( d_k = d_v = d_{\\text{model}} / h = 64 \\).\n",
       "\n",
       "##### 3.2.3 Applications of Attention in the Model\n",
       "\n",
       "- **Encoder-Decoder Attention**: Queries from the decoder, keys, and values from the encoder.\n",
       "- **Self-Attention in Encoder**: Keys, values, and queries from the same layer output.\n",
       "- **Self-Attention in Decoder**: Similar to encoder but prevents leftward information flow to maintain auto-regressive property.\n",
       "\n",
       "#### 3.3 Position-wise Feed-Forward Networks\n",
       "\n",
       "- Each layer in the encoder and decoder contains a fully connected feed-forward network.\n",
       "- Applied to each position separately and identically.\n",
       "- Consists of two linear transformations with a ReLU activation in between.\n",
       "- Input and output dimensionality is \\( d_{\\text{model}} = 512 \\), and inner-layer dimensionality is \\( d_{ff} = 2048 \\).\n",
       "\n",
       "#### 3.4 Embeddings and Softmax\n",
       "\n",
       "- Uses learned embeddings to convert input and output tokens to vectors of dimension \\( d_{\\text{model}} \\).\n",
       "- Uses a learned linear transformation and softmax function to convert decoder output to predicted next-token probabilities.\n",
       "- Shares the same weight matrix between embedding layers and pre-softmax linear transformation, scaled by \\( \\sqrt{d_{\\text{model}}} \\).\n",
       "\n",
       "#### 3.5 Positional Encoding\n",
       "\n",
       "- Adds positional encodings to input embeddings to incorporate sequence order.\n",
       "- Positional encodings have the same dimension \\( d_{\\text{model}} \\) as embeddings.\n",
       "- Uses sine and cosine functions of different frequencies for encoding.\n",
       "- Allows the model to learn to attend by relative positions.\n",
       "- Experimented with learned positional embeddings but found sinusoidal version to be equally effective and potentially better for extrapolation to longer sequences.\n",
       "\n",
       "This summary encapsulates the key components and mechanisms of the Transformer model as described in the \"Attention is All You Need\" paper."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "assistant('ë…¼ë¬¸ Methodì˜ ê° ìš”ì†Œë¥¼ ìš”ì•½í•´ì¤˜')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset memory ë¡œ ë©”ëª¨ë¦¬ë¥¼ ì´ˆê¸°í™”í•©ë‹ˆë‹¤.\n",
    "assistant.reset_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3më…¼ë¬¸ì˜ ì œëª©ì„ ì•Œë ¤ì£¼ì‹œë©´, í•´ë‹¹ ë…¼ë¬¸ì˜ Method ì„¹ì…˜ì„ ë¶ˆëŸ¬ì™€ ìš”ì•½í•´ë“œë¦¬ê² ìŠµë‹ˆë‹¤.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "ë…¼ë¬¸ì˜ ì œëª©ì„ ì•Œë ¤ì£¼ì‹œë©´, í•´ë‹¹ ë…¼ë¬¸ì˜ Method ì„¹ì…˜ì„ ë¶ˆëŸ¬ì™€ ìš”ì•½í•´ë“œë¦¬ê² ìŠµë‹ˆë‹¤."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "assistant('ë…¼ë¬¸ Methodì˜ ê° ìš”ì†Œë¥¼ ìš”ì•½í•´ì¤˜')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "assistant.reset_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `recommendpaper` with `{'query': 'PROMPTAGENT: STRATEGIC PLANNING WITH LANGUAGE MODELS ENABLES EXPERT-LEVEL PROMPT OPTIMIZATION', 'rec_type': 'citation'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[33;1m\u001b[1;3m[{'title': 'Agent-Pro: Learning to Evolve via Policy-Level Reflection and Optimization', 'abstract': \"Large Language Models (LLMs) exhibit robust problem-solving capabilities for diverse tasks. However, most LLM-based agents are designed as specific task solvers with sophisticated prompt engineering, rather than agents capable of learning and evolving through interactions. These task solvers necessitate manually crafted prompts to inform task rules and regulate LLM behaviors, inherently incapacitating to address complex dynamic scenarios e.g., large interactive games. In light of this, we propose Agent-Pro: an LLM-based Agent with Policy-level Reflection and Optimization that can learn a wealth of expertise from interactive experiences and progressively elevate its behavioral policy. Specifically, it involves a dynamic belief generation and reflection process for policy evolution. Rather than action-level reflection, Agent-Pro iteratively reflects on past trajectories and beliefs, fine-tuning its irrational beliefs for a better policy. Moreover, a depth-first search is employed for policy optimization, ensuring continual enhancement in policy payoffs. Agent-Pro is evaluated across two games: Blackjack and Texas Hold'em, outperforming vanilla LLM and specialized models. Our results show Agent-Pro can learn and evolve in complex and dynamic scenes, which also benefits numerous LLM-based applications.\", 'influentialCitationCount': 1, 'publicationDate': '2024-02-27'}, {'title': 'RoT: Enhancing Large Language Models with Reflection on Search Trees', 'abstract': 'Large language models (LLMs) have demonstrated impressive capability in reasoning and planning when integrated with tree-search-based prompting methods. However, since these methods ignore the previous search experiences, they often make the same mistakes in the search process. To address this issue, we introduce Reflection on search Trees (RoT), an LLM reflection framework designed to improve the performance of tree-search-based prompting methods. It uses a strong LLM to summarize guidelines from previous tree search experiences to enhance the ability of a weak LLM. The guidelines are instructions about solving this task through tree search which can prevent the weak LLMs from making similar mistakes in the past search process. In addition, we proposed a novel state selection method, which identifies the critical information from historical search processes to help RoT generate more specific and meaningful guidelines. In our extensive experiments, we find that RoT significantly improves the performance of LLMs in reasoning or planning tasks with various tree-search-based prompting methods (e.g., BFS and MCTS). Non-tree-search-based prompting methods such as Chain-of-Thought (CoT) can also benefit from RoT guidelines since RoT can provide task-specific knowledge collected from the search experience.', 'influentialCitationCount': 1, 'publicationDate': '2024-04-08'}, {'title': 'Fairer Preferences Elicit Improved Human-Aligned Large Language Model Judgments', 'abstract': 'Large language models (LLMs) have shown promising abilities as cost-effective and reference-free evaluators for assessing language generation quality. In particular, pairwise LLM evaluators, which compare two generated texts and determine the preferred one, have been employed in a wide range of applications. However, LLMs exhibit preference biases and worrying sensitivity to prompt designs. In this work, we first reveal that the predictive preference of LLMs can be highly brittle and skewed, even with semantically equivalent instructions. We find that fairer predictive preferences from LLMs consistently lead to judgments that are better aligned with humans. Motivated by this phenomenon, we propose an automatic Zero-shot Evaluation-oriented Prompt Optimization framework, ZEPO, which aims to produce fairer preference decisions and improve the alignment of LLM evaluators with human judgments. To this end, we propose a zero-shot learning objective based on the preference decision fairness. ZEPO demonstrates substantial performance improvements over state-of-the-art LLM evaluators, without requiring labeled data, on representative meta-evaluation benchmarks. Our findings underscore the critical correlation between preference fairness and human alignment, positioning ZEPO as an efficient prompt optimizer for bridging the gap between LLM evaluators and human judgments.', 'influentialCitationCount': 0, 'publicationDate': '2024-06-17'}, {'title': 'Dual-Phase Accelerated Prompt Optimization', 'abstract': 'Gradient-free prompt optimization methods have made significant strides in enhancing the performance of closed-source Large Language Models (LLMs) across a wide range of tasks. However, existing approaches make light of the importance of high-quality prompt initialization and the identification of effective optimization directions, thus resulting in substantial optimization steps to obtain satisfactory performance. In this light, we aim to accelerate prompt optimization process to tackle the challenge of low convergence rate. We propose a dual-phase approach which starts with generating high-quality initial prompts by adopting a well-designed meta-instruction to delve into task-specific information, and iteratively optimize the prompts at the sentence level, leveraging previous tuning experience to expand prompt candidates and accept effective ones. Extensive experiments on eight datasets demonstrate the effectiveness of our proposed method, achieving a consistent accuracy gain over baselines with less than five optimization steps.', 'influentialCitationCount': 0, 'publicationDate': '2024-06-19'}, {'title': 'Trace is the New AutoDiff -- Unlocking Efficient Optimization of Computational Workflows', 'abstract': \"We study a class of optimization problems motivated by automating the design and update of AI systems like coding assistants, robots, and copilots. We propose an end-to-end optimization framework, Trace, which treats the computational workflow of an AI system as a graph akin to neural networks, based on a generalization of back-propagation. Optimization of computational workflows often involves rich feedback (e.g. console output or user's responses), heterogeneous parameters (e.g. prompts, hyper-parameters, codes), and intricate objectives (beyond maximizing a score). Moreover, its computation graph can change dynamically with the inputs and parameters. We frame a new mathematical setup of iterative optimization, Optimization with Trace Oracle (OPTO), to capture and abstract these properties so as to design optimizers that work across many domains. In OPTO, an optimizer receives an execution trace along with feedback on the computed output and updates parameters iteratively. Trace is the tool to implement OPTO in practice. Trace has a Python interface that efficiently converts a computational workflow into an OPTO instance using a PyTorch-like interface. Using Trace, we develop a general-purpose LLM-based optimizer called OptoPrime that can effectively solve OPTO problems. In empirical studies, we find that OptoPrime is capable of first-order numerical optimization, prompt optimization, hyper-parameter tuning, robot controller design, code debugging, etc., and is often competitive with specialized optimizers for each domain. We believe that Trace, OptoPrime and the OPTO framework will enable the next generation of interactive agents that automatically adapt using various kinds of feedback. Website: https://microsoft.github.io/Trace\", 'influentialCitationCount': 0, 'publicationDate': '2024-06-23'}]\u001b[0m\u001b[32;1m\u001b[1;3mHere are some recent papers that cite \"PROMPTAGENT: STRATEGIC PLANNING WITH LANGUAGE MODELS ENABLES EXPERT-LEVEL PROMPT OPTIMIZATION\":\n",
      "\n",
      "1. **Agent-Pro: Learning to Evolve via Policy-Level Reflection and Optimization**\n",
      "   - **Abstract**: Large Language Models (LLMs) exhibit robust problem-solving capabilities for diverse tasks. However, most LLM-based agents are designed as specific task solvers with sophisticated prompt engineering, rather than agents capable of learning and evolving through interactions. These task solvers necessitate manually crafted prompts to inform task rules and regulate LLM behaviors, inherently incapacitating to address complex dynamic scenarios e.g., large interactive games. In light of this, we propose Agent-Pro: an LLM-based Agent with Policy-level Reflection and Optimization that can learn a wealth of expertise from interactive experiences and progressively elevate its behavioral policy. Specifically, it involves a dynamic belief generation and reflection process for policy evolution. Rather than action-level reflection, Agent-Pro iteratively reflects on past trajectories and beliefs, fine-tuning its irrational beliefs for a better policy. Moreover, a depth-first search is employed for policy optimization, ensuring continual enhancement in policy payoffs. Agent-Pro is evaluated across two games: Blackjack and Texas Hold'em, outperforming vanilla LLM and specialized models. Our results show Agent-Pro can learn and evolve in complex and dynamic scenes, which also benefits numerous LLM-based applications.\n",
      "   - **Publication Date**: 2024-02-27\n",
      "   - **Influential Citation Count**: 1\n",
      "\n",
      "2. **RoT: Enhancing Large Language Models with Reflection on Search Trees**\n",
      "   - **Abstract**: Large language models (LLMs) have demonstrated impressive capability in reasoning and planning when integrated with tree-search-based prompting methods. However, since these methods ignore the previous search experiences, they often make the same mistakes in the search process. To address this issue, we introduce Reflection on search Trees (RoT), an LLM reflection framework designed to improve the performance of tree-search-based prompting methods. It uses a strong LLM to summarize guidelines from previous tree search experiences to enhance the ability of a weak LLM. The guidelines are instructions about solving this task through tree search which can prevent the weak LLMs from making similar mistakes in the past search process. In addition, we proposed a novel state selection method, which identifies the critical information from historical search processes to help RoT generate more specific and meaningful guidelines. In our extensive experiments, we find that RoT significantly improves the performance of LLMs in reasoning or planning tasks with various tree-search-based prompting methods (e.g., BFS and MCTS). Non-tree-search-based prompting methods such as Chain-of-Thought (CoT) can also benefit from RoT guidelines since RoT can provide task-specific knowledge collected from the search experience.\n",
      "   - **Publication Date**: 2024-04-08\n",
      "   - **Influential Citation Count**: 1\n",
      "\n",
      "3. **Fairer Preferences Elicit Improved Human-Aligned Large Language Model Judgments**\n",
      "   - **Abstract**: Large language models (LLMs) have shown promising abilities as cost-effective and reference-free evaluators for assessing language generation quality. In particular, pairwise LLM evaluators, which compare two generated texts and determine the preferred one, have been employed in a wide range of applications. However, LLMs exhibit preference biases and worrying sensitivity to prompt designs. In this work, we first reveal that the predictive preference of LLMs can be highly brittle and skewed, even with semantically equivalent instructions. We find that fairer predictive preferences from LLMs consistently lead to judgments that are better aligned with humans. Motivated by this phenomenon, we propose an automatic Zero-shot Evaluation-oriented Prompt Optimization framework, ZEPO, which aims to produce fairer preference decisions and improve the alignment of LLM evaluators with human judgments. To this end, we propose a zero-shot learning objective based on the preference decision fairness. ZEPO demonstrates substantial performance improvements over state-of-the-art LLM evaluators, without requiring labeled data, on representative meta-evaluation benchmarks. Our findings underscore the critical correlation between preference fairness and human alignment, positioning ZEPO as an efficient prompt optimizer for bridging the gap between LLM evaluators and human judgments.\n",
      "   - **Publication Date**: 2024-06-17\n",
      "   - **Influential Citation Count**: 0\n",
      "\n",
      "4. **Dual-Phase Accelerated Prompt Optimization**\n",
      "   - **Abstract**: Gradient-free prompt optimization methods have made significant strides in enhancing the performance of closed-source Large Language Models (LLMs) across a wide range of tasks. However, existing approaches make light of the importance of high-quality prompt initialization and the identification of effective optimization directions, thus resulting in substantial optimization steps to obtain satisfactory performance. In this light, we aim to accelerate prompt optimization process to tackle the challenge of low convergence rate. We propose a dual-phase approach which starts with generating high-quality initial prompts by adopting a well-designed meta-instruction to delve into task-specific information, and iteratively optimize the prompts at the sentence level, leveraging previous tuning experience to expand prompt candidates and accept effective ones. Extensive experiments on eight datasets demonstrate the effectiveness of our proposed method, achieving a consistent accuracy gain over baselines with less than five optimization steps.\n",
      "   - **Publication Date**: 2024-06-19\n",
      "   - **Influential Citation Count**: 0\n",
      "\n",
      "5. **Trace is the New AutoDiff -- Unlocking Efficient Optimization of Computational Workflows**\n",
      "   - **Abstract**: We study a class of optimization problems motivated by automating the design and update of AI systems like coding assistants, robots, and copilots. We propose an end-to-end optimization framework, Trace, which treats the computational workflow of an AI system as a graph akin to neural networks, based on a generalization of back-propagation. Optimization of computational workflows often involves rich feedback (e.g. console output or user's responses), heterogeneous parameters (e.g. prompts, hyper-parameters, codes), and intricate objectives (beyond maximizing a score). Moreover, its computation graph can change dynamically with the inputs and parameters. We frame a new mathematical setup of iterative optimization, Optimization with Trace Oracle (OPTO), to capture and abstract these properties so as to design optimizers that work across many domains. In OPTO, an optimizer receives an execution trace along with feedback on the computed output and updates parameters iteratively. Trace is the tool to implement OPTO in practice. Trace has a Python interface that efficiently converts a computational workflow into an OPTO instance using a PyTorch-like interface. Using Trace, we develop a general-purpose LLM-based optimizer called OptoPrime that can effectively solve OPTO problems. In empirical studies, we find that OptoPrime is capable of first-order numerical optimization, prompt optimization, hyper-parameter tuning, robot controller design, code debugging, etc., and is often competitive with specialized optimizers for each domain. We believe that Trace, OptoPrime and the OPTO framework will enable the next generation of interactive agents that automatically adapt using various kinds of feedback.\n",
      "   - **Publication Date**: 2024-06-23\n",
      "   - **Influential Citation Count**: 0\n",
      "\n",
      "These papers build upon the concepts introduced in \"PROMPTAGENT\" and explore various aspects of prompt optimization, reflection, and learning in large language models.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Here are some recent papers that cite \"PROMPTAGENT: STRATEGIC PLANNING WITH LANGUAGE MODELS ENABLES EXPERT-LEVEL PROMPT OPTIMIZATION\":\n",
       "\n",
       "1. **Agent-Pro: Learning to Evolve via Policy-Level Reflection and Optimization**\n",
       "   - **Abstract**: Large Language Models (LLMs) exhibit robust problem-solving capabilities for diverse tasks. However, most LLM-based agents are designed as specific task solvers with sophisticated prompt engineering, rather than agents capable of learning and evolving through interactions. These task solvers necessitate manually crafted prompts to inform task rules and regulate LLM behaviors, inherently incapacitating to address complex dynamic scenarios e.g., large interactive games. In light of this, we propose Agent-Pro: an LLM-based Agent with Policy-level Reflection and Optimization that can learn a wealth of expertise from interactive experiences and progressively elevate its behavioral policy. Specifically, it involves a dynamic belief generation and reflection process for policy evolution. Rather than action-level reflection, Agent-Pro iteratively reflects on past trajectories and beliefs, fine-tuning its irrational beliefs for a better policy. Moreover, a depth-first search is employed for policy optimization, ensuring continual enhancement in policy payoffs. Agent-Pro is evaluated across two games: Blackjack and Texas Hold'em, outperforming vanilla LLM and specialized models. Our results show Agent-Pro can learn and evolve in complex and dynamic scenes, which also benefits numerous LLM-based applications.\n",
       "   - **Publication Date**: 2024-02-27\n",
       "   - **Influential Citation Count**: 1\n",
       "\n",
       "2. **RoT: Enhancing Large Language Models with Reflection on Search Trees**\n",
       "   - **Abstract**: Large language models (LLMs) have demonstrated impressive capability in reasoning and planning when integrated with tree-search-based prompting methods. However, since these methods ignore the previous search experiences, they often make the same mistakes in the search process. To address this issue, we introduce Reflection on search Trees (RoT), an LLM reflection framework designed to improve the performance of tree-search-based prompting methods. It uses a strong LLM to summarize guidelines from previous tree search experiences to enhance the ability of a weak LLM. The guidelines are instructions about solving this task through tree search which can prevent the weak LLMs from making similar mistakes in the past search process. In addition, we proposed a novel state selection method, which identifies the critical information from historical search processes to help RoT generate more specific and meaningful guidelines. In our extensive experiments, we find that RoT significantly improves the performance of LLMs in reasoning or planning tasks with various tree-search-based prompting methods (e.g., BFS and MCTS). Non-tree-search-based prompting methods such as Chain-of-Thought (CoT) can also benefit from RoT guidelines since RoT can provide task-specific knowledge collected from the search experience.\n",
       "   - **Publication Date**: 2024-04-08\n",
       "   - **Influential Citation Count**: 1\n",
       "\n",
       "3. **Fairer Preferences Elicit Improved Human-Aligned Large Language Model Judgments**\n",
       "   - **Abstract**: Large language models (LLMs) have shown promising abilities as cost-effective and reference-free evaluators for assessing language generation quality. In particular, pairwise LLM evaluators, which compare two generated texts and determine the preferred one, have been employed in a wide range of applications. However, LLMs exhibit preference biases and worrying sensitivity to prompt designs. In this work, we first reveal that the predictive preference of LLMs can be highly brittle and skewed, even with semantically equivalent instructions. We find that fairer predictive preferences from LLMs consistently lead to judgments that are better aligned with humans. Motivated by this phenomenon, we propose an automatic Zero-shot Evaluation-oriented Prompt Optimization framework, ZEPO, which aims to produce fairer preference decisions and improve the alignment of LLM evaluators with human judgments. To this end, we propose a zero-shot learning objective based on the preference decision fairness. ZEPO demonstrates substantial performance improvements over state-of-the-art LLM evaluators, without requiring labeled data, on representative meta-evaluation benchmarks. Our findings underscore the critical correlation between preference fairness and human alignment, positioning ZEPO as an efficient prompt optimizer for bridging the gap between LLM evaluators and human judgments.\n",
       "   - **Publication Date**: 2024-06-17\n",
       "   - **Influential Citation Count**: 0\n",
       "\n",
       "4. **Dual-Phase Accelerated Prompt Optimization**\n",
       "   - **Abstract**: Gradient-free prompt optimization methods have made significant strides in enhancing the performance of closed-source Large Language Models (LLMs) across a wide range of tasks. However, existing approaches make light of the importance of high-quality prompt initialization and the identification of effective optimization directions, thus resulting in substantial optimization steps to obtain satisfactory performance. In this light, we aim to accelerate prompt optimization process to tackle the challenge of low convergence rate. We propose a dual-phase approach which starts with generating high-quality initial prompts by adopting a well-designed meta-instruction to delve into task-specific information, and iteratively optimize the prompts at the sentence level, leveraging previous tuning experience to expand prompt candidates and accept effective ones. Extensive experiments on eight datasets demonstrate the effectiveness of our proposed method, achieving a consistent accuracy gain over baselines with less than five optimization steps.\n",
       "   - **Publication Date**: 2024-06-19\n",
       "   - **Influential Citation Count**: 0\n",
       "\n",
       "5. **Trace is the New AutoDiff -- Unlocking Efficient Optimization of Computational Workflows**\n",
       "   - **Abstract**: We study a class of optimization problems motivated by automating the design and update of AI systems like coding assistants, robots, and copilots. We propose an end-to-end optimization framework, Trace, which treats the computational workflow of an AI system as a graph akin to neural networks, based on a generalization of back-propagation. Optimization of computational workflows often involves rich feedback (e.g. console output or user's responses), heterogeneous parameters (e.g. prompts, hyper-parameters, codes), and intricate objectives (beyond maximizing a score). Moreover, its computation graph can change dynamically with the inputs and parameters. We frame a new mathematical setup of iterative optimization, Optimization with Trace Oracle (OPTO), to capture and abstract these properties so as to design optimizers that work across many domains. In OPTO, an optimizer receives an execution trace along with feedback on the computed output and updates parameters iteratively. Trace is the tool to implement OPTO in practice. Trace has a Python interface that efficiently converts a computational workflow into an OPTO instance using a PyTorch-like interface. Using Trace, we develop a general-purpose LLM-based optimizer called OptoPrime that can effectively solve OPTO problems. In empirical studies, we find that OptoPrime is capable of first-order numerical optimization, prompt optimization, hyper-parameter tuning, robot controller design, code debugging, etc., and is often competitive with specialized optimizers for each domain. We believe that Trace, OptoPrime and the OPTO framework will enable the next generation of interactive agents that automatically adapt using various kinds of feedback.\n",
       "   - **Publication Date**: 2024-06-23\n",
       "   - **Influential Citation Count**: 0\n",
       "\n",
       "These papers build upon the concepts introduced in \"PROMPTAGENT\" and explore various aspects of prompt optimization, reflection, and learning in large language models."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# semantic scholar error ë°œìƒ ì‹œ ë‹¤ì‹œ í•œ ë²ˆ ì‹¤í–‰í•˜ë©´ ë©ë‹ˆë‹¤.\n",
    "assistant('PROMPTAGENT: STRATEGIC PLANNING WITH LANGUAGE MODELS ENABLES EXPERT-LEVEL PROMPT OPTIMIZATION í›„ì† ë…¼ë¬¸ ì¶”ì²œí•´ì¤˜') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "assistant.reset_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `loadpaper` with `{'title': 'A Survey on Recent Advances in LLM-Based Multi-turn Dialogue Systems', 'arxiv_id': '2402.18013'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3mHere is the title and section of the paper in HTML\n",
      "title\n",
      "A Survey on Recent Advances in LLM-Based Multi-turn Dialogue Systems\n",
      "sections\n",
      "          Abstract.\n",
      "  1. INTRODUCTION\n",
      "    1.1. What is Multi-turn Dialogue System?\n",
      "    1.2. Why a Survey on LLM-based Multi-turn Dialogue System?\n",
      "    1.3. Contribution of this Survey\n",
      "  2. GENERAL METHODS\n",
      "    2.1. Decoder-only Transformer Architecture\n",
      "      2.1.1. Causal Decoder\n",
      "        GPT Series\n",
      "          GPT-1.\n",
      "          GPT-2.\n",
      "          GPT-3.\n",
      "          GPT-3.5.\n",
      "          GPT-4.\n",
      "          ChatGPT.\n",
      "          GPTs.\n",
      "        LLAMA Series\n",
      "          LLAMA.\n",
      "          LLAMA2.\n",
      "          LLAMA2 CHAT.\n",
      "          CODE LLAMA.\n",
      "      2.1.2. Prefix Decoder\n",
      "        GLM\n",
      "        ChatGLM series\n",
      "          ChatGLM-6B.\n",
      "          ChatGLM2-6B.\n",
      "          ChatGLM3-6B.\n",
      "    2.2. Encoder-only Transformer Architecture\n",
      "      2.2.1. BERT Series\n",
      "        BERT\n",
      "        RoBERTa\n",
      "      2.2.2. UNiLM\n",
      "    2.3. Encoder-decoder Transformer Architecture\n",
      "      2.3.1. BART\n",
      "      2.3.2. Text-to-Text Transfer Transformer\n",
      "  3. Fine-Tuning\n",
      "    3.1. Full Fine-Tuning\n",
      "    3.2. Parameter-efficient Fine-Tuning\n",
      "      3.2.1. Adapters\n",
      "      3.2.2. LoRA\n",
      "      3.2.3. Instruction Fine-Tuning\n",
      "    3.3. Mitigating Fine-Tuning Instabilities\n",
      "  4. Prompt Engineering\n",
      "    4.1. Prompt Tuning\n",
      "      4.1.1. Discrete Prompts\n",
      "        D1:Pattern-Exploiting Training\n",
      "        D2:LM-BFF\n",
      "        D3:Prompt Tuning with Rules\n",
      "        D4:Knowledgeable Prompt-tuning\n",
      "      4.1.2. Continuous Prompts\n",
      "        C1:P-tuning\n",
      "        C2:Prefix Tuning\n",
      "        C3: P-tuning V2\n",
      "    4.2. Tuning-free Prompting\n",
      "      4.2.1. In-Context Learning\n",
      "      4.2.2. Chain-of-Thought\n",
      "  5. LLM Based Task-oriented Dialogue Systems\n",
      "    5.1. Pipeline-based Methods\n",
      "      5.1.1. Natural Language Understanding\n",
      "        Intent detection\n",
      "        Joint intent detection and slot filling\n",
      "      5.1.2. Dialogue State Tracking\n",
      "      5.1.3. Policy Learning\n",
      "      5.1.4. Natural Language Generation\n",
      "    5.2. End-to-End Methods\n",
      "  6. LLM Based Open-Domain Dialogue Systems\n",
      "    6.1. Retrieval-based Methods\n",
      "    6.2. Generation-based Methods\n",
      "      6.2.1. Knowledge-Enhanced Generation\n",
      "      6.2.2. Personalization and Consistency\n",
      "    6.3. Hybrid Methods\n",
      "      6.3.1. Integrating Retrieval and Generation\n",
      "      6.3.2. Enhancing Dialogue with External Knowledge\n",
      "  7. EVALUATION APPROACHES\n",
      "    7.1. Automatic Evaluation\n",
      "      7.1.1. Automatic Evaluation Approaches for Task-oriented Dialogue Systems\n",
      "        Joint Goal Accuracy\n",
      "        Slot Accuracy\n",
      "        Average Goal Accuracy\n",
      "        Requested Slot F1\n",
      "        BLEU\n",
      "        Entity F1\n",
      "      7.1.2. Automatic Evaluation Approaches for Open-domain Dialogue Systems\n",
      "        Perplexity\n",
      "        DIST-n\n",
      "        Recall@K\n",
      "    7.2. Human Evaluation\n",
      "  8. DATASETS\n",
      "    8.1. Datasets for Task-oriented Dialogue Systems\n",
      "        MultiWOZ\n",
      "        RiSAWOZ\n",
      "        CrossWOZ\n",
      "        PersuasionForGood\n",
      "        WOZ 2.0\n",
      "        Stanford Multi-Domain\n",
      "    8.2. Datasets for Open-domain Dialogue Systems\n",
      "        PersonaChat\n",
      "        MMdialog\n",
      "        Dailydialog\n",
      "        Pchatbot\n",
      "        PersonalDialogue\n",
      "        Douban\n",
      "  9. DISCUSSION\n",
      "  10. CONCLUSION\n",
      "  References\n",
      "\n",
      "\n",
      " Use the 'loadpaper' tool again, specifying the section list you want to view in detail.\u001b[0m\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `loadpaper` with `{'title': 'A Survey on Recent Advances in LLM-Based Multi-turn Dialogue Systems', 'sections': ['Abstract'], 'arxiv_id': '2402.18013'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3mAbstract.This survey provides a comprehensive review of research on multi-turn dialogue systems, with a particular focus on multi-turn dialogue systems based on large language models (LLMs). This paper aims to (a) give a summary of existing LLMs and approaches for adapting LLMs to downstream tasks; (b) elaborate recent advances in multi-turn dialogue systems, covering both LLM-based open-domain dialogue (ODD) and task-oriented dialogue (TOD) systems, along with datasets and evaluation metrics; (c) discuss some future emphasis and recent research problems arising from the development of LLMs and the increasing demands on multi-turn dialogue systems.\u001b[0m\u001b[32;1m\u001b[1;3mì´ ë…¼ë¬¸ì˜ ì´ˆë¡ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:\n",
      "\n",
      "ì´ ì„¤ë¬¸ì¡°ì‚¬ëŠ” ë‹¤ì¤‘ í„´ ëŒ€í™” ì‹œìŠ¤í…œì— ëŒ€í•œ ì—°êµ¬ë¥¼ í¬ê´„ì ìœ¼ë¡œ ê²€í† í•˜ë©°, íŠ¹íˆ ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLM)ì„ ê¸°ë°˜ìœ¼ë¡œ í•œ ë‹¤ì¤‘ í„´ ëŒ€í™” ì‹œìŠ¤í…œì— ì¤‘ì ì„ ë‘¡ë‹ˆë‹¤. ì´ ë…¼ë¬¸ì€ ë‹¤ìŒì„ ëª©í‘œë¡œ í•©ë‹ˆë‹¤:\n",
      "(a) ê¸°ì¡´ LLM ë° LLMì„ ë‹¤ìš´ìŠ¤íŠ¸ë¦¼ ì‘ì—…ì— ì ì‘ì‹œí‚¤ê¸° ìœ„í•œ ì ‘ê·¼ ë°©ì‹ì„ ìš”ì•½í•©ë‹ˆë‹¤.\n",
      "(b) LLM ê¸°ë°˜ì˜ ì˜¤í”ˆ ë„ë©”ì¸ ëŒ€í™”(ODD) ë° ì‘ì—… ì§€í–¥ ëŒ€í™”(TOD) ì‹œìŠ¤í…œì„ í¬í•¨í•œ ë‹¤ì¤‘ í„´ ëŒ€í™” ì‹œìŠ¤í…œì˜ ìµœê·¼ ë°œì „, ë°ì´í„°ì…‹ ë° í‰ê°€ ì§€í‘œë¥¼ ìì„¸íˆ ì„¤ëª…í•©ë‹ˆë‹¤.\n",
      "(c) LLMì˜ ë°œì „ê³¼ ë‹¤ì¤‘ í„´ ëŒ€í™” ì‹œìŠ¤í…œì— ëŒ€í•œ ì¦ê°€í•˜ëŠ” ìš”êµ¬ë¡œ ì¸í•´ ë°œìƒí•˜ëŠ” ë¯¸ë˜ì˜ ì¤‘ì  ì‚¬í•­ê³¼ ìµœê·¼ ì—°êµ¬ ë¬¸ì œë¥¼ ë…¼ì˜í•©ë‹ˆë‹¤.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "ì´ ë…¼ë¬¸ì˜ ì´ˆë¡ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:\n",
       "\n",
       "ì´ ì„¤ë¬¸ì¡°ì‚¬ëŠ” ë‹¤ì¤‘ í„´ ëŒ€í™” ì‹œìŠ¤í…œì— ëŒ€í•œ ì—°êµ¬ë¥¼ í¬ê´„ì ìœ¼ë¡œ ê²€í† í•˜ë©°, íŠ¹íˆ ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLM)ì„ ê¸°ë°˜ìœ¼ë¡œ í•œ ë‹¤ì¤‘ í„´ ëŒ€í™” ì‹œìŠ¤í…œì— ì¤‘ì ì„ ë‘¡ë‹ˆë‹¤. ì´ ë…¼ë¬¸ì€ ë‹¤ìŒì„ ëª©í‘œë¡œ í•©ë‹ˆë‹¤:\n",
       "(a) ê¸°ì¡´ LLM ë° LLMì„ ë‹¤ìš´ìŠ¤íŠ¸ë¦¼ ì‘ì—…ì— ì ì‘ì‹œí‚¤ê¸° ìœ„í•œ ì ‘ê·¼ ë°©ì‹ì„ ìš”ì•½í•©ë‹ˆë‹¤.\n",
       "(b) LLM ê¸°ë°˜ì˜ ì˜¤í”ˆ ë„ë©”ì¸ ëŒ€í™”(ODD) ë° ì‘ì—… ì§€í–¥ ëŒ€í™”(TOD) ì‹œìŠ¤í…œì„ í¬í•¨í•œ ë‹¤ì¤‘ í„´ ëŒ€í™” ì‹œìŠ¤í…œì˜ ìµœê·¼ ë°œì „, ë°ì´í„°ì…‹ ë° í‰ê°€ ì§€í‘œë¥¼ ìì„¸íˆ ì„¤ëª…í•©ë‹ˆë‹¤.\n",
       "(c) LLMì˜ ë°œì „ê³¼ ë‹¤ì¤‘ í„´ ëŒ€í™” ì‹œìŠ¤í…œì— ëŒ€í•œ ì¦ê°€í•˜ëŠ” ìš”êµ¬ë¡œ ì¸í•´ ë°œìƒí•˜ëŠ” ë¯¸ë˜ì˜ ì¤‘ì  ì‚¬í•­ê³¼ ìµœê·¼ ì—°êµ¬ ë¬¸ì œë¥¼ ë…¼ì˜í•©ë‹ˆë‹¤."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "assistant('A Survey on Recent Advances in LLM-Based Multi-turn Dialogue Systems, arxiv id 2402.18013 ë…¼ë¬¸ ì´ˆë¡ ìš”ì•½í•´ì¤˜')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assistant('')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
