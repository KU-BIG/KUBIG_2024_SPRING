# -*- coding: utf-8 -*-
"""ModelTraining.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/19wEfoP71hVFgyiQsFnT-C-IHCHq6TCOr
"""

import pandas as pd

# data = pd.read_csv("sample_prop_0619.csv")
raw = pd.read_csv("sentiment_example500.csv")

raw.drop(columns=['emotions'], inplace=True)
raw

"""## 신경망모델"""

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping

# 특징과 레이블 분리
X = data['문장']
y = data.drop(columns=['문장'])

# 데이터셋 분할
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# TF-IDF 벡터화
tfidf = TfidfVectorizer(max_features=5000)
X_train_tfidf = tfidf.fit_transform(X_train).toarray()
X_test_tfidf = tfidf.transform(X_test).toarray()

# 모델 신경망 구축
model = Sequential([
    Dense(64, activation='relu', input_shape=(X_train_tfidf.shape[1],)),
    Dropout(0.5),
    Dense(32, activation='relu'),
    Dropout(0.5),
    Dense(y.shape[1], activation='sigmoid')
])

model.compile(optimizer=Adam(learning_rate=0.001),
              loss='mean_squared_error',
              metrics=['mean_squared_error'])

# 모델 학습
early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)
history = model.fit(X_train_tfidf, y_train, epochs=50, batch_size=16,
                    validation_data=(X_test_tfidf, y_test), callbacks=[early_stopping])

# 모델 평가
loss, mse = model.evaluate(X_test_tfidf, y_test)
print(f'Test Loss: {loss}')
print(f'Test Mean Squared Error: {mse}')

# 새로운 문장 예측
new_sentence = ["나 네가 너무 좋아"]
new_sentence_tfidf = tfidf.transform(new_sentence).toarray()
predictions = model.predict(new_sentence_tfidf)

# 예측 결과 출력
pred_df = pd.DataFrame(predictions, columns=y.columns)
print(pred_df)

"""## KoBERT"""

# pip install transformers torch pandas scikit-learn

import pandas as pd
from sklearn.model_selection import train_test_split
from transformers import BertTokenizer, BertModel
import torch
from torch import nn
from torch.utils.data import DataLoader, Dataset
from sklearn.preprocessing import MinMaxScaler

# 특징과 레이블 분리
X = data['문장']
y = data.drop(columns=['문장'])

# 데이터셋 분할
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Tokenizer와 모델 불러오기
tokenizer = BertTokenizer.from_pretrained('monologg/kobert')
model = BertModel.from_pretrained('monologg/kobert')

# Device 설정
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")


# 데이터셋 클래스 정의
class KoBERTDataset(Dataset):
    def __init__(self, texts, targets, tokenizer, max_len):
        self.texts = texts
        self.targets = targets
        self.tokenizer = tokenizer
        self.max_len = max_len

    def __len__(self):
        return len(self.texts)

    def __getitem__(self, idx):
        text = self.texts.iloc[idx]
        target = self.targets.iloc[idx]

        encoding = self.tokenizer.encode_plus(
            text,
            add_special_tokens=True,
            max_length=self.max_len,
            return_token_type_ids=False,
            padding='max_length',
            return_attention_mask=True,
            return_tensors='pt',
            truncation=True
        )

        return {
            'input_ids': encoding['input_ids'].flatten(),
            'attention_mask': encoding['attention_mask'].flatten(),
            'targets': torch.tensor(target, dtype=torch.float)
        }

# 데이터셋 생성
MAX_LEN = 128
train_dataset = KoBERTDataset(X_train, y_train, tokenizer, MAX_LEN)
test_dataset = KoBERTDataset(X_test, y_test, tokenizer, MAX_LEN)

train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)
test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False)

class EmotionRegressor(nn.Module):
    def __init__(self, n_classes):
        super(EmotionRegressor, self).__init__()
        self.bert = BertModel.from_pretrained('monologg/kobert')
        self.drop = nn.Dropout(p=0.3)
        self.out = nn.Linear(self.bert.config.hidden_size, n_classes)
        self.sigmoid = nn.Sigmoid()  # Sigmoid activation function

    def forward(self, input_ids, attention_mask):
        pooled_output = self.bert(
            input_ids=input_ids,
            attention_mask=attention_mask
        )[1]
        output = self.drop(pooled_output)
        output = self.out(output)
        return self.sigmoid(output)  # Apply sigmoid activation

model = EmotionRegressor(n_classes=y.shape[1])
model = model.to(device)

import torch.optim as optim
import numpy as np

# 손실 함수 및 옵티마이저 정의
criterion = nn.MSELoss()
optimizer = optim.Adam(model.parameters(), lr=2e-5)

# 모델 학습 함수 정의
def train_epoch(model, data_loader, criterion, optimizer, device):
    model = model.train()
    losses = []

    for d in data_loader:
        input_ids = d['input_ids'].to(device)
        attention_mask = d['attention_mask'].to(device)
        targets = d['targets'].to(device)

        outputs = model(input_ids=input_ids, attention_mask=attention_mask)
        loss = criterion(outputs, targets)

        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        losses.append(loss.item())

    return np.mean(losses)

# 모델 평가 함수 정의
def eval_model(model, data_loader, criterion, device):
    model = model.eval()
    losses = []

    with torch.no_grad():
        for d in data_loader:
            input_ids = d['input_ids'].to(device)
            attention_mask = d['attention_mask'].to(device)
            targets = d['targets'].to(device)

            outputs = model(input_ids=input_ids, attention_mask=attention_mask)
            loss = criterion(outputs, targets)

            losses.append(loss.item())

    return np.mean(losses)

# 학습 및 평가
NUM_EPOCHS = 50

for epoch in range(NUM_EPOCHS):
    train_loss = train_epoch(model, train_loader, criterion, optimizer, device)
    val_loss = eval_model(model, test_loader, criterion, device)

    print(f'Epoch {epoch + 1}/{NUM_EPOCHS}')
    print(f'Train Loss: {train_loss:.4f}')
    print(f'Validation Loss: {val_loss:.4f}')

# 새로운 문장 예측
def predict(model, tokenizer, text, max_len):
    encoding = tokenizer.encode_plus(
        text,
        add_special_tokens=True,
        max_length=max_len,
        return_token_type_ids=False,
        padding='max_length',
        return_attention_mask=True,
        return_tensors='pt',
        truncation=True
    )

    input_ids = encoding['input_ids'].to(device)
    attention_mask = encoding['attention_mask'].to(device)

    model = model.eval()
    with torch.no_grad():
        outputs = model(input_ids=input_ids, attention_mask=attention_mask)

    return outputs.cpu().numpy()

new_sentence = "나 네가 너무 좋아"
predictions = predict(model, tokenizer, new_sentence, MAX_LEN)

# 예측 결과 출력
pred_df = pd.DataFrame(predictions, columns=y.columns)
print(pred_df)

# 테스트 데이터 평가
y_test_pred = []
y_test_true = []

model = model.eval()
with torch.no_grad():
    for d in test_loader:
        input_ids = d['input_ids'].to(device)
        attention_mask = d['attention_mask'].to(device)
        targets = d['targets'].to(device)

        outputs = model(input_ids=input_ids, attention_mask=attention_mask)

        y_test_pred.extend(outputs.cpu().numpy())
        y_test_true.extend(targets.cpu().numpy())

# Mean Squared Error 계산
mse = mean_squared_error(y_test_true, y_test_pred)
print(f'Test Mean Squared Error: {mse}')

"""## klue/roberta-large"""

# pip install transformers torch pandas scikit-learn

import pandas as pd
from sklearn.model_selection import train_test_split
from transformers import AutoTokenizer, AutoModel
import torch
from torch import nn
from torch.utils.data import DataLoader, Dataset

# 특징과 레이블 분리
X = data['문장']
y = data.drop(columns=['문장'])

# 데이터셋 분할
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Tokenizer와 모델 불러오기
tokenizer = AutoTokenizer.from_pretrained('klue/roberta-large')
model = AutoModel.from_pretrained('klue/roberta-large')

# Device 설정
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# 데이터셋 클래스 정의
class KoBERTDataset(Dataset):
    def __init__(self, texts, targets, tokenizer, max_len):
        self.texts = texts
        self.targets = targets
        self.tokenizer = tokenizer
        self.max_len = max_len

    def __len__(self):
        return len(self.texts)

    def __getitem__(self, idx):
        text = self.texts.iloc[idx]
        target = self.targets.iloc[idx]

        encoding = self.tokenizer.encode_plus(
            text,
            add_special_tokens=True,
            max_length=self.max_len,
            return_token_type_ids=False,
            padding='max_length',
            return_attention_mask=True,
            return_tensors='pt',
            truncation=True
        )

        return {
            'input_ids': encoding['input_ids'].flatten(),
            'attention_mask': encoding['attention_mask'].flatten(),
            'targets': torch.tensor(target, dtype=torch.float)
        }

# 데이터셋 생성
MAX_LEN = 128
train_dataset = KoBERTDataset(X_train, y_train, tokenizer, MAX_LEN)
test_dataset = KoBERTDataset(X_test, y_test, tokenizer, MAX_LEN)

train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)
test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False)

class EmotionRegressor(nn.Module):
    def __init__(self, n_classes):
        super(EmotionRegressor, self).__init__()
        self.bert = AutoModel.from_pretrained('klue/roberta-large')
        self.drop = nn.Dropout(p=0.3)
        self.out = nn.Linear(self.bert.config.hidden_size, n_classes)
        self.sigmoid = nn.Sigmoid()  # Sigmoid activation function

    def forward(self, input_ids, attention_mask):
        pooled_output = self.bert(
            input_ids=input_ids,
            attention_mask=attention_mask
        )[1]
        output = self.drop(pooled_output)
        output = self.out(output)
        return self.sigmoid(output)  # Apply sigmoid activation

model = EmotionRegressor(n_classes=y.shape[1])
model = model.to(device)

import torch.optim as optim
import numpy as np

# 손실 함수 및 옵티마이저 정의
criterion = nn.MSELoss()
optimizer = optim.Adam(model.parameters(), lr=2e-5)

# 모델 학습 함수 정의
def train_epoch(model, data_loader, criterion, optimizer, device):
    model = model.train()
    losses = []

    for d in data_loader:
        input_ids = d['input_ids'].to(device)
        attention_mask = d['attention_mask'].to(device)
        targets = d['targets'].to(device)

        outputs = model(input_ids=input_ids, attention_mask=attention_mask)
        loss = criterion(outputs, targets)

        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        losses.append(loss.item())

    return np.mean(losses)

# 학습 및 평가
NUM_EPOCHS = 50

for epoch in range(NUM_EPOCHS):
    train_loss = train_epoch(model, train_loader, criterion, optimizer, device)
    val_loss = eval_model(model, test_loader, criterion, device)

    print(f'Epoch {epoch + 1}/{NUM_EPOCHS}')
    print(f'Train Loss: {train_loss:.4f}')
    print(f'Validation Loss: {val_loss:.4f}')

# 새로운 문장 예측
def predict(model, tokenizer, text, max_len):
    encoding = tokenizer.encode_plus(
        text,
        add_special_tokens=True,
        max_length=max_len,
        return_token_type_ids=False,
        padding='max_length',
        return_attention_mask=True,
        return_tensors='pt',
        truncation=True
    )

    input_ids = encoding['input_ids'].to(device)
    attention_mask = encoding['attention_mask'].to(device)

    model = model.eval()
    with torch.no_grad():
        outputs = model(input_ids=input_ids, attention_mask=attention_mask)

    return outputs.cpu().numpy()

new_sentence = "나 네가 너무 좋아"
predictions = predict(model, tokenizer, new_sentence, MAX_LEN)

# 예측 결과 출력
pred_df = pd.DataFrame(predictions, columns=y.columns)
print(pred_df)

# 테스트 데이터 평가
y_test_pred = []
y_test_true = []

model = model.eval()
with torch.no_grad():
    for d in test_loader:
        input_ids = d['input_ids'].to(device)
        attention_mask = d['attention_mask'].to(device)
        targets = d['targets'].to(device)

        outputs = model(input_ids=input_ids, attention_mask=attention_mask)

        y_test_pred.extend(outputs.cpu().numpy())
        y_test_true.extend(targets.cpu().numpy())

# Mean Squared Error 계산
mse = mean_squared_error(y_test_true, y_test_pred)
print(f'Test Mean Squared Error: {mse}')

"""## KOTE"""

!pip install datasets

from datasets import load_dataset
import pandas as pd

# 데이터셋 로드
dataset = load_dataset("searle-j/kote")
print(dataset)

# 데이터셋을 데이터프레임으로 변환
train_df = pd.DataFrame(dataset['train'])
test_df = pd.DataFrame(dataset['test'])
val_df = pd.DataFrame(dataset['validation'])

print(train_df.head())
print(test_df.head())
print(val_df.head())

from transformers import AutoTokenizer, AutoModelForSequenceClassification, TextClassificationPipeline

model_name = "searle-j/kote_for_easygoing_people"
model = AutoModelForSequenceClassification.from_pretrained(model_name)
tokenizer = AutoTokenizer.from_pretrained(model_name)

pipe = TextClassificationPipeline(
        model=model,
        tokenizer=tokenizer,
        device=0, # gpu number, -1 if cpu used
        return_all_scores=True,
        function_to_apply='sigmoid'
    )

for output in pipe("""나는 지금 집도착해서 옷갈아입었고 바로 잘꺼예요~ 잘자요 내사랑 아직도 자고 있나요~~??""")[0]:
    if output["score"]>0.4:
        print(output)

for output in pipe("""나는 지금 집도착해서 옷갈아입었고 바로 잘꺼예요~ 잘자요 내사랑 아직도 자고 있나요~~??
                      나 밥먹고 책읽으러가요~ 카페가려고 나왓어요~""")[0]:
    if output["score"]>0.4:
        print(output)

for output in pipe("""나는 지금 집도착해서 옷갈아입었고 바로 잘꺼예요~ 잘자요 내사랑 아직도 자고 있나요~~??
                      나 밥먹고 책읽으러가요~ 카페가려고 나왓어요~
                      아구 밥 뭐 먹었어요?? 나는 점심 준비중이셔서 잠깐 깼어요..""")[0]:
    if output["score"]>0.4:
        print(output)

for output in pipe("""나는 지금 집도착해서 옷갈아입었고 바로 잘꺼예요~ 잘자요 내사랑 아직도 자고 있나요~~??
                      나 밥먹고 책읽으러가요~ 카페가려고 나왓어요~
                      아구 밥 뭐 먹었어요?? 나는 점심 준비중이셔서 잠깐 깼어요..
                      언니가 된장찌개 끓여쥬ㅓ어요 청국장은 냄새때뮨에 ㅠ 밥 잘먹그 푹 쉬어요~""")[0]:
    if output["score"]>0.4:
        print(output)

for output in pipe("""나는 지금 집 도착해서 옷갈아입었고 바로 잘꺼예요~ 잘자요 내사랑 아직도 자고 있나요~~?? 나 밥먹고 책읽으러가요~ 카페가려고 나왓어요~ 아구 밥 뭐 먹었어요?? 나는 점심 준비중이셔서 잠깐 깼어요.. 언니가 된장찌개 끓여쥬ㅓ어요 청국장은 냄새때뮨에 ㅠ 밥 잘먹그 푹 쉬어요~ 으이구~~약은 먹었어요??""")[0]:
    if output["score"]>0.4:
        print(output)

# 예측 결과 저장을 위한 함수 정의
def predict_emotions(text):
    outputs = pipe(text)[0]
    results = {output['label']: output['score'] for output in outputs if output['score'] > 0.4}
    return results

# 데이터프레임에 예측 결과 추가
raw['예측결과'] = raw['utterance'].apply(predict_emotions)

# 예측 결과 출력
print(raw)

result = raw
result[25:35]

"""## 감정태그 전체 데이터"""

from transformers import AutoTokenizer, AutoModelForSequenceClassification, TextClassificationPipeline

model_name = "searle-j/kote_for_easygoing_people"
model = AutoModelForSequenceClassification.from_pretrained(model_name)
tokenizer = AutoTokenizer.from_pretrained(model_name)

pipe = TextClassificationPipeline(
        model=model,
        tokenizer=tokenizer,
        device=0, # gpu number, -1 if cpu used
        return_all_scores=True,
        function_to_apply='sigmoid'
    )

from google.colab import drive

# Mount Google Drive
drive.mount('/content/drive')

import pandas as pd

data = pd.read_csv("/content/drive/MyDrive/preprocessed0610.csv")
data

data = data[["dialogueID2", "participantID", "utterance"]]
data

# 예측 결과 저장을 위한 함수 정의
def predict_emotions(text):
    outputs = pipe(text)[0]
    results = {output['label']: output['score'] for output in outputs if output['score'] > 0.4}
    return results

sample = data[:10]


# 데이터프레임에 예측 결과 추가
sample['emotion'] = sample['utterance'].apply(predict_emotions)

# 결과 출력
sample.head()

# 데이터프레임에 예측 결과 추가
data['emotion'] = data['utterance'].apply(predict_emotions)

# 결과 출력
print(data.head())

data.to_csv("kote_tagged_each.csv")

accum = pd.read_csv("/content/drive/MyDrive/preprocessed0610.csv")
accum = accum[["dialogueID2", "participantID", "utterance"]]
accum

# 대화 누적 함수 정의
def accumulate_dialogue(group):
    accumulated = []
    for idx, row in group.iterrows():
        if accumulated:
            accumulated.append(accumulated[-1] + " " + row['utterance'])
        else:
            accumulated.append(row['utterance'])
    return accumulated

# 그룹별로 대화 누적
accum['accumulated_utterance'] = accum.groupby('dialogueID2').apply(lambda x: accumulate_dialogue(x)).explode().reset_index(drop=True)

accum.head()

"""## 시나리오 추출"""

data = pd.read_csv("/content/drive/MyDrive/preprocessed0610.csv")
data = data[["dialogueID2", "participantID", "utterance"]]
data

len(data['dialogueID2'].unique())

import pandas as pd

# dialogueID2 별로 발화 묶기
grouped_data = data.groupby('dialogueID2')['utterance'].apply(' '.join).reset_index()

# 결과 출력
grouped_data

import pandas as pd

# 각 utterance에서 마지막 공백 제거
grouped_data['utterance'] = grouped_data['utterance'].apply(lambda x: x.rstrip())

# 결과 출력
print(grouped_data)

grouped_data

grouped_data.to_csv("/content/drive/MyDrive/grouped_data.csv")

sample_grouped = grouped_data[:500]

# -*- coding: utf-8 -*-

import base64
import json
import http.client


class CompletionExecutor:
    def __init__(self, host, api_key, api_key_primary_val, request_id):
        self._host = host
        self._api_key = api_key
        self._api_key_primary_val = api_key_primary_val
        self._request_id = request_id

    def _send_request(self, completion_request):
        headers = {
            'Content-Type': 'application/json; charset=utf-8',
            'X-NCP-CLOVASTUDIO-API-KEY': self._api_key,
            'X-NCP-APIGW-API-KEY': self._api_key_primary_val,
            'X-NCP-CLOVASTUDIO-REQUEST-ID': self._request_id
        }

        conn = http.client.HTTPSConnection(self._host)
        conn.request('POST', '/testapp/v1/api-tools/summarization/v2/54e034ee84b24a53a5b13c1dc72e6bad', json.dumps(completion_request), headers)
        response = conn.getresponse()
        result = json.loads(response.read().decode(encoding='utf-8'))
        conn.close()
        return result

    def execute(self, completion_request):
        res = self._send_request(completion_request)
        if res['status']['code'] == '20000':
            return res['result']['text']
        else:
            return 'Error'

if __name__ == '__main__':
    completion_executor = CompletionExecutor(
        host='clovastudio.apigw.ntruss.com',
        api_key='NTA0MjU2MWZlZTcxNDJiY43td99e9wRJTWj0W/9R99v7Bs5EPsjO+zS0iCEo8+30',
        api_key_primary_val='IgRUrRCvvPh59Voa0tJY6679HDBxVTO4Y6sQx33v',
        request_id='7be5f80c-12c4-4f25-9ffe-49e0d9f1cf8c'
    )

    def summarize_text(utterance):
        request_data = {
            "texts": [utterance],
            "segMinSize": 300,
            "includeAiFilters": True,
            "autoSentenceSplitter": True,
            "segCount": -1,
            "segMaxSize": 1000
        }
        response_text = completion_executor.execute(request_data)
        return response_text

sample_sum = grouped_data[:48000]

from tqdm import tqdm
tqdm.pandas()

# 'utterance' 컬럼의 텍스트를 요약하고, 진행 상황을 tqdm으로 표시
sample_sum['summary'] = sample_sum['utterance'].progress_apply(summarize_text)

sample_sum.to_csv("/content/drive/MyDrive/sample_sum.csv")

"""## 시나리오 손으로 추출"""

data = pd.read_csv("/content/drive/MyDrive/preprocessed0610.csv")
# data = data[["dialogueID2", "gender", "utterance"]]
data

# 각 dialogueID2별 gender 종류 확인
gender_groups = data.groupby('dialogueID2')['gender'].nunique()

# 여성과 남성 모두 포함된 dialogueID2 추출
mixed_gender_dialogues = gender_groups[gender_groups > 1].index

# 조건을 만족하는 행들 필터링
filtered_data = data[data['dialogueID2'].isin(mixed_gender_dialogues)]

# 결과 출력
filtered_data

import pandas as pd

# dialogueID2 별로 발화 묶기
grouped_data = filtered_data.groupby('dialogueID2')['utterance'].apply(' '.join).reset_index()

# 결과 출력
grouped_data

grouped_data.to_csv("/content/drive/MyDrive/grouped_data.csv")

# 각 dialogueID2별로 gender가 모두 남성인 경우를 필터링
male_only_dialogues = data.groupby('dialogueID2').filter(lambda x: x['gender'].nunique() == 1 and x['gender'].iloc[0] == '남성')

# 결과 출력
male_only_dialogues

import pandas as pd

# dialogueID2 별로 발화 묶기
friends_grouped_data = male_only_dialogues.groupby('dialogueID2')['utterance'].apply(' '.join).reset_index()

# 결과 출력
friends_grouped_data

friends_grouped_data.to_csv("/content/drive/MyDrive/friends_grouped_data.csv")

"""## 1에 더 가까운 모델"""

grouped_data = pd.read_csv("/content/drive/MyDrive/grouped_data.csv")
grouped_data

samp = grouped_data[:200]

# -*- coding: utf-8 -*-

import base64
import json
import http.client


class CompletionExecutor:
    def __init__(self, host, api_key, api_key_primary_val, request_id):
        self._host = host
        self._api_key = api_key
        self._api_key_primary_val = api_key_primary_val
        self._request_id = request_id

    def _send_request(self, completion_request):
        headers = {
            'Content-Type': 'application/json; charset=utf-8',
            'X-NCP-CLOVASTUDIO-API-KEY': self._api_key,
            'X-NCP-APIGW-API-KEY': self._api_key_primary_val,
            'X-NCP-CLOVASTUDIO-REQUEST-ID': self._request_id
        }

        conn = http.client.HTTPSConnection(self._host)
        conn.request('POST', '/testapp/v1/tasks/00vh3vtf/search', json.dumps(completion_request), headers)
        response = conn.getresponse()
        result = json.loads(response.read().decode(encoding='utf-8'))
        conn.close()
        return result

    def execute(self, completion_request):
        res = self._send_request(completion_request)
        if res['status']['code'] == '20000':
            return res['result']['outputText']
        else:
            return 'Error'

from tqdm import tqdm

if __name__ == '__main__':
    completion_executor = CompletionExecutor(
        host='clovastudio.apigw.ntruss.com',
        api_key='NTA0MjU2MWZlZTcxNDJiY43td99e9wRJTWj0W/9R99v7Bs5EPsjO+zS0iCEo8+30',
        api_key_primary_val='IgRUrRCvvPh59Voa0tJY6679HDBxVTO4Y6sQx33v',
        request_id='dcf99ef9-a47f-444b-9b67-a32e9281f661'
    )

    # 진행 상황을 표시하기 위해 tqdm 적용
    tqdm.pandas()

    # 새로운 데이터프레임에 분류 결과 추가
    samp['classification'] = samp['utterance'].progress_apply(
        lambda x: completion_executor.execute({'text': x, 'includeAiFilters': True})
    )

    # 분류 결과가 1인 행들만 남김
    samp_filtered_df_1 = samp[samp['classification'] == 1]

samp[samp['classification'] == '1']

"""## 0에 더 가까운 모델"""

grouped_data = pd.read_csv("/content/drive/MyDrive/grouped_data.csv")
grouped_data

samp = grouped_data[:200]

# -*- coding: utf-8 -*-

import base64
import json
import http.client


class CompletionExecutor:
    def __init__(self, host, api_key, api_key_primary_val, request_id):
        self._host = host
        self._api_key = api_key
        self._api_key_primary_val = api_key_primary_val
        self._request_id = request_id

    def _send_request(self, completion_request):
        headers = {
            'Content-Type': 'application/json; charset=utf-8',
            'X-NCP-CLOVASTUDIO-API-KEY': self._api_key,
            'X-NCP-APIGW-API-KEY': self._api_key_primary_val,
            'X-NCP-CLOVASTUDIO-REQUEST-ID': self._request_id
        }

        conn = http.client.HTTPSConnection(self._host)
        conn.request('POST', '/testapp/v1/tasks/2wjye45d/search', json.dumps(completion_request), headers)
        response = conn.getresponse()
        result = json.loads(response.read().decode(encoding='utf-8'))
        conn.close()
        return result

    def execute(self, completion_request):
        res = self._send_request(completion_request)
        if res['status']['code'] == '20000':
            return res['result']['outputText']
        else:
            return 'Error'

from tqdm import tqdm

if __name__ == '__main__':
    completion_executor = CompletionExecutor(
        host='clovastudio.apigw.ntruss.com',
        api_key='NTA0MjU2MWZlZTcxNDJiY43td99e9wRJTWj0W/9R99v7Bs5EPsjO+zS0iCEo8+30',
        api_key_primary_val='IgRUrRCvvPh59Voa0tJY6679HDBxVTO4Y6sQx33v',
        request_id='27eab36c-40bd-4fa5-8f8d-9474167e5910'
    )

    # 진행 상황을 표시하기 위해 tqdm 적용
    tqdm.pandas()

    # 새로운 데이터프레임에 분류 결과 추가
    samp['classification'] = samp['utterance'].progress_apply(
        lambda x: completion_executor.execute({'text': x, 'includeAiFilters': True})
    )

    # 분류 결과가 1인 행들만 남김
    samp_filtered_df = samp[samp['classification'] == 1]

# 분류 결과가 1인 행들만 남김
samp_filtered_df = samp[samp['classification'] == 1]

samp_filtered_df

samp

"""## 남남 / 연인 대화 식별"""

import pandas as pd

sample_grouped = pd.read_csv("/content/drive/MyDrive/sample_grouped.csv")

from google.colab import drive

# Mount Google Drive
drive.mount('/content/drive')

# -*- coding: utf-8 -*-

import base64
import json
import http.client


class CompletionExecutor:
    def __init__(self, host, api_key, api_key_primary_val, request_id):
        self._host = host
        self._api_key = api_key
        self._api_key_primary_val = api_key_primary_val
        self._request_id = request_id

    def _send_request(self, completion_request):
        headers = {
            'Content-Type': 'application/json; charset=utf-8',
            'X-NCP-CLOVASTUDIO-API-KEY': self._api_key,
            'X-NCP-APIGW-API-KEY': self._api_key_primary_val,
            'X-NCP-CLOVASTUDIO-REQUEST-ID': self._request_id
        }

        conn = http.client.HTTPSConnection(self._host)
        conn.request('POST', '/testapp/v1/tasks/5s8fochb/search', json.dumps(completion_request), headers)
        response = conn.getresponse()
        result = json.loads(response.read().decode(encoding='utf-8'))
        conn.close()
        return result

    def execute(self, completion_request):
        res = self._send_request(completion_request)
        if res['status']['code'] == '20000':
            return res['result']['outputText']
        else:
            return 'Error'

if __name__ == '__main__':
    completion_executor = CompletionExecutor(
        host='clovastudio.apigw.ntruss.com',
        api_key='NTA0MjU2MWZlZTcxNDJiY43td99e9wRJTWj0W/9R99v7Bs5EPsjO+zS0iCEo8+30',
        api_key_primary_val='IgRUrRCvvPh59Voa0tJY6679HDBxVTO4Y6sQx33v',
        request_id='5db7dc28-7ec7-45b8-af49-c3c6c61bad50'
    )

    preset_text = '자기야 보고싶어 많이 나도 많이 보고싶어'

    request_data = {
        'text': preset_text,
        'includeAiFilters': True
    }

    response_text = completion_executor.execute(request_data)
    print(preset_text)
    print(response_text)

if __name__ == '__main__':
    completion_executor = CompletionExecutor(
        host='clovastudio.apigw.ntruss.com',
        api_key='NTA0MjU2MWZlZTcxNDJiY43td99e9wRJTWj0W/9R99v7Bs5EPsjO+zS0iCEo8+30',
        api_key_primary_val='IgRUrRCvvPh59Voa0tJY6679HDBxVTO4Y6sQx33v',
        request_id='5db7dc28-7ec7-45b8-af49-c3c6c61bad50'
    )

ss = sample_sum[:100]

import pandas as pd
from tqdm import tqdm
tqdm.pandas()

# sample_sum = pd.read_csv("/content/drive/MyDrive/sample_sum.csv")

# 새로운 데이터프레임에 분류 결과 추가
ss['classification'] = ss['utterance'].progress_apply(
        lambda x: completion_executor.execute({'text': x, 'includeAiFilters': True})
    )

ss

sample_sum_label = ss[ss['classification'] == '1']
sample_sum_label

# sample_sum_label.to_csv("/content/drive/MyDrive/sample_sum_label.csv")

"""## 데이터 샘플링"""

data = pd.read_csv("/content/drive/MyDrive/preprocessed0610.csv")
data

# 각 dialogueID2별 gender 종류 확인
gender_groups = data.groupby('dialogueID2')['gender'].nunique()

# 여성과 남성 모두 포함된 dialogueID2 추출
mixed_gender_dialogues = gender_groups[gender_groups > 1].index

# 조건을 만족하는 행들 필터링
filtered_data = data[data['dialogueID2'].isin(mixed_gender_dialogues)]

# 결과 출력
filtered_data

filtered_data = filtered_data[["dialogueID2", "utterance"]]
filtered_data

import pandas as pd
from tqdm import tqdm

# tqdm 적용
tqdm.pandas()

# 그룹별로 새로운 컬럼 추가 함수
def combine_utterances(group):
    group['combined'] = group['utterance'] + group['utterance'].shift(-1)
    return group.dropna(subset=['combined'])

# 그룹별로 처리
filtered_data = filtered_data.groupby('dialogueID2').progress_apply(combine_utterances).reset_index(drop=True)

# 결과 출력
print(filtered_data)

abba_data = filtered_data

# 문자열 길이 계산
lengths = abba_data['combined'].apply(len)
min_length = lengths.min()
max_length = lengths.max()
mean_length = lengths.mean()

print(min_length, max_length, mean_length)

lengths = lengths[lengths != 3915]
# 3918, 3915, 29403

# Plotting the box plot
plt.figure(figsize=(8, 6))
plt.boxplot(lengths)
plt.title('Box Plot of Lengths of Combined Strings')
plt.ylabel('Length')
plt.grid(True)
plt.show()

lengths.mean()

abba_data = abba_data[abba_data['combined'].apply(len) != 3915]
abba_data = abba_data[abba_data['combined'].apply(len) != 3918]
abba_data = abba_data[abba_data['combined'].apply(len) != 29403]

len(abba_data)

abba_data = abba_data[['combined']]

abba_data = pd.read_csv("/content/drive/MyDrive/abba_data.csv")

# 길이가 35보다 긴 행들
long_rows = abba_data[abba_data['combined'].apply(len) > 35]

# 길이가 35보다 짧거나 같은 행들
short_rows = abba_data[abba_data['combined'].apply(len) <= 35]

# 샘플링 (각각 4만 개씩)
sampled_long_rows = long_rows.sample(n=25000, random_state=1) if len(long_rows) >= 25000 else long_rows
sampled_short_rows = short_rows.sample(n=25000, random_state=1) if len(short_rows) >= 25000 else short_rows

# 필요시 샘플링된 데이터프레임 합치기
abba_sampled_data = pd.concat([sampled_long_rows, sampled_short_rows]).reset_index(drop=True)

abba_sampled_data

abba_sampled_data = abba_sampled_data[["combined"]]

abba_sampled_data.to_csv("/content/drive/MyDrive/abba_sampled_data.csv")

from google.colab import drive

# Mount Google Drive
drive.mount('/content/drive')

import pandas as pd
abba_sampled_data = pd.read_csv("/content/drive/MyDrive/abba_sampled_data.csv")

# -*- coding: utf-8 -*-

import requests


class CompletionExecutor:
    def __init__(self, host, api_key, api_key_primary_val, request_id):
        self._host = host
        self._api_key = api_key
        self._api_key_primary_val = api_key_primary_val
        self._request_id = request_id

    def execute(self, completion_request):
        headers = {
            'X-NCP-CLOVASTUDIO-API-KEY': self._api_key,
            'X-NCP-APIGW-API-KEY': self._api_key_primary_val,
            'X-NCP-CLOVASTUDIO-REQUEST-ID': self._request_id,
            'Content-Type': 'application/json; charset=utf-8',
            'Accept': 'text/event-stream'
        }

        with requests.post(self._host + '/testapp/v1/chat-completions/HCX-003',
                           headers=headers, json=completion_request, stream=True) as r:
            for line in r.iter_lines():
                if line:
                    print(line.decode("utf-8"))


if __name__ == '__main__':
    completion_executor = CompletionExecutor(
        host='https://clovastudio.stream.ntruss.com',
        api_key='NTA0MjU2MWZlZTcxNDJiY43td99e9wRJTWj0W/9R99v7Bs5EPsjO+zS0iCEo8+30',
        api_key_primary_val='IgRUrRCvvPh59Voa0tJY6679HDBxVTO4Y6sQx33v',
        request_id='05f192b0-dd58-4701-bed5-f2d8f47904ad'
    )

abba_sampled_data

"""## 긍정부정 추출 모델 클로바"""

# 네이버 영화 감성분석 데이터 다운로드
!git clone https://github.com/e9t/nsmc.git

train = pd.read_table("nsmc/"+"ratings_train.txt")

train = train[:1000]

train.to_csv("train.csv")

from google.colab import drive

# Mount Google Drive
drive.mount('/content/drive')

import pandas as pd
abba_sampled_data = pd.read_csv("/content/drive/MyDrive/abba_sampled_data.csv")

# -*- coding: utf-8 -*-

import base64
import json
import http.client


class CompletionExecutor:
    def __init__(self, host, api_key, api_key_primary_val, request_id):
        self._host = host
        self._api_key = api_key
        self._api_key_primary_val = api_key_primary_val
        self._request_id = request_id

    def _send_request(self, completion_request):
        headers = {
            'Content-Type': 'application/json; charset=utf-8',
            'X-NCP-CLOVASTUDIO-API-KEY': self._api_key,
            'X-NCP-APIGW-API-KEY': self._api_key_primary_val,
            'X-NCP-CLOVASTUDIO-REQUEST-ID': self._request_id
        }

        conn = http.client.HTTPSConnection(self._host)
        conn.request('POST', '/testapp/v1/tasks/8nwwykt9/search', json.dumps(completion_request), headers)
        response = conn.getresponse()
        result = json.loads(response.read().decode(encoding='utf-8'))
        conn.close()
        return result

    def execute(self, completion_request):
        res = self._send_request(completion_request)
        if res['status']['code'] == '20000':
            return res['result']['outputText']
        else:
            return 'Error'


if __name__ == '__main__':
    completion_executor = CompletionExecutor(
        host='clovastudio.apigw.ntruss.com',
        api_key='NTA0MjU2MWZlZTcxNDJiY43td99e9wRJTWj0W/9R99v7Bs5EPsjO+zS0iCEo8+30',
        api_key_primary_val='IgRUrRCvvPh59Voa0tJY6679HDBxVTO4Y6sQx33v',
        request_id='09ba5112-e1ac-4b3d-b470-08df7d0a798f'
    )

import pandas as pd
from tqdm import tqdm
tqdm.pandas()

# 새로운 데이터프레임에 분류 결과 추가
abba_sampled_data['classification'] = abba_sampled_data['combined'].progress_apply(
        lambda x: completion_executor.execute({'text': x, 'includeAiFilters': True})
    )

abba_sampled_data

len(abba_sampled_data[abba_sampled_data['classification'] != '1' & abba_sampled_data['classification'] != '0'])

len(abba_sampled_data[abba_sampled_data['classification'] == '1'])

# 긍정인 행들
pos_rows = abba_sampled_data[abba_sampled_data['classification'] == '1']

# 부정인 행들
neg_rows = abba_sampled_data[abba_sampled_data['classification'] == '0']

# 긍정인 행들에서 3만개 샘플링
sampled_pos_rows = pos_rows.sample(n=30000, random_state=1) if len(pos_rows) >= 30000 else pos_rows

# 부정인 행들에서 3만개 샘플링
sampled_neg_rows = neg_rows.sample(n=30000, random_state=1) if len(neg_rows) >= 30000 else neg_rows

# 필요시 샘플링된 데이터프레임 합치기
posneg_data = pd.concat([sampled_pos_rows, sampled_neg_rows]).reset_index(drop=True)

"""## KOTE easygoing으로 답 달기"""

from google.colab import drive

# Mount Google Drive
drive.mount('/content/drive')

import pandas as pd
abba_sampled_data = pd.read_csv("/content/drive/MyDrive/abba_sampled_data.csv")

# 671, 3149, 5222, 8948, 10064, 18252, 23918
abba_sampled_data = abba_sampled_data.drop([671, 3149, 5222, 8948, 10064, 18252, 23918])
abba_sampled_data

!pip install datasets

from datasets import load_dataset
import pandas as pd

# 데이터셋 로드
dataset = load_dataset("searle-j/kote")
print(dataset)

from transformers import AutoTokenizer, AutoModelForSequenceClassification, TextClassificationPipeline

model_name = "searle-j/kote_for_easygoing_people"
model = AutoModelForSequenceClassification.from_pretrained(model_name)
tokenizer = AutoTokenizer.from_pretrained(model_name)

pipe = TextClassificationPipeline(
        model=model,
        tokenizer=tokenizer,
        device=-1, # gpu number, -1 if cpu used
        return_all_scores=True,
        function_to_apply='sigmoid'
    )

# from transformers import AutoTokenizer, AutoModelForSequenceClassification, TextClassificationPipeline

# tqdm을 pandas와 함께 사용할 수 있도록 설정
tqdm.pandas()

# 예측 결과 저장을 위한 함수 정의
def predict_emotions(text):
    # 텍스트 토큰화
    tokenized_text = tokenizer(text, truncation=True, max_length=512, return_tensors='pt')
    if tokenized_text.input_ids.size(1) > tokenizer.model_max_length:
        return None  # 최대 길이를 초과하는 경우 None 반환
    outputs = pipe(text)[0]
    results = {output['label']: output['score'] for output in outputs if output['score'] > 0.4}
    return results

# 데이터프레임에 예측 결과 추가 (progress_apply를 사용하여 진행 상황을 표시)
abba_sampled_data['emotion'] = abba_sampled_data['combined'].progress_apply(predict_emotions)

abba_emotion = abba_sampled_data

abba_emotion.to_csv("/content/drive/MyDrive/abba_emotion.csv")
abba_emotion

abba_emotion = pd.read_csv("/content/drive/MyDrive/abba_emotion.csv")

abba_emotion

abba_emotion = abba_emotion[['combined', 'emotion']]
abba_emotion.to_csv("/content/drive/MyDrive/abba_emotion.csv")
abba_emotion

"""### 토큰 수 구하기"""

from transformers import AutoTokenizer
from tqdm import tqdm
import pandas as pd

# 모델과 토크나이저 로드
model_name = "searle-j/kote_for_easygoing_people"
tokenizer = AutoTokenizer.from_pretrained(model_name)

# tqdm을 pandas와 함께 사용할 수 있도록 설정
tqdm.pandas()

# 각 행의 토큰 길이를 계산하여 출력하는 함수 정의
def calculate_token_length(text):
    tokenized_text = tokenizer(text, truncation=False)
    return len(tokenized_text['input_ids'])

# 데이터프레임의 각 행에 대해 토큰 길이를 계산하고 출력
abba_sampled_data['token_length'] = abba_sampled_data['combined'].progress_apply(calculate_token_length)

# 토큰 길이 출력
for idx, length in enumerate(abba_sampled_data['token_length']):
    print(f"Row {idx}: Token Length = {length}")

abba_sampled_data[abba_sampled_data['token_length'] >= 512]

abba_sampled_data[abba_sampled_data['token_length'] >= 512]

# -*- coding: utf-8 -*-

import base64
import json
import http.client


class CompletionExecutor:
    def __init__(self, host, api_key, api_key_primary_val, request_id):
        self._host = host
        self._api_key = api_key
        self._api_key_primary_val = api_key_primary_val
        self._request_id = request_id

    def _send_request(self, completion_request):
        headers = {
            'Content-Type': 'application/json; charset=utf-8',
            'X-NCP-CLOVASTUDIO-API-KEY': self._api_key,
            'X-NCP-APIGW-API-KEY': self._api_key_primary_val,
            'X-NCP-CLOVASTUDIO-REQUEST-ID': self._request_id
        }

        conn = http.client.HTTPSConnection(self._host)
        conn.request('POST', '/v1/api-tools/tokenize/{modelName}', json.dumps(completion_request), headers)
        response = conn.getresponse()
        result = json.loads(response.read().decode(encoding='utf-8'))
        conn.close()
        return result

    def execute(self, completion_request):
        res = self._send_request(completion_request)
        if res['status']['code'] == '20000':
            return res['result']['numTokens']
        else:
            return 'Error'


if __name__ == '__main__':
    completion_executor = CompletionExecutor(
        host='clovastudio.apigw.ntruss.com',
        api_key='NTA0MjU2MWZlZTcxNDJiY43td99e9wRJTWj0W/9R99v7Bs5EPsjO+zS0iCEo8+30',
        api_key_primary_val = 'IgRUrRCvvPh59Voa0tJY6679HDBxVTO4Y6sQx33v',
        request_id='fa8ef027-9d2f-4b76-be96-b3f9d60d8390'
    )

import pandas as pd
from tqdm import tqdm
tqdm.pandas()


# 새로운 데이터프레임에 분류 결과 추가
abba_sampled_data['token'] = abba_sampled_data['combined'].progress_apply(
        lambda x: completion_executor.execute({'text': x, 'includeAiFilters': True})
    )

abba_sampled_data

# 각 문자열의 토큰 수를 계산하여 새로운 칼럼에 추가
def get_token_count(response):
    try:
        response_json = json.loads(response)
        return response_json.get("total_tokens", 0)
    except json.JSONDecodeError:
        return 0

abba_sampled_data['token'] = abba_sampled_data['combined'].progress_apply(
    lambda x: get_token_count(completion_executor.execute({'text': x}))
)

"""## 급조 사진"""

abba_samp = abba_data[13:18]
abba_samp = abba_samp[['combined']]

# 예측 결과 저장을 위한 함수 정의
def predict_emotions(text):
    # 텍스트 토큰화
    tokenized_text = tokenizer(text, truncation=True, max_length=512, return_tensors='pt')
    if tokenized_text.input_ids.size(1) > tokenizer.model_max_length:
        return None  # 최대 길이를 초과하는 경우 None 반환
    outputs = pipe(text)[0]
    results = {output['label']: output['score'] for output in outputs if output['score'] > 0.4}
    return results

# 데이터프레임에 예측 결과 추가 (progress_apply를 사용하여 진행 상황을 표시)
abba_samp['emotion'] = abba_samp['combined'].progress_apply(predict_emotions)

abba_samp

"""## 메트릭"""

from google.colab import drive

# Mount Google Drive
drive.mount('/content/drive')

abba_emotion_label = pd.read_csv("/content/drive/MyDrive/abba_emotion_label.csv", index_col=0)

abba_emotion_label = abba_emotion_label.drop('Unnamed: 0', axis=1)

abba_emotion_label

import pandas as pd
import ast

# 유사도 값을 매핑하는 딕셔너리 정의
similarity_scores = {
    '즐거움/신남': 0.1,
    '아껴주는': 0.17124822752706000,
    '감동/감탄': 0.23354440692795400,
    '존경': 0.2734879651490610,
    '비장함': 0.32459286308393100,
    '환영/호의': 0.3529984458654700,
    '신기함/관심': 0.5765880081075960,
    '안심/신뢰': 0.5968421852184190,
    '뿌듯함': 0.597977450393608,
    '편안/쾌적': 0.6241742773873270,
    '깨달음': 0.631739770242752,
    '고마움': 0.6473006564807980,
    '흐뭇함(귀여움/예쁨)': 0.6561344189267170,
    '기대감': 0.6710605400168370,
    '놀람': 0.7513536035844380,
    '부끄러움': 0.7570255169916290,
    '기쁨': 0.9004628994899500,
    '행복': 1.0,
    '없음': 0,
    '지긋지긋': -1.0,
    '역겨움/징그러움': -0.7673891177930400,
    '패배/자기혐오': -0.7368156529130150,
    '증오/혐오': -0.592759382340589,
    '경악': -0.5269861407803300,
    '한심함': -0.483511390372089,
    '불안/걱정': -0.3782807131962080,
    '안타까움/실망': -0.3518024871863400,
    '불평/불만': -0.35155494523721200,
    '부담/안 내킴': -0.3466742066296180,
    '화남/분노': -0.31486459674446300,
    '서러움': -0.3068287153668160,
    '힘듦/지침': -0.30084546059388800,
    '의심/불신': -0.2625669004806260,
    '어이없음': -0.22949266731300900,
    '우쭐댐/무시함': -0.2148385594623300,
    '불쌍함/연민': -0.15493278187705000,
    '공포/무서움': -0.1506228606085340,
    '귀찮음': -0.10631785888419000,
    '절망': -0.09344098330797410,
    '짜증': -0.07095753982734030,
    '슬픔': -0.06738774082138870,
    '죄책감': -0.019189351753041400,
    '재미없음': 0.0
}

sam = abba_emotion_label[:1000]
sam

sam_new = pd.read_csv("sam-3.csv")
sam_new

# Counting the occurrences of each label in the 'Completion' column
label_counts = sam_new['Completion'].value_counts().sort_index()
label_counts

# Mapping function
def map_completion_to_emoticon(completion):
    if -4 <= completion <= 0:
        return ':D'
    elif 1 <= completion <= 5:
        return ';('
    else:
        return ''

# Apply the mapping function to the DataFrame
sam_new['Emoticon'] = sam_new['Completion'].apply(map_completion_to_emoticon)

sam_new = sam_new[['Text', 'Emoticon']]
sam_new.to_csv("sam_new.csv")

import pandas as pd
sam_new = pd.read_csv("sam_new.csv")

sam_new.rename(columns={'Emoticon': 'Completion'}, inplace=True)
sam_new

# 누적된 문자열 생성
cumulative_text = []
current_text = ""
for text in sam_new['Text']:
    current_text += text + " "
    cumulative_text.append(current_text.strip())

sam_new['Cumulative_Text'] = cumulative_text
sam_new = sam_new[['Cumulative_Text', 'Completion']]

sam_new.rename(columns={'Cumulative_Text': 'Text'}, inplace=True)
sam_new

sam_new.head()

# 각 턴 별score 계산 함수 정의
def calculate_weight_score(df, similarity_scores, i):
    emotion_series = df['emotion'][i]
    emotion_dict = ast.literal_eval(emotion_series)  # 문자열을 딕셔너리로 변환
    total_score = 0
    for emotion, score in emotion_dict.items():
        total_score += similarity_scores.get(emotion, 0) * score
    return total_score

sam['turn_score'] = sam.apply(lambda x: calculate_weight_score(x, similarity_scores, 0), axis=1)

# for i in range(1000) :
#   sam['turn_score'][i] = calculate_weight_score(sam, similarity_scores, i)

calculate_weight_score(sam, similarity_scores)

# Function to calculate 누적 score
def calculate_total_score(df, similarity_scores):
    emotion_series = df['emotion']
    weighted_scores = {}
    for idx, emotion_str in enumerate(emotion_series):
        emotion_dict = ast.literal_eval(emotion_str)
        weight = (idx + 1) / sum(range(1, len(emotion_series) + 1))

        for emotion, score in emotion_dict.items():
            if emotion in weighted_scores:
                weighted_scores[emotion].append(score * weight)
            else:
                weighted_scores[emotion] = [score * weight]

    averaged_scores = {emotion: sum(scores) / len(scores) for emotion, scores in weighted_scores.items()}

    total_score = sum(averaged_scores.get(emotion, 0) * weight for emotion, weight in similarity_scores.items())
    return total_score

calculate_total_score(sam, similarity_scores)

"""## 하나의 API로 합치기"""

import ast
import torch
from transformers import AutoTokenizer, AutoModelForSequenceClassification, TextClassificationPipeline
import pandas as pd
import joblib
from flask import Flask, request, jsonify

# 모델 초기 설정 (emo_model) -> 모델 완성 안돼서 코드 수정해야함
model_name = "searle-j/kote_for_easygoing_people"
model = AutoModelForSequenceClassification.from_pretrained(model_name)
tokenizer = AutoTokenizer.from_pretrained(model_name)

pipe = TextClassificationPipeline(
    model=model,
    tokenizer=tokenizer,
    device=-1,  # GPU number, -1 if CPU used
    top_k=None,  # top_k=None to get all scores
    function_to_apply='sigmoid',
    truncation=True
)

# 유사도 점수 딕셔너리
similarity_scores = {
    '즐거움/신남': 0.1,
    '아껴주는': 0.17124822752706000,
    '감동/감탄': 0.23354440692795400,
    '존경': 0.2734879651490610,
    '비장함': 0.32459286308393100,
    '환영/호의': 0.3529984458654700,
    '신기함/관심': 0.5765880081075960,
    '안심/신뢰': 0.5968421852184190,
    '뿌듯함': 0.597977450393608,
    '편안/쾌적': 0.6241742773873270,
    '깨달음': 0.631739770242752,
    '고마움': 0.6473006564807980,
    '흐뭇함(귀여움/예쁨)': 0.6561344189267170,
    '기대감': 0.6710605400168370,
    '놀람': 0.7513536035844380,
    '부끄러움': 0.7570255169916290,
    '기쁨': 0.9004628994899500,
    '행복': 1.0,
    '없음': 0,
    '지긋지긋': -1.0,
    '역겨움/징그러움': -0.7673891177930400,
    '패배/자기혐오': -0.7368156529130150,
    '증오/혐오': -0.592759382340589,
    '경악': -0.5269861407803300,
    '한심함': -0.483511390372089,
    '불안/걱정': -0.3782807131962080,
    '안타까움/실망': -0.3518024871863400,
    '불평/불만': -0.35155494523721200,
    '부담/안 내킴': -0.3466742066296180,
    '화남/분노': -0.31486459674446300,
    '서러움': -0.3068287153668160,
    '힘듦/지침': -0.30084546059388800,
    '의심/불신': -0.2625669004806260,
    '어이없음': -0.22949266731300900,
    '우쭐댐/무시함': -0.2148385594623300,
    '불쌍함/연민': -0.15493278187705000,
    '공포/무서움': -0.1506228606085340,
    '귀찮음': -0.10631785888419000,
    '절망': -0.09344098330797410,
    '짜증': -0.07095753982734030,
    '슬픔': -0.06738774082138870,
    '죄책감': -0.019189351753041400,
    '재미없음': 0.0
}

# 개별 점수 계산 함수
def calculate_weight_score(emotion_dict, similarity_scores):
    total_score = 0
    for emotion, score in emotion_dict.items():
        total_score += similarity_scores.get(emotion, 0) * score
    return total_score

# 누적 점수 계산 함수
def calculate_total_score(df, similarity_scores):
    emotion_series = df['emotion']
    weighted_scores = {}
    for idx, emotion_str in enumerate(emotion_series):
        emotion_dict = ast.literal_eval(emotion_str)
        weight = (idx + 1) / sum(range(1, len(emotion_series) + 1))
        for emotion, score in emotion_dict.items():
            if emotion in weighted_scores:
                weighted_scores[emotion].append(score * weight)
            else:
                weighted_scores[emotion] = [score * weight]
    averaged_scores = {emotion: sum(scores) / len(scores) for emotion, scores in weighted_scores.items()}
    total_score = sum(averaged_scores.get(emotion, 0) * weight for emotion, weight in similarity_scores.items())
    return total_score


# 감정 라벨 추출 및 점수 계산 함수
def process_text(text):
    emotions = pipe(text)[0]
    emotion_dict = {output["label"]: output["score"] for output in emotions}
    return emotion_dict

# 각 턴 별 딕셔너리 뽑아서 한 컬럼의 데이터프레임으로 만드는 함수
def emotion_dict(df) :
    df['emotion'] = df['utterance'].apply(process_text)
    return df

# 개별 점수, 누적 점수 출력
def result_score(df) :
    df = emotion_dict(df)
    emotion_dict = df['emotion']
    weight_score = calculate_weight_score(emotion_dict, similarity_scores)
    total_score = calculate_total_score(df, similarity_scores)
    return weight_score, total_score

# Flask 앱 생성
app = Flask(__name__)

@app.route('/calculate_scores', methods=['POST'])
def calculate_scores():
    if 'file' not in request.files:
        return jsonify({"error": "No file provided"}), 400

    file = request.files['file']

    try:
        df = pd.read_csv(file)
        if 'utterance' not in df.columns:
            return jsonify({"error": "CSV file must contain 'utterance' column"}), 400

        weight_score, total_score = result_score(df)
        return jsonify({
            "weight_score": weight_score,
            "total_score": total_score
        })
    except Exception as e:
        return jsonify({"error": str(e)}), 500

if __name__ == '__main__':
    app.run(host='0.0.0.0', port=5000)

curl http://172.28.0.12:5000

from google.colab import drive

# Mount Google Drive
drive.mount('/content/drive')

abba_emotion_label = pd.read_csv("/content/drive/MyDrive/abba_emotion_label.csv", index_col=0)

# Initialize the 'sam' DataFrame
sam = pd.DataFrame()

# Assign the first 5 values of the 'combined' column to the 'utterance' column of 'sam'
sam['utterance'] = abba_emotion_label['combined'][:5]

# Display the 'sam' DataFrame
sam

sam.to_csv("/content/drive/MyDrive/sam.csv")

import requests

url = 'http://127.0.0.1:5000/calculate_scores'
file_path = '/content/drive/MyDrive/sam.csv'

with open(file_path, 'rb') as f:
    files = {'file': f}
    response = requests.post(url, files=files)

print(response.text)

"""## 누적 분위기 인퍼런스"""

# -*- coding: utf-8 -*-

import base64
import json
import http.client


class CompletionExecutor:
    def __init__(self, host, api_key, api_key_primary_val, request_id):
        self._host = host
        self._api_key = api_key
        self._api_key_primary_val = api_key_primary_val
        self._request_id = request_id

    def _send_request(self, completion_request):
        headers = {
            'Content-Type': 'application/json; charset=utf-8',
            'X-NCP-CLOVASTUDIO-API-KEY': self._api_key,
            'X-NCP-APIGW-API-KEY': self._api_key_primary_val,
            'X-NCP-CLOVASTUDIO-REQUEST-ID': self._request_id
        }

        conn = http.client.HTTPSConnection(self._host)
        conn.request('POST', '/testapp/v1/tasks/hayct9t3/search', json.dumps(completion_request), headers)
        response = conn.getresponse()
        result = json.loads(response.read().decode(encoding='utf-8'))
        conn.close()
        return result

    def execute(self, completion_request):
        res = self._send_request(completion_request)
        if res['status']['code'] == '20000':
            return res['result']['outputText']
        else:
            return 'Error'


if __name__ == '__main__':
    completion_executor = CompletionExecutor(
        host='clovastudio.apigw.ntruss.com',
        api_key='NTA0MjU2MWZlZTcxNDJiY43td99e9wRJTWj0W/9R99v7Bs5EPsjO+zS0iCEo8+30',
        api_key_primary_val='IgRUrRCvvPh59Voa0tJY6679HDBxVTO4Y6sQx33v',
        request_id='bd196af9-a4d8-41a9-966e-39844d955fb5'
    )

preset_text = '나 네가 너무 좋아 나도 네가 너무 좋아!!'

request_data = {
        'text': preset_text,
        'includeAiFilters': True
}

response_text = completion_executor.execute(request_data)
print(preset_text)
print(response_text)

"""## 턴 감정 점수 인퍼런스"""

# -*- coding: utf-8 -*-

import base64
import json
import http.client


class CompletionExecutor:
    def __init__(self, host, api_key, api_key_primary_val, request_id):
        self._host = host
        self._api_key = api_key
        self._api_key_primary_val = api_key_primary_val
        self._request_id = request_id

    def _send_request(self, completion_request):
        headers = {
            'Content-Type': 'application/json; charset=utf-8',
            'X-NCP-CLOVASTUDIO-API-KEY': self._api_key,
            'X-NCP-APIGW-API-KEY': self._api_key_primary_val,
            'X-NCP-CLOVASTUDIO-REQUEST-ID': self._request_id
        }

        conn = http.client.HTTPSConnection(self._host)
        conn.request('POST', '/testapp/v1/tasks/fv4465n4/search', json.dumps(completion_request), headers)
        response = conn.getresponse()
        result = json.loads(response.read().decode(encoding='utf-8'))
        conn.close()
        return result

    def execute(self, completion_request):
        res = self._send_request(completion_request)
        if res['status']['code'] == '20000':
            return res['result']['outputText']
        else:
            return 'Error'


if __name__ == '__main__':
    completion_executor = CompletionExecutor(
        host='clovastudio.apigw.ntruss.com',
        api_key='NTA0MjU2MWZlZTcxNDJiY1QtvFM+zuln9DqRrtQMT5Xq1wrs69nE9dzZ3+hGRAH8',
        api_key_primary_val='jgsmH29fjM0b3GEEPQV008G7bAyzEG48L6DMXxvb',
        request_id='7619af78-5dfb-47e9-90fc-07fa15e2b101'
    )

preset_text = '나 네가 너무 좋아'

request_data = {
        'text': preset_text,
        'includeAiFilters': True
}

response_text = completion_executor.execute(request_data)
print(preset_text)
print(response_text)

"""## 챗봇 인퍼런스"""

# -*- coding: utf-8 -*-

import base64
import json
import http.client


class CompletionExecutor:
    def __init__(self, host, api_key, api_key_primary_val, request_id):
        self._host = host
        self._api_key = api_key
        self._api_key_primary_val = api_key_primary_val
        self._request_id = request_id

    def _send_request(self, completion_request):
        headers = {
            'Content-Type': 'application/json; charset=utf-8',
            'X-NCP-CLOVASTUDIO-API-KEY': self._api_key,
            'X-NCP-APIGW-API-KEY': self._api_key_primary_val,
            'X-NCP-CLOVASTUDIO-REQUEST-ID': self._request_id
        }

        conn = http.client.HTTPSConnection(self._host)
        conn.request('POST', '/testapp/v1/tasks/a3r5loz8/completions', json.dumps(completion_request), headers)
        response = conn.getresponse()
        result = json.loads(response.read().decode(encoding='utf-8'))
        conn.close()
        return result

    def execute(self, completion_request):
        res = self._send_request(completion_request)
        if res['status']['code'] == '20000':
            return res['result']['text']
        else:
            return 'Error'


if __name__ == '__main__':
    completion_executor = CompletionExecutor(
        host='clovastudio.apigw.ntruss.com',
        api_key='NTA0MjU2MWZlZTcxNDJiYzpCdGaklIyXuEPpOZHGfsvYxzac629fvGv456Y10sgW',
        api_key_primary_val = 'H9SKYi4EHagBD0Nep7SWJ0T9LD4Z5vurEIuSQnpY',
        request_id='0ba0e9b4-4c54-4ee3-9fb3-5203aede1552'
    )

preset_text = '야 너 왜그래'

request_data = {
        'text': preset_text,
        'start': '',
        'restart': '',
        'includeTokens': False,
        'topP': 0.8,
        'topK': 4,
        'maxTokens': 300,
        'temperature': 0.85,
        'repeatPenalty': 5.0,
        'stopBefore': ['<|endoftext|>'],
        'includeAiFilters': True,
        'includeProbs': False
}

response_text = completion_executor.execute(request_data)
print(preset_text)
print(response_text)

"""## 최종 웹 주소

https://4737-35-233-183-211.ngrok-free.app/
"""

